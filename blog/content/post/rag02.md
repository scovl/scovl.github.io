+++
title = "T√©cnicas Avan√ßadas para RAG em Produ√ß√£o"
description = "Explorando t√©cnicas para otimizar sistemas RAG para uso em produ√ß√£o"
date = 2025-03-28T12:00:00-00:00
tags = ["RAG", "LLM", "AI", "Optimiza√ß√£o", "Produ√ß√£o", "PostgreSQL", "Ollama"]
draft = false
weight = 3
author = "Vitor Lobo Ramos"
+++

# Sum√°rio

- **[Introdu√ß√£o](#introdu√ß√£o)**
- **[Da Teoria √† Produ√ß√£o: Os Desafios Reais](#da-teoria-√†-produ√ß√£o-os-desafios-reais)**
- **[Armadilhas Comuns e Como Evit√°-las](#armadilhas-comuns-e-como-evit√°-las)**
  - [Armadilha 1: A Falsa Sensa√ß√£o de Relev√¢ncia](#armadilha-1-a-falsa-sensa√ß√£o-de-relev√¢ncia)
  - [Armadilha 2: Tamanho Inadequado de Chunks](#armadilha-2-tamanho-inadequado-de-chunks)
  - [Armadilha 3: Falta de Monitoramento Cont√≠nuo](#armadilha-3-falta-de-monitoramento-cont√≠nuo)
  - [Armadilha 4: Consultas Complexas em Pipelines Simples](#armadilha-4-consultas-complexas-em-pipelines-simples)
- **[T√©cnicas Avan√ßadas de Otimiza√ß√£o](#t√©cnicas-avan√ßadas-de-otimiza√ß√£o)**
  - [Re-ranqueamento de Chunks](#re-ranqueamento-de-chunks)
  - [Estrat√©gias de Chunking Din√¢mico](#estrat√©gias-de-chunking-din√¢mico)
  - [Workflows com Agentes para Consultas Complexas](#workflows-com-agentes-para-consultas-complexas)
    - [Arquitetura de Agentes Avan√ßada](#arquitetura-de-agentes-avan√ßada)
    - [Casos de Uso para Workflows de Agentes](#casos-de-uso-para-workflows-de-agentes)
  - [Pipelines Multimodais](#pipelines-multimodais)
    - [Arquitetura Multimodal Completa](#arquitetura-multimodal-completa)
    - [Esquema PostgreSQL para Dados Multimodais](#esquema-postgresql-para-dados-multimodais)
    - [Desafios de Implementa√ß√£o Multimodal](#desafios-de-implementa√ß√£o-multimodal)
  - [Estrat√©gias de Cache](#estrat√©gias-de-cache)
    - [Estrat√©gias Avan√ßadas de Cache para RAG](#estrat√©gias-avan√ßadas-de-cache-para-rag)
- **[Monitoramento e M√©tricas: LLMOps na Pr√°tica](#monitoramento-e-m√©tricas-llmops-na-pr√°tica)**
  - [M√©tricas de Qualidade Espec√≠ficas para RAG](#m√©tricas-de-qualidade-espec√≠ficas-para-rag)
  - [Automa√ß√£o da Avalia√ß√£o com LLMs como Ju√≠zes](#automa√ß√£o-da-avalia√ß√£o-com-llms-como-ju√≠zes)
  - [Configura√ß√£o de um Dashboard de Qualidade RAG](#configura√ß√£o-de-um-dashboard-de-qualidade-rag)
  - [Integra√ß√£o com Sistemas de Feedback do Usu√°rio](#integra√ß√£o-com-sistemas-de-feedback-do-usu√°rio)
- **[Implementando no DocAI](#implementando-no-docai)**
  - [Arquitetura Atual do DocAI](#arquitetura-atual-do-docai)
  - [Diferenciais do DocAI](#diferenciais-do-docai)
  - [Pr√≥ximos Passos para o DocAI](#pr√≥ximos-passos-para-o-docai)
- **[Integra√ß√£o com o Ecossistema](#integra√ß√£o-com-o-ecossistema)**
- **[Conclus√£o](#conclus√£o)**
- **[Refer√™ncias](#refer√™ncias)**

## Introdu√ß√£o

Ol√° pessoal! üëã

Nos artigos anteriores, exploramos como [implementar um RAG b√°sico em Clojure](/post/rag/) em mem√≥ria e como [construir um sistema de busca sem√¢ntica com PostgreSQL e Ollama](/post/semantic-postgresql/). Agora, vamos dar o pr√≥ximo passo: transformar nosso prot√≥tipo em um sistema RAG pronto para produ√ß√£o.

Como muitos desenvolvedores j√° descobriram, criar um prot√≥tipo funcional de [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) com alguns documentos √© relativamente simples. O verdadeiro desafio come√ßa quando precisamos escalar esse sistema para lidar com milhares de documentos, garantir respostas precisas e manter o desempenho sob carga. Neste artigo, vamos explorar t√©cnicas avan√ßadas para superar esses desafios e levar nosso [DocAI](https://github.com/scovl/docai) para um novo patamar de qualidade e confiabilidade.

## Da Teoria √† Produ√ß√£o: Os Desafios Reais

> "No papel, implementar um sistema [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) parece simples‚Äîconectar um banco de dados vetorial, processar documentos, incorporar os dados, incorporar a consulta, consultar o [banco de dados vetorial](https://en.wikipedia.org/wiki/Vector_database) e gerar a resposta com o [LLM](https://en.wikipedia.org/wiki/Large_language_model). Mas na pr√°tica, transformar um prot√≥tipo em uma aplica√ß√£o de alto desempenho √© um desafio completamente diferente."

Ao migrarmos do [TF-IDF em mem√≥ria](/post/rag/) para [PostgreSQL/pgvector/pgai](/post/semantic-postgresql/), demos um grande salto de qualidade. Por√©m, √† medida que o volume de dados cresce e os casos de uso se tornam mais complexos, novos desafios surgem:

- **Escalabilidade**: Como lidar com milh√µes de documentos sem degradar o desempenho?
- **Precis√£o**: Como garantir que estamos recuperando o contexto mais relevante para cada consulta?
- **Efici√™ncia**: Como reduzir lat√™ncia e custos de processamento?
- **Confiabilidade**: Como evitar alucina√ß√µes e respostas incorretas?
- **Manuten√ß√£o**: Como monitorar e melhorar continuamente o sistema?

Antes de mergulharmos nas t√©cnicas avan√ßadas, precisamos entender que o impacto mais significativo no desempenho de um sistema [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) n√£o vem apenas de usar o modelo de linguagem mais recente. Os verdadeiros ganhos v√™m de tr√™s fatores fundamentais:

- **Qualidade dos dados**: Dados bem estruturados e relevantes s√£o a base de todo sistema RAG eficaz.
- **Prepara√ß√£o adequada**: Como os dados s√£o processados, limpos e organizados.
- **Processamento eficiente**: Como os dados s√£o recuperados e utilizados durante a infer√™ncia.

Mesmo com o avan√ßo dos [LLMs](https://en.wikipedia.org/wiki/Large_language_model), esperar que modelos maiores corrijam magicamente problemas em dados defeituosos n√£o √© uma estrat√©gia vi√°vel. O futuro da [IA](https://en.wikipedia.org/wiki/Artificial_intelligence) n√£o est√° em um √∫nico modelo que sabe tudo, mas em sistemas que combinam [LLMs](https://en.wikipedia.org/wiki/Large_language_model), modelos multimodais e ferramentas de suporte que trabalham juntos de forma integrada. Dito isto, para construir um sistema [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) robusto, precisamos responder a v√°rias perguntas importantes como:

   - Como construir [mecanismos de recupera√ß√£o robustos](https://en.wikipedia.org/wiki/Retrieval-augmented_generation)?
   - Qual o papel da [qualidade dos embeddings](https://en.wikipedia.org/wiki/Embedding_model) no desempenho da recupera√ß√£o?
   - Como adaptar estrat√©gias de [chunking](https://en.wikipedia.org/wiki/Chunking_(data_storage)) dinamicamente?
   - Como o [LLM](https://en.wikipedia.org/wiki/Large_language_model) pode interpretar dados de forma eficaz?
   - Uma cadeia de [LLMs](https://en.wikipedia.org/wiki/Large_language_model) ajudaria a refinar as respostas? Vale o custo?
   - Como prevenir alucina√ß√µes mantendo a diversidade das respostas?
   - Como integrar entradas [multimodais](https://en.wikipedia.org/wiki/Multimodal_learning) (texto, imagens, tabelas) em um pipeline [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation)?
   - Quais estrat√©gias de [cache](https://en.wikipedia.org/wiki/Cache_(computing)) reduzem chamadas de API redundantes e lat√™ncia?
   - Como automatizar a [avalia√ß√£o da recupera√ß√£o](https://en.wikipedia.org/wiki/Evaluation_of_retrieval_systems) para melhoria cont√≠nua?

## Armadilhas Comuns e Como Evit√°-las

Baseado na nossa experi√™ncia com o [DocAI](https://github.com/scovl/docai) e nos desafios relatados pela comunidade, identificamos quatro armadilhas principais que podem comprometer sistemas RAG:

### Armadilha 1: A Falsa Sensa√ß√£o de Relev√¢ncia

Uma busca por vizinhos mais pr√≥ximos sempre retornar√° algum resultado, mas como saber se √© realmente √∫til? Alguns documentos podem parecer relevantes com base na similaridade vetorial, mas n√£o fornecem o contexto adequado para responder √† pergunta do usu√°rio.

> **Solu√ß√£o**: Implementar verifica√ß√£o de relev√¢ncia p√≥s-recupera√ß√£o usando [cross-encoders](https://huggingface.co/cross-encoder) ou filtros baseados em regras. No [PostgreSQL](https://www.postgresql.org/), podemos fazer isso com:

```sql
-- Primeiro recuperamos candidatos usando busca vetorial
WITH candidatos AS (
  SELECT id, titulo, conteudo, embedding <=> query_embedding AS distancia
  FROM documentos_embeddings
  ORDER BY distancia
  LIMIT 20
),
-- Depois aplicamos filtro secund√°rio para verificar relev√¢ncia real
filtrados AS (
  SELECT id, titulo, conteudo, distancia
  FROM candidatos
  WHERE 
    -- Filtro baseado em regras (exemplo: deve conter palavras-chave)
    conteudo ILIKE '%' || 'palavra_chave' || '%'
    -- Ou usar um modelo secund√°rio para avaliar relev√¢ncia
    -- ai.evaluate_relevance(conteudo, 'consulta_original') > 0.7  -- ‚ö†Ô∏è Nota: Fun√ß√£o experimental no pgai
)
SELECT * FROM filtrados ORDER BY distancia LIMIT 5;
```

Este c√≥digo SQL demonstra uma abordagem de duas fases para melhorar a qualidade da recupera√ß√£o em sistemas RAG. Na primeira fase, utilizamos a [busca vetorial](https://en.wikipedia.org/wiki/Vector_database) para recuperar 20 candidatos iniciais ordenados por [similaridade vetorial](https://en.wikipedia.org/wiki/Vector_database) (usando o operador `<=>` do [pgvector](https://en.wikipedia.org/wiki/Vector_database) para calcular a dist√¢ncia entre embeddings). Esta etapa prioriza a velocidade e a amplitude da recupera√ß√£o.

Na segunda fase, aplicamos filtros mais refinados para verificar a relev√¢ncia real dos documentos recuperados. Isso pode incluir filtros baseados em regras (como busca por palavras-chave usando `ILIKE`) ou at√© mesmo modelos secund√°rios de avalia√ß√£o de relev√¢ncia (como sugerido no coment√°rio sobre a fun√ß√£o experimental do [pgai](https://github.com/timescale/pgai)). Esta abordagem em duas etapas equilibra efici√™ncia e precis√£o, permitindo que o sistema primeiro capture um conjunto amplo de candidatos potenciais e depois refine os resultados para apresentar apenas os documentos verdadeiramente relevantes para a consulta do usu√°rio.


### Armadilha 2: Tamanho Inadequado de Chunks

Dividir documentos em chunks menores √© uma pr√°tica padr√£o, mas qual √© o tamanho ideal?

- Chunks muito pequenos perdem contexto crucial
- Chunks muito grandes diluem a recupera√ß√£o com detalhes irrelevantes

> **Solu√ß√£o**: Adaptar a estrat√©gia de chunking ao tipo de conte√∫do. No nosso [PostgreSQL RAG](/post/semantic-postgresql/), usamos chunking recursivo:

```sql
-- Podemos ajustar os par√¢metros de chunking para diferentes tipos de documentos
SELECT ai.create_vectorizer(
   'documentos_tecnicos'::regclass,
   destination => 'embeddings_tecnicos',
   embedding => ai.embedding_ollama('nomic-embed-text', 768),
   -- Chunks maiores para documentos t√©cnicos que precisam de mais contexto
   chunking => ai.chunking_recursive_character_text_splitter('conteudo', 
                                                           chunk_size => 1500, 
                                                           chunk_overlap => 200)
);
```

Para documentos t√©cnicos, que geralmente cont√™m informa√ß√µes densas e interconectadas, configuramos chunks maiores (1500 caracteres) com uma sobreposi√ß√£o significativa (200 caracteres). 

Isso permite preservar mais contexto dentro de cada chunk, o que √© crucial para a compreens√£o de conceitos t√©cnicos complexos. O uso do `chunking_recursive_character_text_splitter` implementa uma estrat√©gia de divis√£o recursiva que respeita a estrutura natural do texto, enquanto o modelo de embedding `nomic-embed-text` com 768 dimens√µes captura as nuances sem√¢nticas do conte√∫do t√©cnico. Esta [abordagem adaptativa de chunking](https://en.wikipedia.org/wiki/Chunking_(data_storage)) √© fundamental para equilibrar a granularidade da recupera√ß√£o com a preserva√ß√£o do contexto necess√°rio para respostas precisas em sistemas [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation).


### Armadilha 3: Falta de Monitoramento Cont√≠nuo

Como garantir que seu sistema permane√ßa eficaz ao longo do tempo? [LLMOps](https://www.databricks.com/br/glossary/llmops) n√£o √© apenas sobre implanta√ß√£o, mas sobre o monitoramento cont√≠nuo da qualidade.

> **Solu√ß√£o**: Implementar m√©tricas de avalia√ß√£o como:
> - Compara√ß√µes com respostas conhecidas (ground truth)
> - Detec√ß√£o de drift em embeddings
> - Monitoramento de lat√™ncia e taxa de falhas

### Armadilha 4: Consultas Complexas em Pipelines Simples

Muitas consultas do mundo real s√£o complexas demais para uma √∫nica etapa de recupera√ß√£o. Se uma pergunta requer sintetizar v√°rias informa√ß√µes, um pipeline RAG padr√£o pode falhar.

**Solu√ß√£o**: Implementar fluxos de trabalho mais sofisticados:
- Workflows com agentes
- Recupera√ß√£o multi-hop
- Gera√ß√£o din√¢mica de prompts

## T√©cnicas Avan√ßadas de Otimiza√ß√£o

Agora que entendemos os fundamentos e as armadilhas comuns, vamos explorar t√©cnicas espec√≠ficas para melhorar cada componente do nosso sistema RAG.

### Re-ranqueamento de Chunks

```mermaid
flowchart LR
    subgraph "Primeira Fase"
        Q[Consulta] --> EMB[Embedding da Consulta]
        EMB --> SIM[Busca por Similaridade Vetorial]
        DB[(Base Vetorial)] --> SIM
        SIM --> IC[Chunks Iniciais]
    end
    
    subgraph "Re-ranqueamento"
        IC --> PAIR[Pares Consulta-Chunk]
        Q2[Consulta Original] --> PAIR
        PAIR --> CENC[Cross-Encoder]
        CENC --> SCORE[Scores de Relev√¢ncia]
        SCORE --> SORT[Ordena√ß√£o por Relev√¢ncia]
        SORT --> RC[Chunks Re-ranqueados]
    end
    
    IC -.-> |Top-K Chunks| PAIR
    RC --> GEN[Gera√ß√£o de Resposta]
    
    style Q fill:#f9f,stroke:#333,stroke-width:2px
    style Q2 fill:#f9f,stroke:#333,stroke-width:2px
    style CENC fill:#ffc,stroke:#333,stroke-width:2px
    style RC fill:#9f9,stroke:#333,stroke-width:2px
    style GEN fill:#99f,stroke:#333,stroke-width:2px
```

O diagrama acima ilustra o processo de re-ranqueamento em um sistema RAG, dividido em duas fases principais: 

1. Na "Primeira Fase", o fluxo come√ßa com a consulta do usu√°rio que √© transformada em um embedding vetorial. Este embedding √© ent√£o utilizado para realizar uma busca por [similaridade vetorial](https://en.wikipedia.org/wiki/Vector_database) na base de dados vetoriais, resultando em um conjunto inicial de chunks relevantes.

2. A segunda fase, "Re-ranqueamento", representa o refinamento desses resultados iniciais. Os chunks recuperados s√£o combinados com a consulta original para formar pares consulta-chunk. Estes pares s√£o processados por um [cross-encoder](https://en.wikipedia.org/wiki/Cross-encoder), um modelo especializado que avalia a relev√¢ncia contextual entre a consulta e cada chunk. O cross-encoder gera scores de relev√¢ncia que permitem uma ordena√ß√£o mais precisa, resultando em chunks re-ranqueados que s√£o finalmente utilizados para a gera√ß√£o da resposta final.

Esta abordagem em duas etapas combina a efici√™ncia computacional dos embeddings (que permitem busca r√°pida em grandes bases de dados) com a precis√£o dos cross-encoders (que capturam melhor as rela√ß√µes sem√¢nticas entre consulta e documento), superando as limita√ß√µes de cada m√©todo quando usado isoladamente. Abordagem conceitual de como implementar re-ranqueamento com cross-encoder em Clojure:

```clojure
;; Exemplo conceitual de como implementar re-ranqueamento com cross-encoder
(defn rerank-results
  "Re-classifica resultados usando cross-encoder para melhorar a precis√£o"
  [query initial-results n]
  (let [;; Em um cen√°rio real, usar√≠amos uma biblioteca Clojure para acessar modelos
        ;; Como o clj-huggingface ou wrapper Java para transformers
        cross-encoder (load-cross-encoder "cross-encoder/ms-marco-MiniLM-L-6-v2")
        
        ;; Preparar pares de consulta-documento para avalia√ß√£o
        pairs (map (fn [doc] [query (:conteudo doc)]) initial-results)
        
        ;; Obter scores de relev√¢ncia do cross-encoder
        scores (predict-with-cross-encoder cross-encoder pairs)
        
        ;; Associar scores aos resultados originais
        results-with-scores (map-indexed 
                              (fn [idx doc] 
                                (assoc doc :relevance_score (nth scores idx)))
                              initial-results)
        
        ;; Ordenar por score de relev√¢ncia (do maior para o menor)
        reranked-results (sort-by :relevance_score > results-with-scores)]
    
    ;; Retornar apenas os top-n resultados
    (take n reranked-results)))

;; Fun√ß√µes auxiliares (implementa√ß√µes dependeriam da biblioteca espec√≠fica usada)
(defn load-cross-encoder [model-name]
  ;; Carregar modelo cross-encoder usando Java interop ou biblioteca espec√≠fica
  (println "Carregando modelo" model-name)
  {:model-name model-name})

(defn predict-with-cross-encoder [model pairs]
  ;; Executar predi√ß√£o do cross-encoder nos pares consulta-documento
  ;; Retorna um vetor de scores de relev√¢ncia
  (println "Avaliando" (count pairs) "pares com" (:model-name model))
  (vec (repeatedly (count pairs) #(rand))))
```

No contexto do [DocAI com PostgreSQL](/post/semantic-postgresql/), podemos implementar isso como:

```clojure
;; Exemplo de implementa√ß√£o de re-ranqueamento em Clojure para DocAI
(defn rerank-results
  "Re-classifica resultados usando cross-encoder"
  [query initial-results]
  (let [conn (jdbc/get-connection db-spec)
        ;; Construir array de IDs para consulta SQL
        ids (str/join "," (map :id initial-results))
        ;; Consulta SQL que utiliza fun√ß√£o do pgai para re-classifica√ß√£o
        sql (str "SELECT d.id, d.titulo, d.conteudo, 
                 ai.relevance_score('" query "', d.conteudo) AS relevance  -- ‚ö†Ô∏è Nota: Fun√ß√£o experimental no pgai
                 FROM documentos d 
                 WHERE d.id IN (" ids ") 
                 ORDER BY relevance DESC")]
    (jdbc/execute! conn [sql])))
```

O primeiro c√≥digo demonstra uma implementa√ß√£o conceitual de re-ranqueamento usando um cross-encoder em Clojure. Ele recebe uma consulta e resultados iniciais, utiliza um modelo cross-encoder para avaliar a relev√¢ncia de cada documento em rela√ß√£o √† consulta, e ent√£o reordena os resultados com base nos scores obtidos. As fun√ß√µes auxiliares simulam a integra√ß√£o com modelos de machine learning, embora em um cen√°rio real seria necess√°rio utilizar bibliotecas espec√≠ficas para acessar modelos de linguagem.

O segundo exemplo mostra uma implementa√ß√£o mais pr√°tica no contexto de um sistema [DocAI integrado com PostgreSQL](/post/semantic-postgresql/). Neste caso, o re-ranqueamento √© delegado a uma fun√ß√£o SQL (`ai.relevance_score`) que avalia a relev√¢ncia entre a consulta e o conte√∫do do documento diretamente no banco de dados. Esta abordagem aproveita as capacidades de IA incorporadas no PostgreSQL atrav√©s de extens√µes como pgai, simplificando a arquitetura ao mover o processamento de relev√¢ncia para o banco de dados.

Ambas as implementa√ß√µes ilustram diferentes estrat√©gias para melhorar a precis√£o dos resultados em sistemas RAG. A primeira abordagem oferece mais controle e flexibilidade ao processar o re-ranqueamento na aplica√ß√£o, enquanto a segunda aproveita as capacidades do banco de dados para simplificar a arquitetura e potencialmente melhorar o desempenho ao reduzir a transfer√™ncia de dados entre a aplica√ß√£o e o banco de dados. A escolha entre estas abordagens depender√° dos requisitos espec√≠ficos do sistema, incluindo considera√ß√µes de desempenho, escalabilidade e facilidade de manuten√ß√£o.

---

### Estrat√©gias de Chunking Din√¢mico

Em vez de usar um tamanho fixo para todos os chunks, podemos implementar estrat√©gias din√¢micas que se adaptam ao conte√∫do:

- **Chunking Sem√¢ntico**: Dividir o texto em unidades semanticamente coerentes
- **Chunking Hier√°rquico**: Manter m√∫ltiplas granularidades do mesmo conte√∫do
- **Chunking Adaptativo**: Ajustar tamanho com base em caracter√≠sticas do documento

```clojure
;; Fun√ß√£o conceitual para chunking hier√°rquico
(defn create-hierarchical-chunks
  "Cria chunks em m√∫ltiplos n√≠veis de granularidade"
  [document]
  (let [;; Divis√£o em par√°grafos
        paragraphs (split-paragraphs document)
        ;; Divis√£o em se√ß√µes
        sections (split-sections document)
        ;; Documento completo
        full-doc [{:content document :level "document"}]
        ;; Combinar todos os n√≠veis
        all-chunks (concat full-doc
                          (map #(hash-map :content % :level "section") sections)
                          (map #(hash-map :content % :level "paragraph") paragraphs))]
    ;; Inserir no PostgreSQL com metadados sobre o n√≠vel
    (doseq [chunk all-chunks]
      (jdbc/execute! db-spec
                    ["INSERT INTO documentos_hierarquicos 
                     (conteudo, nivel_granularidade) VALUES (?, ?)"
                     (:content chunk) (:level chunk)]))))
```

O c√≥digo acima implementa uma estrat√©gia de [chunking hier√°rquico](https://en.wikipedia.org/wiki/Chunking_(data_storage)) em [Clojure](https://clojure.org/), uma t√©cnica avan√ßada para sistemas [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) que mant√©m m√∫ltiplas representa√ß√µes do mesmo conte√∫do em diferentes n√≠veis de granularidade. A fun√ß√£o `create-hierarchical-chunks` divide um documento em tr√™s n√≠veis: documento completo, se√ß√µes e par√°grafos, preservando assim tanto o contexto amplo quanto os detalhes espec√≠ficos. 

Esta abordagem permite que o sistema de recupera√ß√£o escolha a granularidade mais apropriada dependendo da consulta, oferecendo flexibilidade que um chunking de tamanho fixo n√£o consegue proporcionar.

A implementa√ß√£o utiliza fun√ß√µes auxiliares como `split-paragraphs` e `split-sections` (n√£o mostradas no c√≥digo) para segmentar o documento de forma inteligente, respeitando a estrutura sem√¢ntica do texto. Cada [chunk](https://en.wikipedia.org/wiki/Chunk_(data_storage)) √© armazenado no [PostgreSQL](https://www.postgresql.org/) junto com metadados sobre seu n√≠vel de granularidade, permitindo consultas que podem priorizar diferentes n√≠veis dependendo do tipo de pergunta. 

Esta t√©cnica √© particularmente valiosa para documentos longos e estruturados, como artigos t√©cnicos ou documenta√ß√£o, onde tanto o contexto geral quanto detalhes espec√≠ficos podem ser relevantes dependendo da natureza da consulta do usu√°rio.

---

### Workflows com Agentes para Consultas Complexas

Para consultas que exigem racioc√≠nio em v√°rias etapas, podemos implementar agentes que decomp√µem o problema:

```mermaid
flowchart TB
    Q[Consulta Original] --> AN[Analisador de Consulta]
    AN --> SQ1[Sub-quest√£o 1]
    AN --> SQ2[Sub-quest√£o 2]
    AN --> SQ3[Sub-quest√£o 3]
    
    SQ1 --> R1[RAG Espec√≠fico 1]
    SQ2 --> R2[RAG Espec√≠fico 2]
    SQ3 --> R3[RAG Espec√≠fico 3]
    
    R1 --> A1[Resposta Parcial 1]
    R2 --> A2[Resposta Parcial 2]
    R3 --> A3[Resposta Parcial 3]
    
    A1 --> S[Sintetizador]
    A2 --> S
    A3 --> S
    
    S --> FR[Resposta Final]
    
    style Q fill:#f9f,stroke:#333,stroke-width:2px
    style S fill:#bbf,stroke:#333,stroke-width:2px
    style FR fill:#bfb,stroke:#333,stroke-width:2px
```

Este diagrama ilustra uma arquitetura de [workflow](https://en.wikipedia.org/wiki/Workflow) baseada em agentes para processamento de consultas complexas em sistemas [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation). O fluxo come√ßa com uma consulta do usu√°rio que √© analisada por um componente [Analisador](https://en.wikipedia.org/wiki/Query_parser), respons√°vel por decompor a pergunta original em sub-quest√µes mais espec√≠ficas e gerenci√°veis. Cada sub-quest√£o √© ent√£o direcionada para um pipeline [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) especializado, permitindo recupera√ß√µes contextuais mais precisas.

A abordagem [divide-e-conquista](https://en.wikipedia.org/wiki/Divide_and_conquer_algorithm) demonstrada no diagrama permite que o sistema lide com perguntas que exigiriam conhecimento de diferentes dom√≠nios ou documentos. Cada [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) especializado pode utilizar diferentes bases de conhecimento, estrat√©gias de recupera√ß√£o ou at√© mesmo modelos de linguagem otimizados para dom√≠nios espec√≠ficos, resultando em respostas parciais de alta qualidade para cada aspecto da consulta.

O componente Sintetizador atua como o elemento integrador final, combinando as respostas parciais em uma resposta coerente e abrangente. Esta arquitetura modular n√£o apenas melhora a precis√£o das respostas para consultas complexas, mas tamb√©m oferece maior transpar√™ncia no processo de racioc√≠nio, permitindo identificar quais fontes contribu√≠ram para cada parte da resposta final. O resultado √© um sistema RAG mais robusto, capaz de lidar com consultas que exigem racioc√≠nio em m√∫ltiplas etapas e integra√ß√£o de informa√ß√µes de diversas fontes.

```clojure
(defn agent-rag-workflow
  "Implementa um workflow de agente para consultas complexas"
  [query]
  (let [;; Passo 1: Analisar a consulta e identificar sub-quest√µes
        sub-questions (analyze-query query)
        ;; Passo 2: Buscar informa√ß√µes para cada sub-quest√£o
        sub-answers (map #(retrieve-and-generate %) sub-questions)
        ;; Passo 3: Sintetizar respostas parciais em uma resposta final
        final-context (str/join "\n\n" sub-answers)
        final-prompt (str "Com base nas seguintes informa√ß√µes:\n\n" 
                         final-context 
                         "\n\nResponda √† pergunta original: " query)
        final-answer (generate-response final-prompt)]
    final-answer))

(defn analyze-query
  "Divide uma consulta complexa em sub-quest√µes"
  [query]
  (let [prompt (str "Divida a seguinte pergunta em sub-quest√µes independentes:\n\n" query)
        response (call-ollama-api prompt)
        ;; Parsear a resposta para extrair as sub-quest√µes
        sub-questions (parse-sub-questions response)]
    sub-questions))
```

Uma implementa√ß√£o mais robusta de workflows com agentes envolve v√°rias etapas adicionais. Trataremos deste assunto em um pr√≥ximo artigo.

---

#### Arquitetura de Agentes Avan√ßada

Os sistemas de agentes RAG mais sofisticados aplicam o conceito de **ReAct** (Racioc√≠nio + A√ß√£o) para processar consultas complexas:

```mermaid
flowchart TB
    subgraph "Arquitetura ReAct para RAG"
    Q[Consulta do Usu√°rio] --> PL[Planejador]
    PL --> PLAN[Plano de Execu√ß√£o]
    PLAN --> RT[Roteador]
    
    RT -->|Sub-tarefa 1| AS[Agente de Pesquisa]
    RT -->|Sub-tarefa 2| AR[Agente de Racioc√≠nio]
    RT -->|Sub-tarefa 3| AC[Agente de C√°lculo]
    
    AS --> OR[Orquestrador]
    AR --> OR
    AC --> OR
    
    OR --> SI[Sintetizador]
    SI --> RES[Resposta Final]
    end
    
    subgraph "Ferramentas e Recursos"
    AS -.-> VDB[(Base Vetorial)]
    AR -.-> LLM[Modelo de Linguagem]
    AC -.-> CALC[Ferramentas de C√°lculo]
    end
    
    style Q fill:#f9f,stroke:#333,stroke-width:2px
    style PLAN fill:#ffc,stroke:#333,stroke-width:2px
    style RT fill:#9cf,stroke:#333,stroke-width:2px
    style AS fill:#bbf,stroke:#333,stroke-width:2px
    style AR fill:#bbf,stroke:#333,stroke-width:2px
    style AC fill:#bbf,stroke:#333,stroke-width:2px
    style SI fill:#bfb,stroke:#333,stroke-width:2px
    style RES fill:#f99,stroke:#333,stroke-width:2px
```

Este diagrama ilustra uma arquitetura avan√ßada ReAct para sistemas [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation), mostrando como uma consulta complexa √© processada atrav√©s de m√∫ltiplos componentes especializados. O fluxo come√ßa com a consulta do usu√°rio sendo analisada por um [Planejador](https://en.wikipedia.org/wiki/Workflow), que cria um plano estruturado de execu√ß√£o. 

Este plano √© ent√£o gerenciado por um [Roteador](https://en.wikipedia.org/wiki/Routing) que distribui sub-tarefas para agentes especializados (Pesquisa, Racioc√≠nio e C√°lculo), cada um interagindo com recursos espec√≠ficos como bases de dados vetoriais, LLMs ou ferramentas de c√°lculo.

A for√ßa desta arquitetura est√° na sua capacidade de decompor problemas complexos em tarefas gerenci√°veis e especializadas, permitindo que cada componente se concentre no que faz melhor. O [Orquestrador](https://en.wikipedia.org/wiki/Orchestration) coordena os resultados dos diferentes agentes, enquanto o [Sintetizador](https://en.wikipedia.org/wiki/Synthesis) integra todas as informa√ß√µes em uma resposta final coerente. Esta abordagem modular n√£o apenas melhora a precis√£o das respostas, mas tamb√©m aumenta a transpar√™ncia do processo de racioc√≠nio e facilita a depura√ß√£o e otimiza√ß√£o de componentes individuais do sistema RAG.

- **Planejador**: Analisa a consulta e cria um plano de execu√ß√£o
- **Roteador**: Direciona sub-consultas para ferramentas especializadas
- **Agentes Especializados**: Executam tarefas espec√≠ficas
   - Agente de Pesquisa: Recupera informa√ß√µes da base de conhecimento
   - Agente de Racioc√≠nio: Realiza infer√™ncias l√≥gicas sobre os dados recuperados
   - Agente de C√°lculo: Processa c√°lculos e an√°lises num√©ricas
- **Orquestrador**: Gerencia o fluxo de informa√ß√µes entre agentes
- **Sintetizador**: Combina as respostas em um resultado coerente

Vamos analisar o c√≥digo abaixo para entender como funciona um sistema ReAct para RAG:

```clojure
;; Exemplo conceitual de um sistema ReAct para RAG
(defn react-agent
  "Implementa um agente ReAct para consultas complexas"
  [query]
  (let [;; Determinar se a consulta precisa de um plano
        plan-needed? (complex-query? query)
        ;; Se necess√°rio, criar um plano
        execution-plan (when plan-needed?
                         (create-execution-plan query))
        ;; Executar o plano ou a consulta direta
        result (if plan-needed?
                 (execute-plan execution-plan)
                 (simple-rag-query query))]
    result))

(defn execute-plan
  "Executa um plano com agentes especializados"
  [plan]
  (loop [steps (:steps plan)
         context {}
         responses []]
    (if (empty? steps)
      ;; Sintetizar respostas em um resultado final
      (synthesize-responses responses (:query plan))
      (let [current-step (first steps)
            agent-type (:agent current-step)
            ;; Determinar qual agente especializado usar
            agent-fn (case agent-type
                       :search search-agent
                       :reasoning reasoning-agent
                       :calculation calculation-agent
                       :default default-agent)
            ;; Executar o agente com o contexto atual
            step-result (agent-fn (:input current-step) context)
            ;; Atualizar o contexto com o resultado
            updated-context (assoc context (:id current-step) step-result)]
        (recur (rest steps) 
               updated-context 
               (conj responses step-result))))))
```

O c√≥digo implementa um agente ReAct (Reasoning + Acting) para consultas complexas em um sistema [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation). A fun√ß√£o principal `react-agent` avalia se a consulta requer um plano de execu√ß√£o complexo ou pode ser processada diretamente. Para consultas complexas, cria-se um plano estruturado que √© executado pela fun√ß√£o `execute-plan`, que utiliza um loop para processar cada etapa do plano sequencialmente. 

O sistema emprega agentes especializados (busca, racioc√≠nio, c√°lculo) selecionados dinamicamente com base no tipo de tarefa. Cada agente contribui com resultados parciais que s√£o acumulados em um contexto compartilhado, permitindo que etapas posteriores utilizem informa√ß√µes de etapas anteriores. Finalmente, todas as respostas s√£o sintetizadas em um resultado coerente. 

Esta arquitetura modular permite decompor problemas complexos em tarefas gerenci√°veis, melhorando a precis√£o e facilitando a manuten√ß√£o do sistema.Para implementa√ß√µes detalhadas de sistemas de agentes RAG, consulte:

- [LlamaIndex - Implementando ReAct Agents](https://docs.llamaindex.ai/en/stable/examples/agent/react_agent.html)
- [LangChain - Multi-Agent Systems](https://python.langchain.com/docs/modules/agents/agent_types/multi_agent)
- [HuggingFace - Agentes Aut√¥nomos](https://huggingface.co/blog/autonomous-agents)

#### Casos de Uso para Workflows de Agentes

Os workflows com agentes s√£o particularmente √∫teis em cen√°rios como:

- **Pesquisa Cient√≠fica**: Onde diversas fontes precisam ser consultadas e relacionadas
- **Diagn√≥stico de Problemas**: Quando √© necess√°rio seguir uma √°rvore de decis√£o
- **An√°lise de Documentos Complexos**: Como contratos ou documenta√ß√£o t√©cnica
- **Planejamento Estrat√©gico**: Onde m√∫ltiplas dimens√µes precisam ser consideradas

---

### Pipelines Multimodais

Integrar entradas multimodais (texto, imagens, tabelas) em um pipeline RAG pode enriquecer significativamente o contexto:

```mermaid
flowchart LR
    subgraph "Documento Misto"
    TXT[Texto]
    IMG[Imagens]
    TBL[Tabelas]
    end
    
    subgraph "Processadores Espec√≠ficos"
    TXT --> TXT_P[Processador de Texto]
    IMG --> IMG_P[Processador de Imagem]
    TBL --> TBL_P[Processador de Tabela]
    end
    
    subgraph "Embeddings"
    TXT_P --> TXT_E[Embedding de Texto]
    IMG_P --> IMG_E[Embedding de Imagem]
    TBL_P --> TBL_E[Embedding de Tabela]
    end
    
    TXT_E --> FUS[Fus√£o de Representa√ß√µes]
    IMG_E --> FUS
    TBL_E --> FUS
    
    FUS --> DB[(Base de Dados Multimodal)]
    Q[Consulta do Usu√°rio] --> Q_PROC[Processador de Consulta]
    Q_PROC --> RAG[Motor RAG]
    DB --> RAG
    RAG --> RES[Resposta Multimodal]
    
    style TXT fill:#f9f,stroke:#333,stroke-width:2px
    style IMG fill:#9cf,stroke:#333,stroke-width:2px
    style TBL fill:#fcf,stroke:#333,stroke-width:2px
    style FUS fill:#ff9,stroke:#333,stroke-width:2px
    style DB fill:#9f9,stroke:#333,stroke-width:2px
    style RES fill:#f99,stroke:#333,stroke-width:2px
```

O diagrama acima ilustra uma arquitetura de pipeline multimodal para sistemas [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation), demonstrando como diferentes tipos de conte√∫do (texto, imagens e tabelas) podem ser processados e integrados em um √∫nico sistema de recupera√ß√£o. O fluxo come√ßa com a extra√ß√£o desses diferentes elementos de um documento misto, cada um seguindo para processadores especializados que compreendem as caracter√≠sticas √∫nicas de cada modalidade.

Na camada de embeddings, cada tipo de conte√∫do √© transformado em representa√ß√µes vetoriais espec√≠ficas para sua modalidade - textos s√£o processados por modelos de linguagem, imagens por modelos de vis√£o computacional, e tabelas por processadores estruturados. O componente de fus√£o de representa√ß√µes √© crucial nesta arquitetura, pois combina estas diferentes representa√ß√µes vetoriais em um formato unificado que pode ser armazenado e consultado eficientemente na base de dados multimodal.

Quando uma consulta do usu√°rio √© recebida, ela passa pelo processador de consulta que determina quais modalidades s√£o relevantes para a pergunta, e o motor [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) recupera as informa√ß√µes apropriadas da base de dados multimodal. Esta abordagem permite que o sistema forne√ßa respostas enriquecidas que incorporam conhecimento de m√∫ltiplas modalidades, resultando em uma experi√™ncia mais completa e contextualmente relevante para o usu√°rio, especialmente para consultas que se beneficiam de informa√ß√µes visuais ou estruturadas al√©m do texto puro.

```clojure
(defn process-multimodal-document
  "Processa um documento que cont√©m texto e imagens"
  [doc-path]
  (let [;; Extrair texto
        text-content (extract-text doc-path)
        ;; Identificar e extrair imagens
        image-paths (extract-images doc-path)
        ;; Gerar descri√ß√µes para as imagens usando um modelo de vis√£o
        image-descriptions (map #(describe-image %) image-paths)
        ;; Combinar texto e descri√ß√µes de imagens
        enriched-content (str text-content "\n\n"
                             "O documento cont√©m as seguintes imagens:\n"
                             (str/join "\n" image-descriptions))]
    ;; Inserir no banco de dados
    (jdbc/execute! db-spec
                  ["INSERT INTO documentos (titulo, conteudo) VALUES (?, ?)"
                   (extract-title doc-path) enriched-content])))
```

---

#### Arquitetura Multimodal Completa

Uma implementa√ß√£o mais completa de pipelines multimodais requer v√°rios componentes especializados:

```mermaid
flowchart TD
    DOC[Documento Multimodal] --> DETECT[Detector de Tipo]
    DETECT --> EXTRACT[Extra√ß√£o de Componentes]
    
    EXTRACT --> TX[Componentes de Texto]
    EXTRACT --> IMG[Componentes de Imagem]
    EXTRACT --> TBL[Componentes de Tabela]
    EXTRACT --> AUD[Componentes de √Åudio]
    
    TX --> TX_PROC[Processador de Texto]
    IMG --> IMG_PROC[Processador de Imagem]
    TBL --> TBL_PROC[Processador de Tabela]
    AUD --> AUD_PROC[Processador de √Åudio]
    
    TX_PROC --> TX_EMB[Embedding de Texto]
    IMG_PROC --> IMG_EMB[Embedding de Imagem]
    TBL_PROC --> TBL_EMB[Embedding de Tabela]
    AUD_PROC --> AUD_EMB[Embedding de √Åudio]
    
    TX_EMB --> FUSION[Fus√£o de Representa√ß√µes]
    IMG_EMB --> FUSION
    TBL_EMB --> FUSION
    AUD_EMB --> FUSION
    
    FUSION --> META[Adi√ß√£o de Metadados]
    META --> STORE[Armazenamento em PostgreSQL]
    
    subgraph "Modelos Espec√≠ficos"
        TX_PROC -.- TEXT_MODEL[Modelo de Texto]
        IMG_PROC -.- CLIP[CLIP]
        TBL_PROC -.- TABLE_MODEL[Modelo de Tabela]
        AUD_PROC -.- AUDIO_MODEL[Modelo de √Åudio]
        FUSION -.- FLAMINGO[Flamingo]
    end
    
    style DOC fill:#f9f,stroke:#333,stroke-width:2px
    style FUSION fill:#ff9,stroke:#333,stroke-width:2px
    style META fill:#9cf,stroke:#333,stroke-width:2px
    style STORE fill:#9f9,stroke:#333,stroke-width:2px
```

O diagrama acima ilustra uma arquitetura para [processamento de documentos multimodais](https://en.wikipedia.org/wiki/Multimodal_AI) em sistemas RAG avan√ßados. O fluxo come√ßa com um documento multimodal que passa por um [detector de tipo](https://en.wikipedia.org/wiki/Type_detection), seguido pela [extra√ß√£o de componentes](https://en.wikipedia.org/wiki/Component_extraction) que separa o conte√∫do em diferentes modalidades: texto, imagem, tabela e √°udio. Cada tipo de componente √© ent√£o direcionado para um processador especializado, projetado para extrair informa√ß√µes significativas espec√≠ficas daquela modalidade.

Ap√≥s o processamento inicial, cada componente √© transformado em uma [representa√ß√£o vetorial (embedding)](https://en.wikipedia.org/wiki/Embedding_model) usando modelos especializados para cada modalidade - [modelos de texto para componentes textuais](https://en.wikipedia.org/wiki/Text_embedding), [CLIP para imagens](https://en.wikipedia.org/wiki/CLIP), [modelos espec√≠ficos para tabelas](https://en.wikipedia.org/wiki/Table_embedding) e [√°udio](https://en.wikipedia.org/wiki/Audio_embedding). Estes embeddings s√£o ent√£o combinados atrav√©s de um processo de fus√£o de representa√ß√µes, que cria uma compreens√£o unificada e coerente do documento multimodal, potencialmente utilizando modelos como o [Flamingo](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model) que s√£o projetados para integra√ß√£o multimodal.

A etapa final do pipeline envolve a adi√ß√£o de [metadados estruturados](https://en.wikipedia.org/wiki/Metadata) √† [representa√ß√£o unificada](https://en.wikipedia.org/wiki/Unified_representation) e seu armazenamento em um banco de dados [PostgreSQL](https://www.postgresql.org/) otimizado para [busca vetorial](https://en.wikipedia.org/wiki/Vector_database) com [pgvector](https://github.com/pgvector/pgvector). 

Esta arquitetura modular permite que o sistema [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) processe eficientemente documentos complexos contendo m√∫ltiplos tipos de m√≠dia, mantendo as rela√ß√µes sem√¢nticas entre diferentes componentes e possibilitando recupera√ß√£o mais precisa quando consultado. Os modelos espec√≠ficos destacados no diagrama (`TEXT_MODEL`, `CLIP`, `TABLE_MODEL`, `AUDIO_MODEL` e `FLAMINGO`) representam as tecnologias de ponta que podem ser empregadas em cada etapa do processamento.

O c√≥digo abaixo implementa um pipeline avan√ßado para processamento de documentos multimodais em [Clojure](https://clojure.org/), demonstrando uma abordagem sofisticada para lidar com conte√∫do heterog√™neo em sistemas [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation):

```clojure
;; Exemplo de pipeline multimodal mais elaborado
(defn advanced-multimodal-processor
  "Pipeline completo para processamento multimodal"
  [document-path]
  (let [;; Determinar tipo de documento
        doc-type (detect-document-type document-path)
        
        ;; Extrair componentes por tipo
        components (case doc-type
                     :pdf (extract-pdf-components document-path)
                     :doc (extract-doc-components document-path)
                     :webpage (extract-webpage-components document-path)
                     (extract-text-components document-path))
        
        ;; Processar cada componente com seu processador especializado
        processed-components (map process-component components)
        
        ;; Gerar embeddings multimodais
        embeddings (map #(generate-multimodal-embedding % doc-type) processed-components)
        
        ;; Criar representa√ß√£o unificada
        unified-representation {:components processed-components
                               :embeddings embeddings
                               :metadata {:doc-type doc-type
                                         :path document-path
                                         :extracted-at (java.util.Date.)}}]
    
    ;; Armazenar no PostgreSQL com schema adequado para multimodalidade
    (store-multimodal-document unified-representation)))

(defn process-component
  "Processa um componente baseado em seu tipo"
  [component]
  (case (:type component)
    :text (process-text (:content component))
    :image (process-image (:content component))
    :table (process-table (:content component))
    :chart (process-chart (:content component))
    :audio (process-audio (:content component))
    (:content component))) ;; Fallback para tipos desconhecidos
```

A fun√ß√£o principal `advanced-multimodal-processor` orquestra todo o fluxo, come√ßando pela detec√ß√£o do tipo de documento, seguida pela extra√ß√£o de componentes espec√≠ficos para cada formato (PDF, DOC, p√°ginas web), processamento especializado de cada componente, gera√ß√£o de embeddings multimodais e finalmente o armazenamento da representa√ß√£o unificada no PostgreSQL. Esta arquitetura modular permite que o sistema processe de forma inteligente diferentes tipos de m√≠dia dentro do mesmo documento.

A fun√ß√£o auxiliar `process-component` exemplifica o tratamento especializado para cada modalidade, direcionando o conte√∫do para processadores espec√≠ficos com base no tipo do componente (texto, imagem, tabela, gr√°fico ou √°udio). Esta abordagem granular garante que cada tipo de conte√∫do receba o tratamento mais apropriado, maximizando a qualidade da informa√ß√£o extra√≠da e sua representa√ß√£o vetorial. 

O resultado √© um sistema [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) verdadeiramente [multimodal](https://en.wikipedia.org/wiki/Multimodal_AI), capaz de compreender e recuperar informa√ß√µes de documentos complexos que combinam texto, elementos visuais e dados estruturados, proporcionando respostas mais completas e contextualmente ricas para as consultas dos usu√°rios.

---

#### Esquema PostgreSQL para Dados Multimodais

Para armazenar e recuperar eficientemente dados multimodais no PostgreSQL:

```sql
-- Tabela principal para documentos multimodais
CREATE TABLE documentos_multimodais (
    id SERIAL PRIMARY KEY,
    titulo TEXT NOT NULL,
    doc_type TEXT,
    metadata JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Tabela para componentes espec√≠ficos
CREATE TABLE componentes_documento (
    id SERIAL PRIMARY KEY,
    documento_id INTEGER REFERENCES documentos_multimodais(id) ON DELETE CASCADE,
    tipo_componente TEXT NOT NULL,
    conteudo TEXT,
    posicao INTEGER,
    metadados JSONB
);

-- Tabela para embeddings de texto
CREATE TABLE embeddings_texto (
    id SERIAL PRIMARY KEY,
    componente_id INTEGER REFERENCES componentes_documento(id) ON DELETE CASCADE,
    embedding VECTOR(768)
);

-- Tabela para embeddings de imagem
CREATE TABLE embeddings_imagem (
    id SERIAL PRIMARY KEY,
    componente_id INTEGER REFERENCES componentes_documento(id) ON DELETE CASCADE,
    embedding VECTOR(512)
);
```

Este esquema (scheme) [SQL](https://en.wikipedia.org/wiki/SQL) estabelece uma estrutura robusta para armazenar e gerenciar documentos multimodais no [PostgreSQL](https://www.postgresql.org/). A arquitetura √© composta por quatro tabelas interconectadas: uma tabela principal (`documentos_multimodais`) que armazena metadados gerais dos documentos, uma tabela para componentes espec√≠ficos (`componentes_documento`) que fragmenta cada documento em suas partes constituintes (texto, imagens, etc.), e duas tabelas especializadas para armazenar embeddings vetoriais de diferentes modalidades (`embeddings_texto` e `embeddings_imagem`). Esta estrutura relacional permite uma organiza√ß√£o hier√°rquica do conte√∫do, mantendo a integridade referencial atrav√©s de chaves estrangeiras.

A separa√ß√£o dos [embeddings](https://en.wikipedia.org/wiki/Embedding_model) por tipo de modalidade √© particularmente importante, pois diferentes tipos de conte√∫do geralmente requerem modelos de embedding distintos com dimensionalidades variadas (768 para texto e 512 para imagens no exemplo). Esta abordagem modular facilita a implementa√ß√£o de consultas multimodais eficientes, permitindo buscas por similaridade em cada modalidade separadamente ou de forma combinada. 

Al√©m disso, o uso de campos [JSONB](https://www.postgresql.org/docs/current/datatype-json.html) para metadados oferece flexibilidade para armazenar informa√ß√µes adicionais sem necessidade de alterar o esquema, tornando o sistema adapt√°vel a diferentes tipos de documentos e requisitos de aplica√ß√£o. Para implementa√ß√µes detalhadas de [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) multimodal, consulte:

- [MultiModal RAG com LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/multi_modal/)
- [Comprehensive Guide to MultiModal RAG](https://towardsdatascience.com/a-comprehensive-guide-to-multimodal-rag-ea72c387c6e8)
- [Projeto IDEFICS para RAG Multimodal](https://huggingface.co/blog/idefics)
- [Supabase - Image Search com pgvector](https://supabase.com/blog/image-search-using-ai-embeddings)

---	

#### Desafios de Implementa√ß√£o Multimodal

A implementa√ß√£o de pipelines multimodais traz desafios espec√≠ficos:

1. **Alinhamento de Representa√ß√µes**: Garantir que diferentes modalidades possam ser comparadas
2. **Gerenciamento de Recursos**: Modelos multimodais s√£o computacionalmente exigentes
3. **Estrat√©gias de Fus√£o**: Decidir quando fundir informa√ß√µes de diferentes modalidades
   - Fus√£o Precoce: Combinar antes do embedding
   - Fus√£o Tardia: Manter embeddings separados e combinar apenas no ranking final

> No pr√≥ximo artigo, exploraremos em profundidade como expandir o DocAI para oferecer suporte total a conte√∫do multimodal, com exemplos pr√°ticos de implementa√ß√£o e otimiza√ß√£o de desempenho.

---

### Estrat√©gias de Cache

Implementar caching pode reduzir drasticamente a lat√™ncia e os custos:

```mermaid
flowchart TD
    Q[Consulta] --> CH1{Cache L1?}
    CH1 -->|Sim| RES1[Resposta do Cache L1]
    CH1 -->|N√£o| CH2{Cache L2?}
    
    CH2 -->|Sim| RES2[Resposta do Cache L2]
    CH2 -->|N√£o| CH3{Cache L3?}
    
    CH3 -->|Sim| RES3[Resposta do Cache L3]
    CH3 -->|N√£o| PROC[Processamento RAG Completo]
    
    PROC --> RES4[Nova Resposta]
    RES4 --> STORE[Armazenar em Cache]
    STORE --> RES[Resposta Final]
    
    RES1 --> RES
    RES2 --> RES
    RES3 --> RES
    
    subgraph "Camadas de Cache"
    CH1
    CH2
    CH3
    end
    
    style Q fill:#f9f,stroke:#333,stroke-width:2px
    style PROC fill:#ffc,stroke:#333,stroke-width:2px
    style RES fill:#9f9,stroke:#333,stroke-width:2px
    style STORE fill:#9cf,stroke:#333,stroke-width:2px
```

O diagrama acima ilustra uma estrat√©gia de cache em m√∫ltiplas camadas para sistemas RAG, uma t√©cnica fundamental para otimizar tanto a lat√™ncia quanto os custos operacionais. A arquitetura implementa tr√™s n√≠veis de cache `(L1, L2 e L3)`, cada um representando diferentes compromissos entre velocidade e abrang√™ncia. O cache `L1` tipicamente armazena respostas exatas para consultas id√™nticas, oferecendo resposta instant√¢nea quando h√° correspond√™ncia perfeita. O cache `L2` pode armazenar respostas para consultas semanticamente similares, enquanto o cache `L3` pode conter resultados parciais como embeddings pr√©-calculados ou chunks recuperados anteriormente.

Esta abordagem em cascata permite que o sistema evite o processamento [RAG](https://en.wikipedia.org/wiki/Retrieval-Augmented_Generation) completo sempre que poss√≠vel, reduzindo significativamente o tempo de resposta e a carga computacional. Quando uma consulta n√£o encontra correspond√™ncia em nenhum n√≠vel de cache, apenas ent√£o o sistema executa o fluxo completo de [RAG](https://en.wikipedia.org/wiki/Retrieval-Augmented_Generation), que inclui gera√ß√£o de embeddings, recupera√ß√£o de contexto e infer√™ncia do [LLM](https://en.wikipedia.org/wiki/Large_language_model). 

> A nova resposta gerada √© ent√£o armazenada no cache apropriado para uso futuro, criando um sistema que se torna progressivamente mais eficiente √† medida que processa mais consultas. A implementa√ß√£o de uma estrat√©gia de cache multicamada como esta pode reduzir custos operacionais em at√© 70% em sistemas de produ√ß√£o com padr√µes de consulta repetitivos. 

Al√©m da economia de recursos, a redu√ß√£o na lat√™ncia melhora significativamente a experi√™ncia do usu√°rio, com respostas quase instant√¢neas para consultas frequentes. Para maximizar a efic√°cia, √© importante implementar pol√≠ticas de expira√ß√£o de cache e estrat√©gias de invalida√ß√£o para garantir que as informa√ß√µes permane√ßam atualizadas, especialmente em dom√≠nios onde os dados subjacentes mudam com frequ√™ncia. Abaixo, um exemplo de implementa√ß√£o de cache de dois n√≠veis em Clojure:

```clojure
;; Implementa√ß√£o de cache de dois n√≠veis em Clojure
(def embedding-cache (atom {}))
(def response-cache (atom {}))

(defn cached-embed
  "Gera embedding para texto com cache"
  [text]
  (if-let [cached (@embedding-cache text)]
    cached
    (let [embedding (generate-embedding text)]
      (swap! embedding-cache assoc text embedding)
      embedding)))

(defn cached-rag-query
  "Executa consulta RAG com cache"
  [query]
  (if-let [cached (@response-cache query)]
    (do
      (println "Cache hit for query!")
      cached)
    (let [;; Processo RAG normal
          response (full-rag-process query)]
      ;; Armazenar no cache apenas para consultas n√£o-pessoais
      (when (not (personal-query? query))
        (swap! response-cache assoc query response))
      response)))
```

O c√≥digo acima implementa uma estrat√©gia de cache de dois n√≠veis em [Clojure](https://clojure.org/) para otimizar sistemas [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation). O primeiro n√≠vel (`embedding-cache`) armazena embeddings j√° calculados para textos, evitando a regenera√ß√£o desses vetores que √© computacionalmente intensiva. O segundo n√≠vel (`response-cache`) armazena respostas completas para consultas anteriores, permitindo retornar resultados instantaneamente quando uma consulta id√™ntica √© feita novamente. 

A fun√ß√£o `cached-embed` verifica primeiro se o embedding j√° existe no cache antes de ger√°-lo, enquanto `cached-rag-query` implementa l√≥gica similar para respostas completas, incluindo uma verifica√ß√£o inteligente para evitar o cache de consultas pessoais.

Em produ√ß√£o com maior escala, esta abordagem poderia ser estendida para utilizar [Redis](https://redis.io/) ou outras solu√ß√µes de cache distribu√≠do, mantendo os mesmos princ√≠pios fundamentais. Para o [PostgreSQL](https://www.postgresql.org/), podemos implementar [cache de embeddings diretamente no banco](https://www.postgresql.org/docs/current/pgvector-embeddings.html):

```sql
-- Criar tabela de cache para embeddings de consultas frequentes
CREATE TABLE IF NOT EXISTS query_embedding_cache (
  query_text TEXT PRIMARY KEY,
  embedding VECTOR(768),
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  hit_count INTEGER DEFAULT 1
);

-- Fun√ß√£o para obter embedding com cache
CREATE OR REPLACE FUNCTION get_cached_embedding(query TEXT)
RETURNS VECTOR AS $$
DECLARE
  cached_embedding VECTOR(768);
BEGIN
  -- Verificar se existe no cache
  SELECT embedding INTO cached_embedding
  FROM query_embedding_cache
  WHERE query_text = query;
  
  -- Se existe, atualizar contador e retornar
  IF FOUND THEN
    UPDATE query_embedding_cache 
    SET hit_count = hit_count + 1 
    WHERE query_text = query;
    RETURN cached_embedding;
  ELSE
    -- Gerar novo embedding
    cached_embedding := ai.ollama_embed('nomic-embed-text', query);  -- ‚ö†Ô∏è Nota: Verifique a disponibilidade desta fun√ß√£o na sua instala√ß√£o
    
    -- Armazenar no cache
    INSERT INTO query_embedding_cache (query_text, embedding)
    VALUES (query, cached_embedding);
    
    RETURN cached_embedding;
  END IF;
END;
$$ LANGUAGE plpgsql;
```

Este c√≥digo SQL implementa um sistema de cache para embeddings de consultas no PostgreSQL, otimizando significativamente o desempenho de sistemas RAG em produ√ß√£o. A tabela `query_embedding_cache` armazena o texto da consulta como chave prim√°ria, junto com seu [embedding vetorial](https://www.postgresql.org/docs/current/pgvector-embeddings.html), [timestamp de cria√ß√£o](https://www.postgresql.org/docs/current/functions-datetime.html) e um [contador de acessos](https://www.postgresql.org/docs/current/functions-math.html). Esta estrutura n√£o apenas evita o rec√°lculo de embeddings para consultas repetidas, mas tamb√©m fornece dados valiosos sobre padr√µes de uso atrav√©s do campo `hit_count`.

A fun√ß√£o `get_cached_embedding` encapsula a l√≥gica de cache com uma interface limpa: quando uma consulta √© recebida, ela primeiro verifica se o embedding j√° existe no cache. Se encontrado, incrementa o contador de acessos e retorna imediatamente o embedding armazenado, economizando o custo computacional da gera√ß√£o de embeddings. Caso contr√°rio, gera um novo embedding usando o modelo 'nomic-embed-text' via [Ollama](https://ollama.com/), armazena-o no cache para uso futuro e o retorna.

Esta implementa√ß√£o reduz significativamente a lat√™ncia para consultas repetidas, diminui a carga nos servi√ßos de embedding, e proporciona uma base para an√°lises de desempenho e otimiza√ß√£o cont√≠nua. A abordagem √© particularmente eficaz em cen√°rios onde os usu√°rios tendem a fazer perguntas semelhantes ou quando o sistema processa grandes volumes de consultas, resultando em economia de recursos computacionais e melhoria na experi√™ncia do usu√°rio com respostas mais r√°pidas.

#### Estrat√©gias Avan√ßadas de Cache para RAG

Para sistemas RAG em produ√ß√£o, podemos implementar estrat√©gias de cache mais sofisticadas:

1. [**Cache em M√∫ltiplas Camadas**](https://en.wikipedia.org/wiki/Multilevel_cache):
   - L1: Cache em mem√≥ria para consultas muito frequentes
   - L2: Cache em banco de dados para persist√™ncia entre reinicializa√ß√µes
   - L3: Cache distribu√≠do (como [Redis](https://redis.io/)) para sistemas escal√°veis

2. [**Pol√≠ticas de Expira√ß√£o Inteligentes**](https://en.wikipedia.org/wiki/Time_to_live):
   - TTL (Time-to-Live) baseado na frequ√™ncia de uso
   - Invalida√ß√£o seletiva quando documentos relacionados s√£o atualizados
   - Cache sem√¢ntico que agrupa consultas similares

3. **Pr√©-Computa√ß√£o e Cache Preditivo**:
   - Analisar padr√µes de consulta para pr√©-computar respostas prov√°veis
   - Gerar embeddings para varia√ß√µes comuns de consultas

```clojure
;; Exemplo de implementa√ß√£o de cache com Redis para alta disponibilidade
(defn distributed-cached-rag-query
  "Executa consulta RAG com cache distribu√≠do"
  [query]
  (let [cache-key (str "rag:query:" (digest/md5 query))
        ;; Verificar no Redis
        cached-response (redis/get cache-key)]
    (if cached-response
      ;; Usar resposta em cache
      (do
        (redis/incr (str cache-key ":hits"))
        (json/read-str cached-response))
      ;; Gerar nova resposta
      (let [response (full-rag-process query)
            ;; Serializar e armazenar no Redis com TTL
            _ (redis/setex cache-key 
                          (* 60 60 24) ;; 24 horas
                          (json/write-str response))
            ;; Registrar metadados para an√°lise
            _ (redis/hmset (str cache-key ":meta")
                          {"timestamp" (System/currentTimeMillis)
                           "query_length" (count query)
                           "query_type" (determine-query-type query)})]
        response))))
```

Para implementa√ß√µes detalhadas de estrat√©gias de cache para RAG, consulte:

- [LlamaIndex - Query Engine Caching](https://docs.llamaindex.ai/en/stable/module_guides/querying/query_engine/query_engine_caching)
- [LangChain - Caching para LLM Applications](https://python.langchain.com/docs/modules/model_io/llms/llm_caching)
- [Redis Vector Database for RAG](https://redis.io/docs/stack/search/reference/vectors/)

---

## Monitoramento e M√©tricas: LLMOps na Pr√°tica

Para garantir que nosso sistema RAG continue funcionando bem em produ√ß√£o, precisamos monitorar m√©tricas chave:

```mermaid
flowchart TB
    subgraph "Ciclo de Monitoramento RAG"
    direction TB
    LOG[Logs de Intera√ß√µes] --> METR[C√°lculo de M√©tricas]
    METR --> ANOM[Detec√ß√£o de Anomalias]
    ANOM --> ALER[Alertas e Relat√≥rios]
    ALER --> OPT[Otimiza√ß√£o do Sistema]
    OPT --> LOG
    end
    
    subgraph "M√©tricas RAG"
    direction LR
    METR_OP[M√©tricas Operacionais]
    METR_Q[M√©tricas de Qualidade]
    METR_F[M√©tricas de Feedback]
    end
    
    METR --- METR_OP
    METR --- METR_Q
    METR --- METR_F
    
    style LOG fill:#f9f,stroke:#333,stroke-width:2px
    style METR fill:#ffc,stroke:#333,stroke-width:2px
    style ANOM fill:#f99,stroke:#333,stroke-width:2px
    style OPT fill:#9f9,stroke:#333,stroke-width:2px
```

O diagrama acima ilustra o ciclo completo de [monitoramento](https://en.wikipedia.org/wiki/Monitoring) para [sistemas RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) em produ√ß√£o. No centro do processo est√£o os "[Logs de Intera√ß√µes](https://en.wikipedia.org/wiki/Log_file)", que capturam dados detalhados sobre cada [consulta](https://en.wikipedia.org/wiki/Query) processada pelo sistema, incluindo a pergunta original, os [documentos recuperados](https://en.wikipedia.org/wiki/Information_retrieval), a [resposta gerada](https://en.wikipedia.org/wiki/Natural_language_generation) e [m√©tricas de desempenho](https://en.wikipedia.org/wiki/Performance_metric). 

Estes logs alimentam o "[C√°lculo de M√©tricas](https://en.wikipedia.org/wiki/Metric_(mathematics))", que transforma [dados brutos](https://en.wikipedia.org/wiki/Raw_data) em [indicadores acion√°veis](https://en.wikipedia.org/wiki/Key_performance_indicator) distribu√≠dos em tr√™s categorias principais: [operacionais](https://en.wikipedia.org/wiki/Operational_efficiency) ([lat√™ncia](https://en.wikipedia.org/wiki/Latency_(engineering)), [throughput](https://en.wikipedia.org/wiki/Throughput)), [qualidade](https://en.wikipedia.org/wiki/Data_quality) ([precis√£o](https://en.wikipedia.org/wiki/Precision_and_recall), [relev√¢ncia](https://en.wikipedia.org/wiki/Relevance_(information_retrieval))) e [feedback](https://en.wikipedia.org/wiki/Feedback) (avalia√ß√µes dos usu√°rios). A "[Detec√ß√£o de Anomalias](https://en.wikipedia.org/wiki/Anomaly_detection)" monitora continuamente estas m√©tricas para identificar desvios significativos dos padr√µes esperados, gerando "[Alertas e Relat√≥rios](https://en.wikipedia.org/wiki/Alert_management)" que orientam a "[Otimiza√ß√£o do Sistema](https://en.wikipedia.org/wiki/System_optimization)", fechando assim o ciclo de [melhoria cont√≠nua](https://en.wikipedia.org/wiki/Continuous_improvement).

Este fluxo de trabalho representa a ess√™ncia do [LLMOps](https://en.wikipedia.org/wiki/MLOps) aplicado a sistemas RAG, onde o monitoramento n√£o √© apenas [reativo](https://en.wikipedia.org/wiki/Reactive_programming), mas [proativo](https://en.wikipedia.org/wiki/Proactive) na identifica√ß√£o de oportunidades de melhoria. A estrutura tripartite das m√©tricas garante uma [vis√£o hol√≠stica](https://en.wikipedia.org/wiki/Holism) do desempenho: enquanto as m√©tricas operacionais asseguram a [efici√™ncia t√©cnica](https://en.wikipedia.org/wiki/Technical_efficiency) do sistema, as m√©tricas de qualidade avaliam a [precis√£o sem√¢ntica](https://en.wikipedia.org/wiki/Semantic_similarity) das respostas, e as m√©tricas de feedback incorporam a [perspectiva humana](https://en.wikipedia.org/wiki/Human-centered_design) na avalia√ß√£o. 

Esta abordagem [integrada](https://en.wikipedia.org/wiki/System_integration) permite que [equipes de engenharia](https://en.wikipedia.org/wiki/Engineering_team) identifiquem rapidamente [gargalos](https://en.wikipedia.org/wiki/Bottleneck_(software)), ajustem [par√¢metros de recupera√ß√£o](https://en.wikipedia.org/wiki/Information_retrieval) e melhorem continuamente a [experi√™ncia do usu√°rio](https://en.wikipedia.org/wiki/User_experience) final, mesmo √† medida que o [volume de dados](https://en.wikipedia.org/wiki/Big_data) e a [complexidade das consultas](https://en.wikipedia.org/wiki/Query_complexity) aumentam. O c√≥digo abaixo mostra como implementar o log e a avalia√ß√£o de respostas em [Clojure](https://en.wikipedia.org/wiki/Clojure):

```clojure
;; Estrutura para log e avalia√ß√£o de respostas
(defn log-rag-interaction
  "Registra uma intera√ß√£o RAG para an√°lise posterior"
  [query retrieved-docs response latency]
  (jdbc/execute! db-spec
                ["INSERT INTO rag_logs 
                 (query, retrieved_docs, response, latency_ms, timestamp)
                 VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP)"
                 query
                 (json/write-str retrieved-docs)
                 response
                 latency]))

;; Fun√ß√£o para calcular m√©tricas de desempenho
(defn calculate-rag-metrics
  "Calcula m√©tricas de desempenho para um per√≠odo"
  [start-date end-date]
  (let [logs (jdbc/execute! db-spec
                           ["SELECT * FROM rag_logs 
                            WHERE timestamp BETWEEN ? AND ?"
                            start-date end-date])
        ;; M√©tricas de lat√™ncia
        avg-latency (average-latency logs)
        p95-latency (percentile-latency logs 95)
        ;; Taxa de falhas (quando resposta cont√©m erros espec√≠ficos)
        failure-rate (failure-rate logs)
        ;; Distribui√ß√£o de consultas por t√≥pico
        topic-distribution (topic-distribution logs)]
    {:avg_latency avg-latency
     :p95_latency p95-latency
     :failure_rate failure-rate
     :topic_distribution topic-distribution}))
```

A fun√ß√£o `log-rag-interaction` captura cada aspecto da intera√ß√£o desde a consulta original at√© os documentos recuperados, a resposta gerada e o tempo de lat√™ncia armazenando-os em um banco de dados relacional para an√°lise posterior. Esta abordagem permite rastrear o hist√≥rico completo de intera√ß√µes, criando um registro valioso para depura√ß√£o, otimiza√ß√£o e avalia√ß√£o de desempenho ao longo do tempo.

A fun√ß√£o `calculate-rag-metrics` complementa o sistema de logging ao transformar os dados brutos em m√©tricas acion√°veis, calculando indicadores cr√≠ticos como lat√™ncia m√©dia, percentil 95 de lat√™ncia (importante para entender outliers), taxa de falhas e distribui√ß√£o de consultas por t√≥pico. 

Esta an√°lise multidimensional permite que as equipes identifiquem n√£o apenas problemas t√©cnicos (como gargalos de desempenho), mas tamb√©m padr√µes de uso e √°reas tem√°ticas que podem requerer otimiza√ß√£o espec√≠fica. A combina√ß√£o destas duas fun√ß√µes estabelece um ciclo de feedback cont√≠nuo que √© essencial para [sistemas RAG em produ√ß√£o](https://en.wikipedia.org/wiki/Retrieval-Augmented_Generation), permitindo melhorias iterativas baseadas em dados reais de uso.

### M√©tricas de Qualidade Espec√≠ficas para RAG

Al√©m das m√©tricas operacionais comuns (lat√™ncia, disponibilidade), sistemas RAG requerem m√©tricas espec√≠ficas para avaliar a qualidade das respostas:

#### 1. M√©tricas de Relev√¢ncia do Contexto

- **Precision@K**: Propor√ß√£o de chunks recuperados que s√£o realmente relevantes para a consulta
- **Recall@K**: Propor√ß√£o de chunks relevantes na base de conhecimento que foram recuperados
- **NDCG (Normalized Discounted Cumulative Gain)**: Avalia se os chunks mais relevantes est√£o no topo da lista

Abaixo, um exemplo de implementa√ß√£o de m√©tricas de relev√¢ncia do contexto em Clojure:

```clojure
(defn calculate-precision-at-k
  "Calcula Precision@K para uma consulta e seus chunks recuperados"
  [query chunks k expert-judgments]
  (let [retrieved-top-k (take k chunks)
        relevant-count (count (filter #(is-chunk-relevant? % query expert-judgments) 
                                     retrieved-top-k))]
    (double (/ relevant-count (min k (count retrieved-top-k))))))

(defn calculate-recall-at-k
  "Calcula Recall@K para uma consulta"
  [query chunks k all-relevant-chunks expert-judgments]
  (let [retrieved-top-k (take k chunks)
        retrieved-relevant (filter #(is-chunk-relevant? % query expert-judgments) 
                                  retrieved-top-k)
        total-relevant-count (count all-relevant-chunks)]
    (if (pos? total-relevant-count)
      (double (/ (count retrieved-relevant) total-relevant-count))
      1.0))) ;; Se n√£o h√° chunks relevantes, recall √© 1
```

A fun√ß√£o `calculate-precision-at-k` mede a propor√ß√£o de chunks relevantes entre os `k` primeiros resultados recuperados, comparando-os com julgamentos de especialistas. J√° a fun√ß√£o `calculate-recall-at-k` avalia a propor√ß√£o de chunks relevantes que foram efetivamente recuperados em rela√ß√£o ao total de chunks relevantes dispon√≠veis. 

Ambas as m√©tricas s√£o fundamentais para entender a efic√°cia do sistema de recupera√ß√£o: `precision` indica qu√£o precisa √© a recupera√ß√£o (minimizando falsos positivos), enquanto `recall` mostra qu√£o completa √© a recupera√ß√£o (minimizando falsos negativos). A implementa√ß√£o inclui tratamento para casos especiais, como quando n√£o h√° chunks relevantes dispon√≠veis, garantindo resultados matematicamente consistentes. 

#### 2. M√©tricas de Qualidade da Resposta

Para avaliar a qualidade das respostas geradas por sistemas RAG, √© essencial implementar m√©tricas espec√≠ficas que capturem diferentes dimens√µes de efic√°cia. Estas m√©tricas v√£o al√©m de simples avalia√ß√µes bin√°rias (correto/incorreto) e permitem uma an√°lise nuan√ßada da performance do sistema. Implementamos as seguintes m√©tricas qualitativas em nosso framework de avalia√ß√£o:

- **Faithfulness (Fidelidade)**: O grau em que a resposta √© suportada pelo contexto fornecido, sem alucina√ß√µes
- **Answer Relevancy (Relev√¢ncia da Resposta)**: Qu√£o bem a resposta aborda a consulta do usu√°rio
- **Contextual Precision (Precis√£o Contextual)**: Propor√ß√£o do contexto utilizado que foi relevante para a resposta
- **Helpfulness (Utilidade)**: Avalia√ß√£o subjetiva de qu√£o √∫til foi a resposta para o usu√°rio

Abaixo, um exemplo de implementa√ß√£o de m√©tricas de qualidade da resposta em Clojure:

```clojure
(defn evaluate-response-quality
  "Avalia m√©tricas qualitativas de uma resposta RAG"
  [query context response]
  (let [;; Usar LLM como avaliador
        prompt-faithfulness (str "Avalie a fidelidade da seguinte resposta ao contexto fornecido.\n\n"
                                "Consulta: " query "\n\n"
                                "Contexto: " context "\n\n"
                                "Resposta: " response "\n\n"
                                "A resposta cont√©m informa√ß√µes que n√£o est√£o no contexto? "
                                "A resposta contradiz o contexto em algum ponto? "
                                "Atribua uma pontua√ß√£o de 1 a 10, onde 10 significa perfeita fidelidade ao contexto.")
        
        prompt-relevancy (str "Avalie qu√£o relevante √© a resposta para a consulta.\n\n"
                             "Consulta: " query "\n\n"
                             "Resposta: " response "\n\n"
                             "A resposta aborda diretamente a consulta? "
                             "Alguma parte importante da consulta foi ignorada? "
                             "Atribua uma pontua√ß√£o de 1 a 10, onde 10 significa perfeitamente relevante.")
        
        ;; Chamar LLM para avalia√ß√£o
        faithfulness-result (parse-score (call-evaluation-llm prompt-faithfulness))
        relevancy-result (parse-score (call-evaluation-llm prompt-relevancy))]
    
    ;; Retornar resultados agregados
    {:faithfulness faithfulness-result
     :relevancy relevancy-result
     :composite_score (/ (+ faithfulness-result relevancy-result) 2.0)}))
```

A fun√ß√£o recebe tr√™s par√¢metros principais: a consulta original do usu√°rio (`query`), o contexto recuperado pelo sistema (`context`) e a resposta gerada pelo modelo (`response`). Utilizando esses inputs, a fun√ß√£o constr√≥i dois prompts espec√≠ficos para avaliar diferentes dimens√µes da qualidade da resposta.

O primeiro prompt avalia a "fidelidade" [(faithfulness)](https://en.wikipedia.org/wiki/Faithfulness_(literary_theory)) da resposta, verificando se ela se mant√©m fiel ao contexto fornecido sem adicionar informa√ß√µes n√£o presentes ou contradizer o material de refer√™ncia. O segundo prompt avalia a "relev√¢ncia" [(relevancy)](https://en.wikipedia.org/wiki/Relevance_(information_retrieval)), analisando se a resposta aborda diretamente a consulta do usu√°rio e se cobre todos os aspectos importantes da pergunta. Ambos os prompts s√£o enviados para um [LLM avaliador atrav√©s da fun√ß√£o `call-evaluation-llm`](https://github.com/langchain-ai/langchain/blob/main/libs/langchain-core/langchain_core/prompts/prompt.py), que retorna uma avalia√ß√£o textual que √© ent√£o convertida em uma pontua√ß√£o num√©rica pela fun√ß√£o `parse-score`.

Por fim, a fun√ß√£o agrega os resultados em um mapa contendo as pontua√ß√µes individuais de fidelidade e relev√¢ncia, al√©m de calcular uma pontua√ß√£o composta que √© a m√©dia das duas m√©tricas. Esta abordagem de "LLM como avaliador" representa uma t√©cnica avan√ßada no campo de RAG, permitindo avalia√ß√µes automatizadas que capturam nuances qualitativas dif√≠ceis de medir com m√©tricas puramente estat√≠sticas. 

> O c√≥digo demonstra como implementar um sistema de avalia√ß√£o que pode ser usado para monitoramento cont√≠nuo da qualidade das respostas e identifica√ß√£o de √°reas para melhoria.

#### 3. M√©tricas de Consenso entre Modelos

Uma t√©cnica eficaz √© comparar respostas de m√∫ltiplos modelos ou configura√ß√µes:

- [**Model Agreement (Concord√¢ncia de Modelos)**](https://en.wikipedia.org/wiki/Model_agreement): Grau de concord√¢ncia entre diferentes LLMs para a mesma consulta/contexto
- [**Embedding Stability (Estabilidade de Embeddings)**](https://en.wikipedia.org/wiki/Embedding): Consist√™ncia de embeddings entre atualiza√ß√µes de modelos
- [**Context Utilization Variance (Vari√¢ncia de Utiliza√ß√£o de Contexto)**](https://en.wikipedia.org/wiki/Context_utilization_variance): Diferen√ßas na forma como os modelos utilizam o contexto

Abaixo, um exemplo de implementa√ß√£o de m√©tricas de consenso entre modelos em Clojure:

```clojure
(defn measure-model-agreement
  "Mede concord√¢ncia entre diferentes modelos para mesma consulta"
  [query context models]
  (let [;; Gerar respostas de cada modelo
        responses (map #(generate-response-with-model % query context) models)
        
        ;; Calcular similaridade sem√¢ntica entre cada par de respostas
        similarities (for [i (range (count responses))
                          j (range (inc i) (count responses))]
                      (calculate-semantic-similarity 
                        (nth responses i) 
                        (nth responses j)))
        
        ;; M√©dia das similaridades como medida de concord√¢ncia
        avg-similarity (if (seq similarities)
                         (/ (reduce + similarities) (count similarities))
                         1.0)]
    avg-similarity))
```

Esta fun√ß√£o implementa uma m√©trica de concord√¢ncia entre modelos, uma t√©cnica valiosa para avaliar a robustez de sistemas RAG. Ao gerar respostas para a mesma consulta usando diferentes modelos, a fun√ß√£o calcula a similaridade sem√¢ntica entre cada par de respostas. Uma alta concord√¢ncia (similaridade) entre modelos diversos sugere que a resposta √© mais confi√°vel, enquanto baixa concord√¢ncia pode indicar ambiguidade nos dados ou quest√µes com a recupera√ß√£o de contexto.

A implementa√ß√£o utiliza uma abordagem de compara√ß√£o par a par, onde cada resposta √© comparada com todas as outras. A fun√ß√£o `calculate-semantic-similarity` (n√£o mostrada) provavelmente utiliza embeddings para medir qu√£o semanticamente pr√≥ximas est√£o duas respostas. O resultado final √© uma pontua√ß√£o m√©dia de similaridade que quantifica o n√≠vel geral de consenso entre os modelos. Esta m√©trica √© particularmente √∫til para identificar consultas problem√°ticas onde diferentes modelos divergem significativamente, sinalizando potenciais √°reas para melhoria no pipeline RAG.

### Automa√ß√£o da Avalia√ß√£o com LLMs como Ju√≠zes

```mermaid
flowchart TD
    Q[Consulta do Usu√°rio] --> RAG[Sistema RAG]
    CTX[Contexto Recuperado] --> RAG
    
    RAG --> RESP[Resposta Gerada]
    
    subgraph "Avalia√ß√£o Automatizada"
        RESP --> JUDGE[LLM Avaliador]
        Q --> JUDGE
        CTX --> JUDGE
        CRIT[Crit√©rios de Avalia√ß√£o] --> JUDGE
        
        JUDGE --> EVAL[Avalia√ß√£o Estruturada]
        EVAL --> DB[(Banco de Dados)]
        
        EVAL --> METRICS[M√©tricas de Qualidade]
        METRICS --> DASH[Dashboard]
        
        EVAL --> INSIGHT[Insights para Melhoria]
        INSIGHT --> REFINE[Refinamento do Sistema]
        REFINE -.-> RAG
    end
    
    style Q fill:#f9f,stroke:#333,stroke-width:2px
    style RESP fill:#9cf,stroke:#333,stroke-width:2px
    style JUDGE fill:#fc9,stroke:#333,stroke-width:2px
    style EVAL fill:#9f9,stroke:#333,stroke-width:2px
    style REFINE fill:#f99,stroke:#333,stroke-width:2px
```

O diagrama acima representando o fluxo desde a consulta do usu√°rio at√© o refinamento cont√≠nuo do sistema. No centro do processo est√° o "LLM Avaliador" (JUDGE), que recebe tr√™s entradas cruciais: a consulta original do usu√°rio, o contexto recuperado e a resposta gerada pelo sistema RAG. Adicionalmente, o avaliador utiliza crit√©rios de avalia√ß√£o predefinidos para realizar uma an√°lise estruturada e imparcial.

O aspecto mais valioso deste fluxo √© o ciclo de feedback que ele estabelece: a avalia√ß√£o estruturada n√£o apenas alimenta um banco de dados para registro hist√≥rico e gera m√©tricas de qualidade para visualiza√ß√£o em dashboards, mas tamb√©m produz insights acion√°veis que direcionam o refinamento do sistema. Esta abordagem c√≠clica permite que o sistema RAG evolua continuamente, aprendendo com suas pr√≥prias limita√ß√µes e melhorando progressivamente a qualidade das respostas, sem necessidade de interven√ß√£o humana constante em cada etapa do processo de avalia√ß√£o. Uma abordagem emergente √© usar LLMs como "ju√≠zes" para avaliar automaticamente a qualidade das respostas:

```clojure
(defn llm-judge-evaluation
  "Utiliza LLM como juiz para avaliar respostas RAG"
  [query context response evaluation-criteria]
  (let [;; Construir prompt para avalia√ß√£o
        evaluation-prompt (str "Voc√™ √© um avaliador especializado em sistemas RAG. "
                              "Analise a seguinte intera√ß√£o e avalie de acordo com os crit√©rios especificados.\n\n"
                              "Consulta do usu√°rio: " query "\n\n"
                              "Contexto recuperado: " context "\n\n"
                              "Resposta gerada: " response "\n\n"
                              "Crit√©rios de avalia√ß√£o:\n"
                              evaluation-criteria "\n\n"
                              "Para cada crit√©rio, forne√ßa:\n"
                              "1. Uma pontua√ß√£o de 1-10\n"
                              "2. Justificativa para a pontua√ß√£o\n"
                              "3. Sugest√µes espec√≠ficas para melhoria\n"
                              "Formate sua resposta como JSON.")
        
        ;; Chamar LLM avaliador (preferivelmente um modelo diferente do usado para gerar a resposta para evitar vi√©s de auto-avalia√ß√£o)](https://en.wikipedia.org/wiki/Self-assessment)
        judge-response (call-evaluation-llm evaluation-prompt)
        
        ;; Parsear resposta estruturada
        evaluation-results (json/read-str judge-response)]
    
    ;; Registrar avalia√ß√£o no banco de dados
    (log-evaluation query context response evaluation-results)
    
    ;; Retornar resultados estruturados
    evaluation-results))
```

A implementa√ß√£o segue um padr√£o elegante e pr√°tico: primeiro constr√≥i um prompt detalhado que enquadra a tarefa de avalia√ß√£o, depois chama um modelo [LLM dedicado (preferencialmente diferente do usado na gera√ß√£o da resposta para evitar vi√©s de auto-avalia√ß√£o)](https://en.wikipedia.org/wiki/Self-assessment), processa a resposta estruturada e finalmente registra os resultados para an√°lise posterior. Esta abordagem permite avalia√ß√£o cont√≠nua e escal√°vel da qualidade do sistema RAG, fornecendo insights acion√°veis para refinamento do pipeline sem necessidade de interven√ß√£o humana constante. 

A fun√ß√£o representa uma evolu√ß√£o importante nas pr√°ticas de avalia√ß√£o de RAG, combinando a capacidade de compreens√£o contextual dos LLMs com a necessidade de feedback estruturado e quantific√°vel.

#### Configura√ß√£o de um Dashboard de Qualidade RAG

Para monitoramento cont√≠nuo, √© essencial configurar um dashboard que acompanhe a evolu√ß√£o das m√©tricas ao longo do tempo:

```clojure
(defn generate-rag-quality-report
  "Gera relat√≥rio di√°rio de qualidade do sistema RAG"
  []
  (let [;; Per√≠odo de avalia√ß√£o (√∫ltimo dia)
        end-date (java.util.Date.)
        start-date (-> (java.util.Calendar/getInstance)
                       (doto (.setTime end-date)
                             (.add java.util.Calendar/DAY_OF_MONTH -1))
                       (.getTime))
        
        ;; Recuperar logs do per√≠odo
        logs (jdbc/execute! db-spec
                           ["SELECT * FROM rag_logs 
                             WHERE timestamp BETWEEN ? AND ?"
                            start-date end-date])
        
        ;; Calcular m√©tricas operacionais
        operational-metrics (calculate-operational-metrics logs)
        
        ;; Selecionar amostra aleat√≥ria para avalia√ß√£o qualitativa
        evaluation-sample (take 50 (shuffle logs))
        
        ;; Avaliar qualidade das respostas na amostra
        quality-metrics (evaluate-sample-quality evaluation-sample)
        
        ;; Identificar tend√™ncias e anomalias
        trends (detect-quality-trends quality-metrics)
        anomalies (detect-quality-anomalies quality-metrics)
        
        ;; Compilar relat√≥rio
        report {:date (format-date end-date)
                :sample_size (count evaluation-sample)
                :operational_metrics operational-metrics
                :quality_metrics quality-metrics
                :trends trends
                :anomalies anomalies
                :recommendations (generate-recommendations trends anomalies)}]
    
    ;; Salvar relat√≥rio e enviar notifica√ß√µes se houver anomalias
    (save-quality-report report)
    (when (not-empty anomalies)
      (send-quality-alert report))
    
    report))
```

O c√≥digo acima implementa uma fun√ß√£o Clojure chamada `generate-rag-quality-report` que automatiza a gera√ß√£o de relat√≥rios di√°rios de qualidade para um sistema RAG. A fun√ß√£o come√ßa definindo um per√≠odo de avalia√ß√£o (√∫ltimo dia), recupera logs de intera√ß√µes RAG desse per√≠odo do banco de dados, e calcula m√©tricas operacionais b√°sicas. Em seguida, seleciona uma amostra aleat√≥ria de 50 intera√ß√µes para uma avalia√ß√£o qualitativa mais profunda.

O n√∫cleo da fun√ß√£o est√° na avalia√ß√£o da qualidade das respostas na amostra selecionada, seguida pela identifica√ß√£o de tend√™ncias e anomalias nos dados de qualidade. Isso permite que o sistema n√£o apenas me√ßa o desempenho atual, mas tamb√©m detecte padr√µes emergentes ou problemas que possam exigir aten√ß√£o. O relat√≥rio final √© estruturado como um [mapa Clojure](https://clojure.org/reference/data_structures) contendo a data, tamanho da amostra, m√©tricas operacionais, m√©tricas de qualidade, tend√™ncias identificadas, anomalias detectadas e recomenda√ß√µes geradas automaticamente.

Um aspecto importante da fun√ß√£o √© seu mecanismo de alerta: ap√≥s salvar o relat√≥rio no sistema, ela verifica se foram detectadas anomalias e, em caso positivo, envia alertas para os respons√°veis. Esta abordagem proativa para monitoramento de qualidade permite que equipes de engenharia e produto intervenham rapidamente quando o desempenho do sistema RAG come√ßa a degradar, antes que os usu√°rios sejam significativamente afetados. O c√≥digo exemplifica uma implementa√ß√£o pr√°tica de [LLMOps](https://en.wikipedia.org/wiki/LLMOps), focando na avalia√ß√£o cont√≠nua e sistem√°tica da qualidade das respostas em um sistema RAG.

### Integra√ß√£o com Sistemas de Feedback do Usu√°rio

O feedback direto dos usu√°rios √© uma fonte valiosa para avaliar a qualidade das respostas:

```clojure
(defn process-user-feedback
  "Processa feedback expl√≠cito do usu√°rio"
  [query-id response-id feedback-type feedback-text]
  (let [;; Registrar feedback no banco de dados
        _ (jdbc/execute! db-spec
                        ["INSERT INTO user_feedback 
                          (query_id, response_id, feedback_type, feedback_text, timestamp) 
                          VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP)"
                         query-id response-id feedback-type feedback-text])
        
        ;; Recuperar detalhes da intera√ß√£o
        interaction (jdbc/execute-one! db-spec
                                     ["SELECT query, retrieved_docs, response 
                                       FROM rag_logs WHERE id = ?"
                                      query-id])
        
        ;; Analisar feedback para extrair insights
        feedback-analysis (analyze-user-feedback feedback-type 
                                               feedback-text 
                                               (:query interaction)
                                               (:response interaction))]
    
    ;; Atualizar m√©tricas agregadas
    (update-feedback-metrics feedback-type)
    
    ;; Para feedback negativo, adicionar √† fila de revis√£o manual
    (when (= feedback-type "negative")
      (add-to-manual-review-queue query-id feedback-analysis))
    
    feedback-analysis))
```
A fun√ß√£o registra o feedback no banco de dados, recupera os detalhes da intera√ß√£o original, analisa o feedback para extrair insights valiosos e atualiza m√©tricas agregadas. Um aspecto importante √© o tratamento especial para [feedback negativo](https://en.wikipedia.org/wiki/Negative_feedback), que √© automaticamente adicionado a uma fila de revis√£o manual, permitindo que a equipe investigue e corrija problemas espec√≠ficos.

Esta implementa√ß√£o representa um componente crucial de um sistema LLMOps maduro, pois estabelece um ciclo de feedback cont√≠nuo entre usu√°rios e desenvolvedores. Ao capturar sistematicamente as avalia√ß√µes dos usu√°rios e vincul√°-las √†s consultas e respostas espec√≠ficas, a fun√ß√£o permite an√°lises detalhadas sobre o desempenho do sistema, identifica√ß√£o de padr√µes de falha e oportunidades de melhoria. 

---

## Implementando no DocAI

Agora que exploramos v√°rias t√©cnicas avan√ßadas, vamos ver como elas s√£o implementadas no projeto DocAI. Nosso sistema atual j√° incorpora muitas dessas t√©cnicas para criar um pipeline RAG avan√ßado.

> Caso n√£o saiba o que √© o DocAI, voc√™ pode ver os artigos anteriores [RAG Simples com Clojure e Ollama](https://scovl.github.io/2025/03/23/rag/) e [Busca Sem√¢ntica com Ollama e PostgreSQL](https://scovl.github.io/2025/03/25/semantic-postgresql/).

### Arquitetura Atual do DocAI

A arquitetura do DocAI implementa um sistema RAG completo com suporte a agentes para consultas complexas. Os principais componentes s√£o:

1. [**Core (core.clj)**](https://github.com/docai-ai/docai/blob/main/src/core.clj): Coordena√ß√£o central do sistema, implementando a interface CLI e gerenciando o fluxo de dados entre componentes.

2. [**LLM (llm.clj)**](https://github.com/docai-ai/docai/blob/main/src/llm.clj): Interface com o Ollama para gera√ß√£o de texto e embeddings, abstraindo detalhes de comunica√ß√£o com a API.

3. [**PostgreSQL (pg.clj)**](https://github.com/docai-ai/docai/blob/main/src/pg.clj): Implementa√ß√£o da busca sem√¢ntica com pgvector, incluindo configura√ß√£o e consultas otimizadas.

4. [**Processamento de Documentos (document.clj)**](https://github.com/docai-ai/docai/blob/main/src/document.clj): Respons√°vel pela extra√ß√£o, limpeza e prepara√ß√£o de texto de diferentes formatos.

5. [**Advanced RAG (advanced_rag.clj)**](https://github.com/docai-ai/docai/blob/main/src/advanced_rag.clj): 
   - Cache em m√∫ltiplos n√≠veis (embeddings e respostas)
   - Chunking din√¢mico adaptado ao tipo de documento
   - Re-ranqueamento de resultados para melhorar precis√£o

6. [**Sistema de Agentes (agents.clj)**](https://github.com/docai-ai/docai/blob/main/src/agents.clj):
   - An√°lise de complexidade de consultas
   - Decomposi√ß√£o em sub-tarefas
   - Agentes especializados (busca, racioc√≠nio, c√°lculo)
   - Verifica√ß√£o de qualidade das respostas
   - S√≠ntese de resultados parciais

7. [**M√©tricas (metrics.clj)**](https://github.com/docai-ai/docai/blob/main/src/metrics.clj): Monitoramento de desempenho e qualidade das respostas.

O fluxo de processamento de consultas inicia em `core.clj`, que identifica se a consulta requer um pipeline RAG simples ou avan√ßado com agentes:

```clojure
(defn query-advanced-rag
  "Processa uma consulta usando o pipeline RAG avan√ßado"
  [query]
  (println "DEBUG - Processando query com RAG avan√ßado:" query)
  (let [start-time (System/currentTimeMillis)
        ;; Verificar se a consulta precisa do workflow com agentes
        need-agents (agents/needs-agent-workflow? query)
        _ (when need-agents
            (println "DEBUG - Consulta identificada como complexa, usando workflow com agentes"))
        
        ;; Escolher o processamento adequado
        response (if need-agents
                   (agents/process-with-agents query)
                   (adv-rag/advanced-rag-query query))
        
        end-time (System/currentTimeMillis)
        latency (- end-time start-time)]
    
    ;; Registrar m√©tricas
    (metrics/log-rag-interaction query [] response latency)
    
    response))
```

Para consultas simples, o pipeline `advanced-rag-query` realiza:
1. Verifica√ß√£o de cache
2. An√°lise de complexidade da consulta
3. Busca sem√¢ntica com chunking din√¢mico
4. Formata√ß√£o de prompt contextualizado
5. Gera√ß√£o de resposta com o LLM

Para consultas complexas, o sistema de agentes em `agents.clj` entra em a√ß√£o:

```clojure
(defn execute-agent-workflow
  "Executa o workflow completo de agentes para uma consulta complexa"
  [query]
  (let [;; Verificar cache primeiro
        cached (@agent-cache query)]
    (if cached
      cached
      (let [start-time (System/currentTimeMillis)
            
            ;; Analisar a consulta para determinar inten√ß√£o e sub-quest√µes
            analysis (analyze-query query)
            primary-intent (get-agent-type (:intent analysis))
            subtasks (or (:sub_questions analysis) [query])
            
            ;; Resultados parciais
            results (atom [])
            
            ;; Executar cada subtarefa em sequ√™ncia
            _ (doseq [subtask subtasks]
                (let [agent-result (execute-subtask 
                                     subtask 
                                     primary-intent
                                     @results)]
                  (swap! results conj (:response agent-result))))
            
            ;; Gerar resposta final sintetizada
            synthesis-prompt (str "Com base nas seguintes informa√ß√µes:\n\n"
                                 (str/join "\n\n" @results)
                                 "\n\nResponda √† pergunta original de forma completa e coerente: " query)
            
            initial-response (llm/call-ollama-api synthesis-prompt)
            
            ;; Obter contexto combinado para verifica√ß√£o
            combined-context (str/join "\n\n" @results)
            
            ;; Verificar a qualidade da resposta
            final-response (verify-response query combined-context initial-response)
            
            duration (- (System/currentTimeMillis) start-time)]
        
        ;; Registrar m√©tricas e resultados
        final-response))))
```

O sistema de agentes implementa um workflow sofisticado para consultas complexas:
1. An√°lise da consulta para identificar inten√ß√£o e subtarefas
2. Execu√ß√£o de cada subtarefa com agentes especializados
3. Acumula√ß√£o de resultados parciais
4. S√≠ntese de uma resposta final coerente
5. Verifica√ß√£o da qualidade da resposta
6. Armazenamento em cache para consultas futuras

### Diferenciais do DocAI

O DocAI se destaca por implementar v√°rias t√©cnicas avan√ßadas de RAG em um sistema integrado e modular:

- **Chunking Adaptativo**: Diferentes estrat√©gias de chunking baseadas no tipo de documento:
  ```clojure
  (defn adaptive-chunking-strategy
    "Determina estrat√©gia de chunking com base no tipo de documento"
    [document-type]
    (case document-type
      "article" {:chunk-size 1000 :chunk-overlap 150}
      "code" {:chunk-size 500 :chunk-overlap 50}
      "legal" {:chunk-size 1500 :chunk-overlap 200}
      "qa" {:chunk-size 800 :chunk-overlap 100}
      ;; Default
      {:chunk-size 1000 :chunk-overlap 100}))
  ```

O sistema implementa estrat√©gias de [chunking adaptativas](https://en.wikipedia.org/wiki/Chunking_(data_storage)) que otimizam a segmenta√ß√£o de documentos conforme seu tipo espec√≠fico. Esta abordagem reconhece que diferentes conte√∫dos possuem caracter√≠sticas √∫nicas que afetam como devem ser divididos para processamento:

- **Artigos**: Chunks maiores (1000 tokens) com sobreposi√ß√£o significativa (150 tokens), preservando o fluxo narrativo e argumentativo
- **C√≥digo-fonte**: Chunks menores (500 tokens) com sobreposi√ß√£o reduzida (50 tokens), respeitando a estrutura modular do c√≥digo
- **Documentos legais**: Chunks extensos (1500 tokens) com alta sobreposi√ß√£o (200 tokens), mantendo intactas cl√°usulas e refer√™ncias cruzadas
- **Conte√∫do Q&A**: Chunks de tamanho m√©dio (800 tokens) com sobreposi√ß√£o moderada (100 tokens), preservando pares de perguntas e respostas

Esta estrat√©gia contextual melhora significativamente a qualidade da recupera√ß√£o, garantindo que cada tipo de documento seja processado de forma otimizada para seu formato e densidade informacional espec√≠ficos. A fun√ß√£o `adaptive-chunking-strategy` demonstra uma implementa√ß√£o elegante deste conceito, utilizando pattern matching para selecionar par√¢metros otimizados para cada categoria de documento. 

Documentos legais, por exemplo, recebem chunks maiores (1500 tokens) devido √† sua natureza densa e interconectada, enquanto documentos de perguntas e respostas utilizam uma configura√ß√£o intermedi√°ria (800 tokens). Esta estrat√©gia de chunking contextual melhora significativamente a qualidade da recupera√ß√£o, garantindo que o contexto sem√¢ntico seja preservado de forma apropriada para cada tipo espec√≠fico de conte√∫do.


- **Cache Multin√≠vel**: Implementa√ß√£o de cache para embeddings e respostas, reduzindo lat√™ncia e custos:
  ```clojure
  ;; Cache para embeddings
  (def embedding-cache (atom {}))
  ;; Cache para respostas
  (def response-cache (atom {}))
  ;; Cache para resultados de agentes
  (def agent-cache (atom {}))
  ```

O sistema implementa uma estrat√©gia de [cache multin√≠vel](https://en.wikipedia.org/wiki/Cache_hierarchy) para otimizar o desempenho e reduzir custos operacionais. Utilizando estruturas de dados at√¥micas [(`atom`)](https://en.wikipedia.org/wiki/Atom_(data_structure)), o [DocAI](https://github.com/scovl/docai) mant√©m tr√™s camadas distintas de cache: para embeddings, respostas completas e resultados de agentes. Esta abordagem permite reutilizar c√°lculos computacionalmente intensivos como a gera√ß√£o de embeddings, evitando processamento redundante de textos id√™nticos. 

O cache de respostas armazena resultados finais para consultas frequentes, enquanto o cache de agentes preserva resultados intermedi√°rios de subtarefas espec√≠ficas. Esta implementa√ß√£o reduz significativamente a lat√™ncia do sistema, especialmente para consultas recorrentes, e diminui custos associados a chamadas de API para modelos externos. A estrutura at√¥mica escolhida garante [thread-safety](https://en.wikipedia.org/wiki/Thread_safety) em ambientes concorrentes, permitindo atualiza√ß√µes seguras do cache mesmo com m√∫ltiplas consultas simult√¢neas.

- **Verifica√ß√£o de Respostas**: Sistema que avalia e melhora automaticamente as respostas:
  ```clojure
  (defn verify-response
    "Usa um agente cr√≠tico para verificar e melhorar uma resposta"
    [query context response]
    (let [prompt (str "Avalie criticamente a seguinte resposta para a consulta do usu√°rio. 
                      Verifique se a resposta √©:\n"
                      "1. Fiel ao contexto fornecido\n"
                      "2. Completa (responde todos os aspectos da pergunta)\n"
                      "3. Precisa (n√£o cont√©m informa√ß√µes incorretas)\n\n"
                      "Consulta: " query "\n\n"
                      "Contexto: " (if (> (count context) 300) 
                                    (str (subs context 0 300) "...") context) "\n\n"
                      "Resposta: " response "\n\n"
                      "Se a resposta for adequada, apenas responda 'A resposta est√° correta'. "
                      "Caso contr√°rio, forne√ßa uma vers√£o melhorada.")
          verification (llm/call-ollama-api prompt)]
      
      (if (str/includes? verification "A resposta est√° correta")
        response
        (let [improved-version (str/replace verification 
                                           #"(?i).*?\b(a resposta melhorada seria:|vers√£o melhorada:|resposta corrigida:|sugest√£o de resposta:|aqui est√° uma vers√£o melhorada:)\s*" 
                                           "")]
          improved-version))))
  ```

O c√≥digo acima implementa um sistema de verifica√ß√£o e melhoria autom√°tica de respostas, um componente cr√≠tico em sistemas [RAG](https://en.wikipedia.org/wiki/Retrieval-Augmented_Generation) avan√ßados. A fun√ß√£o `verify-response` atua como um "agente cr√≠tico" que avalia a qualidade das respostas geradas com base em tr√™s crit√©rios fundamentais: fidelidade ao contexto fornecido, completude em rela√ß√£o √† pergunta original e precis√£o factual. Este mecanismo de auto-verifica√ß√£o representa uma camada adicional de controle de qualidade que ajuda a mitigar alucina√ß√µes e imprecis√µes comuns em sistemas baseados em LLMs.

A implementa√ß√£o utiliza uma abordagem elegante de [prompt engineering](https://en.wikipedia.org/wiki/Prompt_engineering), onde o sistema solicita explicitamente uma avalia√ß√£o cr√≠tica da resposta original. O prompt estruturado inclui a consulta do usu√°rio, um resumo do contexto (limitado a 300 caracteres para evitar sobrecarga) e a resposta gerada, orientando o modelo a realizar uma an√°lise meticulosa. A fun√ß√£o ent√£o analisa o resultado da verifica√ß√£o, mantendo a resposta original quando considerada adequada ou extraindo uma vers√£o aprimorada quando necess√°rio, utilizando express√µes regulares para limpar metadados desnecess√°rios da resposta melhorada.

Este mecanismo de verifica√ß√£o representa uma implementa√ß√£o pr√°tica do conceito de [Constitutional AI](https://en.wikipedia.org/wiki/Constitutional_AI) ou "AI com princ√≠pios orientadores", onde um sistema √© projetado para avaliar criticamente suas pr√≥prias sa√≠das. Ao incorporar esta camada de verifica√ß√£o no pipeline [RAG](https://en.wikipedia.org/wiki/Retrieval-Augmented_Generation), o [DocAI](https://github.com/scovl/docai) consegue oferecer respostas mais confi√°veis e precisas, reduzindo significativamente o risco de fornecer informa√ß√µes incorretas ou incompletas. Esta abordagem reflexiva √© particularmente valiosa em dom√≠nios onde a precis√£o √© crucial, como documenta√ß√£o t√©cnica, informa√ß√µes m√©dicas ou an√°lises legais.


- **M√©tricas Detalhadas**: Sistema de monitoramento que registra todos os aspectos das intera√ß√µes:
  ```clojure
  (metrics/log-rag-interaction query [] response latency)
  ```

O c√≥digo acima implementa um sistema de monitoramento que registra todos os aspectos das intera√ß√µes, incluindo a consulta do usu√°rio, o tempo de resposta, e a resposta gerada. Este sistema permite acompanhar o desempenho do sistema ao longo do tempo e identificar poss√≠veis problemas ou pontos de melhoria. Isso √© essencial para manter o sistema funcionando de forma eficiente e para continuar evoluindo para novas funcionalidades.

Estas implementa√ß√µes demonstram como as t√©cnicas avan√ßadas de RAG discutidas neste artigo podem ser integradas em um sistema coeso, resultando em um assistente de documenta√ß√£o mais inteligente e eficiente.

### Pr√≥ximos Passos para o DocAI

Conforme detalhado no `plan.md`, o DocAI evoluir√° para um sistema RAG Ag√™ntico mais completo, implementando as seguintes melhorias:

1. **Reescrita de Consultas**
   - M√≥dulo de reformula√ß√£o para melhorar a precis√£o da busca
   - Expans√£o de consultas curtas e foco em consultas abrangentes

2. **Sele√ß√£o Din√¢mica de Fontes**
   - Workflow de agentes aprimorado para decidir quais fontes consultar
   - Integra√ß√£o com APIs externas e pesquisa web

3. **Framework de Ferramentas para Agentes**
   - Sistema de ferramentas para a√ß√µes espec√≠ficas
   - Executores de c√≥digo, calculadoras e formatadores

4. **Interface Multimodal**
   - Processamento de imagens e gera√ß√£o de gr√°ficos
   - Suporte a diversos formatos al√©m de texto

Estas evolu√ß√µes manter√£o a arquitetura modular e extens√≠vel do DocAI, permitindo adapta√ß√£o a diferentes casos de uso e dom√≠nios de conhecimento.

## Integra√ß√£o com o Ecossistema

```mermaid
flowchart TB
    subgraph "Ecossistema DocAI"
        direction TB
        
        DOCAI[Sistema DocAI] --- OLLAMA[Ollama]
        DOCAI --- POSTGRES[PostgreSQL + pgvector]
        
        DOCAI --- API_GATE[API Gateway]
        API_GATE --- WEB_APP[Aplica√ß√£o Web]
        API_GATE --- CLI[Interface CLI]
        
        DOCAI --- MONITORING[Sistema de Monitoramento]
        MONITORING --- DASHBOARD[Dashboard de M√©tricas]
        
        DOCAI -.-> FUTURE_INT[Integra√ß√µes Futuras]
        FUTURE_INT -.-> EXT_API[APIs Externas]
        FUTURE_INT -.-> SEARCH[Motores de Busca]
        FUTURE_INT -.-> TOOLS[Ferramentas de Produtividade]
        
        style DOCAI fill:#f99,stroke:#333,stroke-width:3px
        style OLLAMA fill:#9f9,stroke:#333,stroke-width:2px
        style POSTGRES fill:#99f,stroke:#333,stroke-width:2px
        style FUTURE_INT fill:#ddd,stroke:#333,stroke-width:2px,stroke-dasharray: 5 5
    end
```

O diagrama acima mostra como o DocAI se integra ao ecossistema mais amplo de ferramentas e servi√ßos. No centro est√° o sistema [DocAI](https://github.com/docai-ai/docai), que se conecta diretamente com [Ollama](https://github.com/ollama/ollama) para gera√ß√£o de texto e embeddings, e com [PostgreSQL](https://www.postgresql.org/) (com [pgvector](https://github.com/pgvector/pgvector)) para armazenamento e recupera√ß√£o de dados vetoriais.

Para intera√ß√£o com usu√°rios, o DocAI se conecta a um [API Gateway](https://en.wikipedia.org/wiki/API_gateway) que fornece acesso tanto para uma aplica√ß√£o web quanto para uma interface de linha de comando (CLI). Um sistema dedicado de monitoramento coleta m√©tricas e as exibe em um dashboard para an√°lise de desempenho.

As linhas tracejadas indicam integra√ß√µes futuras planejadas, incluindo [APIs externas](https://en.wikipedia.org/wiki/API) para busca de informa√ß√µes adicionais, [motores de busca](https://en.wikipedia.org/wiki/Search_engine) para ampliar o alcance de recupera√ß√£o, e [ferramentas de produtividade](https://en.wikipedia.org/wiki/Productivity) para aumentar as capacidades do sistema.

Esta arquitetura modular permite que o DocAI se mantenha flex√≠vel e adapt√°vel, podendo ser expandido conforme novos requisitos e oportunidades surgem, sempre mantendo seu n√∫cleo robusto de funcionalidades RAG avan√ßadas.

---

## Conclus√£o

Transformar um sistema [RAG](https://en.wikipedia.org/wiki/Retrieval-Augmented_Generation) de prot√≥tipo para produ√ß√£o requer mais do que apenas escolher as melhores ferramentas - exige uma compreens√£o profunda de cada componente e como eles trabalham juntos para produzir resultados confi√°veis.

O projeto [DocAI](https://github.com/scovl/docai) representa uma implementa√ß√£o robusta das t√©cnicas avan√ßadas de RAG discutidas neste artigo. Sua arquitetura modular, com componentes especializados em diferentes aspectos do processo (como Core, LLM, PostgreSQL, Sistema de Agentes e M√©tricas), demonstra a import√¢ncia de um design bem estruturado para sistemas RAG em produ√ß√£o.

As t√©cnicas que exploramos - desde re-ranqueamento e chunking din√¢mico at√© workflows com agentes e monitoramento avan√ßado - representam as pr√°ticas que separam implementa√ß√µes amadoras de sistemas robustos e prontos para uso em escala.

```mermaid
flowchart LR
    subgraph "Evolu√ß√£o do DocAI"
    direction LR
    BASIC[RAG B√°sico com TF-IDF] --> PGSQL[PostgreSQL + Embeddings] --> ADV[Sistema RAG Avan√ßado] --> AGT[Sistema RAG Ag√™ntico]
    end
    
    style BASIC fill:#ddf,stroke:#333,stroke-width:2px
    style PGSQL fill:#fdf,stroke:#333,stroke-width:2px
    style ADV fill:#dfd,stroke:#333,stroke-width:2px
    style AGT fill:#ffd,stroke:#333,stroke-width:2px
```

Nossa jornada com o [DocAI](https://github.com/scovl/docai) evoluiu significativamente, de uma implementa√ß√£o b√°sica com TF-IDF, passando por um sistema com PostgreSQL e embeddings, e agora para uma arquitetura avan√ßada com agentes que pode lidar com casos de uso complexos do mundo real. O pr√≥ximo passo, conforme detalhado no plano de evolu√ß√£o, ser√° expandir ainda mais essas capacidades para criar um sistema RAG Ag√™ntico completo.

O futuro dos sistemas de IA n√£o est√° em modelos cada vez maiores, mas na combina√ß√£o inteligente de componentes especializados que trabalham juntos para superar limita√ß√µes individuais. O [DocAI](https://github.com/scovl/docai) exemplifica esta abordagem, demonstrando como a integra√ß√£o de t√©cnicas avan√ßadas de RAG pode resultar em um sistema mais inteligente, preciso e √∫til para seus usu√°rios.

---

## Refer√™ncias

- [Artigo anterior: Busca Sem√¢ntica com Ollama e PostgreSQL](/post/semantic-postgresql/) - Nossa implementa√ß√£o b√°sica com PostgreSQL.
- [CLIP - OpenAI](https://openai.com/research/clip) - Modelo para unificar vis√£o e linguagem.
- [Comprehensive Guide to MultiModal RAG](https://towardsdatascience.com/a-comprehensive-guide-to-multimodal-rag-ea72c387c6e8) - Guia detalhado para implementa√ß√£o de RAG multimodal.
- [Cross-Encoders - Hugging Face](https://huggingface.co/cross-encoder) - Modelos para re-ranking em sistemas de recupera√ß√£o.
- [Daily Dose of Data Science: RAG Techniques](https://dailydoseofds.com) - Artigo sobre t√©cnicas para otimizar sistemas RAG.
- [Documenta√ß√£o do pgai](https://github.com/timescale/pgai) - Extens√£o do PostgreSQL para aplica√ß√µes de IA.
- [Documenta√ß√£o do pgvector](https://github.com/pgvector/pgvector) - Extens√£o do PostgreSQL para embeddings vetoriais.
- [Flamingo - DeepMind](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model) - Modelo visual de linguagem para tarefas multimodais.
- [JSONB no PostgreSQL](https://www.postgresql.org/docs/current/datatype-json.html) - Documenta√ß√£o sobre o tipo de dados JSONB.
- [LangChain - Multi-Agent Systems](https://python.langchain.com/docs/modules/agents/agent_types/multi_agent) - Implementa√ß√£o de sistemas multi-agentes.
- [LangChain](https://python.langchain.com/) - Biblioteca para desenvolvimento de aplica√ß√µes baseadas em LLM.
- [LlamaIndex - Implementando ReAct Agents](https://docs.llamaindex.ai/en/stable/examples/agent/react_agent.html) - Guia para implementa√ß√£o de agentes ReAct.
- [LlamaIndex](https://docs.llamaindex.ai/) - Framework para construir aplica√ß√µes alimentadas por LLM.
- [MultiModal RAG com LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/multi_modal/) - Exemplos de implementa√ß√£o multimodal.
- [Ollama - Rodando LLMs localmente](https://ollama.com/) - Ferramenta para executar LLMs localmente.
- [PostgreSQL](https://www.postgresql.org/) - Sistema de gerenciamento de banco de dados relacional.
- [Projeto DocAI](https://github.com/scovl/docai) - Reposit√≥rio do projeto DocAI.