+++
title = "RAG Simples com Clojure e Ollama"
description = "Um prot√≥tipo funcional do zero"
date = 2024-01-23T19:00:00-00:00
tags = ["RAG", "LLM", "AI", "Langchain"]
draft = false
weight = 1
author = "Vitor Lobo Ramos"
+++

# Sum√°rio

- **[Introdu√ß√£o](#introdu√ß√£o)**
- **[Fundamentos do RAG](#fundamentos-do-rag)**
    - [O que √© RAG?](#o-que-√©-rag)
    - [Por que precisamos do RAG?](#por-que-precisamos-do-rag)
    - [Os Tr√™s Pilares do RAG](#os-tr√™s-pilares-do-rag)
    - [Por que o DeepSeek R1?](#por-que-o-deepseek-r1)
- **[Implementa√ß√£o Pr√°tica](#implementa√ß√£o-pr√°tica)**
    - [Preparando o Ambiente](#preparando-o-ambiente)
    - [Estrutura do Projeto](#estrutura-do-projeto)
    - [Processamento de Documentos](#processamento-de-documentos)
    - [Sistema de Embeddings](#sistema-de-embeddings)
    - [Interface com Ollama](#interface-com-ollama)
    - [M√≥dulo Principal](#m√≥dulo-principal)
- **[Como Usar](#como-usar)**
    - [Instala√ß√£o do Ollama](#instala√ß√£o-do-ollama)
    - [Configura√ß√£o do Projeto](#configura√ß√£o-do-projeto)
    - [Executando a Aplica√ß√£o](#executando-a-aplica√ß√£o)
- **[Considera√ß√µes T√©cnicas](#considera√ß√µes-t√©cnicas)**
    - [Performance e Otimiza√ß√µes](#performance-e-otimiza√ß√µes)
    - [Tratamento de Erros](#tratamento-de-erros)
    - [Prompt Engineering](#prompt-engineering)
- **[Pr√≥ximos Passos](#pr√≥ximos-passos)**
    - [Melhorias Propostas](#melhorias-propostas)
    - [Usando Langchain4j](#usando-langchain4j)
- **[Refer√™ncias](#refer√™ncias)**

## Introdu√ß√£o

Ol√°, pessoal! üëã 

Neste artigo, vamos explorar como construir uma aplica√ß√£o [RAG (Retrieval-Augmented Generation)](https://pt.wikipedia.org/wiki/Gera√ß√£o_aumentada_por_recupera√ß√£o) completa do zero usando [Clojure](https://clojure.org/). Vamos mergulhar em uma implementa√ß√£o pr√°tica que combina processamento de texto, busca sem√¢ntica e gera√ß√£o de respostas com LLMs locais. Se voc√™ est√° interessado em melhorar a precis√£o e relev√¢ncia das respostas dos seus modelos de linguagem com informa√ß√µes atualizadas, este guia √© para voc√™!

## Fundamentos do RAG

### O que √© RAG?

Os Modelos de Linguagem de Grande Escala (LLMs), como o GPT, ChatGPT e outros, revolucionaram a forma como interagimos com a intelig√™ncia artificial. Eles s√£o capazes de gerar textos coerentes, responder perguntas complexas e at√© mesmo criar conte√∫do criativo. No entanto, esses modelos possuem uma limita√ß√£o fundamental: seu conhecimento √© "congelado" no tempo.

```mermaid
graph TD
    A[LLM Treinado] --> B[Data de Corte]
    B --> C[Conhecimento Congelado]
    C --> D[Limita√ß√µes]
    D --> E[N√£o sabe eventos recentes]
    D --> F[N√£o tem dados atualizados]
    D --> G[N√£o conhece novas tecnologias]
```

### Por que precisamos do RAG?

Ao desenvolver aplica√ß√µes inteligentes, como assistentes financeiros que precisam de cota√ß√µes de a√ß√µes em tempo real, chatbots de suporte que devem conhecer os produtos mais recentes da empresa ou sistemas de recomenda√ß√£o que se baseiam nas √∫ltimas tend√™ncias, nos deparamos com uma limita√ß√£o crucial dos Modelos de Linguagem de Grande Escala (LLMs) tradicionais: seu conhecimento est√°tico.

O problema fundamental reside no fato de que esses modelos, por mais sofisticados que sejam, possuem uma base de conhecimento "congelada" no momento de seu treinamento. Eles carecem de acesso inerente a informa√ß√µes atualizadas, o que restringe drasticamente sua aplicabilidade em cen√°rios que exigem dados em tempo real ou conhecimento sobre eventos recentes.

> Confiar exclusivamente em um LLM "puro" nesses contextos resultar√° em respostas desatualizadas, potencialmente imprecisas e, consequentemente, em uma experi√™ncia do usu√°rio comprometida. A efic√°cia da aplica√ß√£o √© diretamente afetada.

### Os Tr√™s Pilares do RAG

```mermaid
graph LR
    A[RAG] --> B[Base de Dados Atual]
    A --> C[Pesquisa em Tempo Real]
    A --> D[Combina√ß√£o de Conhecimento]
    
    B --> E[Documentos Atualizados]
    B --> F[Dados em Tempo Real]
    
    C --> G[Busca Ativa]
    C --> H[Sele√ß√£o de Informa√ß√µes]
    
    D --> I[Integra√ß√£o com LLM]
    D --> J[Contextualiza√ß√£o]
```

1. **Conex√£o com uma base de dados atual:** Em vez de depender apenas do conhecimento est√°tico adquirido durante seu treinamento (que pode se tornar obsoleto rapidamente), o LLM ganha acesso a uma fonte de informa√ß√µes din√¢mica e constantemente atualizada. Isso pode ser uma base de dados de not√≠cias, um reposit√≥rio de documentos corporativos, uma cole√ß√£o de artigos cient√≠ficos, ou qualquer outra fonte relevante para a tarefa em quest√£o.
2. **Pesquisa em tempo real:** O LLM n√£o est√° mais limitado a "lembrar" de informa√ß√µes. Ele adquire a capacidade de "procurar" ativamente por dados relevantes para responder a uma pergunta ou gerar um texto. Isso √© semelhante a como n√≥s, humanos, usamos mecanismos de busca para encontrar informa√ß√µes que n√£o temos memorizadas. O LLM, equipado com RAG, pode formular consultas, analisar os resultados e selecionar as informa√ß√µes mais pertinentes.
3. **Combina√ß√£o de conhecimento base com dados novos:** Este √© o ponto crucial que diferencia o RAG de uma simples busca em uma base de dados. O LLM n√£o apenas recupera informa√ß√µes, mas tamb√©m as integra ao seu conhecimento pr√©-existente. Ele usa sua capacidade de racioc√≠nio e compreens√£o para contextualizar os novos dados, identificar contradi√ß√µes, e formular respostas coerentes e informadas.

Segundo um [whitepaper recente dos pesquisadores do Google](https://arxiv.org/abs/2309.01066), existem v√°rias t√©cnicas para turbinar o desempenho dos LLMs, e o RAG √© uma das mais promissoras. Isso ocorre porque o RAG aborda algumas das limita√ß√µes fundamentais desses modelos:

O RAG resolve v√°rios problemas de uma vez s√≥: diminui aquelas "viagens" dos LLMs quando inventam respostas (as famosas alucina√ß√µes), mant√©m tudo atualizado em vez de ficar preso no passado, deixa as respostas mais transparentes porque voc√™ sabe de onde veio a informa√ß√£o, e ainda melhora o desempenho do modelo quando ele precisa lidar com documentos ou dados espec√≠ficos da sua empresa. √â como dar ao modelo um Google particular que ele pode consultar antes de responder!

> O RAG representa um avan√ßo significativo na evolu√ß√£o dos LLMs, permitindo que eles se tornem ferramentas mais confi√°veis, precisas e √∫teis para uma ampla gama de aplica√ß√µes. Ele transforma o LLM de um "sabe-tudo" desatualizado em um pesquisador √°gil e bem-informado, capaz de combinar conhecimento profundo com informa√ß√µes atualizadas em tempo real.

### Por que o DeepSeek R1?

Ele trabalha muito bem com documenta√ß√£o t√©cnica, o que √© perfeito para nosso sistema [RAG](https://pt.wikipedia.org/wiki/Gera√ß√£o_aumentada_por_recupera√ß√£o) focado em docs t√©cnicas. O DeepSeek R1 consegue equilibrar qualidade e velocidade melhor que outros modelos do Ollama, rodando na sua m√°quina sem ficar alucinando com respostas que n√£o fazem sentido.

O modelo tamb√©m se d√° super bem com v√°rias linguagens de programa√ß√£o, incluindo [Clojure](https://clojure.org/), ent√£o ele responde numa boa sobre implementa√ß√µes t√©cnicas e documenta√ß√£o de c√≥digo. E o melhor: mesmo quando voc√™ joga informa√ß√µes pela metade ou todas bagun√ßadas, ele ainda consegue manter o contexto e dar respostas que fazem sentido. Por isso ele √© perfeito para o que estamos construindo!

## Implementa√ß√£o Pr√°tica

### Preparando o Ambiente

Pre-requisitos:
- [Clojure](https://clojure.org/guides/getting_started): Linguagem de programa√ß√£o funcional que vamos usar para construir a aplica√ß√£o
- [Leiningen](https://leiningen.org/): Ferramenta de build para Clojure
- [Ollama](https://ollama.com/): Modelo de linguagem local

### Estrutura do Projeto

Nossa aplica√ß√£o ter√° tr√™s componentes principais:
1. **Processamento de documenta√ß√£o (Markdown/HTML)**
   - Extra√ß√£o de texto
   - Pr√©-processamento de texto
2. **Sistema de embeddings**
   - Cria√ß√£o de embeddings para o texto usando [TF-IDF](https://pt.wikipedia.org/wiki/TF-IDF)
   - Busca por similaridade sem√¢ntica
3. **Interface com o LLM**
   - Gera√ß√£o de resposta usando o LLM

> **Observa√ß√£o:** Embora o RAG moderno utilize embeddings densos gerados por modelos de linguagem para capturar a sem√¢ntica de forma mais rica, neste artigo, usaremos uma implementa√ß√£o simplificada de [TF-IDF (Term Frequency-Inverse Document Frequency)](https://pt.wikipedia.org/wiki/TF-IDF).

### Configura√ß√£o do Projeto

1. Crie um novo projeto Clojure:
```bash
lein new app docai
cd docai
```

2. Configure o `project.clj`:
```clojure
(defproject docai "0.1.0-SNAPSHOT"
  :description "Um assistente RAG para consulta de documenta√ß√£o t√©cnica"
  :url "http://example.com/FIXME"
  :license {:name "EPL-2.0 OR GPL-2.0-or-later WITH Classpath-exception-2.0"
            :url "https://www.eclipse.org/legal/epl-2.0/"}
  :dependencies [[org.clojure/clojure "1.11.1"]
                 [markdown-to-hiccup "0.6.2"]
                 [hickory "0.7.1"]
                 [org.clojure/data.json "2.4.0"]
                 [http-kit "2.6.0"]
                 [org.clojure/tools.logging "1.2.4"]
                 [org.clojure/tools.namespace "1.4.4"]
                 [org.clojure/core.async "1.6.681"]
                 [org.clojure/core.memoize "1.0.257"]
                 [org.clojure/core.cache "1.0.225"]]
  :main ^:skip-aot docai.core
  :target-path "target/%s"
  :profiles {:uberjar {:aot :all
                       :jvm-opts ["-Dclojure.compiler.direct-linking=true"]}})
```

A estrutura do projeto acima define um aplicativo Clojure para RAG (Retrieval-Augmented Generation) com v√°rias depend√™ncias essenciais: `markdown-to-hiccup` e `hickory` para processar documentos em Markdown e HTML, `data.json` e `http-kit` para comunica√ß√£o com APIs (como a do Ollama), `tools.logging` para registro de eventos, `tools.namespace` para gerenciamento de namespaces, `core.async` para opera√ß√µes ass√≠ncronas (√∫til ao lidar com processamento de documentos grandes), e `core.memoize` e `core.cache` para implementar cache de resultados (como embeddings ou respostas do LLM), o que melhora significativamente a performance ao evitar rec√°lculos desnecess√°rios, especialmente em consultas repetidas ou similares.


### Implementa√ß√£o dos Componentes

Agora vamos implementar os tr√™s componentes principais do nosso sistema RAG e vamos come√ßar com o processamento de documentos. Pois, ele √© o ponto de entrada para o RAG onde vamos processar os documentos e extrair o texto para ser usado nos outros componentes.

#### Processamento de Documentos

```clojure
;; src/docai/document.clj
(ns docai.document
  (:require [markdown-to-hiccup.core :as md]
            [hickory.core :as html]
            [clojure.string :as str]))

(defn is-string? [x]
  (instance? String x))

(defn extract-text-from-markdown [content]
  (try
    (let [hiccup-result (md/md->hiccup content)
          text-nodes (filter is-string? (flatten hiccup-result))]
      text-nodes)
    (catch Exception e
      (println "Erro ao processar Markdown:" (.getMessage e))
      [content])))

(defn extract-text-from-html [content]
  (try
    (let [dom (html/parse content)
          hiccup-result (html/as-hiccup dom)
          text-nodes (filter is-string? (flatten hiccup-result))]
      text-nodes)
    (catch Exception e
      (println "Erro ao processar HTML:" (.getMessage e))
      [content])))

(defn extract-text
  "Extrai texto de documenta√ß√£o (Markdown ou HTML)"
  [doc-path]
  (println "Extraindo texto de:" doc-path)
  (let [content (slurp doc-path)
        _ (println "Tamanho do conte√∫do:" (count content) "caracteres")
        _ (println "Amostra do conte√∫do:" (subs content 0 (min 100 (count content))))
        text (if (.endsWith doc-path ".md")
               (extract-text-from-markdown content)
               (extract-text-from-html content))
        _ (println "Quantidade de n√≥s de texto extra√≠dos:" (count text))
        chunks (partition-all 512 text)]  ; 512 tokens por chunk
    (println "Quantidade de chunks gerados:" (count chunks))
    chunks))

(defn preprocess-chunks
  "Limpa e prepara os chunks de texto"
  [chunks]
  (let [processed (map #(-> %
                            (str/join " ")
                            (str/replace #"\s+" " ")
                            (str/trim))
                       chunks)]
    (println "Primeiro chunk processado:" (first processed))
    processed))
```

Este trecho de c√≥digo implementa a parte de processamento de documentos do nosso sistema RAG. Basicamente, ele pega arquivos Markdown ou HTML e extrai o texto puro deles para que possamos usar depois na busca sem√¢ntica. O c√≥digo usa bibliotecas como `markdown-to-hiccup` e `hickory` para converter os documentos em estruturas de dados que facilitam a extra√ß√£o do texto.

```mermaid
graph TD
    A[Documento] --> B{√â Markdown?}
    B -->|Sim| C[Processa Markdown]
    B -->|N√£o| D[Processa HTML]
    C --> E[Extrai Texto]
    D --> E
    E --> F[Divide em Chunks]
    F --> G[Limpa e Formata]
    G --> H[Chunks Prontos]
```

O fluxo √© bem direto: primeiro verificamos se estamos lidando com Markdown ou HTML, depois extra√≠mos o texto usando a fun√ß√£o apropriada, dividimos em peda√ßos menores (chunks) de 512 tokens cada, e finalmente limpamos esses chunks removendo espa√ßos extras e formatando tudo direitinho. O c√≥digo tamb√©m inclui bastante logging para ajudar a depurar o processo, mostrando informa√ß√µes como o tamanho do documento, quantidade de texto extra√≠do e n√∫mero de chunks gerados. Essa abordagem de dividir o texto em peda√ßos menores √© crucial para o RAG, j√° que permite processar documentos grandes sem sobrecarregar o modelo de linguagem.


#### Sistema de Embeddings

Agora vamos implementar o sistema de embeddings. Ele √© respons√°vel por criar embeddings para o texto para que possamos usar na busca sem√¢ntica.

```clojure
;; src/docai/embedding.clj
(ns docai.embedding
  (:require [clojure.string :as str]
            [clojure.core.memoize :as memo]))

;; Implementa√ß√£o de embeddings usando TF-IDF simples
;; N√£o depende de modelos externos, ao contr√°rio do Ollama que usa o deepseek-r1 para o LLM

(defn tokenize
  "Divide o texto em tokens"
  [text]
  (if (string? text)
    (-> text
        str/lower-case
        (str/split #"\s+")
        (->> (filter #(> (count %) 2))))
    []))

(defn term-freq
  "Calcula a frequ√™ncia dos termos"
  [tokens]
  (frequencies tokens))

(defn string-doc? [x]
  (instance? String x))

(defn doc-freq
  "Calcula a frequ√™ncia dos documentos"
  [docs]
  (let [string-docs (filter string-doc? docs)  ; Use our own predicate function
        _ (println (str "Processando " (count string-docs) " documentos v√°lidos de " (count docs) " total"))
        doc-tokens (map tokenize string-docs)  
        all-tokens (distinct (flatten doc-tokens))
        doc-count (count string-docs)]
    (if (zero? doc-count)
      {}
      (zipmap all-tokens
              (map #(count (filter (fn [tokens] (some #{%} tokens)) doc-tokens))
                   all-tokens)))))

(defn tf-idf
  "Calcula TF-IDF para um documento"
  [doc doc-freq]
  (if (empty? doc-freq)
    {}
    (let [tokens (tokenize doc)
          tf (term-freq tokens)
          n-docs (count (keys doc-freq))]
      (zipmap (keys tf)
              (map #(* (get tf %) (Math/log (/ n-docs (get doc-freq % 1))))
                   (keys tf))))))

(defn vectorize
  "Converte um documento em um vetor TF-IDF"
  [doc doc-freq]
  (let [tf-idf-scores (tf-idf doc doc-freq)]
    (if (empty? doc-freq)
      []
      (map #(get tf-idf-scores % 0.0)
           (keys doc-freq)))))

(defn create-embeddings
  "Gera embeddings para uma lista de textos usando TF-IDF"
  [texts]
  (try
    (let [doc-freq (doc-freq texts)]
      (map #(vectorize % doc-freq) texts))
    (catch Exception e
      (println "Erro ao criar embeddings: " (.getMessage e))
      (vec (repeat (count texts) [])))))

(defn cosine-similarity
  "Calcula a similaridade do cosseno entre dois vetores"
  [v1 v2]
  (if (or (empty? v1) (empty? v2))
    0.0
    (let [dot-product (reduce + (map * v1 v2))
          norm1 (Math/sqrt (reduce + (map #(* % %) v1)))
          norm2 (Math/sqrt (reduce + (map #(* % %) v2)))]
      (if (or (zero? norm1) (zero? norm2))
        0.0
        (/ dot-product (* norm1 norm2))))))

(defn similarity-search
  "Encontra os N chunks mais similares"
  [query-embedding doc-embeddings n]
  (if (or (empty? query-embedding) (empty? doc-embeddings))
    (take (min n (count doc-embeddings)) (range))
    (let [scores (map #(cosine-similarity query-embedding %) doc-embeddings)]
      (->> (map vector scores (range))
           (sort-by first >)
           (take n)
           (map second)))))
```

O c√≥digo acima implementa um sistema simples de embeddings usando TF-IDF (Term Frequency-Inverse Document Frequency) para transformar textos em vetores num√©ricos. Basicamente, ele pega documentos de texto, quebra em palavras (tokens), calcula a import√¢ncia de cada palavra considerando tanto sua frequ√™ncia no documento quanto sua raridade na cole√ß√£o inteira, e cria vetores que representam cada documento. √â como transformar textos em coordenadas matem√°ticas para que o computador possa entender a "semelhan√ßa" entre eles.

```mermaid
graph TD
    A[Documentos] -->|Tokeniza√ß√£o| B[Tokens]
    B -->|TF-IDF| C[Vetores Num√©ricos]
    C -->|Similaridade do Cosseno| D[Documentos Similares]
```

A parte mais legal √© a fun√ß√£o `similarity_search`, que usa a similaridade do cosseno para encontrar documentos parecidos com uma consulta. Imagine que cada documento √© um ponto num espa√ßo multidimensional - quanto menor o √¢ngulo entre dois pontos, mais similares eles s√£o. O c√≥digo n√£o usa nenhum modelo de IA sofisticado para isso, apenas matem√°tica b√°sica, o que o torna leve e r√°pido, embora menos poderoso que embeddings modernos baseados em redes neurais. √â como um GPS simples que te leva ao destino sem todos os recursos de um Google Maps.


#### Entendendo o TF-IDF

O TF-IDF √© uma t√©cnica fundamental para representar documentos como vetores num√©ricos. Vamos entender como ele funciona atrav√©s de um exemplo pr√°tico:

##### Exemplo Num√©rico

Suponha que temos tr√™s documentos sobre programa√ß√£o:

1. Doc1: "Clojure √© uma linguagem funcional"
2. Doc2: "Clojure √© uma linguagem Lisp"
3. Doc3: "Python √© uma linguagem din√¢mica"

Vamos calcular o TF-IDF passo a passo:

1. **Tokeniza√ß√£o e TF (Term Frequency)**
   - Primeiro, convertemos para min√∫sculas e dividimos em palavras
   - Removemos palavras muito curtas (menos de 3 caracteres)
   - Calculamos a frequ√™ncia de cada termo em cada documento

   ```
   Doc1: {"clojure": 1, "linguagem": 1, "funcional": 1}
   Doc2: {"clojure": 1, "linguagem": 1, "lisp": 1}
   Doc3: {"python": 1, "linguagem": 1, "din√¢mica": 1}
   ```

2. **IDF (Inverse Document Frequency)**
   - Contamos em quantos documentos cada termo aparece
   - Aplicamos a f√≥rmula: IDF = log(N/DF), onde:
     - N = n√∫mero total de documentos (3)
     - DF = n√∫mero de documentos que cont√™m o termo

   ```bash
   "clojure": log(3/2) = 0.405
   "linguagem": log(3/3) = 0
   "funcional": log(3/1) = 1.099
   "lisp": log(3/1) = 1.099
   "python": log(3/1) = 1.099
   "din√¢mica": log(3/1) = 1.099
   ```

3. **TF-IDF Final**
   - Multiplicamos TF pelo IDF para cada termo

   ```bash
   Doc1: {"clojure": 0.405, "linguagem": 0, "funcional": 1.099}
   Doc2: {"clojure": 0.405, "linguagem": 0, "lisp": 1.099}
   Doc3: {"python": 1.099, "linguagem": 0, "din√¢mica": 1.099}
   ```

4. **Vetoriza√ß√£o**
   - Convertemos para vetores usando todos os termos √∫nicos como dimens√µes
   - Preenchemos com 0 para termos ausentes

   ```bash
   Doc1: [0.405, 0, 1.099, 0, 0, 0]
   Doc2: [0.405, 0, 0, 1.099, 0, 0]
   Doc3: [0, 0, 0, 0, 1.099, 1.099]
   ```

##### Por que usar logaritmo no IDF?

O logaritmo no IDF serve para dois prop√≥sitos principais:

1. **Suaviza√ß√£o**: Reduz o impacto de termos muito raros ou muito comuns
2. **Escala**: Mant√©m os valores em uma faixa mais gerenci√°vel

Por exemplo, sem o logaritmo:
- Um termo que aparece em 1/1000 documentos teria IDF = 1000
- Um termo que aparece em 1/2 documentos teria IDF = 2

Com o logaritmo:
- `log(1000) ‚âà 6.9`
- `log(2) ‚âà 0.7`

#### Similaridade do Cosseno

A similaridade do cosseno mede o √¢ngulo entre dois vetores TF-IDF. Quanto menor o √¢ngulo, mais similares s√£o os documentos. A f√≥rmula √©:

```bash
cos(Œ∏) = (A¬∑B) / (||A|| ||B||)
```

Onde:
- `A¬∑B` √© o produto escalar dos vetores
- `||A||` e `||B||` s√£o as normas (comprimentos) dos vetores

#### Limita√ß√µes do TF-IDF

1. **Sem√¢ntica**: TF-IDF n√£o captura o significado das palavras. Por exemplo:
   - "carro" e "autom√≥vel" s√£o tratados como palavras diferentes
   - "bom" e "ruim" s√£o tratados como palavras diferentes
2. **Ordem**: N√£o considera a ordem das palavras
   - "gato come rato" e "rato come gato" teriam o mesmo vetor TF-IDF
3. **Contexto**: N√£o captura o contexto das palavras
   - "banco" (financeiro) e "banco" (assento) s√£o tratados como a mesma palavra
4. **Dimens√£o**: O vetor final pode ser muito grande (uma dimens√£o para cada termo √∫nico)

> Por isso, em sistemas RAG modernos, √© mais comum usar embeddings gerados por modelos de linguagem, que capturam melhor a sem√¢ntica e o contexto das palavras.

#### Interface com Ollama

Agora vamos implementar a interface com o Ollama. Ele √© respons√°vel por gerar a resposta para a query do usu√°rio (essa parte aqui √© super divertida, pois √© onde vamos usar o LLM).

```clojure
;; src/docai/llm.clj
(ns docai.llm
  (:require [clojure.data.json :as json]
            [org.httpkit.client :as http]))

(def ollama-url "http://localhost:11434/api/generate")
(def model-name "deepseek-r1") ; Modelo DeepSeek para melhor qualidade

(defn call-ollama-api
  "Chama a API do Ollama para gerar uma resposta"
  [prompt]
  (let [request-body {:model model-name
                      :prompt prompt
                      :stream false}
        options {:headers {"Content-Type" "application/json"}
                 :body (json/write-str request-body)}
        response @(http/post ollama-url options)]
    (if (= (:status response) 200)
      (-> response
          :body
          (json/read-str :key-fn keyword)
          :response)
      (str "Erro ao chamar a API do Ollama: " (:status response) " - " (:body response)))))

;; Fun√ß√µes de utilidade para uso futuro:
;;
;; extract-code-blocks: Extrai blocos de c√≥digo do texto usando regex
;; exemplo de uso:
;;   (extract-code-blocks "```clojure\n(+ 1 2)\n```") => ["(+ 1 2)"]
;;
;; extract-summary: Cria um resumo de texto com tamanho m√°ximo especificado
;; exemplo de uso:
;;   (extract-summary "# T√≠tulo\nConte√∫do longo..." 50) => "Conte√∫do longo..."

(defn format-prompt
  "Formata o prompt para o LLM"
  [context query]
  (str "Voc√™ √© um assistente especializado em documenta√ß√£o t√©cnica. Com base no seguinte contexto da documenta√ß√£o:\n\n"
       context
       "\n\nPergunta: " query
       "\n\nForne√ßa uma resposta t√©cnica precisa e, se poss√≠vel, inclua exemplos de c√≥digo. "
       "Se a documenta√ß√£o n√£o contiver informa√ß√µes relevantes para a pergunta, "
       "indique isso claramente e forne√ßa uma resposta geral com base em seu conhecimento."))

(defn generate-response
  "Gera resposta usando o LLM com base no contexto"
  [query context]
  (try
    (let [prompt (format-prompt context query)]
      (println "DEBUG - Enviando prompt para o Ollama usando o modelo" model-name)
      (call-ollama-api prompt))
    (catch Exception e
      (str "Erro ao gerar resposta: " (.getMessage e) 
           "\n\nPor favor, verifique se o Ollama est√° em execu√ß√£o no endere√ßo " 
           ollama-url 
           "\n\nVoc√™ pode iniciar o Ollama com o comando: ollama serve"))))
```

#### M√≥dulo Principal

Agora vamos implementar o m√≥dulo principal que vai ser o ponto de entrada para o RAG. Ele vai ser respons√°vel por carregar os documentos, processar os chunks, criar os embeddings e gerar a resposta para a query do usu√°rio. 

```clojure
;; src/docai/core.clj
(ns docai.core
  (:require [docai.document :as doc]
            [docai.embedding :as emb]
            [docai.llm :as llm]
            [clojure.java.io :as io]
            [clojure.string :as str])
  (:gen-class))

(def docs-path "resources/docs")

(defn load-documentation
  "Carrega todos os arquivos de documenta√ß√£o do diret√≥rio"
  []
  (->> (file-seq (io/file docs-path))
       (filter #(.isFile %))
       (map #(.getPath %))))

(defn setup-knowledge-base
  "Configura a base de conhecimento inicial"
  []
  (let [doc-files (load-documentation)
        _ (when (empty? doc-files)
            (println "Aviso: Nenhum arquivo de documenta√ß√£o encontrado em resources/docs/"))
        _ (doseq [file doc-files]
            (println "Arquivo encontrado:" file))
        all-chunks (mapcat doc/extract-text doc-files)
        processed-chunks (doc/preprocess-chunks all-chunks)
        _ (println (str "Processando " (count processed-chunks) " chunks de texto..."))
        _ (when (< (count processed-chunks) 5)
            (println "DEBUG - Primeiros chunks:")
            (doseq [chunk (take 5 processed-chunks)]
              (println (str "Chunk: '" (subs chunk 0 (min 50 (count chunk))) "...'"))))
        embeddings (emb/create-embeddings processed-chunks)]
    {:chunks processed-chunks
     :embeddings embeddings
     :original-files doc-files}))

(defn get-file-content
  "L√™ o conte√∫do completo de um arquivo"
  [file-path]
  (try
    (slurp file-path)
    (catch Exception _
      (println "Erro ao ler arquivo:" file-path)
      "")))

(defn query-rag
  "Processa uma query usando o pipeline RAG"
  [knowledge-base query]
  (println "DEBUG - Processando query:" query)
  (if (and (seq (:chunks knowledge-base)) 
           (seq (:embeddings knowledge-base)))
    (let [query-emb (first (emb/create-embeddings [query]))
          similar-idxs (emb/similarity-search query-emb 
                                            (:embeddings knowledge-base)
                                            3)
          _ (println "DEBUG - √çndices similares:" similar-idxs)
          
          ;; Obter contexto relevante
          context-chunks (->> similar-idxs
                              (map #(nth (:chunks knowledge-base) %))
                              (str/join "\n\n"))
          
          ;; Se n√£o houver chunks relevantes, use o conte√∫do original
          context (if (str/blank? context-chunks)
                    (if (seq (:original-files knowledge-base))
                      (get-file-content (first (:original-files knowledge-base)))
                      "N√£o foi poss√≠vel encontrar informa√ß√µes relevantes.")
                    context-chunks)]
      
      (println "DEBUG - Tamanho do contexto:" (count context) "caracteres")
      (println "DEBUG - Amostra do contexto:" (subs context 0 (min 200 (count context))) "...")
      
      ;; Gerar resposta usando o LLM
      (llm/generate-response query context))
    "N√£o foi poss√≠vel encontrar informa√ß√µes relevantes na base de conhecimento."))

(defn -main
  "Fun√ß√£o principal que inicializa a aplica√ß√£o DocAI"
  [& _]
  (println "Inicializando DocAI...")
  
  ;; Verificar se o Ollama est√° acess√≠vel
  (println "Para usar o Ollama, certifique-se de que ele est√° em execu√ß√£o com o comando: ollama serve")
  (println "Usando o modelo deepseek-r1. Se voc√™ ainda n√£o o baixou, execute: ollama pull deepseek-r1")
  
  (let [kb (setup-knowledge-base)]
    (println "Base de conhecimento pronta! Fa√ßa sua pergunta:")
    (try
      (loop []
        (when-let [input (read-line)]
          (when-not (= input "sair")
            (println "Processando...")
            (println (query-rag kb input))
            (println "\nPr√≥xima pergunta (ou 'sair' para terminar):")
            (recur))))
      (catch Exception e
        (println "Erro: " (.getMessage e))
        (println "Detalhes: " (ex-data e))))
    (println "Obrigado por usar o DocAI. At√© a pr√≥xima!")))
```

---

## Como Usar

### Instala√ß√£o do Ollama

1. **Instala√ß√£o**:
   - **Windows**: Baixe o instalador do [site oficial do Ollama](https://ollama.com/download) e execute-o
   - **Linux**: Execute o comando:
     ```bash
     curl https://ollama.ai/install.sh | sh
     ```
   - **macOS**: Use o Homebrew:
     ```bash
     brew install ollama
     ```

2. **Iniciando o Servidor**:
   ```bash
   ollama serve
   ```

3. **Baixando o Modelo**:
   ```bash
   ollama pull deepseek-r1
   ```

4. **Verificando a Instala√ß√£o**:
   - Execute um teste simples:
     ```bash
     ollama run deepseek-r1 "Ol√°! Como voc√™ est√°?"
     ```
   - Se tudo estiver funcionando, voc√™ receber√° uma resposta do modelo

> **Dica**: O Ollama mant√©m os modelos em cache local. Se voc√™ precisar liberar espa√ßo, pode usar `ollama rm deepseek-r1` para remover o modelo.

### Executando a Aplica√ß√£o

1. Coloque seus documentos na pasta `resources/docs/` (j√° inclu√≠mos dois exemplos: `example.md`)
2. Execute o projeto:

```bash
lein run
```

3. Fa√ßa suas perguntas! Exemplo:

```bash
Como implementar autentica√ß√£o JWT em Clojure?
Como implementar auth saml em python?
Como integrar o auth0 em uma aplica√ß√£o Clojure?
etc...
``` 	

O DocAI processa sua pergunta em v√°rias etapas:

```mermaid
flowchart TD
    A[In√≠cio] --> B[Carrega Documenta√ß√£o]
    B --> C[Processa Documentos]
    C --> D[Gera Embeddings]
    D --> E[Base de Conhecimento]
    
    F[Consulta do Usu√°rio] --> G[Processa Consulta]
    G --> H[Gera Embedding da Consulta]
    H --> I[Busca Similaridade]
    I --> J[Seleciona Chunks Relevantes]
    J --> K[Combina Contexto]
    K --> L[Gera Prompt]
    L --> M[LLM DeepSeek R1]
    M --> N[Resposta Final]
    
    E --> I
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style N fill:#9ff,stroke:#333,stroke-width:2px
```

1. **Processamento da Consulta**: A pergunta √© convertida em um vetor TF-IDF
2. **Busca por Similaridade**: O sistema encontra os chunks mais relevantes
3. **Gera√ß√£o de Contexto**: Os chunks s√£o combinados em um contexto coeso
4. **Gera√ß√£o de Resposta**: O LLM gera uma resposta baseada no contexto

Voc√™ pode ver o processo em a√ß√£o nos logs:

```bash
DEBUG - Processando query: Como implementar autentica√ß√£o JWT em Clojure?
DEBUG - √çndices similares: [2, 5, 8]
DEBUG - Tamanho do contexto: 1234 caracteres
DEBUG - Amostra do contexto: "Para implementar autentica√ß√£o JWT em Clojure..."
```

> **NOTA:** A prop√≥sito, o projeto docai est√° dispon√≠vel no [https://github.com/scovl/docai](https://github.com/scovl/docai) caso voc√™ queira contribuir com o projeto ou usar em outro projeto.

## Considera√ß√µes T√©cnicas

### Performance e Otimiza√ß√µes

1. **Performance**: Esta implementa√ß√£o √© b√°sica e pode ser otimizada:
   - Usando um banco de dados vetorial como [Milvus](https://milvus.io/) ou [FAISS](https://github.com/facebookresearch/faiss)
   - Implementando cache de embeddings
   - Paralelizando o processamento de chunks

2. **Mem√≥ria**: Para documenta√ß√µes muito extensas, considere:
   - Processar os chunks em lotes
   - Implementar indexa√ß√£o incremental
   - Usar streaming para arquivos grandes

3. **Modelos**: Diferentes modelos do Ollama t√™m diferentes caracter√≠sticas:
   - DeepSeek R1: Bom para compreens√£o geral e gera√ß√£o de texto
   - DeepSeek Coder: Especializado em c√≥digo
   - Llama 3: Boa alternativa geral
   - Mistral: Bom para tarefas espec√≠ficas
   - Gemma: Leve e eficiente

### Tratamento de Erros

O sistema implementa v√°rias camadas de tratamento de erros para lidar com diferentes cen√°rios:

1. **Ollama Offline**
   - **Sintoma**: O sistema n√£o consegue se conectar ao servidor Ollama
   - **Tratamento**: O c√≥digo verifica a disponibilidade do servidor e fornece mensagens claras de erro:
   ```clojure
   (catch Exception e
     (str "Erro ao gerar resposta: " (.getMessage e) 
          "\n\nPor favor, verifique se o Ollama est√° em execu√ß√£o no endere√ßo " 
          ollama-url 
          "\n\nVoc√™ pode iniciar o Ollama com o comando: ollama serve"))
   ```

2. **Documenta√ß√£o Muito Grande**
   - **Sintoma**: Arquivos de documenta√ß√£o que excedem a mem√≥ria dispon√≠vel
   - **Tratamento**: O sistema implementa:
     - Chunking de documentos (512 tokens por chunk)
     - Processamento em lotes
     - Logs de progresso para monitoramento
   ```clojure
   (let [content (slurp doc-path)
         chunks (partition-all 512 text)]
     (println "Quantidade de chunks gerados:" (count chunks)))
   ```

3. **Consultas sem Rela√ß√£o com a Documenta√ß√£o**
   - **Sintoma**: Nenhum chunk relevante √© encontrado para a consulta
   - **Tratamento**: O sistema:
     - Verifica se h√° chunks dispon√≠veis
     - Usa fallback para conte√∫do original se necess√°rio
     - Fornece feedback claro ao usu√°rio
   ```clojure
   (if (str/blank? context-chunks)
     (if (seq (:original-files knowledge-base))
       (get-file-content (first (:original-files knowledge-base)))
       "N√£o foi poss√≠vel encontrar informa√ß√µes relevantes.")
     context-chunks)
   ```

4. **Melhorias Futuras**
   - Implementar retry com backoff exponencial para falhas de conex√£o
   - Adicionar cache de embeddings para melhor performance
   - Implementar streaming para arquivos muito grandes
   - Adicionar valida√ß√£o de formato de documentos
   - Implementar rate limiting para evitar sobrecarga do Ollama

### Prompt Engineering

O Prompt Engineering √© uma habilidade crucial para obter bons resultados com LLMs. Um prompt bem estruturado pode fazer a diferen√ßa entre uma resposta vaga e uma resposta precisa e √∫til.

#### Estrutura do Prompt

```clojure
(defn format-prompt
  "Formata o prompt para o LLM"
  [context query]
  (str "Voc√™ √© um assistente especializado em documenta√ß√£o t√©cnica. "
       "Com base no seguinte contexto da documenta√ß√£o:\n\n"
       context
       "\n\nPergunta: " query
       "\n\nForne√ßa uma resposta t√©cnica precisa e, se poss√≠vel, "
       "inclua exemplos de c√≥digo. Se a documenta√ß√£o n√£o contiver "
       "informa√ß√µes relevantes para a pergunta, indique isso claramente "
       "e forne√ßa uma resposta geral com base em seu conhecimento."))
```

#### T√©cnicas de Prompt Engineering

- **Role Prompting**: Define um papel espec√≠fico para o modelo ("Voc√™ √© um especialista em...")
- **Few-shot Learning**: Fornece exemplos de entradas e sa√≠das desejadas
- **Chain of Thought**: Pede ao modelo para explicar seu racioc√≠nio
- **Format Specification**: Especifica o formato desejado da resposta
- **Constraints**: Define limites e requisitos para a resposta

#### Exemplo de Prompt Avan√ßado

```clojure
(defn format-advanced-prompt
  "Formata um prompt mais sofisticado para o LLM"
  [context query]
  (str "Voc√™ √© um especialista em documenta√ß√£o t√©cnica de software, "
       "com foco em Clojure e desenvolvimento web.\n\n"
       "Contexto da documenta√ß√£o:\n"
       context
       "\n\nPergunta: " query
       "\n\nPor favor, siga estas diretrizes:\n"
       "1. Seja preciso e t√©cnico\n"
       "2. Inclua exemplos de c√≥digo quando relevante\n"
       "3. Cite as partes da documenta√ß√£o que voc√™ est√° usando\n"
       "4. Se a informa√ß√£o n√£o estiver na documenta√ß√£o, indique claramente\n"
       "5. Mantenha a resposta concisa mas completa\n"
       "6. Use formata√ß√£o Markdown para melhor legibilidade"))
```

#### Dicas para Prompts Efetivos

- Seja espec√≠fico e claro nas instru√ß√µes
- Use formata√ß√£o para melhorar a legibilidade
- Inclua exemplos quando poss√≠vel
- Defina limites e restri√ß√µes claras
- Pe√ßa ao modelo para explicar seu racioc√≠nio
- Use itera√ß√£o para refinar o prompt

#### Avalia√ß√£o de Prompts

- Teste diferentes varia√ß√µes do mesmo prompt
- Compare as respostas para identificar a melhor estrutura
- Colete feedback dos usu√°rios
- Mantenha um registro dos prompts que funcionam bem

> **Nota**: O [Prompt Engineering](https://www.promptingguide.ai/) √© uma √°rea em constante evolu√ß√£o. Novas t√©cnicas e melhores pr√°ticas surgem regularmente √† medida que os modelos evoluem.

## Pr√≥ximos Passos

### Melhorias Propostas

```mermaid
mindmap
  root((Melhorias))
    Tokeniza√ß√£o
      BPE
      WordPiece
      Tokenizador do Modelo
    Embeddings
      Pr√©-treinados
      Via Ollama
      Cache
    Banco de Dados
      Milvus
      FAISS
      Qdrant
    Cache
      Embeddings
      Respostas
    Erros
      Conex√£o
      Modelo
      Rede
    Logging
      Framework
      Rastreamento
    Testes
      Unit√°rios
      Integra√ß√£o
    Prompt
      Few-shot
      Chain-of-thought
      Formato
    Langchain4j
      Abstra√ß√£o
      Integra√ß√£o
```

1. **Tokeniza√ß√£o Avan√ßada**
   - Usar um tokenizador de *subpalavras* (como BPE ou WordPiece)
   - Idealmente, o mesmo usado no treinamento do modelo (ex: `deepseek-r1`)

2. **Embeddings Pr√©-treinados**
   - Usar embeddings do pr√≥prio modelo (via Ollama)
   - Mais simples e *muito* melhor para busca sem√¢ntica

3. **Banco de Dados Vetorial**
   - Usar um banco de dados vetorial (Milvus, FAISS, Qdrant, etc.)
   - Para lidar com *muitos* documentos de forma eficiente

4. **Cache**
   - Usar cache para os embeddings
   - Opcionalmente, cache de respostas

5. **Erros**
   - Tratar mais erros (Ollama offline, modelo indispon√≠vel, rede, arquivos inv√°lidos)

6. **Logging**
   - Usar um framework de logging para rastreamento e depura√ß√£o

7. **Testes**
   - Adicionar testes unit√°rios e de integra√ß√£o

8. **Prompt Engineering**
   - Refinar o prompt (em `format-prompt`)
   - Experimentar com:
     * Exemplos no prompt (few-shot learning)
     * Instru√ß√µes passo a passo (chain-of-thought)
     * Instru√ß√µes claras sobre formato, tamanho, etc.
     * Pedir a fonte da informa√ß√£o (qual chunk)

9. **Usar langchain4j**
   - Criar RAG atrav√©s do [langchain4j](https://github.com/langchain4j/langchain4j)
   - Via interop java com o clojure

### Usando Langchain4j

Langchain4j √© uma biblioteca Java que oferece uma abstra√ß√£o de alto n√≠vel para construir aplica√ß√µes de IA generativa, incluindo sistemas RAG. Ela se integra bem com Clojure atrav√©s da interoperabilidade Java.

Vantagens de usar Langchain4j:
- **Abstra√ß√£o**: Fornece componentes pr√©-constru√≠dos para tarefas comuns
- **Modularidade**: Permite trocar implementa√ß√µes facilmente
- **Integra√ß√£o**: Oferece integra√ß√µes com v√°rias ferramentas e servi√ßos
- **Comunidade e Suporte**: Possui uma comunidade ativa e boa documenta√ß√£o

> Em um pr√≥ximo artigo, escreverei sobre como usar [Langchain4j](https://github.com/langchain4j/langchain4j) para criar um sistema RAG ainda neste mesmo projeto.

## Refer√™ncias

- [RAG](https://www.pinecone.io/learn/rag/) - Documenta√ß√£o do Pinecone
- [Embedding](https://www.pinecone.io/learn/embeddings/) - Documenta√ß√£o do Pinecone
- [LLM](https://www.pinecone.io/learn/llms/) - Documenta√ß√£o do Pinecone
- [Ollama](https://ollama.com/) - Ferramenta para rodar LLMs localmente
- [Clojure](https://clojure.org/) - Documenta√ß√£o do Clojure
- [http-kit](https://github.com/http-kit/http-kit) - Cliente HTTP para Clojure
- [data.json](https://github.com/clojure/data.json) - Biblioteca JSON para Clojure
- [clojure.test](https://clojure.github.io/clojure/clojure.test-api.html) - Documenta√ß√£o da biblioteca de testes do Clojure
- [clj-kondo](https://github.com/clj-kondo/clj-kondo) - Linter para Clojure
