+++
title = "01 - RAG Simples com Clojure e Ollama"
description = "Um prot√≥tipo funcional do zero"
date = 2025-03-23T19:00:00-00:00
tags = ["RAG", "LLM", "AI", "Langchain"]
draft = false
weight = 1
author = "Vitor Lobo Ramos"
+++


## Introdu√ß√£o

Ol√°, pessoal! üëã 

Neste artigo, vamos explorar como construir uma aplica√ß√£o [RAG (Retrieval-Augmented Generation)](https://pt.wikipedia.org/wiki/Gera√ß√£o_aumentada_por_recupera√ß√£o) completa do zero usando [Clojure](https://clojure.org/). Vamos mergulhar em uma implementa√ß√£o pr√°tica que combina processamento de texto, busca sem√¢ntica e gera√ß√£o de respostas com LLMs locais. Se voc√™ est√° interessado em melhorar a precis√£o e relev√¢ncia das respostas dos seus modelos de linguagem com informa√ß√µes atualizadas, este guia √© para voc√™!

## Fundamentos do RAG

### O que √© RAG?

Os Modelos de Linguagem de Grande Escala ([LLMs](https://en.wikipedia.org/wiki/Large_language_model)), como o [GPT](https://openai.com/api/), [ChatGPT](https://openai.com/api/) e outros, revolucionaram a forma como interagimos com a intelig√™ncia artificial. Eles s√£o capazes de gerar textos coerentes, responder perguntas complexas e at√© mesmo criar conte√∫do criativo. No entanto, esses modelos possuem uma limita√ß√£o fundamental: seu conhecimento √© "congelado" no tempo.

```mermaid
graph TD
    A[LLM Treinado] --> B[Data de Corte]
    B --> C[Conhecimento Congelado]
    C --> D[Limita√ß√µes]
    D --> E[N√£o sabe eventos recentes]
    D --> F[N√£o tem dados atualizados]
    D --> G[N√£o conhece novas tecnologias]
```

### Por que precisamos do RAG?

Ao desenvolver aplica√ß√µes inteligentes, como assistentes financeiros que precisam de cota√ß√µes de a√ß√µes em tempo real, chatbots de suporte que devem conhecer os produtos mais recentes da empresa ou sistemas de recomenda√ß√£o que se baseiam nas √∫ltimas tend√™ncias, nos deparamos com uma limita√ß√£o crucial dos Modelos de Linguagem de Grande Escala ([LLMs](https://en.wikipedia.org/wiki/Large_language_model)) tradicionais: seu conhecimento est√°tico.

O problema fundamental reside no fato de que esses modelos, por mais sofisticados que sejam, possuem uma base de conhecimento "congelada" no momento de seu treinamento. Eles carecem de acesso inerente a informa√ß√µes atualizadas, o que restringe drasticamente sua aplicabilidade em cen√°rios que exigem dados em tempo real ou conhecimento sobre eventos recentes.

> Confiar exclusivamente em um [LLM "puro"](https://en.wikipedia.org/wiki/Large_language_model) nesses contextos resultar√° em respostas desatualizadas, potencialmente imprecisas e, consequentemente, em uma experi√™ncia do usu√°rio comprometida. A efic√°cia da aplica√ß√£o √© diretamente afetada.

### Os Tr√™s Pilares do RAG

```mermaid
graph LR
    A[RAG] --> B[Base de Dados Atual]
    A --> C[Pesquisa em Tempo Real]
    A --> D[Combina√ß√£o de Conhecimento]
    
    B --> E[Documentos Atualizados]
    B --> F[Dados em Tempo Real]
    
    C --> G[Busca Ativa]
    C --> H[Sele√ß√£o de Informa√ß√µes]
    
    D --> I[Integra√ß√£o com LLM]
    D --> J[Contextualiza√ß√£o]
```

1. **Conex√£o com uma base de dados atual:** Em vez de depender apenas do conhecimento est√°tico adquirido durante seu treinamento (que pode se tornar obsoleto rapidamente), o [LLM](https://en.wikipedia.org/wiki/Large_language_model) ganha acesso a uma fonte de informa√ß√µes din√¢mica e constantemente atualizada. Isso pode ser uma base de dados de not√≠cias, um reposit√≥rio de documentos corporativos, uma cole√ß√£o de artigos cient√≠ficos, ou qualquer outra fonte relevante para a tarefa em quest√£o.
2. **Pesquisa em tempo real:** O [LLM](https://en.wikipedia.org/wiki/Large_language_model) n√£o est√° mais limitado a "lembrar" de informa√ß√µes. Ele adquire a capacidade de "procurar" ativamente por dados relevantes para responder a uma pergunta ou gerar um texto. Isso √© semelhante a como n√≥s, humanos, usamos mecanismos de busca para encontrar informa√ß√µes que n√£o temos memorizadas. O [LLM](https://en.wikipedia.org/wiki/Large_language_model), equipado com RAG, pode formular consultas, analisar os resultados e selecionar as informa√ß√µes mais pertinentes.
3. **Combina√ß√£o de conhecimento base com dados novos:** Este √© o ponto crucial que diferencia o [RAG](https://pt.wikipedia.org/wiki/Gera√ß√£o_aumentada_por_recupera√ß√£o) de uma simples busca em uma base de dados. O [LLM](https://en.wikipedia.org/wiki/Large_language_model) n√£o apenas recupera informa√ß√µes, mas tamb√©m as integra ao seu conhecimento pr√©-existente. Ele usa sua capacidade de racioc√≠nio e compreens√£o para contextualizar os novos dados, identificar contradi√ß√µes, e formular respostas coerentes e informadas.

### RAG em Produ√ß√£o

Sistemas RAG em produ√ß√£o frequentemente incluem etapas adicionais para melhorar a precis√£o: **re-ranking** (onde um modelo especializado re-avalia a relev√¢ncia dos documentos recuperados) e **merge-rerank** (que combina resultados de m√∫ltiplas estrat√©gias de busca como sem√¢ntica, lexical e h√≠brida). Essas t√©cnicas aumentam significativamente a qualidade das respostas, mas adicionam complexidade ao sistema.

> **Nota**: Nossa implementa√ß√£o atual usa apenas busca sem√¢ntica simples com TF-IDF, focando na compreens√£o dos fundamentos do RAG. Para aplica√ß√µes em produ√ß√£o, considere implementar essas t√©cnicas avan√ßadas.

Segundo um [whitepaper recente dos pesquisadores do Google](https://arxiv.org/abs/2309.01066), existem v√°rias t√©cnicas para turbinar o desempenho dos [LLMs](https://en.wikipedia.org/wiki/Large_language_model), e o RAG √© uma das mais promissoras. Isso ocorre porque o RAG aborda algumas das limita√ß√µes fundamentais desses modelos:

O RAG resolve v√°rios problemas de uma vez s√≥: diminui aquelas "viagens" dos [LLMs](https://en.wikipedia.org/wiki/Large_language_model) quando inventam respostas (as famosas alucina√ß√µes), mant√©m tudo atualizado em vez de ficar preso no passado, deixa as respostas mais transparentes porque voc√™ sabe de onde veio a informa√ß√£o, e ainda melhora o desempenho do modelo quando ele precisa lidar com documentos ou dados espec√≠ficos da sua empresa. √â como dar ao modelo um Google particular que ele pode consultar antes de responder!

> O RAG representa um avan√ßo significativo na evolu√ß√£o dos [LLMs](https://en.wikipedia.org/wiki/Large_language_model), permitindo que eles se tornem ferramentas mais confi√°veis, precisas e √∫teis para uma ampla gama de aplica√ß√µes. Ele transforma o [LLM](https://en.wikipedia.org/wiki/Large_language_model) de um "sabe-tudo" desatualizado em um pesquisador √°gil e bem-informado, capaz de combinar conhecimento profundo com informa√ß√µes atualizadas em tempo real.

### Por que o DeepSeek R1?

Ele trabalha muito bem com documenta√ß√£o t√©cnica, o que √© perfeito para nosso sistema [RAG](https://pt.wikipedia.org/wiki/Gera√ß√£o_aumentada_por_recupera√ß√£o) focado em docs t√©cnicas. O DeepSeek R1 consegue equilibrar qualidade e velocidade melhor que outros modelos do Ollama, rodando na sua m√°quina sem ficar alucinando com respostas que n√£o fazem sentido.

O modelo tamb√©m se d√° super bem com v√°rias linguagens de programa√ß√£o, incluindo [Clojure](https://clojure.org/), ent√£o ele responde numa boa sobre implementa√ß√µes t√©cnicas e documenta√ß√£o de c√≥digo. E o melhor: mesmo quando voc√™ joga informa√ß√µes pela metade ou todas bagun√ßadas, ele ainda consegue manter o contexto e dar respostas que fazem sentido. Por isso ele √© perfeito para o que estamos construindo!

## Implementa√ß√£o Pr√°tica

### Preparando o Ambiente

Pre-requisitos:
- [Clojure](https://clojure.org/guides/getting_started): Linguagem de programa√ß√£o funcional que vamos usar para construir a aplica√ß√£o
- [Leiningen](https://leiningen.org/): Ferramenta de build para Clojure
- [Ollama](https://ollama.com/): Modelo de linguagem local

### Estrutura do Projeto

Nossa aplica√ß√£o ter√° tr√™s componentes principais:
1. **Processamento de documenta√ß√£o (Markdown/HTML)**
   - Extra√ß√£o de texto
   - Pr√©-processamento de texto
2. **Sistema de embeddings**
   - Cria√ß√£o de embeddings para o texto usando [TF-IDF](https://pt.wikipedia.org/wiki/TF-IDF)
   - Busca por similaridade sem√¢ntica
3. **Interface com o LLM**
   - Gera√ß√£o de resposta usando o LLM

> **Observa√ß√£o:** Embora o RAG moderno utilize embeddings densos gerados por modelos de linguagem para capturar a sem√¢ntica de forma mais rica, neste artigo, usaremos uma implementa√ß√£o simplificada de [TF-IDF (Term Frequency-Inverse Document Frequency)](https://pt.wikipedia.org/wiki/TF-IDF) como **prova de conceito**. Para aplica√ß√µes em produ√ß√£o, recomendamos fortemente o uso de embeddings densos.


### TF-IDF

O [TF-IDF](https://pt.wikipedia.org/wiki/TF-IDF) (Term Frequency-Inverse Document Frequency) √© uma t√©cnica estat√≠stica usada para avaliar a import√¢ncia de uma palavra em um documento, em rela√ß√£o a uma cole√ß√£o de documentos. Vamos entender como funciona:

1. **Term Frequency (TF)**: Mede a frequ√™ncia de uma palavra em um documento.
   ```
   TF(termo) = (N√∫mero de vezes que o termo aparece no documento) / (Total de termos no documento)
   ```

2. **Inverse Document Frequency (IDF)**: Mede a raridade de um termo na cole√ß√£o de documentos.
   ```
   IDF(termo) = log(N√∫mero total de documentos / N√∫mero de documentos contendo o termo)
   ```

3. **TF-IDF**: √â o produto desses dois valores.
   ```
   TF-IDF(termo) = TF(termo) √ó IDF(termo)
   ```

Vamos imaginar um cen√°rio pr√°tico com tr√™s documentos t√©cnicos: 

- Doc1: "Clojure √© uma linguagem funcional baseada em [Lisp](https://en.wikipedia.org/wiki/Lisp_(programming_language))"
- Doc2: "Python √© uma linguagem de programa√ß√£o vers√°til"
- Doc3: "Clojure e Python s√£o linguagens de programa√ß√£o populares"

O TF-IDF √© uma t√©cnica que nos ajuda a identificar quais palavras s√£o mais importantes em cada documento, comparando a frequ√™ncia de um termo no documento (TF) com a raridade desse termo em toda a cole√ß√£o (IDF). Por exemplo, se "Clojure" aparece uma vez em um documento de oito palavras, seu TF √© 0,125; como est√° presente em dois de tr√™s documentos, seu IDF √© log(3/2) ‚âà 0,176, resultando em um TF-IDF de aproximadamente 0,022. J√° termos muito comuns, como "linguagem", acabam com TF-IDF zero, pois n√£o ajudam a diferenciar os documentos.

Esse m√©todo √© fundamental em sistemas de busca, pois destaca os termos que realmente caracterizam cada texto. No contexto do RAG, o TF-IDF permite indexar e encontrar rapidamente os documentos mais relevantes para uma consulta, servindo como uma base simples e eficiente para recupera√ß√£o de informa√ß√µes, que pode ser aprimorada com t√©cnicas mais avan√ßadas como embeddings densos.

### Requisitos M√≠nimos

Este experimento funciona com hardware b√°sico: **4 cores de CPU e 8GB de RAM** s√£o suficientes. Para m√°quinas mais lentas, use `ollama pull deepseek-r1:3b` (vers√£o otimizada). 

> Para requisitos detalhados de produ√ß√£o e otimiza√ß√µes avan√ßadas, consulte o [ap√™ndice de hardware](#requisitos-de-hardware-detalhados) ao final do artigo.

### Configura√ß√£o do Projeto

1. Crie um novo projeto Clojure:
```bash
lein new app docai
cd docai
```

2. Configure o `project.clj`:
```clojure
(defproject docai "0.1.0-SNAPSHOT"
  :description "Um assistente RAG para consulta de documenta√ß√£o t√©cnica"
  :url "http://example.com/FIXME"
  :license {:name "EPL-2.0 OR GPL-2.0-or-later WITH Classpath-exception-2.0"
            :url "https://www.eclipse.org/legal/epl-2.0/"}
  :dependencies [[org.clojure/clojure "1.11.1"]
                 [markdown-to-hiccup "0.6.2"]
                 [hickory "0.7.1"]
                 [org.clojure/data.json "2.4.0"]
                 [http-kit "2.6.0"]
                 [org.clojure/tools.logging "1.2.4"]
                 [org.clojure/tools.namespace "1.4.4"]
                 [org.clojure/core.async "1.6.681"]
                 [org.clojure/core.memoize "1.0.257"]
                 [org.clojure/core.cache "1.0.225"]]
  :main ^:skip-aot docai.core
  :target-path "target/%s"
  :profiles {:uberjar {:aot :all
                       :jvm-opts ["-Dclojure.compiler.direct-linking=true"]}})
```

A estrutura do projeto acima define um aplicativo Clojure para RAG (Retrieval-Augmented Generation) com v√°rias depend√™ncias essenciais. Entre elas, `markdown-to-hiccup` e `hickory` s√£o usadas para processar documentos em Markdown e HTML, enquanto `data.json` e `http-kit` facilitam a comunica√ß√£o com APIs externas, como a do Ollama. Al√©m disso, `tools.logging` √© respons√°vel pelo registro de eventos e logs, e `tools.namespace` auxilia no gerenciamento de namespaces do projeto.

J√° `core.async` permite opera√ß√µes ass√≠ncronas, o que √© especialmente √∫til ao lidar com o processamento de documentos grandes. Por fim, `core.memoize` e `core.cache` s√£o utilizados para implementar cache de resultados, como embeddings ou respostas do LLM, melhorando significativamente a performance ao evitar rec√°lculos desnecess√°rios, principalmente em consultas repetidas ou similares.

### Implementa√ß√£o dos Componentes

Agora vamos implementar os tr√™s componentes principais do nosso sistema RAG e vamos come√ßar com o processamento de documentos. Pois, ele √© o ponto de entrada para o RAG onde vamos processar os documentos e extrair o texto para ser usado nos outros componentes.

#### Processamento de Documentos

```clojure
;; src/docai/document.clj
(ns docai.document
  (:require [markdown-to-hiccup.core :as md]
            [hickory.core :as html]
            [clojure.string :as str]))

(defn extract-text-from-markdown [content]
  (try
    (let [hiccup-result (md/md->hiccup content)
          text-nodes (filter string? (flatten hiccup-result))]
      text-nodes)
    (catch Exception e
      (println "Erro ao processar Markdown:" (.getMessage e))
      [content])))

(defn extract-text-from-html [content]
  (try
    (let [dom (html/parse content)
          hiccup-result (html/as-hiccup dom)
          text-nodes (filter string? (flatten hiccup-result))]
      text-nodes)
    (catch Exception e
      (println "Erro ao processar HTML:" (.getMessage e))
      [content])))

;; Declare functions that will be defined later
(declare create-token-aware-chunks)

(defn extract-text
  "Extrai texto de documenta√ß√£o (Markdown ou HTML)"
  [doc-path]
  (println "Extraindo texto de:" doc-path)
  (let [content (slurp doc-path)
        _ (println "Tamanho do conte√∫do:" (count content) "caracteres")
        _ (println "Amostra do conte√∫do:" (subs content 0 (min 100 (count content))))
        text (if (.endsWith doc-path ".md")
               (extract-text-from-markdown content)
               (extract-text-from-html content))
        _ (println "Quantidade de n√≥s de texto extra√≠dos:" (count text))
        ;; Usar tokens reais em vez de caracteres para chunking preciso
        chunks (create-token-aware-chunks text 512)]
    (println "Quantidade de chunks gerados:" (count chunks))
    chunks))

(defn count-tokens
  "Conta tokens usando heur√≠stica (para desenvolvimento)"
  [text]
  ;; ‚ö†Ô∏è ATEN√á√ÉO: Esta √© uma heur√≠stica aproximada
  ;; Para produ√ß√£o, use [clojure-tiktoken](https://github.com/justone/clojure-tiktoken)
  ;; ou API do Ollama para contagem precisa
  (try
    (let [words (str/split text #"\s+")
          ;; Estimativa melhorada para portugu√™s brasileiro
          ;; Ainda pode errar 2x em textos muito curtos/longos
          estimated-tokens (reduce + 
                                 (map (fn [word]
                                        (cond
                                          ;; Palavras muito longas (composi√ß√£o)
                                          (> (count word) 15) (* (count word) 0.8)
                                          ;; Palavras longas (deriva√ß√£o)
                                          (> (count word) 10) (* (count word) 0.6)
                                          ;; Palavras m√©dias
                                          (> (count word) 5) (* (count word) 0.4)
                                          ;; Palavras curtas
                                          :else 1.0))
                                      words))]
      (int estimated-tokens))
    (catch Exception e
      (println "Erro ao contar tokens:" (.getMessage e))
      ;; Fallback conservador: 1 token por caractere
      (count text))))

(defn create-token-aware-chunks
  "Cria chunks baseados em tokens reais, n√£o caracteres"
  [text-nodes max-tokens]
  (loop [nodes text-nodes
         current-chunk []
         current-tokens 0
         all-chunks []]
    (if (empty? nodes)
      (if (seq current-chunk)
        (conj all-chunks (str/join " " current-chunk))
        all-chunks)
      (let [node (first nodes)
            node-tokens (count-tokens node)
            new-total (+ current-tokens node-tokens)]
        (if (and (> new-total max-tokens) (seq current-chunk))
          ;; Chunk cheio, salva e inicia novo
          (recur (rest nodes)
                 [node]
                 node-tokens
                 (conj all-chunks (str/join " " current-chunk)))
          ;; Adiciona ao chunk atual
          (recur (rest nodes)
                 (conj current-chunk node)
                 new-total
                 all-chunks))))))

(defn preprocess-chunks
  "Limpa e prepara os chunks de texto"
  [chunks]
  (let [processed (map #(-> %
                            (str/replace #"\s+" " ")
                            (str/trim))
                       chunks)]
    (println "Primeiro chunk processado:" (first processed))
    processed))
```

Este trecho de c√≥digo implementa a parte de processamento de documentos do nosso sistema RAG. Basicamente, ele pega arquivos Markdown ou HTML e extrai o texto puro deles para que possamos usar depois na busca sem√¢ntica. O c√≥digo usa bibliotecas como `markdown-to-hiccup` e `hickory` para converter os documentos em estruturas de dados que facilitam a extra√ß√£o do texto.

```mermaid
graph TD
    A[Documento] --> B{√â Markdown?}
    B -->|Sim| C[Processa Markdown]
    B -->|N√£o| D[Processa HTML]
    C --> E[Extrai Texto]
    D --> E
    E --> F[Divide em Chunks]
    F --> G[Limpa e Formata]
    G --> H[Chunks Prontos]
```

O fluxo √© bem direto: primeiro verificamos se estamos lidando com Markdown ou HTML, depois extra√≠mos o texto usando a fun√ß√£o apropriada, dividimos em peda√ßos menores (chunks) baseados em tokens reais (n√£o caracteres), e finalmente limpamos esses chunks removendo espa√ßos extras e formatando tudo direitinho. 

O c√≥digo tamb√©m inclui bastante logging para ajudar a depurar o processo, mostrando informa√ß√µes como o tamanho do documento, quantidade de texto extra√≠do e n√∫mero de chunks gerados. 

Essa abordagem de dividir o texto em peda√ßos menores √© crucial para o RAG, j√° que permite processar documentos grandes sem sobrecarregar o modelo de linguagem.

> **Importante**: Dividimos o texto em chunks usando tokens (n√£o caracteres) para n√£o ultrapassar o limite do modelo. A contagem de tokens √© aproximada. Para produ√ß√£o, use uma biblioteca como [clojure-tiktoken](https://github.com/justone/clojure-tiktoken) para maior precis√£o.

#### Sistema de Embeddings

Agora vamos implementar o sistema de embeddings. Ele √© respons√°vel por criar embeddings para o texto para que possamos usar na busca sem√¢ntica.

```clojure
;; src/docai/embedding.clj
(ns docai.embedding
  (:require [clojure.string :as str]
            [clojure.core.memoize :as memo]))

;; Implementa√ß√£o de embeddings usando TF-IDF simples
;; N√£o depende de modelos externos, ao contr√°rio do Ollama que usa o deepseek-r1 para o LLM

(defn tokenize
  "Divide o texto em tokens"
  [text]
  (if (string? text)
    (-> text
        str/lower-case
        (str/split #"\s+")
        (->> (filter #(> (count %) 2))))
    []))

(defn term-freq
  "Calcula a frequ√™ncia dos termos"
  [tokens]
  (frequencies tokens))



(defn doc-freq
  "Calcula a frequ√™ncia dos documentos"
  [docs]
  (let [string-docs (filter string? docs)  ; Use Clojure's built-in string? function
        _ (println (str "Processando " (count string-docs) " documentos v√°lidos de " (count docs) " total"))
        doc-tokens (map tokenize string-docs)  
        all-tokens (distinct (flatten doc-tokens))
        doc-count (count string-docs)]
    (if (zero? doc-count)
      {}
      (zipmap all-tokens
              (map #(count (filter (fn [tokens] (some #{%} tokens)) doc-tokens))
                   all-tokens)))))

(defn tf-idf
  "Calcula TF-IDF para um documento"
  [doc doc-freq doc-count]
  (if (empty? doc-freq)
    {}
    (let [tokens (tokenize doc)
          tf (term-freq tokens)]
      (zipmap (keys tf)
              (map #(* (get tf %) (Math/log (/ doc-count (get doc-freq % 1))))
                   (keys tf))))))

(defn vectorize
  "Converte um documento em um vetor TF-IDF"
  [doc doc-freq doc-count vocab]
  (let [tf-idf-scores (tf-idf doc doc-freq doc-count)]
    (if (empty? vocab)
      []
      (map #(get tf-idf-scores % 0.0) vocab))))

(defn create-embeddings
  "Gera embeddings para uma lista de textos usando TF-IDF"
  [texts]
  (try
    (let [doc-freq (doc-freq texts)
          doc-count (count (filter string? texts))
          ;; Vocabul√°rio ordenado para garantir ordem est√°vel
          vocab (sort (keys doc-freq))]
      (map #(vectorize % doc-freq doc-count vocab) texts))
    (catch Exception e
      (println "Erro ao criar embeddings: " (.getMessage e))
      (vec (repeat (count texts) [])))))

(defn cosine-similarity
  "Calcula a similaridade do cosseno entre dois vetores"
  [v1 v2]
  (if (or (empty? v1) (empty? v2))
    0.0
    (let [dot-product (reduce + (map * v1 v2))
          norm1 (Math/sqrt (reduce + (map #(* % %) v1)))
          norm2 (Math/sqrt (reduce + (map #(* % %) v2)))]
      (if (or (zero? norm1) (zero? norm2))
        0.0
        (/ dot-product (* norm1 norm2))))))

(defn similarity-search
  "Encontra os N chunks mais similares"
  [query-embedding doc-embeddings n]
  (if (or (empty? query-embedding) (empty? doc-embeddings))
    (take (min n (count doc-embeddings)) (range))
    (let [scores (map #(cosine-similarity query-embedding %) doc-embeddings)]
      (->> (map vector scores (range))
           (sort-by first >)
           (take n)
           (map second)))))
```

O c√≥digo acima implementa um sistema simples de embeddings usando TF-IDF (Term Frequency-Inverse Document Frequency) para transformar textos em vetores num√©ricos. 

Basicamente, ele pega documentos de texto, quebra em palavras (tokens), calcula a import√¢ncia de cada palavra considerando tanto sua frequ√™ncia no documento quanto sua raridade na cole√ß√£o inteira, e cria vetores que representam cada documento. √â como transformar textos em coordenadas matem√°ticas para que o computador possa entender a "semelhan√ßa" entre eles.

```mermaid
graph TD
    A[Documentos] -->|Tokeniza√ß√£o| B[Tokens]
    B -->|TF-IDF| C[Vetores Num√©ricos]
    C -->|Similaridade do Cosseno| D[Documentos Similares]
```

A parte mais legal √© a fun√ß√£o `similarity_search`, que usa a similaridade do cosseno para encontrar documentos parecidos com uma consulta. Imagine que cada documento √© um ponto num espa√ßo multidimensional ‚Äì quanto menor o √¢ngulo entre dois pontos, mais similares eles s√£o.

O c√≥digo n√£o usa nenhum modelo de IA sofisticado para isso, apenas matem√°tica b√°sica, o que o torna leve e r√°pido, embora menos poderoso que embeddings modernos baseados em redes neurais. √â como um GPS simples que te leva ao destino sem todos os recursos de um Google Maps.

O TF-IDF transforma textos em vetores num√©ricos ao combinar a frequ√™ncia de cada palavra em um documento (TF) com o quanto essa palavra √© rara em toda a cole√ß√£o (IDF): palavras comuns como "linguagem" t√™m peso baixo, enquanto termos mais exclusivos como "Clojure" ganham peso alto, permitindo que o computador compare documentos de forma eficiente e encontre os mais relevantes para cada consulta.

Outra abordagem, √© por meio da similaridade do cosseno, que compara dois vetores TF-IDF calculando o √¢ngulo entre eles: quanto menor o √¢ngulo, mais parecidos s√£o os textos, usando a f√≥rmula cos(Œ∏) = (A¬∑B) / (||A|| ||B||), onde A¬∑B √© o produto escalar e ||A|| e ||B|| s√£o os tamanhos dos vetores; por√©m, o TF-IDF tem limita√ß√µes, pois n√£o entende sin√¥nimos, contexto ou ordem das palavras, tratando termos como "carro" e "autom√≥vel" como diferentes e podendo gerar vetores grandes.

> **Importante**: Esta implementa√ß√£o TF-IDF √© uma **prova de conceito** para demonstrar os fundamentos do RAG. Em aplica√ß√µes reais, embeddings densos modernos como [SBERT](https://www.sbert.net/), [E5](https://huggingface.co/intfloat/e5-large), [BGE](https://huggingface.co/BAAI/bge-large-en) ou modelos via Ollama superam significativamente o TF-IDF em tarefas de busca sem√¢ntica e question-answering.

#### Interface com Ollama

Agora vamos implementar a interface com o Ollama. Ele √© respons√°vel por gerar a resposta para a query do usu√°rio (essa parte aqui √© super divertida, pois √© onde vamos usar o LLM).

```clojure
;; src/docai/llm.clj
(ns docai.llm
  (:require [clojure.data.json :as json]
            [org.httpkit.client :as http]))

(def ollama-url "http://localhost:11434/api/generate")
(def model-name "deepseek-r1") ; Modelo DeepSeek para melhor qualidade

(defn call-ollama-api
  "Chama a API do Ollama para gerar uma resposta"
  [prompt]
  (let [request-body {:model model-name
                      :prompt prompt
                      :stream false}
        options {:headers {"Content-Type" "application/json"}
                 :body (json/write-str request-body)}
        response @(http/post ollama-url options)]
    (if (= (:status response) 200)
      (-> response
          :body
          (json/read-str :key-fn keyword)
          ;; Compat√≠vel com vers√µes antigas (:response) e novas (:message) do Ollama
          (#(or (:response %) (:message %))))
      (str "Erro ao chamar a API do Ollama: " (:status response) " - " (:body response)))))

;; Fun√ß√µes de utilidade para uso futuro:
;;
;; extract-code-blocks: Extrai blocos de c√≥digo do texto usando regex
;; exemplo de uso:
;;   (extract-code-blocks "```clojure\n(+ 1 2)\n```") => ["(+ 1 2)"]
;;
;; extract-summary: Cria um resumo de texto com tamanho m√°ximo especificado
;; exemplo de uso:
;;   (extract-summary "# T√≠tulo\nConte√∫do longo..." 50) => "Conte√∫do longo..."

(defn format-prompt
  "Formata o prompt para o LLM com delimita√ß√£o segura do contexto"
  [context query]
  (str "Voc√™ √© um assistente especializado em documenta√ß√£o t√©cnica. "
       "Use APENAS as informa√ß√µes do contexto fornecido para responder.\n\n"
       "DOCUMENTO:\n"
       "```\n"
       context
       "\n```\n\n"
       "Pergunta: " query
       "\n\n"
       "Instru√ß√µes:\n"
       "- Responda baseado APENAS no contexto fornecido\n"
       "- Se a informa√ß√£o n√£o estiver no contexto, indique claramente\n"
       "- Forne√ßa exemplos de c√≥digo quando relevante\n"
       "- Se o contexto for limitado, mencione essa limita√ß√£o\n"
       "- N√ÉO invente informa√ß√µes que n√£o est√£o no contexto"))

(defn generate-response
  "Gera resposta usando o LLM com base no contexto"
  [query context]
  (try
    (let [prompt (format-prompt context query)]
      (println "DEBUG - Enviando prompt para o Ollama usando o modelo" model-name)
      (call-ollama-api prompt))
    (catch Exception e
      (str "Erro ao gerar resposta: " (.getMessage e) 
           "\n\nPor favor, verifique se o Ollama est√° em execu√ß√£o no endere√ßo " 
           ollama-url 
           "\n\nVoc√™ pode iniciar o Ollama com o comando: ollama serve"))))

;; Exemplo de prompt seguro gerado:
;; Voc√™ √© um assistente especializado em documenta√ß√£o t√©cnica. 
;; Use APENAS as informa√ß√µes do contexto fornecido para responder.
;;
;; DOCUMENTO:
;; ```
;; [contexto aqui]
;; ```
;;
;; Pergunta: [pergunta do usu√°rio]
;;
;; Instru√ß√µes:
;; - Responda baseado APENAS no contexto fornecido
;; - Se a informa√ß√£o n√£o estiver no contexto, indique claramente
;; - Forne√ßa exemplos de c√≥digo quando relevante
;; - Se o contexto for limitado, mencione essa limita√ß√£o
;; - N√ÉO invente informa√ß√µes que n√£o est√£o no contexto
```

A parte mais importante aqui √© a fun√ß√£o `call-ollama-api`, que faz uma requisi√ß√£o HTTP para o servidor Ollama rodando na m√°quina local. Ela envia um prompt de texto e recebe de volta a resposta gerada pelo modelo DeepSeek R1. O c√≥digo tamb√©m inclui uma fun√ß√£o `format-prompt` super importante, que estrutura a mensagem enviada ao modelo. 

Ela combina o contexto (os trechos de documenta√ß√£o relevantes que encontramos) com a pergunta do usu√°rio, e adiciona instru√ß√µes espec√≠ficas para o modelo se comportar como um assistente t√©cnico. Essa "engenharia de prompt" √© crucial para obter respostas de qualidade - estamos essencialmente ensinando o modelo a responder no formato que queremos.

A fun√ß√£o `generate-response` amarra tudo isso, pegando a pergunta e o contexto, formatando o prompt, enviando para o Ollama e tratando poss√≠veis erros. Tem at√© uma mensagem amig√°vel caso o Ollama n√£o esteja rodando, sugerindo como iniciar o servi√ßo. √â um exemplo cl√°ssico de como interfaces com LLMs funcionam: voc√™ prepara um prompt bem estruturado, envia para o modelo, e recebe de volta texto gerado que (esperamos!) responda √† pergunta original com base no contexto fornecido.


#### M√≥dulo Principal

Agora vamos implementar o m√≥dulo principal que vai ser o ponto de entrada para o RAG. Ele vai ser respons√°vel por carregar os documentos, processar os chunks, criar os embeddings e gerar a resposta para a query do usu√°rio. 

```clojure
;; src/docai/core.clj
(ns docai.core
  (:require [docai.document :as doc]
            [docai.embedding :as emb]
            [docai.llm :as llm]
            [clojure.java.io :as io]
            [clojure.string :as str])
  (:gen-class))

(def docs-path "resources/docs")

(defn load-documentation
  "Carrega todos os arquivos de documenta√ß√£o do diret√≥rio"
  []
  (->> (file-seq (io/file docs-path))
       (filter #(.isFile %))
       (map #(.getPath %))))

(defn setup-knowledge-base
  "Configura a base de conhecimento inicial"
  []
  (let [doc-files (load-documentation)
        _ (when (empty? doc-files)
            (println "Aviso: Nenhum arquivo de documenta√ß√£o encontrado em resources/docs/"))
        _ (doseq [file doc-files]
            (println "Arquivo encontrado:" file))
        all-chunks (mapcat doc/extract-text doc-files)
        processed-chunks (doc/preprocess-chunks all-chunks)
        _ (println (str "Processando " (count processed-chunks) " chunks de texto..."))
        _ (when (< (count processed-chunks) 5)
            (println "DEBUG - Primeiros chunks:")
            (doseq [chunk (take 5 processed-chunks)]
              (println (str "Chunk: '" (subs chunk 0 (min 50 (count chunk))) "...'"))))
        doc-freq (emb/doc-freq processed-chunks)
        doc-count (count (filter string? processed-chunks))
        ;; Vocabul√°rio ordenado para garantir ordem est√°vel entre execu√ß√µes
        vocab (sort (keys doc-freq))
        embeddings (map #(emb/vectorize % doc-freq doc-count vocab) processed-chunks)]
          {:chunks processed-chunks
       :embeddings embeddings
       :doc-freq doc-freq
       :doc-count doc-count
            :vocab vocab  ; Persistir vocabul√°rio ordenado
     :original-files doc-files}))

;; Fun√ß√£o para for√ßar rec√°lculo (√∫til para desenvolvimento)
(defn force-recalculate-kb []
  (let [kb-file "resources/knowledge-base.json"]
    (when (.exists (io/file kb-file))
      (.delete (io/file kb-file)))
  (setup-knowledge-base))

(defn get-file-content
  "L√™ o conte√∫do completo de um arquivo"
  [file-path]
  (try
    (slurp file-path)
    (catch Exception _
      (println "Erro ao ler arquivo:" file-path)
      "")))

(defn get-limited-fallback-content
  "Obt√©m conte√∫do limitado para fallback (evita estourar contexto)"
  [file-path]
  (try
    (let [content (slurp file-path)
          max-chars 8000  ; Limite de ~8KB para evitar estourar contexto
          limited-content (if (> (count content) max-chars)
                           (str (subs content 0 max-chars) 
                                "\n\n[Conte√∫do truncado - arquivo muito grande]")
                           content)]
      (str "Informa√ß√µes limitadas da documenta√ß√£o:\n\n" limited-content))
    (catch Exception _
      (println "Erro ao ler arquivo para fallback:" file-path)
      "N√£o foi poss√≠vel acessar a documenta√ß√£o.")))

(defn query-rag
  "Processa uma query usando o pipeline RAG"
  [knowledge-base query]
  (cond
    (str/blank? query)
    "Por favor, digite uma pergunta v√°lida."
    
    (and (seq (:chunks knowledge-base)) 
         (seq (:embeddings knowledge-base)))
    (let [query-emb (emb/vectorize query (:doc-freq knowledge-base) (:doc-count knowledge-base) (:vocab knowledge-base))
          similar-idxs (emb/similarity-search query-emb 
                                            (:embeddings knowledge-base)
                                            3)
          _ (println "DEBUG - √çndices similares:" similar-idxs)
          
          ;; Obter contexto relevante
          context-chunks (->> similar-idxs
                              (map #(nth (:chunks knowledge-base) %))
                              (str/join "\n\n"))
          
          ;; Se n√£o houver chunks relevantes, use fallback inteligente
          context (if (str/blank? context-chunks)
                    (if (seq (:original-files knowledge-base))
                      (get-limited-fallback-content (first (:original-files knowledge-base)))
                      "N√£o foi poss√≠vel encontrar informa√ß√µes relevantes.")
                    context-chunks)]
      
      (println "DEBUG - Tamanho do contexto:" (count context) "caracteres")
      (println "DEBUG - Amostra do contexto:" (subs context 0 (min 200 (count context))) "...")
      
      ;; Gerar resposta usando o LLM
      (llm/generate-response query context))
    
    :else
    "N√£o foi poss√≠vel encontrar informa√ß√µes relevantes na base de conhecimento."))

(defn -main
  "Fun√ß√£o principal que inicializa a aplica√ß√£o DocAI"
  [& _]
  (println "Inicializando DocAI...")
  
  ;; Verificar se o Ollama est√° acess√≠vel
  (println "Para usar o Ollama, certifique-se de que ele est√° em execu√ß√£o com o comando: ollama serve")
  (println "Usando o modelo deepseek-r1. Se voc√™ ainda n√£o o baixou, execute: ollama pull deepseek-r1")
  
  (let [kb (setup-knowledge-base)]
    (println "Base de conhecimento pronta! Fa√ßa sua pergunta:")
    (try
      (loop []
        (when-let [input (read-line)]
          (cond
            (= input "sair") 
            (println "Obrigado por usar o DocAI. At√© a pr√≥xima!")
            
            (str/blank? input)
            (do
              (println "Digite uma pergunta ou 'sair' para terminar.")
              (recur))
            
            :else
            (do
              (println "Processando...")
              (println (query-rag kb input))
              (println "\nPr√≥xima pergunta (ou 'sair' para terminar):")
              (recur)))))
      (catch Exception e
        (println "Erro: " (.getMessage e))
        (println "Detalhes: " (ex-data e))))
    (println "Obrigado por usar o DocAI. At√© a pr√≥xima!")))
```

Basicamente, quando voc√™ faz uma pergunta, o sistema primeiro transforma sua pergunta em n√∫meros (embeddings) e depois procura nos documentos quais partes s√£o mais parecidas com o que voc√™ perguntou. 

√â como se ele estivesse destacando os trechos mais relevantes de um livro para responder sua d√∫vida. Voc√™ pode ver isso acontecendo quando ele imprime os "√≠ndices similares" no console - s√£o as posi√ß√µes dos peda√ßos de texto que ele achou mais √∫teis.

Depois de encontrar os trechos relevantes, o sistema junta tudo em um "contexto" - que √© basicamente um resumo das informa√ß√µes importantes. Se ele n√£o achar nada parecido com sua pergunta, ele tenta usar o documento inteiro ou avisa que n√£o tem informa√ß√£o suficiente. 

D√° para ver que ele √© bem transparente, mostrando no console o tamanho do contexto e at√© uma amostra do que encontrou, para voc√™ entender o que est√° acontecendo nos bastidores.

Por fim, ele passa sua pergunta original junto com o contexto encontrado para o modelo de linguagem (LLM) gerar uma resposta personalizada. √â como dar a um especialista tanto a sua pergunta quanto as p√°ginas relevantes de um manual t√©cnico - assim ele pode dar uma resposta muito mais precisa e fundamentada. 

Todo esse processo acontece em segundos, permitindo que voc√™ tenha uma conversa fluida com seus documentos, como se estivesse conversando com algu√©m que leu tudo e est√° pronto para responder suas d√∫vidas espec√≠ficas. 

---

## Como Usar

Abaixo um guia para voc√™ instalar e usar o DocAI (e ver o processo em a√ß√£o).

### Instala√ß√£o do Ollama

1. **Instala√ß√£o**:
   - **Windows**: Baixe o instalador do [site oficial do Ollama](https://ollama.com/download) e execute-o
   - **Linux**: Execute o comando:
     ```bash
     curl https://ollama.ai/install.sh | sh
     ```
   - **macOS**: Use o Homebrew:
     ```bash
     brew install ollama
     ```

2. **Iniciando o Servidor**:
   ```bash
   ollama serve
   ```

3. **Baixando o Modelo**:
   ```bash
   ollama pull deepseek-r1
   ```

4. **Verificando a Instala√ß√£o**:
   - Execute um teste simples:
     ```bash
     ollama run deepseek-r1 "Ol√°! Como voc√™ est√°?"
     ```
   - Se tudo estiver funcionando, voc√™ receber√° uma resposta do modelo

> **Dica**: O Ollama mant√©m os modelos em cache local. Se voc√™ precisar liberar espa√ßo, pode usar `ollama rm deepseek-r1` para remover o modelo.

### Executando a Aplica√ß√£o

1. Coloque seus documentos na pasta `resources/docs/` (j√° inclu√≠mos dois exemplos: `example.md`)
2. Execute o projeto:

```bash
lein run
```

3. Fa√ßa suas perguntas! Exemplo:

```bash
Como implementar autentica√ß√£o JWT em Clojure?
Como implementar auth saml em python?
Como integrar o auth0 em uma aplica√ß√£o Clojure?
etc...
``` 	

O DocAI processa sua pergunta em v√°rias etapas:

```mermaid
flowchart TD
    A[In√≠cio] --> B[Carrega Documenta√ß√£o]
    B --> C[Processa Documentos]
    C --> D[Gera Embeddings]
    D --> E[Base de Conhecimento]
    
    F[Consulta do Usu√°rio] --> G[Processa Consulta]
    G --> H[Gera Embedding da Consulta]
    H --> I[Busca Similaridade]
    I --> J[Seleciona Chunks Relevantes]
    J --> K[Combina Contexto]
    K --> L[Gera Prompt]
    L --> M[LLM DeepSeek R1]
    M --> N[Resposta Final]
    
    E --> I
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style N fill:#9ff,stroke:#333,stroke-width:2px
```

1. **Processamento da Consulta**: A pergunta √© convertida em um vetor TF-IDF
2. **Busca por Similaridade**: O sistema encontra os chunks mais relevantes
3. **Gera√ß√£o de Contexto**: Os chunks s√£o combinados em um contexto coeso
4. **Gera√ß√£o de Resposta**: O LLM gera uma resposta baseada no contexto

Voc√™ pode ver o processo em a√ß√£o nos logs:

```bash
DEBUG - Processando query: Como implementar autentica√ß√£o JWT em Clojure?
DEBUG - √çndices similares: [2, 5, 8]
DEBUG - Tamanho do contexto: 1234 caracteres
DEBUG - Amostra do contexto: "Para implementar autentica√ß√£o JWT em Clojure..."
```

> **NOTA:** A prop√≥sito, o projeto docai est√° dispon√≠vel no [https://github.com/scovl/docai](https://github.com/scovl/docai) caso voc√™ queira contribuir com o projeto ou usar em outro projeto.

---

## Considera√ß√µes T√©cnicas

### Performance e Otimiza√ß√µes

Nossa implementa√ß√£o atual oferece uma base funcional, mas pode ser significativamente otimizada em termos de performance atrav√©s da ado√ß√£o de bancos de dados vetoriais como [Milvus](https://milvus.io/) ou [FAISS](https://github.com/facebookresearch/faiss), implementa√ß√£o de cache de embeddings e paraleliza√ß√£o do processamento de chunks, permitindo consultas mais r√°pidas mesmo com grandes volumes de dados.

Para lidar com documenta√ß√µes extensas, recomendo estrat√©gias espec√≠ficas de gerenciamento de mem√≥ria, como o processamento de chunks em lotes menores, implementa√ß√£o de indexa√ß√£o incremental que constr√≥i a base de conhecimento gradualmente, e utiliza√ß√£o de t√©cnicas de streaming para processar arquivos grandes sem sobrecarregar a mem√≥ria dispon√≠vel.

Quanto √† escolha de modelos no ecossistema Ollama, cada um apresenta caracter√≠sticas distintas que podem ser exploradas conforme a necessidade: o [DeepSeek R1](https://ollama.com/models/deepseek-r1) destaca-se na compreens√£o geral e gera√ß√£o de texto, o [DeepSeek Coder](https://ollama.com/models/deepseek-coder) √© especializado em c√≥digo, o [Llama 3](https://ollama.com/models/llama3) serve como excelente alternativa geral, o [Mistral](https://ollama.com/models/mistral) demonstra efic√°cia em tarefas espec√≠ficas, enquanto o [Gemma](https://ollama.com/models/gemma) oferece uma solu√ß√£o leve e eficiente para ambientes com recursos limitados.

Outra quest√£o importante √© como estou tratando os erros. O sistema implementa v√°rias camadas de tratamento de erros para lidar com diferentes cen√°rios:

1. **Ollama Offline**
   - **Sintoma**: O sistema n√£o consegue se conectar ao servidor Ollama
   - **Tratamento**: O c√≥digo verifica a disponibilidade do servidor e fornece mensagens claras de erro:
   ```clojure
   (catch Exception e
     (str "Erro ao gerar resposta: " (.getMessage e) 
          "\n\nPor favor, verifique se o Ollama est√° em execu√ß√£o no endere√ßo " 
          ollama-url 
          "\n\nVoc√™ pode iniciar o Ollama com o comando: ollama serve"))
   ```

2. **Documenta√ß√£o Muito Grande**
   - **Sintoma**: Arquivos de documenta√ß√£o que excedem a mem√≥ria dispon√≠vel
   - **Tratamento**: O sistema implementa:
     - Chunking de documentos (512 tokens por chunk)
     - Processamento em lotes
     - Logs de progresso para monitoramento
   ```clojure
   (let [content (slurp doc-path)
         chunks (partition-all 512 text)]
     (println "Quantidade de chunks gerados:" (count chunks)))
   ```

3. **Consultas sem Rela√ß√£o com a Documenta√ß√£o**
   - **Sintoma**: Nenhum chunk relevante √© encontrado para a consulta
   - **Tratamento**: O sistema:
     - Verifica se h√° chunks dispon√≠veis
     - Usa fallback para conte√∫do original se necess√°rio
     - Fornece feedback claro ao usu√°rio
   ```clojure
   (if (str/blank? context-chunks)
     (if (seq (:original-files knowledge-base))
       (get-file-content (first (:original-files knowledge-base)))
       "N√£o foi poss√≠vel encontrar informa√ß√µes relevantes.")
     context-chunks)
   ```

4. **Melhorias Futuras** - Implementar [retry com backoff exponencial](https://en.wikipedia.org/wiki/Exponential_backoff) para falhas de conex√£o, adicionar [cache de embeddings](https://en.wikipedia.org/wiki/Cache_(computing)) para melhor performance, implementar [streaming](https://en.wikipedia.org/wiki/Streaming_media) para arquivos muito grandes, adicionar [valida√ß√£o de formato de documentos](https://en.wikipedia.org/wiki/Document_validation) e implementar [rate limiting](https://en.wikipedia.org/wiki/Rate_limiting) para evitar sobrecarga do Ollama.

---

### Melhorando os Prompts

Para obter melhores respostas do sistema RAG, voc√™ pode usar prompts mais estruturados:

```clojure
(defn format-advanced-prompt
  "Prompt otimizado com diretrizes claras"
  [context query]
  (str "Voc√™ √© um especialista em documenta√ß√£o t√©cnica de software.\n\n"
       "DOCUMENTO:\n```\n" context "\n```\n\n"
       "Pergunta: " query "\n\n"
       "Diretrizes:\n"
       "1. Use APENAS informa√ß√µes do contexto fornecido\n"
       "2. Seja preciso e t√©cnico\n"
       "3. Inclua exemplos de c√≥digo quando relevante\n"
       "4. Se a informa√ß√£o n√£o estiver no contexto, indique claramente\n"
       "5. Use formata√ß√£o Markdown para melhor legibilidade"))
```

> Para t√©cnicas avan√ßadas de prompt engineering, consulte o [Guia Completo de Prompt Engineering](https://www.promptingguide.ai/).

## Pr√≥ximos Passos

Abaixo uma lista de melhorias que podem ser feitas no projeto atual.

### Melhorias R√°pidas (Implementa√ß√£o Imediata)

#### **1. Persist√™ncia da Base de Conhecimento**
```clojure
;; src/docai/persistence.clj
(ns docai.persistence
  (:require [clojure.data.json :as json]
            [clojure.edn :as edn]))

(defn calculate-checksum
  "Calcula checksum dos arquivos de documenta√ß√£o"
  [doc-files]
  (let [checksums (map #(hash (slurp %)) doc-files)]
    (hash checksums)))

(defn save-knowledge-base
  "Salva a base de conhecimento em disco com checksum"
  [kb filename]
  (let [doc-files (:original-files kb)
        checksum (calculate-checksum doc-files)
        serializable-kb (-> kb
                           (select-keys [:chunks :embeddings :doc-freq :doc-count :vocab])
                           (assoc :checksum checksum :doc-files doc-files))]
    (spit filename (json/write-str serializable-kb))))

(defn load-knowledge-base
  "Carrega a base de conhecimento do disco com verifica√ß√£o de mudan√ßas"
  [filename doc-files]
  (try
    (let [content (slurp filename)
          data (json/read-str content :key-fn keyword)
          cached-checksum (:checksum data)
          current-checksum (calculate-checksum doc-files)]
      (if (= cached-checksum current-checksum)
        (do
          (println "Cache v√°lido - carregando embeddings...")
          (assoc data :original-files doc-files))
        (do
          (println "Arquivos modificados - recalculando embeddings...")
          nil)))
    (catch Exception e
      (println "Erro ao carregar KB:" (.getMessage e))
      nil)))

;; Uso no core.clj
(defn setup-knowledge-base
  "Configura a base de conhecimento (com cache inteligente)"
  []
  (let [kb-file "resources/knowledge-base.json"
        doc-files (load-documentation)]
    (if (.exists (io/file kb-file))
      (if-let [cached-kb (load-knowledge-base kb-file doc-files)]
        cached-kb
        (do
          (println "Recriando KB devido a mudan√ßas nos arquivos...")
          (let [kb (create-knowledge-base)]
            (save-knowledge-base kb kb-file)
            kb)))
      (do
        (println "Criando nova KB...")
        (let [kb (create-knowledge-base)]
          (save-knowledge-base kb kb-file)
          kb)))))
```

#### **2. Testes Unit√°rios**
```clojure
;; test/docai/embedding_test.clj
(ns docai.embedding-test
  (:require [clojure.test :refer :all]
            [docai.embedding :as emb]))

(deftest test-tokenize
  (testing "Tokeniza√ß√£o b√°sica"
    (is (= ["hello" "world"] (emb/tokenize "Hello World!")))
    (testing "Filtra palavras curtas"
      (is (= [] (emb/tokenize "a b c")))))

(deftest test-tf-idf
  (testing "C√°lculo TF-IDF"
    (let [doc "hello world hello"
          doc-freq {"hello" 2 "world" 1}
          doc-count 2
          result (emb/tf-idf doc doc-freq doc-count)]
      (is (contains? result "hello"))
      (is (contains? result "world")))))

(deftest test-cosine-similarity
  (testing "Similaridade do cosseno"
    (is (= 1.0 (emb/cosine-similarity [1 0] [1 0])))
    (is (= 0.0 (emb/cosine-similarity [1 0] [0 1])))
    (is (= 0.707 (emb/cosine-similarity [1 1] [1 0]) :delta 0.001))))
```

#### **3. Streaming de Respostas**
```clojure
;; src/docai/streaming.clj
(ns docai.streaming
  (:require [clojure.data.json :as json]
            [org.httpkit.client :as http]))

(defn stream-ollama-response
  "Streaming de resposta do Ollama"
  [prompt]
  (let [url "http://localhost:11434/api/generate"
        request-body {:model "deepseek-r1"
                     :prompt prompt
                     :stream true}]
    (with-open [conn @(http/post url {:body (json/write-str request-body)
                                      :as :stream})]
      (doseq [line (line-seq (:body conn))]
        (when-not (str/blank? line)
          (let [data (json/read-str line :key-fn keyword)]
            ;; Compat√≠vel com vers√µes antigas (:response) e novas (:message) do Ollama
            (when-let [content (or (:response data) (:message data))]
              (print content)
              (flush))))))))
```

#### **4. Cache de Embeddings**
```clojure
;; src/docai/cache.clj
(ns docai.cache
  (:require [clojure.core.cache :as cache]))

;; Cache LRU com limite de mem√≥ria (evita vazamentos)
(def embedding-cache (atom (cache/lru-cache-factory {} :threshold 1000))) ; M√°ximo 1000 embeddings

(defn cached-embedding
  "Embedding com cache LRU"
  [text doc-freq doc-count vocab]
  (if-let [cached (cache/lookup @embedding-cache text)]
    cached
    (let [embedding (emb/vectorize text doc-freq doc-count vocab)]
      (swap! embedding-cache cache/miss text embedding)
      embedding)))

;; Cache para respostas do LLM (tamb√©m LRU)
(def response-cache (atom (cache/lru-cache-factory {} :threshold 500))) ; M√°ximo 500 respostas

(defn cached-llm-response
  "Resposta do LLM com cache LRU"
  [prompt]
  (if-let [cached (cache/lookup @response-cache prompt)]
    cached
    (let [response (llm/call-ollama-api prompt)]
      (swap! response-cache cache/miss prompt response)
      response)))

;; Fun√ß√£o para limpar cache manualmente se necess√°rio
(defn clear-caches []
  (reset! embedding-cache (cache/lru-cache-factory {} :threshold 1000))
  (reset! response-cache (cache/lru-cache-factory {} :threshold 500))
  (println "Caches limpos!"))

;; Monitoramento de cache
(defn cache-stats []
  (let [embedding-size (count @embedding-cache)
        response-size (count @response-cache)]
    (println (str "Embedding cache: " embedding-size "/1000"))
    (println (str "Response cache: " response-size "/500"))))
```

**Vantagens do Cache LRU:**
- **üîÑ Auto-limpeza**: Remove itens menos usados automaticamente
- **üíæ Controle de mem√≥ria**: Limite m√°ximo de itens
- **‚ö° Performance**: Acesso r√°pido a dados frequentes
- **üõ°Ô∏è Estabilidade**: Evita vazamentos de mem√≥ria

**Cache Inteligente de Embeddings:**
- **üìÅ Persist√™ncia**: Embeddings salvos em disco
- **üîç Verifica√ß√£o de Mudan√ßas**: Checksum dos arquivos
- **‚ö° Recarregamento R√°pido**: S√≥ recalcula se necess√°rio
- **üîÑ Invalida√ß√£o Autom√°tica**: Detecta modifica√ß√µes nos arquivos

#### **5. Banco Vetorial Simples (BM25 Manual)**
```clojure
;; src/docai/vector_store.clj
(ns docai.vector-store
  (:require [clojure.string :as str]))

(defn create-simple-vector-store
  "Store vetorial simples com BM25 (implementa√ß√£o manual)"
  [documents]
  (let [index (atom {})
        doc-freq (emb/doc-freq documents)
        vocab (sort (keys doc-freq))]  ; Vocabul√°rio ordenado
    (doseq [[idx doc] (map-indexed vector documents)]
      (let [tokens (emb/tokenize doc)
            tf (emb/term-freq tokens)]
        (swap! index assoc idx {:doc doc :tf tf})))
    {:index index :doc-freq doc-freq :vocab vocab}))

(defn calculate-bm25
  "Calcula score BM25 para um documento"
  [query-tokens doc-tf doc-freq]
  (let [k1 1.2  ; Par√¢metro de satura√ß√£o de termo
        b 0.75   ; Par√¢metro de normaliza√ß√£o de comprimento
        avg-doc-len 100  ; Comprimento m√©dio do documento (aproxima√ß√£o)
        doc-len (reduce + (vals doc-tf))
        
        ;; IDF para cada termo da query
        idf-scores (map (fn [term]
                          (let [df (get doc-freq term 0)
                                n (count doc-freq)]
                            (if (zero? df)
                              0
                              (Math/log (/ (- n df 0.5) (+ df 0.5)))))
                        query-tokens)
        
        ;; TF para cada termo da query no documento
        tf-scores (map (fn [term]
                         (let [tf (get doc-tf term 0)]
                           (/ (* tf (+ k1 1))
                              (+ tf (* k1 (- 1 b (* b (/ doc-len avg-doc-len)))))))
                       query-tokens)]
    
    ;; Soma ponderada de IDF * TF
    (reduce + (map * idf-scores tf-scores))))

(defn search-bm25
  "Busca h√≠brida: BM25 + similaridade sem√¢ntica"
  [query vector-store top-k]
  (let [query-tokens (emb/tokenize query)
        query-embedding (emb/vectorize query (:doc-freq vector-store) (:doc-count vector-store) (:vocab vector-store))
        
        ;; BM25 scores
        bm25-scores (map-indexed 
                      (fn [idx {:keys [tf]}]
                        [idx (calculate-bm25 query-tokens tf (:doc-freq vector-store))])
                      (vals @(:index vector-store)))
        
        ;; Semantic scores
        semantic-scores (map-indexed
                          (fn [idx _]
                            [idx (emb/cosine-similarity query-embedding 
                                                       (emb/vectorize (get-in @(:index vector-store) [idx :doc])
                                                                      (:doc-freq vector-store)
                                                                      (:doc-count vector-store)
                                                                      (:vocab vector-store)))])
                          (vals @(:index vector-store)))
        
        ;; Combine scores (weighted average)
        combined-scores (map (fn [[idx bm25] [idx2 semantic]]
                              [idx (+ (* 0.3 bm25) (* 0.7 semantic))])
                            bm25-scores semantic-scores)]
    
    (->> combined-scores
         (sort-by second >)
         (take top-k)
         (map first))))
```

**Sobre o Algoritmo BM25:**
- **k1 = 1.2**: Controla satura√ß√£o de frequ√™ncia de termos
- **b = 0.75**: Normaliza pelo comprimento do documento
- **IDF**: Mede raridade dos termos na cole√ß√£o
- **TF**: Frequ√™ncia dos termos no documento
- **Combina√ß√£o**: 30% BM25 + 70% similaridade sem√¢ntica

**Nota**: Esta √© uma implementa√ß√£o manual do BM25. Para produ√ß√£o, considere usar Apache Lucene (veja depend√™ncias acima) que oferece BM25 nativo e otimizado.

### Melhorias Avan√ßadas

```mermaid
mindmap
  root((Melhorias))
    Tokeniza√ß√£o
      BPE
      WordPiece
      Tokenizador do Modelo
    Embeddings
      Pr√©-treinados
      Via Ollama
      Cache
    Banco de Dados
      Milvus
      FAISS
      Qdrant
    Cache
      Embeddings
      Respostas
    Erros
      Conex√£o
      Modelo
      Rede
    Logging
      Framework
      Rastreamento
    Testes
      Unit√°rios
      Integra√ß√£o
    Prompt
      Few-shot
      Chain-of-thought
      Formato
```

### Depend√™ncias e Pr√≥ximos Passos

#### **Depend√™ncias Recomendadas**

Para implementar as funcionalidades avan√ßadas mencionadas no artigo, adicione estas depend√™ncias ao `project.clj`:

```clojure
;; Depend√™ncias para produ√ß√£o
[com.github.justone/clojure-tiktoken "0.1.0"]  ; Contagem precisa de tokens
[org.apache.lucene/lucene-core "9.10.0"]       ; Busca textual avan√ßada
[org.apache.lucene/lucene-analyzers-common "9.10.0"]  ; Analisadores de texto
[org.apache.lucene/lucene-queryparser "9.10.0"] ; Parser de queries
[com.github.clojure-lsp/clojure-lsp "2024.01.15-20.32.45"]  ; LSP para IDE
```

#### **Implementa√ß√£o com Lucene**

```clojure
;; src/docai/lucene_store.clj
(ns docai.lucene-store
  (:import [org.apache.lucene.analysis.standard StandardAnalyzer]
           [org.apache.lucene.document Document Field Field$Store]
           [org.apache.lucene.index IndexWriter IndexWriterConfig DirectoryReader]
           [org.apache.lucene.search IndexSearcher QueryParser]
           [org.apache.lucene.store RAMDirectory]))

(defn create-lucene-index
  "Cria √≠ndice Lucene para busca textual"
  [documents]
  (let [analyzer (StandardAnalyzer.)
        directory (RAMDirectory.)
        config (IndexWriterConfig. analyzer)
        writer (IndexWriter. directory config)]
    
    ;; Adiciona documentos ao √≠ndice
    (doseq [[idx doc] (map-indexed vector documents)]
      (let [document (Document.)]
        (.add document (Field. "content" doc Field$Store/YES))
        (.add document (Field. "id" (str idx) Field$Store/YES))
        (.addDocument writer document)))
    
    (.close writer)
    
    {:directory directory
     :analyzer analyzer
     :reader (DirectoryReader/open directory)
     :searcher (IndexSearcher. (DirectoryReader/open directory))}))

(defn search-lucene
  "Busca textual usando Lucene"
  [index query top-k]
  (let [parser (QueryParser. "content" (:analyzer index))
        query-obj (.parse parser query)
        hits (.search (:searcher index) query-obj top-k)]
    (map #(.doc (:searcher index) %) (.scoreDocs hits))))
```

### Upgrade para Embeddings Densos

Para evoluir de TF-IDF para embeddings densos modernos, considere estas op√ß√µes:

#### 1. **Via Ollama Embeddings API**
```clojure
;; Exemplo de upgrade usando Ollama embeddings
(defn create-dense-embeddings [texts]
  (let [embeddings-url "http://localhost:11434/api/embeddings"]
    (map #(call-ollama-embeddings embeddings-url %) texts)))

(defn call-ollama-embeddings [url text]
  (let [request-body {:model "deepseek-r1" :prompt text}
        response @(http/post url {:body (json/write-str request-body)})]
    (if (= (:status response) 200)
      (-> response :body (json/read-str :key-fn keyword) :embedding)
      (throw (ex-info "Erro ao gerar embedding" {:status (:status response)})))))

;; Token counting preciso via Ollama API
(defn count-tokens-ollama [text]
  (let [url "http://localhost:11434/api/generate"
        request-body {:model "deepseek-r1" 
                     :prompt text 
                     :stream false
                     :options {:num_predict 0}}]
    (try
      (let [response @(http/post url {:body (json/write-str request-body)})]
        (if (= (:status response) 200)
          (-> response :body (json/read-str :key-fn keyword) :eval_count)
          0))
      (catch Exception _ 0))))

;; Implementa√ß√£o com clojure-tiktoken (recomendado para produ√ß√£o)
(defn count-tokens-precise [text]
  (try
    ;; Requer: [com.github.justone/clojure-tiktoken "0.1.0"]
    ;; (require '[com.github.justone.clojure-tiktoken :as tiktoken])
    ;; (tiktoken/count-tokens text "cl100k_base")
    (count-tokens text) ; Fallback para implementa√ß√£o atual
    (catch Exception e
      (println "Erro ao usar tiktoken:" (.getMessage e))
      (count-tokens text))))

;; Exemplo de implementa√ß√£o com API do Ollama (mais preciso)
(defn count-tokens-ollama-precise [text]
  (let [url "http://localhost:11434/api/generate"
        request-body {:model "deepseek-r1" 
                     :prompt text 
                     :stream false
                     :options {:num_predict 0}}]
    (try
      (let [response @(http/post url {:body (json/write-str request-body)})]
        (if (= (:status response) 200)
          (-> response :body (json/read-str :key-fn keyword) :eval_count)
          (count-tokens text))) ; Fallback para heur√≠stica
      (catch Exception _
        (count-tokens text))))) ; Fallback para heur√≠stica
```

#### 2. **Via HuggingFace Transformers**
```clojure
;; Exemplo usando interop com Python/HuggingFace
(defn create-hf-embeddings [texts]
  (let [model-name "sentence-transformers/all-MiniLM-L6-v2"]
    ;; Usar interop com Python para carregar modelo
    ;; e gerar embeddings densos
    ))

;; Token counting preciso com tiktoken
(defn count-tokens-tiktoken [text]
  ;; Requer interop com Python tiktoken
  ;; pip install tiktoken
  ;; python -c "import tiktoken; print(len(tiktoken.get_encoding('cl100k_base').encode('texto aqui')))"
  )
```

#### 3. **Compara√ß√£o de Performance**

| M√©todo | Sem√¢ntica | Contexto | Performance | Complexidade | Hardware M√≠nimo | Precis√£o Tokens |
|--------|-----------|----------|-------------|--------------|-----------------|-----------------|
| TF-IDF | ‚ùå | ‚ùå | ‚ö°‚ö°‚ö° | ‚ö° | CPU 4 cores, 8GB RAM | ‚ö†Ô∏è Heur√≠stica |
| Ollama Embeddings | ‚úÖ | ‚úÖ | ‚ö°‚ö° | ‚ö°‚ö° | CPU 8 cores, 16GB RAM | ‚úÖ Preciso |
| SBERT/E5 | ‚úÖ‚úÖ | ‚úÖ‚úÖ | ‚ö° | ‚ö°‚ö°‚ö° | GPU 8GB VRAM, 32GB RAM | ‚úÖ Preciso |

> **Recomenda√ß√£o**: Para aplica√ß√µes em produ√ß√£o, comece com Ollama embeddings (simples de implementar) e evolua para modelos especializados como SBERT ou E5 conforme necess√°rio. Considere seus recursos de hardware ao escolher a abordagem.
> 
> **‚ö†Ô∏è Importante**: A contagem de tokens heur√≠stica pode errar at√© 2x. Para produ√ß√£o, use `count-tokens-ollama-precise` ou `clojure-tiktoken` para precis√£o.

Olha, d√° pra turbinar esse nosso RAG de v√°rias formas! Primeiro, a gente poderia melhorar a tokeniza√ß√£o usando aqueles m√©todos mais avan√ßados tipo [BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding) ou [WordPiece](https://en.wikipedia.org/wiki/WordPiece) - idealmente o mesmo que o modelo usa. 

E os embeddings? Seria muito mais eficiente pegar direto do Ollama em vez de fazer na m√£o. A diferen√ßa na busca sem√¢ntica seria absurda! O TF-IDF que implementamos √© √≥timo para entender os conceitos, mas embeddings densos modernos capturam nuances sem√¢nticas que fazem toda a diferen√ßa em aplica√ß√µes reais.

Quando o projeto crescer, vai ser essencial ter um banco de dados vetorial decente. Imagina lidar com milhares de documentos usando nossa implementa√ß√£o atual? Seria um pesadelo! [Milvus](https://milvus.io/), [FAISS](https://github.com/facebookresearch/faiss) ou [Qdrant](https://qdrant.tech/) resolveriam isso numa boa. E n√£o podemos esquecer do cache - tanto para embeddings quanto para respostas. Economiza um temp√£o e reduz a carga no sistema.

A parte de tratamento de erros e logging tamb√©m precisa de carinho. J√° pensou o usu√°rio esperando resposta e o Ollama t√° offline? Ou um arquivo corrompido? Precisamos de mensagens amig√°veis e um sistema de logging decente pra rastrear problemas. E claro, testes! Sem testes unit√°rios e de integra√ß√£o, qualquer mudan√ßa vira uma roleta-russa.

O prompt engineering √© outro ponto crucial - d√° pra refinar bastante o formato atual. Poder√≠amos experimentar com exemplos no prompt (few-shot), instru√ß√µes passo a passo (chain-of-thought), e ser mais espec√≠fico sobre o formato da resposta.

## Ap√™ndice

### Requisitos de Hardware Detalhados

A performance do sistema RAG depende significativamente do hardware dispon√≠vel. Aqui est√£o as configura√ß√µes recomendadas:

| Componente | M√≠nimo | Recomendado | Alto Desempenho |
|------------|--------|-------------|-----------------|
| **CPU** | 4 cores (Intel i5/AMD Ryzen 5) | 8 cores (Intel i7/AMD Ryzen 7) | 16+ cores (Intel i9/AMD Ryzen 9) |
| **RAM** | 8 GB | 16 GB | 32+ GB |
| **GPU** | Integrada | NVIDIA RTX 3060 (8GB VRAM) | NVIDIA RTX 4090 (24GB VRAM) |
| **VRAM** | - | 8 GB | 16+ GB |
| **Storage** | SSD 256 GB | SSD 512 GB | NVMe 1 TB+ |
| **Rede** | 100 Mbps | 1 Gbps | 10 Gbps |

#### **Configura√ß√µes por Caso de Uso**

**üü¢ Desenvolvimento/Teste**
- CPU: 4 cores, RAM: 8GB
- Modelo: `deepseek-r1` (CPU only)
- Documentos: < 1GB
- Performance: ~2-5 segundos por consulta

**üü° Produ√ß√£o Pequena**
- CPU: 8 cores, RAM: 16GB, GPU: RTX 3060
- Modelo: `deepseek-r1` (GPU)
- Documentos: 1-10GB
- Performance: ~1-3 segundos por consulta

**üî¥ Produ√ß√£o Grande**
- CPU: 16+ cores, RAM: 32GB+, GPU: RTX 4090
- Modelo: `deepseek-r1` + embeddings densos
- Documentos: 10GB+
- Performance: < 1 segundo por consulta

#### **Otimiza√ß√µes por Hardware**

**CPU Only:**
```bash
# Usar modelo otimizado para CPU
ollama pull deepseek-r1:3b  # Vers√£o menor
```

**GPU Dispon√≠vel:**
```bash
# Usar vers√£o completa com acelera√ß√£o GPU
ollama pull deepseek-r1
```

**M√∫ltiplas GPUs:**
```bash
# Distribuir carga entre GPUs
CUDA_VISIBLE_DEVICES=0,1 ollama serve
```

---

## Refer√™ncias

- [RAG](https://www.pinecone.io/learn/rag/) - Documenta√ß√£o do Pinecone
- [Embedding](https://www.pinecone.io/learn/embeddings/) - Documenta√ß√£o do Pinecone
- [LLM](https://www.pinecone.io/learn/llms/) - Documenta√ß√£o do Pinecone
- [Ollama](https://ollama.com/) - Ferramenta para rodar LLMs localmente
- [Clojure](https://clojure.org/) - Documenta√ß√£o do Clojure
- [http-kit](https://github.com/http-kit/http-kit) - Cliente HTTP para Clojure
- [data.json](https://github.com/clojure/data.json) - Biblioteca JSON para Clojure
- [clojure.test](https://clojure.github.io/clojure/clojure.test-api.html) - Documenta√ß√£o da biblioteca de testes do Clojure
- [clj-kondo](https://github.com/clj-kondo/clj-kondo) - Linter para Clojure
