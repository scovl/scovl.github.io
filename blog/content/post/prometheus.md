+++
title = "Prometheus"
description = "Under the hood"
date = 2023-03-21T23:18:18-03:00
tags = ["Prometheus", "Grafana", "Monitoring", "TSDB", "DevOps", "Observability"]
draft = true
weight = 3
+++


O **[Prometheus](https://prometheus.io/)** √© uma ferramenta open-source de monitoramento de sistemas e aplica√ß√µes que revolucionou a forma de pensar observabilidade em ambientes distribu√≠dos. Ele coleta e armazena m√©tricas como s√©ries temporais, ou seja, valores num√©ricos associados a um carimbo de tempo e a pares chave-valor chamados **[labels](https://prometheus.io/docs/concepts/data_model/#metric-names-and-labels)**. A pot√™ncia do Prometheus vem, em parte, da sua linguagem de consulta pr√≥pria, **[PromQL](https://prometheus.io/docs/prometheus/latest/querying/basics/)**, que permite criar consultas complexas para analisar os dados coletados em tempo real. A interface web integrada (Expression browser) facilita visualizar e explorar m√©tricas, possibilitando an√°lises r√°pidas para identificar tend√™ncias e anomalias.

Desenvolvido inicialmente na SoundCloud em 2012 por [Julius Volz](https://github.com/juliusv) e equipe, o Prometheus foi projetado para ser simples, eficiente e altamente dimension√°vel. Em 2016, o projeto foi adotado pela **[Cloud Native Computing Foundation (CNCF)](https://www.cncf.io/)** como o segundo projeto hospedado (logo ap√≥s o [Kubernetes](https://kubernetes.io/)), refor√ßando sua maturidade e ampla ado√ß√£o pela comunidade. Hoje, o Prometheus √© um pilar no ecossistema de observabilidade cloud-native, frequentemente usado em conjunto com o Grafana para visualiza√ß√µes avan√ßadas, formando uma poderosa stack de monitoramento.

> **Nota:** Para um deep dive em PromQL, confira nosso artigo dedicado **[aqui](https://scovl.github.io/2023/03/19/promql/)**.


## √çndice

* **[Tipos de m√©tricas](#tipos-de-m√©tricas)**
* **[Monitoramento pull vs push](#monitoramento-pull-vs-push)**
* **[Arquitetura do Prometheus](#arquitetura-do-prometheus)**
* **[Labels e Samples](#labels-e-samples)**
* **[Instala√ß√£o](#instala√ß√£o)**
  * **[Configura√ß√£o](#configura√ß√£o)**
  * **[Discovery Din√¢mico e Relabeling](#discovery-din√¢mico-e-relabeling)**
  * **[Service Discovery](#service-discovery)**




## Tipos de m√©tricas

O Prometheus suporta quatro tipos principais de m√©tricas:

* **[Counter (Contador)](https://prometheus.io/docs/concepts/metric_types/#counter)**: M√©trica cumulativa que apenas aumenta (ou zera). Indicada para quantificar eventos, como n√∫mero de requisi√ß√µes ou erros. Por exemplo, um contador `http_requests_total` incrementa a cada requisi√ß√£o recebida. Contadores nunca diminuem, exceto quando reiniciados. Consultas comuns envolvem a taxa de aumento usando fun√ß√µes como `rate()` ou `increase()`, calculando, por exemplo, quantas requisi√ß√µes por segundo ocorreram em determinado intervalo.

* **[Gauge (Indicador)](https://prometheus.io/docs/concepts/metric_types/#gauge)**: M√©trica que representa um valor em um instante, podendo tanto aumentar quanto diminuir. Indicado para valores como utiliza√ß√£o de CPU, mem√≥ria ou tamanho de fila ‚Äì que sobem e descem livremente. N√£o possui limite m√≠nimo ou m√°ximo fixo. Fun√ß√µes como `avg_over_time()`, `min()`, `max()` e `sum()` s√£o frequentemente aplicadas sobre gauges para obter m√©dias, m√≠nimos, m√°ximos ou somas ao longo do tempo.

* **[Histogram (Histograma)](https://prometheus.io/docs/concepts/metric_types/#histogram)**: M√©trica que contabiliza a distribui√ß√£o de valores observados em *buckets* (faixas) predefinidos. √â muito utilizada para medir lat√™ncias (e.g., dura√ß√£o de requisi√µes) ou outros valores cuja distribui√ß√£o importa. O Prometheus implementa histogramas atrav√©s de v√°rios contadores ‚Äì um por bucket ‚Äì al√©m de contadores especiais para total de observa√ß√µes (`_count`) e soma dos valores (`_sum`). Consultas tipicamente usam `histogram_quantile()` para extrair percentis a partir dos buckets e fun√ß√µes como `rate()` ou `increase()` nos contadores para ver taxas.

* **[Summary (Sum√°rio)](https://prometheus.io/docs/concepts/metric_types/#summary)**: M√©trica similar ao histograma, mas os c√°lculos de percentis e m√©dias s√£o feitos pelo pr√≥prio alvo instrumentado. O summary fornece diretamente percentis (por exemplo, lat√™ncia p95) e contagens/agregados para um conjunto de observa√ß√µes. Entretanto, summaries t√™m a limita√ß√£o de n√£o poderem ser agregados facilmente entre m√∫ltiplas inst√¢ncias (diferente dos histogramas). Em geral, histogramas s√£o preferidos para m√©tricas de lat√™ncia quando se quer combinar valores de v√°rias fontes, enquanto summaries podem ser √∫teis para percentis muito espec√≠ficos em inst√¢ncias isoladas.

Al√©m desses tipos principais, o Prometheus exp√µe m√©tricas especiais de estado ‚Äì por exemplo, a m√©trica interna `up` indica se um determinado alvo foi coletado com sucesso (valor 1) ou n√£o (0). Essa m√©trica √© muito √∫til para monitorar disponibilidade de servi√ßos: se um **endpoint** monitorado ficar indispon√≠vel, `up{instance="endpoint:porta"} == 0` sinaliza falha. Vale notar que n√£o existe um "tipo" separado para essas m√©tricas de sa√∫de; elas normalmente s√£o gauges (0 ou 1) usadas para esse prop√≥sito.

## Monitoramento pull vs push

Para entender **pull** vs **push**, imagine cuidar de plantas: no modelo **pull** voc√™ vai todo dia verificar se precisam de √°gua; no modelo **push** as pr√≥prias plantas enviam um sinal quando precisam ser regadas. Tecnicamente, no monitoramento **pull** um sistema central (como o Prometheus) consulta periodicamente os alvos para coletar m√©tricas ‚Äì ele "puxa" as informa√ß√µes. J√° no monitoramento **push**, os pr√≥prios alvos enviam (*empurram*) as m√©tricas para um coletor central sem serem solicitados.

![](https://raw.githubusercontent.com/scovl/scovl.github.io/main/post/images/tsdb/prom-pullvspush.png)

No Prometheus, prevalece o modelo pull. O servidor Prometheus periodicamente faz **scrape** (raspagem) dos dados de cada alvo exportador via HTTP, no endpoint padr√£o `/metrics`. Cada scrape coleta o valor atual de todas as s√©ries expostas naquele alvo. Os alvos podem ser aplica√ß√µes instrumentadas que exp√µem suas m√©tricas diretamente, ou **exporters** (exportadores) que traduzem m√©tricas de sistemas externos para o formato do Prometheus. Assim, o Prometheus obt√©m em intervalos regulares (por padr√£o a cada 15s) as m√©tricas atuais de cada servi√ßo, armazenando-as localmente.

Na imagem acima, a compara√ß√£o dos modelos de coleta: √† esquerda, no modo push os clientes enviam suas m√©tricas proativamente a um gateway; √† direita, no modo pull o Prometheus consulta cada cliente periodicamente. O modelo pull tem vantagens em simplicidade e confiabilidade ‚Äì se um servi√ßo cair, o Prometheus sabe (a m√©trica `up` fica 0) e n√£o depende de buffers intermedi√°rios. J√° o modelo push pode ser √∫til para casos espec√≠ficos, como *jobs* de curta dura√ß√£o ou ambientes onde n√£o √© poss√≠vel expor um endpoint (nesses casos usa-se o **Pushgateway**, discutido adiante). Em suma, o Prometheus, por padr√£o, **n√£o** recebe m√©tricas ativamente; ele mesmo vai colet√°-las, evitando sobrecarga nos aplicativos monitorados e detectando automaticamente indisponibilidades.

## Arquitetura do Prometheus

A arquitetura do Prometheus foi concebida para facilitar a coleta de dados de m√∫ltiplas fontes de forma confi√°vel e distribu√≠da. O cora√ß√£o do sistema √© o **[Prometheus Server](https://prometheus.io/docs/prometheus/latest/components/prometheus/)** principal, respons√°vel por agendar e realizar as coletas (*scrapes*) de cada alvo monitorado e armazenar as s√©ries temporais resultantes localmente. A configura√ß√£o dessas coletas √© definida em um arquivo YAML (geralmente `prometheus.yml`), especificando **[jobs](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#job_name)** e **[targets](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#static_configs)** ‚Äì por exemplo, "coletar m√©tricas do servi√ßo X na URL Y a cada 15 segundos". A figura abaixo (extra√≠da da documenta√ß√£o oficial) ilustra a arquitetura e os componentes do ecossistema Prometheus:

![](https://raw.githubusercontent.com/scovl/scovl.github.io/refs/heads/main/blog/content/post/images/tsdb/arch.png)

Em resumo, o fluxo √©: o Prometheus **raspa (pull)** m√©tricas dos jobs instrumentados, diretamente dos servi√ßos ou via um componente intermedi√°rio de push para jobs ef√™meros. Todos os samples coletados s√£o armazenados localmente no banco de dados de s√©ries temporais embutido ([TSDB](https://prometheus.io/docs/prometheus/latest/storage/tsdb/)). Regras definidas podem ser executadas continuamente sobre esses dados ‚Äì seja para gravar novas s√©ries agregadas ([recording rules](https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/)) ou para acionar **[alertas](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/)**. Os alertas gerados pelo Prometheus s√£o ent√£o enviados para o **[Alertmanager](https://prometheus.io/docs/alerting/latest/alertmanager/)** processar. Por fim, ferramentas de visualiza√ß√£o como o **[Grafana](https://grafana.com/)** podem consultar o Prometheus para exibir dashboards das m√©tricas coletadas.

O ecossistema Prometheus possui diversos componentes (muitos opcionais) que interagem nessa arquitetura:

* **[Servidor Prometheus](https://prometheus.io/docs/prometheus/latest/components/prometheus/)** ‚Äì o servidor principal que coleta e armazena as m√©tricas e processa consultas PromQL.
* **[Bibliotecas cliente](https://prometheus.io/docs/instrumenting/clientlibs/)** ‚Äì usadas para instrumentar c√≥digo de aplica√ß√µes (expondo m√©tricas via /metrics). H√° libs oficiais em Go, Java, Ruby, Python, etc.
* **[Exporters](https://prometheus.io/docs/instrumenting/exporters/)** ‚Äì programas externos que coletam m√©tricas de servi√ßos ou sistemas terceiros (bancos de dados, servidores web, sistemas operacionais) e as exp√µem no formato Prometheus. Exemplos: Node Exporter (m√©tricas de sistema Linux), Blackbox Exporter (monitoramento de endpoints externos), etc.
* **[Pushgateway](https://prometheus.io/docs/instrumenting/pushing/)** ‚Äì gateway para receber m√©tricas *pushed* por aplicativos de curta dura√ß√£o ou ambientes onde n√£o d√° para o Prometheus puxar diretamente.
* **[Alertmanager](https://prometheus.io/docs/alerting/latest/alertmanager/)** ‚Äì componente respons√°vel por receber alertas enviados pelo Prometheus e gerenciar o envio de notifica√ß√µes (email, Slack, PagerDuty etc.), realizando agrupamento, deduplica√ß√£o e silenciamento conforme configurado.
* **[Ferramentas de suporte](https://prometheus.io/docs/prometheus/latest/tools/)** ‚Äì englobam utilit√°rios de linha de comando (como o promtool), exportadores de terceiros, dashboards pr√©-configurados, entre outros, que facilitam operar e integrar o Prometheus.

Essa arquitetura descentralizada (com coleta **[pull](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#pull_interval)** e componentes distintos) torna o Prometheus especialmente adequado a ambientes modernos com microsservi√ßos e orquestra√ß√£o de cont√™ineres ([Docker](https://www.docker.com/), [Kubernetes](https://kubernetes.io/)). 

Ele foi projetado para funcionar de forma aut√¥noma em cada n√≥ (cada servidor Prometheus √© independente, sem depend√™ncia de armazenamento distribu√≠do), privilegiando confiabilidade mesmo durante falhas de rede ou de outros servi√ßos. Em caso de problemas graves na infraestrutura, voc√™ ainda consegue acessar m√©tricas recentes localmente no Prometheus, que atua como fonte de verdade para diagnosticar incidentes.

## Labels e Samples

No Prometheus, **[labels](https://prometheus.io/docs/concepts/data_model/#metric-names-and-labels)** (r√≥tulos) e **[samples](https://prometheus.io/docs/concepts/data_model/#samples-and-series)** (amostras) s√£o conceitos-chave para organizar os dados monitorados. Uma analogia simples: imagine um guarda-roupa onde cada roupa tem etiquetas indicando cor, tamanho e tipo. Essas etiquetas ajudam a encontrar rapidamente, por exemplo, "camisetas verdes tamanho M". Da mesma forma, no Prometheus cada m√©trica pode ter v√°rios **[labels](https://prometheus.io/docs/concepts/data_model/#metric-names-and-labels)** (chave=valor) que a qualificam. Por exemplo, uma m√©trica `app_memory_usage_bytes` poderia ter labels como `host="servidor1"` e `region="us-east"`. Assim podemos filtrar/consultar "uso de mem√≥ria no servidor1" apenas buscando por `host="servidor1"`.

Os **[labels](https://prometheus.io/docs/concepts/data_model/#metric-names-and-labels)** permitem um modelo de dados multidimensional ‚Äì ou seja, uma mesma m√©trica (ex: `http_requests_total`) √© armazenada separadamente para cada combina√ß√£o de labels (rota="/login", m√©todo="GET", c√≥digo="200", etc.). Isso enriquece as an√°lises, pois podemos agregar ou dividir m√©tricas por essas dimens√µes conforme necess√°rio.

![](https://raw.githubusercontent.com/scovl/scovl.github.io/master/post/images/tsdb/samples01.png)

J√° os **[samples](https://prometheus.io/docs/concepts/data_model/#samples-and-series)** s√£o as unidades de dado coletadas ao longo do tempo ‚Äì cada medi√ß√£o individual de uma m√©trica em um determinado instante. Voltando √† analogia, se ped√≠ssemos a cada crian√ßa numa pesquisa que escolhesse 3 balas, as balas escolhidas por cada crian√ßa seriam uma **amostra** da prefer√™ncia de balas. No contexto do Prometheus, a cada scrape o valor de cada m√©trica coletada √© um sample (com timestamp e valor). Esses samples ficam armazenados como uma s√©rie temporal etiquetada, permitindo ver a evolu√ß√£o daquele valor no tempo.

Por exemplo, considere a m√©trica gauge `node_cpu_usage` com label `host`. Para cada host monitorado, teremos uma s√©rie separada, e a cada intervalo de coleta obtemos um sample novo do uso de CPU daquele host. Assim, podemos consultar a s√©rie para ver como a CPU variou ao longo de um dia inteiro para cada m√°quina.

> **Exemplo de s√©ries temporais no Prometheus**: cada ponto representa um sample (valor observado) etiquetado por inst√¢ncia ou outra dimens√£o, armazenado em sequ√™ncia temporal.

Em resumo, **[labels](https://prometheus.io/docs/concepts/data_model/#metric-names-and-labels)** fornecem contexto (quem, onde, o qu√™) e **[samples](https://prometheus.io/docs/concepts/data_model/#samples-and-series)** fornecem o valor num√©rico no tempo. Essa combina√ß√£o √© o que torna o Prometheus poderoso para agregar m√©tricas semelhantes e, ao mesmo tempo, permitir recortes por dimens√£o. Vale ressaltar a import√¢ncia de escolher labels com cardinalidade controlada ‚Äì ou seja, evitar labels que possam assumir valores extremamente variados (como IDs √∫nicos, URLs completas ou timestamps). 

> **Nota:** Labels com varia√ß√£o descontrolada podem causar uma explos√£o de s√©ries e sobrecarregar o Prometheus, conforme discutiremos em melhores pr√°ticas.

## Instala√ß√£o

Existem diversas maneiras de instalar e executar o Prometheus. Aqui vou demonstrar uma configura√ß√£o simples usando **[Docker](https://www.docker.com/)** e **[Docker Compose](https://docs.docker.com/compose/)**, incluindo o Grafana e uma ferramenta de simula√ß√£o de m√©tricas chamada **[PromSim](https://github.com/dmitsh/promsim)** (√∫til para testes). Essa stack de exemplo traz:

* **[Prometheus](https://prometheus.io/)** ‚Äì servidor de m√©tricas.
* **[Grafana](https://grafana.com/)** ‚Äì para dashboards e visualiza√ß√£o.
* **[PromSim](https://github.com/dmitsh/promsim)** ‚Äì um simulador que exp√µe m√©tricas aleat√≥rias para exercitar o Prometheus.

Comece criando um arquivo `docker-compose.yml` com o seguinte conte√∫do:

```yaml
version: "3"
services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - "./prometheus.yml:/etc/prometheus/prometheus.yml"
    depends_on:
      - promsim

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"

  promsim:
    image: sysdigtraining/promsim:latest
    container_name: promsim
    ports:
      - "8080:8080"
```

No mesmo diret√≥rio, crie o arquivo de configura√ß√£o `prometheus.yml` para o Prometheus:

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: "promsim"
    static_configs:
      - targets: ["promsim:8080"]
```

Esse arquivo define que o Prometheus far√° scrape a cada 15s (`scrape_interval`) e avalia regras na mesma frequ√™ncia (`evaluation_interval`). Em `scrape_configs`, temos um job chamado "promsim" que coleta m√©tricas do endere√ßo `promsim:8080` (nosso container PromSim simulando um alvo de m√©tricas). Agora suba os servi√ßos:

```bash
docker-compose up -d
```

Isso iniciar√° os containers Prometheus, Grafana e PromSim em segundo plano. Ap√≥s o start, acesse o Grafana em **[http://localhost:3000](http://localhost:3000)** (usu√°rio **admin**, senha **admin** padr√£o). No Grafana, adicione o Prometheus como fonte de dados: v√° em *Configuration (engrenagem) > Data Sources*, adicione nova fonte do tipo Prometheus com URL **[http://prometheus:9090](http://prometheus:9090)** (que, devido ao Docker Compose, resolve para o container do Prometheus).

Feito isso, voc√™ j√° pode importar ou criar pain√©is Grafana usando as m√©tricas do Prometheus (inclusive as geradas pelo PromSim). O PromSim estar√° expondo v√°rias m√©tricas aleat√≥rias ‚Äì por exemplo, simulando CPU, mem√≥ria, requisi√ß√µes ‚Äì permitindo testar consultas e alertas sem precisar de uma aplica√ß√£o real por tr√°s. Para mais detalhes do PromSim, veja **[a documenta√ß√£o oficial](https://github.com/dmitsh/promsim)**.

Caso queira rodar apenas o Prometheus isoladamente, basta executar o container oficial: `docker run -p 9090:9090 prom/prometheus`. Depois acesse **[http://localhost:9090](http://localhost:9090)** para abrir a UI nativa do Prometheus:

![](https://raw.githubusercontent.com/scovl/scovl.github.io/master/post/images/tsdb/ui01.png)

A interface web padr√£o do Prometheus inclui os seguintes menus no topo:

* **[Alerts](/alerts)**: lista os alertas ativos e suas informa√ß√µes. Mostra tamb√©m alertas pendentes e silenciados.
* **[Graph](/graph)**: permite rodar consultas PromQL e visualizar o resultado em formato gr√°fico (ou tabela). √â √∫til para explorar interativamente as m√©tricas.
* **[Status](/status)**: informa√ß√µes sobre o status do servidor Prometheus ‚Äì mem√≥ria usada, n√∫mero de s√©ries ativas, status das coletas, etc.
  * **[Targets](/targets)** (na se√ß√£o Status): mostra todos os alvos configurados e se a coleta est√° OK (up) ou falhou.
  * **[Service Discovery](/service-discovery)** (tamb√©m em Status): lista os servi√ßos descobertos via mecanismos din√¢micos (Kubernetes, DNS, etc.).
* **[Help](/classic/targets)**: link para documenta√ß√£o e ajuda do Prometheus.

Al√©m disso, logo abaixo dos menus, a UI oferece algumas op√ß√µes e campos importantes:

* **[Time range e refresh](https://prometheus.io/docs/prometheus/latest/querying/basics/#time-range-and-resolution-selection)**: controles para selecionar o intervalo de tempo da consulta e atualizar automaticamente.
* **[Use local time](https://prometheus.io/docs/prometheus/latest/querying/basics/#time-range-and-resolution-selection)**: alterna entre exibir os timestamps no seu fuso hor√°rio local ou em UTC.
* **[Query history](https://prometheus.io/docs/prometheus/latest/querying/basics/#query-history)**: op√ß√£o para habilitar hist√≥rico das consultas feitas (facilita repetir queries recentes).
* **[Autocomplete](https://prometheus.io/docs/prometheus/latest/querying/basics/#autocomplete)**: op√ß√£o para habilitar auto-completar de m√©tricas e fun√ß√µes no campo de consulta.
* **[Campo de consulta PromQL](https://prometheus.io/docs/prometheus/latest/querying/basics/#expression-language-promql)**: onde voc√™ escreve a express√£o a ser consultada. O Prometheus traz sugest√µes enquanto voc√™ digita (se autocomplete ligado).
* **[Bot√µes Execute / Reset](https://prometheus.io/docs/prometheus/latest/querying/basics/#execute-and-reset)**: para executar a consulta ou limpar o campo.
* **[Aba Graph / Table](https://prometheus.io/docs/prometheus/latest/querying/basics/#graph-and-table)**: seleciona se o resultado ser√° plotado em um gr√°fico ou mostrado como tabela bruta de valores.
* **[Evaluation time](https://prometheus.io/docs/prometheus/latest/querying/basics/#evaluation-time)**: permite fixar um timestamp espec√≠fico para avaliar a query (por padr√£o √© "now", mas voc√™ pode ver valores hist√≥ricos escolhendo um hor√°rio passado).

> **Dica:** a UI do Prometheus √© √≥tima para explorar e depurar m√©tricas rapidamente, mas para dashboards permanentes e mais bonitos geralmente usamos o Grafana. O Grafana se conecta ao Prometheus via API e permite combinar m√∫ltiplas consultas em gr√°ficos customizados.

### Configura√ß√£o

Ap√≥s instalar, o principal arquivo a ajustar √© o de **[configura√ß√£o do Prometheus](https://prometheus.io/docs/prometheus/latest/configuration/configuration/)** (`prometheus.yml`). Nele definimos os par√¢metros globais, jobs de scrape, regras de alerta, etc. Vamos examinar a estrutura b√°sica e algumas customiza√ß√µes comuns. Um exemplo m√≠nimo de `prometheus.yml` poderia ser:

```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
```

Nesse caso, definimos um intervalo global de scrape de 15s e um job para monitorar o pr√≥prio Prometheus (expondo m√©tricas em [localhost:9090](http://localhost:9090)). Para monitorar outras aplica√ß√µes, adicionamos novos blocos em `scrape_configs`. Por exemplo, para monitorar uma aplica√ß√£o web rodando na porta 8080 de um host chamado `my-app`:

```yaml
scrape_configs:
  - job_name: 'my-app'
    static_configs:
      - targets: ['my-app:8080']
```

Isso instruir√° o Prometheus a coletar periodicamente m√©tricas em **[http://my-app:8080/metrics](http://my-app:8080/metrics)**. Podemos repetir o processo para cada servi√ßo ou componente que queremos incluir, definindo um `job_name` descritivo e a lista de endpoints (targets).

Para ambientes com muitos alvos ou infraestrutura din√¢mica, √© invi√°vel gerenciar esses targets manualmente. Nesses casos, o Prometheus oferece integra√ß√µes de **Service Discovery** ([Kubernetes](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config), [AWS EC2](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#ec2_sd_config), [Consul](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#consul_sd_config), [DNS](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dns_sd_config), etc.) e tamb√©m o **file-based discovery** (descoberta via arquivos). Este √∫ltimo permite apontar para um ou mais arquivos JSON externos contendo a lista de targets. Assim, ferramentas externas ou scripts podem atualizar esses arquivos conforme os servi√ßos mudam, e o Prometheus percebe as altera√ß√µes automaticamente. Por exemplo, poder√≠amos alterar o job acima para usar arquivo:

```yaml
scrape_configs:
  - job_name: 'my-app'
    file_sd_configs:
      - files:
          - /etc/prometheus/targets/my-app.json
```

E no arquivo `/etc/prometheus/targets/my-app.json` colocar algo como:

```json
[
  {
    "labels": {
      "job": "my-app",
      "env": "production"
    },
    "targets": [
      "my-app1:8080",
      "my-app2:8080"
    ]
  }
]
```

Nesse JSON, especificamos dois targets (dois inst√¢ncias da aplica√ß√£o `my-app`) e tamb√©m atribu√≠mos labels adicionais a essas inst√¢ncias (`env: production`, por exemplo). Assim, se futuramente adicionarmos `my-app3:8080`, basta atualizar o JSON ‚Äì o Prometheus recarrega periodicamente ou quando o arquivo muda. Esse m√©todo facilita escalabilidade e automa√ß√£o da configura√ß√£o de alvos.

Outro ponto de configura√ß√£o importante √© a **reten√ß√£o de dados**. Por padr√£o, o Prometheus guarda as s√©ries temporais localmente por 15 dias. Em ambientes de produ√ß√£o, pode ser necess√°rio ajustar esse per√≠odo. Voc√™ pode definir a flag de inicializa√ß√£o `--storage.tsdb.retention.time` (ou configurar no servi√ßo) para algo maior, por exemplo `30d` para reter \~1 m√™s de m√©tricas. Tenha em mente que aumentar a reten√ß√£o aumenta proporcionalmente o consumo de disco e mem√≥ria. 

Tamb√©m √© poss√≠vel limitar por tamanho de disco (`--storage.tsdb.retention.size`), se preferir. Caso precise de reten√ß√£o muito longa (meses/anos), √© recomend√°vel integrar com solu√ß√µes de armazenamento remoto em vez de manter tudo no Prometheus (falaremos disso em *Melhores Pr√°ticas*). Exemplo de defini√ß√£o de reten√ß√£o no **[systemd](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration-file)** (ExecStart):

```bash
/opt/prometheus/prometheus \
  --config.file=/opt/prometheus/prometheus.yml \
  --storage.tsdb.retention.time=30d
```

> **Nota:** O formato aceita unidades como `h`, `d`, `w`, `y`. Voc√™ tamb√©m pode usar a op√ß√£o `--storage.tsdb.retention.size` para definir um tamanho m√°ximo (por ex: `50GB`), o que ocorrer primeiro (tempo ou tamanho) aciona a limpeza de dados antigos.

Em instala√ß√µes via pacote ou container, normalmente a estrutura de diret√≥rios do Prometheus √© assim:

```
/opt/prometheus/
‚îú‚îÄ‚îÄ prometheus (bin√°rio)
‚îú‚îÄ‚îÄ promtool   (bin√°rio utilit√°rio)
‚îú‚îÄ‚îÄ prometheus.yml (configura√ß√£o)
‚îú‚îÄ‚îÄ consoles/  (arquivos HTML da UI "classic")
‚îú‚îÄ‚îÄ console_libraries/ (bibliotecas JS para consoles)
‚îî‚îÄ‚îÄ data/      (armazenamento local das s√©ries temporais)
```

A pasta `data/` merece destaque ‚Äì ali ficam todos os dados das m√©tricas coletadas. Abordaremos sua estrutura interna na se√ß√£o "Under the Hood".

> Em resumo, ap√≥s instalar, voc√™ deve editar o `prometheus.yml` para incluir todos os targets que deseja monitorar (seja listando estaticamente ou via mecanismos din√¢micos) e ajustar par√¢metros globais (intervalos, regras, reten√ß√£o). 

Depois reinicie o servi√ßo/container do Prometheus para aplicar as altera√ß√µes. Para validar se a sintaxe do arquivo est√° correta antes de reiniciar, podemos usar o **[promtool](https://prometheus.io/docs/prometheus/latest/tools/promtool/)** conforme abaixo.

### Discovery Din√¢mico e Relabeling

Em ambientes modernos com infraestrutura din√¢mica (Kubernetes, cloud, microsservi√ßos), configurar targets manualmente no `prometheus.yml` torna-se invi√°vel. O Prometheus oferece mecanismos de **Service Discovery** que permitem descobrir automaticamente alvos para monitoramento, e o **Relabeling** permite transformar dinamicamente essas descobertas durante o processo de configura√ß√£o.

#### Service Discovery

O Prometheus suporta diversos mecanismos de descoberta autom√°tica:

* **[Kubernetes](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config)**: Descobre pods, servi√ßos, endpoints automaticamente baseado em labels e anota√ß√µes.
* **[AWS EC2](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#ec2_sd_config)**: Encontra inst√¢ncias EC2 baseado em tags.
* **[Consul](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#consul_sd_config)**: Usa o Consul como fonte de verdade para servi√ßos.
* **[DNS](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dns_sd_config)**: Resolve nomes DNS para descobrir alvos.
* **[File-based](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#file_sd_config)**: L√™ targets de arquivos JSON/YAML que podem ser atualizados externamente.

**Exemplo de discovery Kubernetes:**

```yaml
scrape_configs:
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
```

#### Relabeling

O **relabeling** √© uma funcionalidade poderosa que permite transformar labels, nomes de targets, endere√ßos e outros metadados durante o processo de discovery. √â fundamental para:

* **Filtrar targets indesejados** (ex: excluir pods de teste)
* **Adicionar/remover labels** dinamicamente
* **Transformar endere√ßos** (ex: mascarar IPs internos)
* **Agrupar targets** logicamente

**Exemplo pr√°tico de relabeling:**

```yaml
relabel_configs:
  # Manter apenas pods com annotation prometheus.io/scrape=true
  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
    action: keep
    regex: true
  
  # Extrair namespace como label
  - source_labels: [__meta_kubernetes_namespace]
    action: replace
    target_label: namespace
  
  # Adicionar label de ambiente baseado no namespace
  - source_labels: [namespace]
    regex: 'prod-.*'
    replacement: 'production'
    target_label: environment
  
  # Remover porta padr√£o se n√£o especificada
  - source_labels: [__address__]
    regex: '(.+):8080'
    target_label: instance
    replacement: '$1'
  
  # Filtrar targets que come√ßam com 'test'
  - action: drop
    source_labels: [__meta_kubernetes_pod_name]
    regex: 'test.*'
```

**Casos de uso comuns:**

* **Filtros de ambiente**: Manter apenas pods de produ√ß√£o, excluindo dev/test
* **Mascaramento de dados sens√≠veis**: Remover IPs internos ou informa√ß√µes de debug
* **Agrega√ß√£o por labels**: Agrupar targets por regi√£o, datacenter, time
* **Normaliza√ß√£o de nomes**: Padronizar nomes de inst√¢ncias ou servi√ßos

> **Importante**: O relabeling √© aplicado **antes** do scrape, ent√£o voc√™ pode usar `__meta_*` labels (metadados do discovery) para tomar decis√µes sobre quais targets monitorar e como rotul√°-los.

## Promtool

O **promtool** √© uma ferramenta de linha de comando que acompanha o Prometheus, fornecendo utilit√°rios para verificar configura√ß√µes e depurar dados. Algumas utiliza√ß√µes comuns do promtool:

* **Checar sintaxe de configura√ß√£o:** Antes de subir uma altera√ß√£o no `prometheus.yml`, rode `promtool check config prometheus.yml`. Ele apontar√° erros de sintaxe ou campos desconhecidos, ajudando a evitar falhas no start do servidor.
* **Validar regras de alerta ou grava√ß√£o:** Se voc√™ definiu arquivos externos de regras (YAML de alertas ou recording rules), use `promtool check rules minhas_regras.yml`. Ele analisar√° as express√µes PromQL e a formata√ß√£o.
* **Testar express√£o de alerta:** O promtool permite avaliar manualmente express√µes em um dado instant√¢neo ou s√©rie de tempo para ver se disparariam alerta. √ötil em CI ou para garantir que a l√≥gica est√° correta.
* **Checar integridade do TSDB:** Com o comando `promtool tsdb check /path/para/dados` √© poss√≠vel inspecionar o banco local de s√©ries temporais em busca de inconsist√™ncias ou corrup√ß√£o.
* **Converter formatos de dados de m√©trica:** H√° como transformar arquivos de m√©tricas entre formatos (por exemplo, de texto para JSON e vice-versa) usando `promtool convert metrics --from=txt --to=json arquivo.txt`.

Essas s√£o apenas algumas fun√ß√µes. Em suma, o promtool √© seu amigo para garantir que o ambiente Prometheus est√° consistente e saud√°vel ‚Äì use-o sempre que fizer mudan√ßas significativas na configura√ß√£o.

## üîç Instrumenta√ß√£o

A **instrumenta√ß√£o** √© o processo de inserir coleta de m√©tricas em sistemas e aplica√ß√µes. No contexto Prometheus, podemos dividir em dois tipos:

### üìä Instrumenta√ß√£o direta (na aplica√ß√£o)

Significa instrumentar o pr√≥prio c√≥digo da aplica√ß√£o ou servi√ßo para expor m√©tricas de neg√≥cio ou de desempenho relevantes. Voc√™ adiciona pontos de m√©trica no c√≥digo ([counters](https://prometheus.io/docs/concepts/metric_types/#counter), [gauges](https://prometheus.io/docs/concepts/metric_types/#gauge), etc.) usando uma biblioteca cliente do Prometheus. Assim, a pr√≥pria aplica√ß√£o passa a expor um endpoint `/metrics` com dados em tempo real sobre si mesma (lat√™ncia de requisi√ß√µes, uso de mem√≥ria interno, tamanho de fila, etc.). Essa abordagem d√° controle granular ‚Äì os desenvolvedores escolhem o que medir ‚Äì e tende a fornecer m√©tricas altamente espec√≠ficas e √∫teis para diagnosticar o comportamento da aplica√ß√£o.

### üîÑ Instrumenta√ß√£o indireta (via exporters)

Refere-se a coletar m√©tricas de sistemas externos ou legados atrav√©s de componentes intermedi√°rios chamados **[exporters](https://prometheus.io/docs/instrumenting/exporters/)**. Em vez de modificar o sistema alvo, voc√™ roda um exporter que coleta informa√ß√µes daquele sistema (geralmente via APIs existentes, comandos ou leitura de arquivos) e as exp√µe no formato Prometheus. O Prometheus ent√£o faz scrape nesse exporter. Essa abordagem √© comum para: sistemas operacionais, bancos de dados, servidores web, ou qualquer software que n√£o tenha suporte nativo ao Prometheus. Por exemplo, h√° exporters para **[MySQL](https://github.com/prometheus/mysqld_exporter)**, **[PostgreSQL](https://github.com/prometheus/postgres_exporter)**, **[Apache/Nginx](https://github.com/nginxinc/nginx-prometheus-exporter)**, **[Redis](https://github.com/oliver006/redis_exporter)**, entre muitos outros, que traduzem m√©tricas desses sistemas para o formato esperado.

Ambos os tipos s√£o importantes. A instrumenta√ß√£o direta fornece m√©tricas sob medida da aplica√ß√£o (por exemplo, quantas transa√ß√µes processou, quantos usu√°rios ativos, etc.), enquanto a indireta garante visibilidade de componentes de infraestrutura e softwares de terceiros sem precisar alterar eles. A seguir, veremos exemplos de instrumenta√ß√£o indireta (principais exporters) e de instrumenta√ß√£o direta em algumas linguagens.

### Instrumenta√ß√£o indireta: Exporters

**Ecossistema nativo:** O Prometheus j√° oferece diversos exporters oficiais ou mantidos pela comunidade para sistemas populares. Alguns exemplos:

* **[Node Exporter](https://github.com/prometheus/node_exporter)** (Linux): Coleta m√©tricas de sistema operacional Linux ‚Äì CPU, mem√≥ria, disco, rede, entropia, stats de kernel, etc. √â imprescind√≠vel para monitorar VMs ou servidores bare metal. Basta executar o bin√°rio do node\_exporter no host; ele abre :9100/metrics com dezenas de m√©tricas padronizadas (cpu\_seconds\_total, node\_filesystem\_usage\_bytes, etc.). Essas m√©tricas d√£o uma visibilidade completa do estado do host, permitindo identificar gargalos de recurso.

* **[Windows Exporter](https://github.com/prometheus/wmic_exporter)** (Windows): Equivalente para plataformas Windows (antigo WMI exporter). Coleta CPU, mem√≥ria, disco, contadores do Windows, etc., expondo em :9182/metrics (porta padr√£o). Assim, ambiente heterog√™neos tamb√©m podem ser monitorados.

* **[Blackbox Exporter](https://github.com/prometheus/blackbox_exporter)**: √ötil para monitorar *externamente* a disponibilidade de servi√ßos. Ele executa *probes* do tipo ICMP (ping), HTTP(S), DNS, TCP, etc., simulando a experi√™ncia do usu√°rio externo. Voc√™ configura m√≥dulos de probe (ex: checar HTTP 200 em determinada URL dentro de 2s) e o Prometheus chama o Blackbox passando o alvo a testar. Se a resposta falha ou excede tempo, m√©tricas como `probe_success`=0 ou `probe_duration_seconds` indicam problema. √â excelente para monitorar uptime de sites e endpoints de fora para dentro.

* **[Exporters de aplica√ß√µes](https://prometheus.io/docs/instrumenting/exporters/)**: H√° muitos: PostgreSQL exporter, Redis exporter, JMX exporter (Java), SNMP exporter (equipamentos de rede), etc. Em geral, se voc√™ usar alguma tecnologia popular, provavelmente j√° existe um exporter pronto (a documenta√ß√£o oficial lista dezenas: **[Exporters e integra√ß√µes](https://prometheus.io/docs/instrumenting/exporters/)**).

> **Como usar exporters?** Normalmente √© executar o bin√°rio do exporter pr√≥ximo do servi√ßo alvo, e ent√£o adicionar um job no `prometheus.yml` apontando para o endpoint do exporter. Por exemplo, para Node Exporter em v√°rias m√°quinas, voc√™ rodaria node\_exporter em cada host (porta 9100) e adicionaria algo como:

```yaml
scrape_configs:
  - job_name: 'node'
    static_configs:
      - targets: ['host1:9100', 'host2:9100', ...]
```

Assim o Prometheus coletar√° as m√©tricas de cada m√°quina. Cada m√©trica vir√° automaticamente com labels como `instance="host1:9100"` e outras espec√≠ficas (o Node Exporter adiciona label `job="node"` e por vezes labels como `cpu="0"` para m√©tricas por CPU, etc.).

> Em resumo, a instrumenta√ß√£o indireta via exporters √© fundamental para trazer para o Prometheus dados de componentes que n√£o exp√µem nativamente as m√©tricas. √â um jeito de *bridge* (ponte) entre sistemas legados e o moderno mundo do Prometheus.

## PromQL na Pr√°tica

O PromQL √© a linguagem de consulta do Prometheus que permite extrair insights valiosos das m√©tricas coletadas. Vamos explorar exemplos pr√°ticos de consultas comuns para uso di√°rio em monitoramento.

### Consultas B√°sicas de Disponibilidade

**Verificar se todos os targets est√£o up:**
```promql
up == 1
```

**Contar quantos targets est√£o down:**
```promql
count(up == 0)
```

**Taxa de disponibilidade por job:**
```promql
avg(up) by (job)
```

### M√©tricas de Sistema (Node Exporter)

**CPU m√©dio por inst√¢ncia:**
```promql
avg(rate(node_cpu_seconds_total{mode="user"}[5m])) by (instance)
```

**Uso de mem√≥ria em porcentagem:**
```promql
(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100
```

**Uso de disco por filesystem:**
```promql
(node_filesystem_size_bytes - node_filesystem_free_bytes) / node_filesystem_size_bytes * 100
```

**Taxa de I/O de disco:**
```promql
rate(node_disk_io_time_seconds_total[5m])
```

### M√©tricas de Aplica√ß√£o Web

**Taxa de requisi√ß√µes por segundo (QPS):**
```promql
rate(http_requests_total[5m])
```

**Taxa de erro por endpoint:**
```promql
rate(http_requests_total{status=~"5.."}[5m])
```

**Lat√™ncia p95 (percentil 95):**
```promql
histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))
```

**Lat√™ncia p99 (percentil 99):**
```promql
histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))
```

**Tempo de resposta m√©dio:**
```promql
rate(http_request_duration_seconds_sum[5m]) / rate(http_request_duration_seconds_count[5m])
```

### M√©tricas de Banco de Dados

**Conex√µes ativas do PostgreSQL:**
```promql
pg_stat_database_numbackends
```

**Taxa de transa√ß√µes por segundo:**
```promql
rate(pg_stat_database_xact_commit[5m]) + rate(pg_stat_database_xact_rollback[5m])
```

**Tamanho de tabelas (PostgreSQL):**
```promql
pg_stat_user_tables_size_bytes
```

### M√©tricas de Container/Kubernetes

**CPU por pod:**
```promql
sum(rate(container_cpu_usage_seconds_total{container!=""}[5m])) by (pod)
```

**Mem√≥ria por pod:**
```promql
sum(container_memory_usage_bytes{container!=""}) by (pod)
```

**Pods por namespace:**
```promql
count(kube_pod_info) by (namespace)
```

### Alertas Comuns

**Alerta de CPU alta:**
```promql
100 - (avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) by (instance) * 100) > 80
```

**Alerta de mem√≥ria alta:**
```promql
(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 85
```

**Alerta de disco cheio:**
```promql
(node_filesystem_size_bytes - node_filesystem_free_bytes) / node_filesystem_size_bytes * 100 > 90
```

**Alerta de lat√™ncia alta:**
```promql
histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le)) > 1
```

**Alerta de taxa de erro alta:**
```promql
rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.05
```

### Consultas Avan√ßadas

**Top 5 inst√¢ncias com maior CPU:**
```promql
topk(5, 100 - (avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) by (instance) * 100))
```

**Soma de m√©tricas por regi√£o:**
```promql
sum(rate(http_requests_total[5m])) by (region)
```

**Diferen√ßa de m√©tricas entre per√≠odos:**
```promql
increase(http_requests_total[1h]) - increase(http_requests_total[1h] offset 1h)
```

**M√©trica com label din√¢mico:**
```promql
rate(http_requests_total{endpoint=~"/api/.*"}[5m])
```

### Dicas de Performance

**Use intervalos apropriados:**
* Para alertas: `[5m]` ou `[1m]`
* Para dashboards: `[1h]` para tend√™ncias
* Evite `[24h]` em consultas frequentes

**Prefira `rate()` sobre `irate()` para alertas:**
```promql
# Bom para alertas (mais est√°vel)
rate(http_requests_total[5m])

# Melhor para dashboards (mais responsivo)
irate(http_requests_total[5m])
```

**Agregue quando poss√≠vel:**
```promql
# Em vez de somar muitas s√©ries
sum(rate(http_requests_total[5m])) by (job)

# Evite isso em m√©tricas com alta cardinalidade
sum(rate(http_requests_total[5m]))
```

### Exemplos de Recording Rules

**Regra para QPS agregado:**
```yaml
groups:
- name: recording_rules
  rules:
    - record: job:http_requests:rate5m
      expr: sum(rate(http_requests_total[5m])) by (job)
```

**Regra para lat√™ncia p95:**
```yaml
    - record: job:http_request_duration_seconds:p95
      expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, job))
```

> **Dica**: Use recording rules para pr√©-calcular consultas complexas e frequentes. Isso melhora a performance e reduz a carga no Prometheus.

### Instrumenta√ß√£o direta: exemplos por linguagem

Agora vejamos como instrumentar aplica√ß√µes escritas em algumas linguagens populares. A ideia geral em qualquer linguagem √©: instalar a biblioteca cliente do Prometheus, criar m√©tricas ([counters](https://prometheus.io/docs/concepts/metric_types/#counter), [gauges](https://prometheus.io/docs/concepts/metric_types/#gauge), etc.) em pontos estrat√©gicos do c√≥digo, e expor um endpoint HTTP `/metrics` onde essas m√©tricas s√£o servidas (em formato de texto). O Prometheus ent√£o coleta nesse endpoint.

#### Java (Micrometer / Cliente Java do Prometheus)

Em Java, uma abordagem comum √© usar o **[Micrometer](https://micrometer.io/)** ‚Äì uma biblioteca de instrumenta√ß√£o que suporta m√∫ltiplos backends (Prometheus, Graphite, etc.). O Micrometer foi adotado pelo Spring Boot, por exemplo, facilitando a exposi√ß√£o de m√©tricas. Passos b√°sicos:

1. **Depend√™ncias:** Adicione ao seu projeto (pom.xml ou build.gradle) a depend√™ncia do Micrometer e do registry Prometheus. Exemplo (Maven):

   ```xml
   <dependency>
       <groupId>io.micrometer</groupId>
       <artifactId>micrometer-core</artifactId>
       <version>1.10.4</version>
   </dependency>
   <dependency>
       <groupId>io.micrometer</groupId>
       <artifactId>micrometer-registry-prometheus</artifactId>
       <version>1.10.4</version>
   </dependency>
   ```

2. **Registrar m√©tricas:** Em sua aplica√ß√£o, configure um `MeterRegistry` do Prometheus e registre m√©tricas. Por exemplo, em uma classe de configura√ß√£o Spring:

   ```java
   @Bean
   PrometheusMeterRegistry prometheusRegistry() {
       return new PrometheusMeterRegistry(PrometheusConfig.DEFAULT);
   }
   ```

   Voc√™ pode ent√£o criar contadores, gauges, etc. usando esse registry:

   ```java
   Counter requestCount = Counter.builder("myapp_requests_total")
                                 .description("Total de requisi√ß√µes")
                                 .register(prometheusRegistry());
   // Usar requestCount.inc(); em pontos apropriados do c√≥digo
   ```

   Ou usar anota√ß√µes/filtros prontos do Spring Boot Actuator que medem tempos de resposta automaticamente.

3. **Expor endpoint /metrics:** Se estiver usando Spring Boot Actuator, habilite a endpoint Prometheus. No application.properties:

   ```
   management.endpoints.web.exposure.include=prometheus
   management.endpoint.prometheus.enabled=true
   ```

   Isso far√° o Actuator expor `/actuator/prometheus` com as m√©tricas no formato Prometheus. O Prometheus pode ent√£o fazer scrape nessa URL. (Alternativamente, sem Spring, voc√™ poderia iniciar um HTTP server manualmente que responda com `prometheusRegistry.scrape()` output).

4. **Verificar m√©tricas:** Ao rodar a aplica√ß√£o, acesse [http://localhost:8080/actuator/prometheus](http://localhost:8080/actuator/prometheus) (por exemplo) e voc√™ ver√° todas as m√©tricas registradas, inclusive padr√µes do JVM (o Micrometer j√° fornece m√©tricas de mem√≥ria, CPU, GC, etc. por padr√£o) e as personalizadas que voc√™ adicionou.

> Em resumo, no Java/Spring o processo pode ser muito simples aproveitando frameworks existentes. Para outras aplica√ß√µes Java sem Spring, existe tamb√©m o cliente Java do Prometheus (simpleclient) onde voc√™ manualmente gerencia as m√©tricas e HTTP endpoint.

#### JavaScript/Node.js

No Node.js podemos usar o pacote **prom-client** para instrumenta√ß√£o:

1. **Instalar pacote:** `npm install prom-client`.

2. **Criar m√©tricas no c√≥digo:** Por exemplo, vamos medir o tempo de resposta de uma rota Express:

   ```js
   const express = require('express');
   const promClient = require('prom-client');
   const app = express();

   // Cria um histogram para tempos de resposta em segundos
   const httpResponseHist = new promClient.Histogram({
     name: 'myapp_http_response_duration_seconds',
     help: 'Tempo de resposta das requisi√ß√µes HTTP (segundos)',
     labelNames: ['route', 'method']
   });
   ```

   Aqui usamos um Histogram (poderia ser Summary tamb√©m). Antes de enviar a resposta na rota, registramos a observa√ß√£o:

   ```js
   app.get('/example', (req, res) => {
     const end = httpResponseHist.startTimer({ route: '/example', method: 'GET' });
     // ... l√≥gica da rota ...
     res.send("Hello World");
     end(); // marca o fim do timer e observa a dura√ß√£o no histogram
   });
   ```

   O *prom-client* possui m√©todos convenientes para medir dura√ß√£o com `Histogram.startTimer()` que retorna uma fun√ß√£o para encerrar e registrar.

3. **Expor as m√©tricas:** Precisamos servir as m√©tricas via HTTP para o Prometheus. Podemos criar um endpoint `/metrics`:

   ```js
   app.get('/metrics', async (req, res) => {
     res.set('Content-Type', promClient.register.contentType);
     res.end(await promClient.register.metrics());
   });
   ```

   Isso coleta todas as m√©tricas registradas e retorna no formato de texto padr√£o.

4. **Iniciar server:** Por fim, inicie seu servidor Node (por ex, `app.listen(3000)`). Ent√£o a URL [http://localhost:3000/metrics](http://localhost:3000/metrics) mostrar√° as m√©tricas.

5. **Configurar Prometheus:** Adicione no `prometheus.yml` um job apontando para o servi√ßo Node, porta 3000 (ou a porta usada) e path `/metrics`. Exemplo:

   ```yaml
   scrape_configs:
     - job_name: 'my-nodeapp'
       static_configs:
         - targets: ['my-node-host:3000']
   ```

   (Se o Node est√° no mesmo Docker Compose do Prometheus, pode usar o nome de servi√ßo do container e porta.)

A partir da√≠, o Prometheus coletar√° as m√©tricas do seu app Node. Voc√™ poder√° consultar coisas como `rate(myapp_http_response_duration_seconds_count[5m])` ou `histogram_quantile(0.9, rate(myapp_http_response_duration_seconds_bucket[5m]))` para ver percentis de lat√™ncia.

#### Python (Flask, etc.)

Em Python, h√° o pacote **prometheus\_client**. Exemplo integrando com Flask:

1. **Instala√ß√£o:** `pip install prometheus_client`.

2. **Cria√ß√£o de m√©tricas:** Digamos que queremos contar requisi√ß√µes e medir dura√ß√£o. Podemos usar um Histogram ou Summary. Aqui um Summary:

   ```python
   from flask import Flask, request
   from prometheus_client import Summary, Counter, start_http_server

   app = Flask(__name__)
   REQUEST_TIME = Summary('myapp_request_processing_seconds', 'Tempo de processamento por rota', ['endpoint'])
   REQUEST_COUNT = Counter('myapp_requests_total', 'Total de requisi√ß√µes', ['endpoint', 'http_status'])
   ```

   Decoramos a rota para coletar m√©tricas:

   ```python
   @app.route("/example")
   def example():
       with REQUEST_TIME.labels(endpoint="/example").time():  # inicia timer autom√°tico
           # ... l√≥gica do endpoint ...
           response = "Hello World"
       REQUEST_COUNT.labels(endpoint="/example", http_status=200).inc()
       return response
   ```

   O `Summary.time()` funciona como context manager medindo o tempo dentro do bloco. Tamb√©m incrementamos um counter de requests totais por endpoint e status.

3. **Expor m√©tricas:** Podemos fazer de duas formas ‚Äì ou usamos o servidor HTTP interno do prometheus\_client ou integramos com Flask. Uma maneira simples: iniciar um *thread* do servidor metrics separado:

   ```python
   if __name__ == "__main__":
       start_http_server(8000)  # inicia servidor em porta 8000
       app.run(host="0.0.0.0", port=5000)
   ```

   O `start_http_server(8000)` far√° com que em [http://localhost:8000/metrics](http://localhost:8000/metrics) tenhamos as m√©tricas (note: ele por default exp√µe em /metrics automaticamente). Nesse caso, o Prometheus deve apontar para porta 8000 do app. Alternativamente, h√° integra√ß√£o para Flask (via middleware) que poderia expor /metrics no pr√≥prio Flask app.

4. **Prometheus config:** Similar aos anteriores, adicionar job apontando para o endpoint do metrics (host e porta usados).

Ap√≥s esses passos, seu app Python estar√° fornecendo m√©tricas. Voc√™ pode conferir acessando [http://localhost:8000/metrics](http://localhost:8000/metrics) e vendo as s√©ries nomeadas `myapp_request_processing_seconds_*` e `myapp_requests_total` entre outras (o client lib Python tamb√©m exp√µe m√©tricas padr√£o do processo Python como uso de mem√≥ria do processo, CPU, etc.).

### Ferramentas legadas e fechadas

Uma dificuldade comum √© monitorar sistemas legados ou softwares propriet√°rios que n√£o oferecem m√©tricas no formato Prometheus. Nesses casos, h√° alguns padr√µes de solu√ß√£o:

* **[Exporters externos](https://prometheus.io/docs/instrumenting/exporters/)**: Como j√° mencionado, se existir um exporter compat√≠vel (oficial ou da comunidade) para aquela ferramenta, ele √© o caminho mais f√°cil ‚Äì rodar o exporter e configur√°-lo como alvo. Por exemplo, para monitorar um servidor Oracle propriet√°rio, pode haver um exporter que conecta no Oracle e extrai estat√≠sticas via queries.

* **[Bridges personalizadas](https://prometheus.io/docs/instrumenting/writing_exporters/#writing-a-bridge-exporter):** Caso n√£o exista um exporter pronto, podemos criar um processo intermedi√°rio (*bridge*) que consulta a ferramenta legada de alguma forma (API REST, CLI, leitura de arquivos de log) e exp√µe resultados em /metrics. Essencialmente, isso √© escrever um pequeno exporter sob medida. Ferramentas de script como Python facilitam isso ‚Äì voc√™ coleta os dados e usa `prometheus_client` para expor.

* **[Integra√ß√µes via gateway ou plugins](https://prometheus.io/docs/instrumenting/writing_exporters/#writing-a-bridge-exporter):** Alguns ambientes possuem hooks para m√©tricas. Por exemplo, aplica√ß√µes .NET legadas podem exportar contadores no Windows Performance Counters ‚Äì a√≠ usar o Windows Exporter para peg√°-los. Em casos extremos, voc√™ pode usar o Pushgateway como ponte: o sistema legado faz push de alguma m√©trica b√°sica para o gateway (n√£o ideal, mas poss√≠vel).

> Em resumo, **sempre** √© poss√≠vel integrar algo ao Prometheus, ainda que indiretamente. A comunidade j√° produziu exporters para muitos sistemas fechados (WebLogic, SAP, etc.). E como √∫ltimo recurso, extrair dados e expor manualmente n√£o √© t√£o complexo gra√ßas √†s bibliotecas cliente dispon√≠veis.

## Alertmanager

O **[Alertmanager](https://prometheus.io/docs/alerting/latest/alertmanager/)** complementa o Prometheus no tratamento de alertas. Enquanto o Prometheus detecta condi√ß√µes de alerta (com base nas m√©tricas e regras definidas), ele delega ao Alertmanager a fun√ß√£o de envio de notifica√ß√µes e gerenciamento desses alertas. Isso inclui agregar alertas similares, evitar duplica√ß√µes, silenciar alertas durante manuten√ß√£o, e encaminh√°-los para canais apropriados (e-mail, sistemas de chat, PagerDuty, etc.).

Como funciona: voc√™ define no Prometheus regras de alerta (no arquivo de configura√ß√£o ou separado) com express√µes PromQL que identificam situa√ß√µes problem√°ticas. Por exemplo: "se a m√©trica `up` de um servidor for 0 por 5 minutos, dispare alerta". Quando a condi√ß√£o √© verdadeira, o Prometheus gera um evento de alerta e o envia para o Alertmanager (que est√° configurado na se√ß√£o `alerting` do prometheus.yml). O Alertmanager ent√£o aplica suas pr√≥prias regras de roteamento: por exemplo, enviar alertas de severidade cr√≠tica para um webhook do Slack e para email da equipe X, alertas menos graves s√≥ para email, etc...

**Exemplo pr√°tico:** Vamos configurar um alerta de servidor fora do ar com notifica√ß√£o no Slack.

1. **Definir regra de alerta (Prometheus):** Crie um arquivo `alert.rules.yml`:

```yaml
groups:
- name: instance_down
  rules:
    - alert: InstanceDown
      expr: up == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Inst√¢ncia {{ $labels.instance }} fora do ar"
        description: "O alvo {{ $labels.instance }} n√£o respondeu √†s coletas por mais de 1 minuto."
```

Essa regra verifica a m√©trica `up` de todos os alvos; se qualquer um estiver com valor 0 (significa alvo inacess√≠vel) por 1 minuto cont√≠nuo, aciona o alerta **InstanceDown** com severidade **critical**. As anota√ß√µes fornecem um resumo e descri√ß√£o usando templating (inserindo o label instance do alvo problem√°tico).

2. **Incluir regra e Alertmanager na config do Prometheus:** No `prometheus.yml`, adicionar:

```yaml
rule_files:
  - "alert.rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets: ['alertmanager:9093']
```

Aqui presumimos que o Alertmanager est√° rodando e acess√≠vel no endere√ßo `alertmanager:9093` (no Docker Compose, por ex.). O Prometheus agora carrega as regras de alerta e sabe para onde enviar notifica√ß√µes.

3. **Configurar o Alertmanager (alertmanager.yml):** Exemplo m√≠nimo para Slack:

```yaml
route:
  group_by: ['alertname']
  receiver: 'time-slack'
receivers:
  - name: 'time-slack'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/T000/B000/XXXXX'  # Webhook do Slack
        channel: '#alerts'
        send_resolved: true
        title: "{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}"
        text: "{{ range .Alerts }}{{ .Annotations.description }}{{ end }}"
```

Esse config muito b√°sico diz: todos alertas (n√£o importa o grupo\_by, etc.) ir√£o para o receptor nomeado 'time-slack', que tem um slack\_config apontando para um webhook do Slack no canal **#alerts**. O `title` e `text` da mensagem aproveitam as anota√ß√µes definidas na regra (summary e description). `send_resolved: true` indica para notificar tamb√©m quando o alerta for resolvido.

Em produ√ß√£o, o Alertmanager pode ter rotas mais elaboradas ‚Äì por exemplo, roteando com base em labels de alerta (team=A vai para equipe A, severidade critical pode mandar SMS, etc.), escalonamento, agrupamento por determinados campos (como agrupar todos alertas do mesmo datacenter numa s√≥ notifica√ß√£o), etc.

4. **Executar e testar:** Rode o Alertmanager com esse config (no Docker ou bin√°rio). Quando um alerta InstanceDown ocorrer, o Prometheus vai enviar para o Alertmanager, que em seguida usar√° a integra√ß√£o [Slack](https://prometheus.io/docs/alerting/latest/configuration/#slack-receiver) para postar no canal configurado uma mensagem com t√≠tulo "Inst√¢ncia X fora do ar" e descri√ß√£o com detalhes.

Esse foi um exemplo focado em Slack, mas o Alertmanager suporta muitos outros **receivers**: e-mail (SMTP), PagerDuty, OpsGenie, VictorOps, Webhooks gen√©ricos, entre outros. Com ele, voc√™ ganha flexibilidade para gerenciar o "barulho" de alertas: por exemplo, suprimir alertas filhos quando um pai j√° ocorreu ([inhibition](https://prometheus.io/docs/alerting/latest/configuration/#inhibition)), ou silenciar certos alertas durante janelas de manuten√ß√£o planejada.

> **Observa√ß√£o:** O Alertmanager n√£o √© obrigat√≥rio ‚Äì voc√™ pode rodar o Prometheus sem ele se n√£o precisar de notifica√ß√µes externas. Por√©m, para qualquer ambiente de produ√ß√£o, √© altamente recomendado configur√°-lo para n√£o depender de ficar olhando a p√°gina /alerts manualmente. Em outro artigo abordaremos em detalhes boas pr√°ticas de configura√ß√£o do Alertmanager.

### Alertmanager Avan√ßado: Silencing e Inhibition

Em ambientes de produ√ß√£o com muitos alertas, o **"alert fatigue"** (fadiga de alertas) pode ser um problema s√©rio. O Alertmanager oferece funcionalidades avan√ßadas para gerenciar esse cen√°rio: **silencing** (silenciamento) e **inhibition** (inibi√ß√£o).

#### Silencing

O **silencing** permite suprimir temporariamente alertas espec√≠ficos, geralmente durante janelas de manuten√ß√£o planejada. Isso evita spam desnecess√°rio quando voc√™ j√° sabe que um servi√ßo estar√° indispon√≠vel.

**Exemplo de configura√ß√£o de silence:**

```yaml
# Via API do Alertmanager (POST /api/v1/silences)
{
  "matchers": [
    {
      "name": "alertname",
      "value": "InstanceDown",
      "isRegex": false
    },
    {
      "name": "instance",
      "value": "web-server-01:9100",
      "isRegex": false
    }
  ],
  "startsAt": "2023-12-01T10:00:00Z",
  "endsAt": "2023-12-01T12:00:00Z",
  "createdBy": "admin",
  "comment": "Manuten√ß√£o programada do servidor web-01"
}
```

**Silencing via interface web:**
O Alertmanager oferece uma interface web em `/silences` onde voc√™ pode criar silences interativamente, especificando:
* **Matchers**: Labels que identificam os alertas a silenciar
* **Dura√ß√£o**: Per√≠odo de silenciamento (in√≠cio e fim)
* **Coment√°rio**: Justificativa para o silence

#### Inhibition

A **inhibition** permite suprimir alertas secund√°rios quando um alerta prim√°rio j√° est√° ativo. Por exemplo, se um servidor caiu (alerta cr√≠tico), n√£o faz sentido alertar sobre "disco quase cheio" ou "alta lat√™ncia" na mesma inst√¢ncia.

**Exemplo de configura√ß√£o de inhibition:**

```yaml
inhibit_rules:
  # Se um alerta critical estiver ativo, suprimir warnings da mesma inst√¢ncia
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['instance', 'job']
  
  # Se um datacenter estiver down, suprimir alertas de servi√ßos internos
  - source_match:
      alertname: 'DatacenterDown'
    target_match:
      severity: 'warning'
    equal: ['datacenter']
  
  # Se CPU estiver 100%, suprimir alertas de alta lat√™ncia
  - source_match:
      alertname: 'HighCPUUsage'
      severity: 'critical'
    target_match:
      alertname: 'HighLatency'
    equal: ['instance']
```

**Casos de uso comuns:**

* **Alertas de infraestrutura**: Se um rack/datacenter caiu, suprimir alertas de servi√ßos que dependem dele
* **Alertas de aplica√ß√£o**: Se um servi√ßo cr√≠tico est√° down, n√£o alertar sobre m√©tricas secund√°rias
* **Alertas de depend√™ncia**: Se um banco de dados est√° inacess√≠vel, suprimir alertas de aplica√ß√µes que dependem dele

#### Routing Avan√ßado

O Alertmanager permite roteamento sofisticado baseado em labels de alerta:

```yaml
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h
  
  routes:
    # Alertas cr√≠ticos v√£o para PagerDuty + Slack
    - match:
        severity: critical
      receiver: 'pager-duty-critical'
      continue: true  # Continua para o pr√≥ximo receiver
    
    # Alertas cr√≠ticos tamb√©m v√£o para Slack
    - match:
        severity: critical
      receiver: 'slack-critical'
    
    # Alertas de infraestrutura v√£o para equipe de infra
    - match:
        job: node
      receiver: 'infra-team'
    
    # Alertas de aplica√ß√£o v√£o para equipe de dev
    - match:
        job: app
      receiver: 'dev-team'
    
    # Default: todos os outros alertas v√£o para Slack geral
    - receiver: 'slack-general'

receivers:
  - name: 'pager-duty-critical'
    pagerduty_configs:
      - service_key: 'your-pagerduty-key'
  
  - name: 'slack-critical'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/...'
        channel: '#alerts-critical'
  
  - name: 'infra-team'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/...'
        channel: '#infra-alerts'
  
  - name: 'dev-team'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/...'
        channel: '#dev-alerts'
  
  - name: 'slack-general'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/...'
        channel: '#monitoring'
```

**Recursos avan√ßados:**

* **Agrupamento inteligente**: `group_by` agrupa alertas similares em uma notifica√ß√£o
* **Tempo de espera**: `group_wait` aguarda antes de enviar o primeiro alerta do grupo
* **Intervalo de repeti√ß√£o**: `repeat_interval` define quando reenviar alertas n√£o resolvidos
* **Roteamento condicional**: `continue: true` permite m√∫ltiplos receivers para o mesmo alerta

## PushGateway

O **[Pushgateway](https://prometheus.io/docs/instrumenting/pushing/)** √© um componente auxiliar do ecossistema Prometheus que permite coletar m√©tricas via modelo *push* em situa√ß√µes espec√≠ficas. A ideia √© que certos jobs ou aplicativos ef√™meros, que n√£o t√™m como serem raspados diretamente (por exemplo, um script cron que executa e termina rapidamente), possam empurrar suas m√©tricas para um gateway intermedi√°rio. O Prometheus ent√£o coleta essas m√©tricas do Pushgateway posteriormente.

Funciona assim: o job de curta dura√ß√£o (ou qualquer processo que n√£o viva tempo suficiente para ser raspado) envia um HTTP POST para o Pushgateway com suas m√©tricas no formato Prometheus. O Pushgateway armazena essas m√©tricas em mem√≥ria e as exp√µe em seu pr√≥prio `/metrics`. O Prometheus configura um scrape no Pushgateway, coletando tudo que estiver l√°.

**Por√©m,** √© importante entender que o Pushgateway deve ser usado com modera√ß√£o e prop√≥sito claro. Ele n√£o √© um agente gen√©rico para substituir o modelo pull. Alguns pontos de aten√ß√£o destacados pela documenta√ß√£o oficial:

* Se m√∫ltiplas inst√¢ncias usam um mesmo Pushgateway, ele vira um ponto central de falha e potencial gargalo.
* Voc√™ perde a detec√ß√£o autom√°tica de *down* (j√° que as m√©tricas s√£o push, o Prometheus n√£o sabe se um job n√£o est√° rodando ou s√≥ n√£o teve m√©tricas recentes).
* O Pushgateway **n√£o expira** automaticamente s√©ries que foram enviadas. Uma vez que uma m√©trica √© empurrada, ela ficar√° l√° at√© ser sobrescrita ou manualmente apagada via API do Pushgateway. Isso significa que m√©tricas de jobs antigos podem ficar persistindo como "fantasmas", exigindo que voc√™ gerencie remo√ß√£o ou inclus√£o de algum label de *instance* para distingui-las.

Devido a esses aspectos, o uso recomendado do Pushgateway √© **capturar resultados de jobs batch de n√≠vel de servi√ßo** ‚Äì isto √©, trabalhos que n√£o pertencem a uma √∫nica m√°quina ou inst√¢ncia espec√≠fica, mas sim algo como "um script de limpeza de banco que roda uma vez por dia". Nesse caso, o job emite (push) uma m√©trica do tipo "usuarios\_deletados\_total{job="cleanup"} 123" e termina. O Pushgateway guarda esse valor. O Prometheus, ao raspar, ter√° essa informa√ß√£o agregada do job. Como esse tipo de job n√£o tem um "endpoint" pr√≥prio para scrap, o Pushgateway serve como cache.

Para outros cen√°rios, onde o push √© considerado porque h√° firewall/NAT impedindo scrapes, a documenta√ß√£o sugere alternativas melhores ‚Äì como rodar Prometheus perto dos alvos (dentro da rede) ou usar algo como o **[PushProx](https://github.com/prometheus/pushprox)** para atravessar firewalls mantendo o modelo pull. E para jobs cron por m√°quina, que t√™m contexto de host, recomenda-se usar o **[Node Exporter Textfile Collector](https://github.com/prometheus/node_exporter#textfile-collector)** (escrever m√©tricas em um arquivo que o Node Exporter l√™), ao inv√©s do Pushgateway.

> Resumindo: o Pushgateway √© √∫til, mas **somente** em casos espec√≠ficos. Evite us√°-lo para coletar m√©tricas de servi√ßos normais (isso seria ‚Äúusar push por pregui√ßa‚Äù, e acarretaria problemas de dados stale e falta de detec√ß√£o de falha). Use-o para jobs batch pontuais, e mesmo assim, sem abusar ‚Äì lembre-se de limpar m√©tricas antigas se necess√°rio, ou projetar os labels de modo que cada job substitua seu pr√≥prio valor sem acumular lixo.

## Federa√ß√£o

A **federa√ß√£o** no Prometheus permite que uma inst√¢ncia do Prometheus (geralmente chamada de **federadora** ou **global**) fa√ßa scrape em endpoints de outras inst√¢ncias do Prometheus (**federadas**) para obter um subconjunto de suas m√©tricas. Em outras palavras, √© uma forma de **hierarquizar** o monitoramento: por exemplo, voc√™ pode ter um Prometheus por data center coletando tudo localmente, e um Prometheus global que apenas busca m√©tricas j√° agregadas de cada data center para ter uma vis√£o geral corporativa. Existem dois casos de uso principais para federa√ß√£o:

1. **[Agrega√ß√£o hier√°rquica](https://prometheus.io/docs/prometheus/latest/federation/)**: como no exemplo acima, onde cada Prometheus local faz o trabalho pesado e calcula agregados (soma de CPU por datacenter, lat√™ncia m√©dia de servi√ßo X por datacenter, etc.), e o Prometheus global s√≥ extrai essas s√©ries agregadas prontas. Isso d√° uma vis√£o do todo sem sobrecarregar a inst√¢ncia global com todas as s√©ries detalhadas.

2. **[Checagens cruzadas ou seletivas](https://prometheus.io/docs/prometheus/latest/federation/)**: Puxar algumas poucas m√©tricas de outra inst√¢ncia para compara√ß√µes. Exemplo: voc√™ tem um Prometheus dedicado a HAProxy e outro para um app front-end, pode federar a m√©trica de QPS do HAProxy no Prometheus do front-end para checar se ambos observam o mesmo tr√°fego. Normalmente, isso √© usado at√© mesmo apenas para alertas (voc√™ pode configurar alertas usando essas poucas m√©tricas federadas).

**[Quando N√ÉO usar federa√ß√£o](https://prometheus.io/docs/prometheus/latest/federation/#when-not-to-use-federation):** a tenta√ß√£o de federar tudo de todos os Prometheus em um ‚Äúsuper Prometheus‚Äù central deve ser evitada. Pegar todas as s√©ries de inst√¢ncias filhas e centralizar em uma s√≥ inst√¢ncia global traz v√°rios problemas:

* **Escalabilidade limitada:** O desempenho do Prometheus √© limitado pelos recursos de um √∫nico n√≥ (n√£o escala horizontalmente). Se voc√™ puxa tudo para um s√≥ servidor global, no fim do dia voc√™ est√° limitado ao throughput e mem√≥ria de uma m√°quina. Isso anula a distribui√ß√£o de carga que m√∫ltiplas inst√¢ncias proporcionam.
* **Performance e carga duplicada:** Al√©m de sobrecarregar a inst√¢ncia global ao ter que armazenar e consultar tudo, a pr√≥pria opera√ß√£o de federa√ß√£o (expor /federate e responder a scraping) gera carga nas inst√¢ncias filhas. Se a consulta federada n√£o for focada (usar express√µes match\[] gen√©ricas demais), pode consumir muitos recursos para as inst√¢ncias fonte servirem esses dados.
* **Confiabilidade reduzida:** Voc√™ adiciona um ponto extra de falha. Se o link entre uma inst√¢ncia local e a global cair, a inst√¢ncia global ‚Äúfica cega‚Äù √†quele segmento. E pior, se voc√™ centralizou a avalia√ß√£o de certos alertas s√≥ no global, pode ficar sem alertas (falso negativo) caso o global perca conex√£o com os locais. A recomenda√ß√£o de especialistas √© sempre que poss√≠vel avaliar alertas o mais localmente poss√≠vel ‚Äì por exemplo, um alerta ‚Äúservi√ßo X caiu‚Äù deve ser definido no Prometheus que coleta servi√ßo X, n√£o em um global distante, exatamente para n√£o depender de rede.
* **Delay e poss√≠veis inconsist√™ncias:** A federa√ß√£o n√£o √© instant√¢nea; h√° lat√™ncia entre um dado ser coletado no Prometheus filho e ser federado pelo pai. Al√©m disso, condi√ß√µes de corrida podem fazer o global perder algumas amostras ou ver valores ligeiramente diferentes (por exemplo, contadores que resetaram podem parecer estranhos). Para uns poucos agregados isso √© toler√°vel, mas se voc√™ federar tudo e depender disso para alertar, pode ter sutilezas indesejadas.
* **Complexidade de configura√ß√£o e seguran√ßa:** √â mais complexo gerenciar dois n√≠veis de Prometheus, com configura√ß√µes de match\[], externas labels √∫nicas por inst√¢ncia, etc. Tamb√©m √© necess√°rio expor o endpoint /federate das inst√¢ncias filhas ‚Äì o que pode ampliar a superf√≠cie de ataque ou requerer configura√ß√µes TLS, autentica√ß√£o, caso atravesse redes n√£o confi√°veis.

Em raz√£o desses fatores, a federa√ß√£o deve ser usada **apenas** para casos de uso bem planejados (tipicamente agrega√ß√µes de baixo volume ou m√©tricas espec√≠ficas). N√£o √© a solu√ß√£o adequada para reten√ß√£o de longo prazo nem para alta disponibilidade.

> **NOTA:** Para necessidades de **escalabilidade horizontal** e **armazenamento de longo prazo**, surgiram outros projetos que complementam o Prometheus, como **Thanos**, **Cortex** e **Mimir** (Grafana Labs). Essas solu√ß√µes armazenam as s√©ries em storage distribu√≠do (objeto, bigtable, etc.) e permitem ‚Äújuntar‚Äù m√∫ltiplas inst√¢ncias como se fossem uma s√≥, suportando consultas globais e reten√ß√£o virtualmente infinita. Exploraremos essas alternativas em outro artigo, mas adianta-se que elas resolvem muitos dos problemas de tentar usar federa√ß√£o pura para esses fins.

## Remote Write e Remote Read

O Prometheus pode ser configurado para enviar suas m√©tricas em tempo real para bancos externos (**remote write**) e buscar dados hist√≥ricos de outros sistemas (**remote read**). Essa funcionalidade √© fundamental para integra√ß√£o com solu√ß√µes de armazenamento de longo prazo, compliance e an√°lise de dados.

### Remote Write

O **remote write** permite que o Prometheus envie amostras coletadas para sistemas externos em tempo real, mantendo uma c√≥pia local. Isso √© √∫til para:

* **Reten√ß√£o de longo prazo**: Enviar dados para sistemas como InfluxDB, TimescaleDB, ou solu√ß√µes cloud
* **Compliance e auditoria**: Manter m√©tricas por meses/anos para requisitos regulat√≥rios
* **Machine Learning**: Integrar com plataformas de ML para an√°lise preditiva
* **Correla√ß√£o de dados**: Combinar m√©tricas com logs e traces em sistemas unificados

**Exemplo de configura√ß√£o:**

```yaml
remote_write:
  - url: "https://longterm.example.com/api/v1/write"
    basic_auth:
      username: "prometheus"
      password: "password"
    write_relabel_configs:
      - source_labels: [__name__]
        regex: 'node_.*'
        action: keep
    queue_config:
      max_samples_per_send: 1000
      max_shards: 30
      capacity: 2500
```

**Configura√ß√µes importantes:**

* **`url`**: Endpoint do sistema de destino
* **`basic_auth`**: Autentica√ß√£o b√°sica (tamb√©m suporta TLS)
* **`write_relabel_configs`**: Filtros para enviar apenas m√©tricas espec√≠ficas
* **`queue_config`**: Configura√ß√µes de buffer e performance

### Remote Read

O **remote read** permite que o Prometheus busque dados hist√≥ricos de sistemas externos, como se fossem parte do seu TSDB local. Isso √© √∫til para:

* **Consultas hist√≥ricas**: Acessar dados antigos sem manter tudo localmente
* **Migra√ß√£o de dados**: Transi√ß√£o gradual entre sistemas de armazenamento
* **An√°lise retrospectiva**: Investigar incidentes passados com dados completos

**Exemplo de configura√ß√£o:**

```yaml
remote_read:
  - url: "https://longterm.example.com/api/v1/read"
    basic_auth:
      username: "prometheus"
      password: "password"
    read_recent: true
    required_matchers:
      - label: "job"
        value: "node"
```

### Casos de Uso T√≠picos

**1. Integra√ß√£o com Grafana Cloud:**
```yaml
remote_write:
  - url: "https://prometheus-prod-XX-XXX.grafana.net/api/prom/push"
    basic_auth:
      username: "12345"
      password: "glc_eyJvIjoiOTk5OTkiLCJuIjoiYWRtaW4iLCJpIjoiMTIzNDU2Nzg5MCJ9"
```

**2. Envio para InfluxDB:**
```yaml
remote_write:
  - url: "http://influxdb:8086/api/v2/prom/write?org=myorg&bucket=prometheus"
    basic_auth:
      username: "admin"
      password: "password"
```

**3. M√∫ltiplos destinos:**
```yaml
remote_write:
  - url: "https://backup-storage.example.com/write"
    write_relabel_configs:
      - source_labels: [__name__]
        regex: '.*'
        action: keep
  - url: "https://ml-platform.example.com/metrics"
    write_relabel_configs:
      - source_labels: [__name__]
        regex: 'app_.*'
        action: keep
```

> **Importante**: Remote write/read n√£o substitui o armazenamento local do Prometheus. O TSDB local continua sendo usado para consultas recentes e alertas. O remote write √© **aditivo** - voc√™ mant√©m os dados locais e envia uma c√≥pia para sistemas externos.

## Under the Hood

Nesta se√ß√£o, vamos dissecar o funcionamento interno do armazenamento de dados do Prometheus ‚Äì o **[Time Series Database](https://prometheus.io/docs/introduction/architecture/#time-series-database)** (TSDB) local ‚Äì e entender por que ele consome recursos como consome.

Quando instalamos o Prometheus, uma pasta de dados (por padr√£o chamada `data/`) √© usada para persistir as s√©ries temporais coletadas. Dentro dela, os dados s√£o organizados em blocos de tempo fixo. Por padr√£o, cada **bloco** cobre 2 horas de m√©tricas. Ap√≥s duas horas de coleta, o Prometheus fecha aquele bloco e inicia outro. Periodicamente, v√°rios blocos menores podem ser compactados em blocos maiores (por exemplo, 5 blocos de 2h podem ser mesclados num bloco de 10h de dados, e assim por diante). A estrutura de arquivos t√≠pica em `data/` √© assim (exemplo simplificado):

```
data/
‚îú‚îÄ‚îÄ 01GZY5ABCD.../       # pasta de um bloco de dados
‚îÇ   ‚îú‚îÄ‚îÄ meta.json        # metadados do bloco
‚îÇ   ‚îú‚îÄ‚îÄ index            # √≠ndice para busca das s√©ries no bloco
‚îÇ   ‚îú‚îÄ‚îÄ chunks/          # peda√ßos contendo os samples comprimidos
‚îÇ   ‚îî‚îÄ‚îÄ tombstones       # (pode estar vazio) marca√ß√µes de dele√ß√£o
‚îú‚îÄ‚îÄ 01GZY1WXYZ.../       # outro bloco (mais antigo, por ex)
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ chunks_head/         # chunks do bloco "head" atual (em uso)
‚îî‚îÄ‚îÄ wal/                 # Write-Ahead Log (log de escrita recente)
    ‚îú‚îÄ‚îÄ 00000000
    ‚îú‚îÄ‚îÄ 00000001
    ‚îî‚îÄ‚îÄ checkpoint.000001/ ...
```

Cada bloco de 2h √© identificado por um **[ULID](https://github.com/prometheus/prometheus/blob/main/tsdb/encoding/ulid.go)** (ID √∫nico lexicograficamente orden√°vel) que comp√µe o nome da pasta. Dentro de um bloco, temos:

* **meta.json:** arquivo JSON com metadados do bloco (faixa de tempo coberta, stats de quantas s√©ries/amostras cont√©m, hist√≥rico de compacta√ß√£o, etc.).
* **index:** arquivo de √≠ndice invertido para permitir procurar s√©ries rapidamente pelo nome e labels, e localizar em quais chunks est√£o seus dados.
* **chunks/**: diret√≥rio contendo os arquivos bin√°rios de chunks de dados. Os *chunks* s√£o os blocos comprimidos de amostras das s√©ries. Cada arquivo (nomeado como 000001, 000002, ...) cont√©m muitos chunks. O tamanho m√°ximo de cada arquivo √© \~512MB para facilitar gerenciamento.
* **tombstones:** arquivo que registra intervalos de dados deletados manualmente (via API de delete), se houver.

Al√©m dos blocos fechados, existe o **[Head block](https://prometheus.io/docs/introduction/architecture/#head-block)** (bloco atual em mem√≥ria) que armazena as m√©tricas em curso. Os dados mais recentes (√∫ltimas \~2h) residem em mem√≥ria para escrita r√°pida e consultas de curt√≠ssimo prazo. A cada 2h, o Prometheus ‚Äúdissolve‚Äù parte do Head em um bloco persistente e libera daquela mem√≥ria. Vamos inspecionar um exemplo de **meta.json** para entender seus campos:

```json
{
    "ulid": "01BKGTZQ1SYQJTR4PB43C8PD98",
    "minTime": 1602237600000,
    "maxTime": 1602244800000,
    "stats": {
        "numSamples": 553673232,
        "numSeries": 1346066,
        "numChunks": 4440437
    },
    "compaction": {
        "level": 1,
        "sources": [
            "01EM65SHSX4VARXBBHBF0M0FDS",
            "01EM6GAJSYWSQQRDY782EA5ZPN"
        ]
    },
    "version": 1
}
```

Explicando os campos principais:

* **ulid:** Identificador √∫nico do bloco (um c√≥digo 128-bit parecido com [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier)). Ele √© tamb√©m o nome da pasta do bloco.
* **minTime e maxTime:** Timestamp inicial e final (epoch em milissegundos) cobertos pelos samples deste bloco. No exemplo, corresponde a um intervalo de 2h.
* **stats:** Estat√≠sticas do bloco ‚Äì quantas amostras ([numSamples](https://prometheus.io/docs/introduction/architecture/#head-block)), s√©ries ([numSeries](https://prometheus.io/docs/introduction/architecture/#head-block)) e chunks ([numChunks](https://prometheus.io/docs/introduction/architecture/#head-block)) est√£o armazenados nele. No exemplo real acima, temos \~1,34 milh√£o de s√©ries distintas, totalizando 553 milh√µes de amostras em \~4,44 milh√µes de chunks dentro desse bloco de 2h. Esses n√∫meros d√£o uma no√ß√£o do volume de dados.
* **compaction:** Informa o hist√≥rico de compacta√ß√£o. **[level](https://prometheus.io/docs/introduction/architecture/#head-block)** indica quantas vezes j√° foi compactado (1 significa um bloco resultante da jun√ß√£o de outros menores). **[sources](https://prometheus.io/docs/introduction/architecture/#head-block)** lista os IDs dos blocos que foram combinados para formar este (no caso, dois blocos anteriores). Se o bloco foi gerado direto do Head (dados ‚Äúoriginais‚Äù), √†s vezes sources cont√©m ele pr√≥prio.
* **version:** Vers√£o do formato do bloco/arquivo (para compatibilidade futura).

Com isso, entendemos que cada bloco √© imut√°vel depois de escrito. Se novos dados chegam daquele intervalo, seria criado um bloco novo via compaction. Isso facilita a confiabilidade ‚Äì dados hist√≥ricos n√£o mudam.

O **arquivo de √≠ndice (index)** serve para mapear as s√©ries e labels aos chunks dentro do bloco. Ele funciona como um √≠ndice invertido: dado um nome de m√©trica e um conjunto de labels, encontra os IDs das s√©ries correspondentes e, ent√£o, aponta para os chunks onde est√£o os dados daquela s√©rie. Assim, ao fazer uma consulta, o Prometheus carrega o √≠ndice do bloco relevante e consegue buscar rapidamente somente os chunks necess√°rios (por exemplo, pula chunks inteiros que est√£o fora do range de tempo consultado, usando informa√ß√µes de minTime/maxtime dos chunks). 

O √≠ndice √© altamente otimizado e comprimido ‚Äì usa conceitos de [posting lists](https://prometheus.io/docs/introduction/architecture/#posting-lists) (listas de IDs de s√©ries para cada label-valor) e [tabelas de s√≠mbolos](https://prometheus.io/docs/introduction/architecture/#symbol-table) para strings √∫nicas. Esses detalhes avan√ßados fogem do escopo aqui, mas o importante √©: o √≠ndice permite que mesmo com milh√µes de s√©ries por bloco, o Prometheus consiga localizar dados sem varrer tudo linearmente.

Finalmente, o **[WAL (Write-Ahead Log)](https://prometheus.io/docs/introduction/architecture/#write-ahead-log)**: √© um log de transa√ß√µes recente onde cada amostra coletada √© gravada imediatamente no disco antes de ser inserida na mem√≥ria do Head. Isso garante que se o Prometheus cair inesperadamente, ao voltar ele pode reprocessar o WAL e recuperar as amostras que ainda n√£o tinham sido compactadas em blocos. O WAL consiste em arquivos sequenciais (`00000000`, `00000001`, etc.) que v√£o acumulando as escritas. Periodicamente, o Prometheus faz um checkpoint (snapshot do head) e limpa parte do WAL j√° aplicado. Em caso de crash, ele l√™ desde o √∫ltimo checkpoint para restaurar o estado do Head.

### Gerenciamento de mem√≥ria pelo Prometheus

O Prometheus armazena as s√©ries temporais em mem√≥ria para r√°pido acesso √†s m√©tricas recentes, enquanto grava continuamente os novos dados no disco (WAL) para durabilidade. Isso pode levar a alto uso de RAM e espa√ßo em disco.

![](https://raw.githubusercontent.com/scovl/scovl.github.io/main/post/images/tsdb/prom-mem02.png)

Como mencionado, o Prometheus mant√©m em RAM todas as s√©ries ativas do bloco atual (tipicamente √∫ltimas 2 horas de dados por s√©rie). Essa decis√£o arquitetural visa desempenho: consultas sobre dados recentes (que s√£o as mais comuns, e.g. alertas e dashboards de curto prazo) n√£o precisam esperar leitura de disco ‚Äì os valores j√° est√£o na mem√≥ria. 

Al√©m disso, novas amostras sendo inseridas a cada segundo/minuto s√£o agregadas a estruturas em mem√≥ria (evitando I/O de disco a cada opera√ß√£o, que seria invi√°vel em alta escala). O resultado √© que o **consumo de RAM** do Prometheus cresce com o n√∫mero de s√©ries ativas e com a frequ√™ncia de coleta. Estima-se, por experi√™ncias reportadas, que cada s√©rie ativa consome em torno de **\~3 KB de RAM** (depende de labels, comprimento do nome, etc.). Portanto, 1 milh√£o de s√©ries pode usar na ordem de 3‚Äì4 GB de RAM apenas para manter o head da TSDB.

Em paralelo, o Prometheus escreve todas as amostras no WAL (em disco) para n√£o perd√™-las em caso de crash. A cada 2 horas, ele ent√£o compacta esses dados quentes em um bloco de 2h comprimido e libera a mem√≥ria correspondente. Ou seja, h√° um ciclo onde a mem√≥ria vai sendo ocupada pelas amostras recentes, e de hora em hora (na verdade 2h) h√° um flush para disco que esvazia um pouco a mem√≥ria (mas novas s√©ries podem surgir e ocupar de novo).

O *design* de manter dados recentes em mem√≥ria traz a consequ√™ncia de que **o uso de RAM aumenta com a carga de m√©tricas e n√£o √© liberado at√© que os blocos sejam fechados ou as s√©ries cessem**. Em per√≠odos de pico (muitas s√©ries novas aparecendo rapidamente), o Prometheus pode chegar a consumir muita mem√≥ria para acompanhar. Se faltar RAM, o processo corre risco de OOM (matar por falta de mem√≥ria) ou, no melhor caso, o sistema operacional vai come√ßar a usar swap ‚Äì o que degrada muito a performance. Na imagem acima, vemos que tanto a RAM quanto o armazenamento em disco podem crescer substancialmente √† medida que aumentamos o volume de dados monitorados.

> **Quanto mais dias de reten√ß√£o mantidos no Prometheus, mais recursos s√£o usados e maior o esfor√ßo para consultas longas. Manter dados hist√≥ricos demais pode sobrecarregar a mem√≥ria e o disco, al√©m de dificultar encontrar informa√ß√µes recentes relevantes.**

Embora possamos configurar reten√ß√µes longas (30, 60 dias), isso n√£o significa que o Prometheus foi otimizado para operar eficientemente com esse hist√≥rico todo localmente. Lembre-se: ele n√£o indexa por data de forma distribu√≠da ‚Äì consultas que abrangem muitos dias ter√£o que ler v√°rios blocos do disco e processar um grande volume de amostras. Na pr√°tica, reter al√©m de algumas semanas come√ßa a tornar as consultas bem lentas e o uso de disco muito alto (sem falar nos backups dessa quantidade de data). A imagem acima ilustra que, √† medida que guardamos mais dias, o custo de recursos cresce e pode inclusive ofuscar tend√™ncias atuais no meio de tanto dado antigo.

![](https://raw.githubusercontent.com/scovl/scovl.github.io/main/post/images/tsdb/prom-mem03.png)

A filosofia do Prometheus √© ser a ferramenta de **monitoramento em tempo real** e de curto/m√©dio prazo. Para an√°lises hist√≥ricas longas ou compliance (guardar m√©tricas por 1 ano, por exemplo), a solu√ß√£o comum √© integrar um back-end de longo prazo (Thanos, Cortex, databases remotas) que arquivem esses dados, enquanto o Prometheus local mant√©m s√≥ o necess√°rio para opera√ß√£o/alertas recentes. Assim voc√™ tem o melhor dos dois mundos: rapidez no real-time e hist√≥rico completo dispon√≠vel quando precisar, sem sobrecarregar o Prometheus diariamente.

> Todas as amostras recentes residem na mem√≥ria principal (Head), com flush peri√≥dico para disco a cada 2 horas. O WAL no disco captura as escritas para garantir durabilidade. Em situa√ß√£o de carga extrema, o OS pode usar swap, mas isso deve ser evitado pois degrada o desempenho.

Vamos recapitular o ciclo de vida dos dados no Prometheus e seu impacto em mem√≥ria/disco:

* **Head Block (mem√≥ria):** Novas s√©ries e amostras entram aqui. As s√©ries ativas ocupam estruturas na heap da aplica√ß√£o Go do Prometheus. A cada amostra recebida, ela tamb√©m √© anexada no **[WAL](https://prometheus.io/docs/introduction/architecture/#write-ahead-log)** (no SSD/disco) para registro permanente. Durante at√© \~2h, os dados ficam dispon√≠veis no Head para consultas instant√¢neas. Por isso, consultas e alertas em dados "frescos" s√£o muito r√°pidas.

* **Flush para bloco persistente:** Quando o intervalo de 2h se completa, o Prometheus corta o bloco (na verdade ele espera 2h ou 1h30 dependendo de certas condi√ß√µes) e escreve um **[novo bloco](https://prometheus.io/docs/introduction/architecture/#head-block)** no diret√≥rio data (contendo aqueles 2h de amostras agora imut√°veis, j√° comprimidas). Em seguida, libera da mem√≥ria boa parte das estruturas referentes √†quele intervalo. O head ent√£o mant√©m somente as s√©ries ainda ativas que extrapolem o pr√≥ximo bloco.

* **Compaction:** Ap√≥s algumas rota√ß√µes de bloco, o Prometheus agrupa blocos menores em blocos maiores (por exemplo, une 5 blocos de 2h em 1 bloco de 10h, e assim por diante). Isso ocorre em segundo plano e ajuda a reduzir o n√∫mero de arquivos e melhorar compress√£o geral. Compaction consome CPU/disk I/O, mas √© intercalado para n√£o interferir muito.

* **Reten√ß√£o e cleanup:** Quando um bloco excede a reten√ß√£o configurada (ex: ficou mais velho que 15 dias), ele √© marcado para dele√ß√£o. A limpeza ocorre periodicamente e remove blocos expirados. Importante: a remo√ß√£o n√£o √© imediata ao passar do prazo ‚Äì o processo de cleanup roda em intervalos (at√© 2h de delay). Durante a limpeza, o Prometheus deleta os diret√≥rios daqueles blocos antigos, liberando espa√ßo em disco.

* **Rein√≠cio e recupera√ß√£o:** Se o Prometheus reiniciar ou cair, na inicializa√ß√£o ele precisa recarregar o estado. Ele vai abrir todos os blocos persistentes (apenas meta e √≠ndice, sem carregar todos os dados) e principalmente processar o WAL para recriar o Head com as amostras que ainda n√£o estavam em bloco. Esse processo de recupera√ß√£o do WAL pode demorar dependendo do tamanho (por isso h√° checkpoint para otimizar). Ao final, o sistema retorna ao estado como se nunca tivesse parado (exceto pelos minutos offline onde dados podem ter se perdido se os alvos n√£o suportam retroativa).

Tudo isso explica por que o Prometheus consome **bastante mem√≥ria**: ele aposta em manter as s√©ries recentes acess√≠veis e indexadas para respostas r√°pidas. Num Prometheus com muitos alvos ou alta cardinalidade (muitas combina√ß√µes de labels), o consumo de RAM pode facilmente ser o principal limitador. Conforme mencionado anteriormente, 1 milh√£o de s√©ries ativas pode exigir v√°rios GB de RAM, portanto planeje a capacidade de acordo com o volume de m√©tricas esperado.

Infelizmente, n√£o h√° muito **tunings** manuais a fazer na mem√≥ria al√©m de reduzir a quantidade de dados: **menos s√©ries ou menor frequ√™ncia de coleta** = menos uso de RAM. O Prometheus n√£o tem um mecanismo interno de shard autom√°tico ou flush mais frequente (o flush √© fixo \~2h por design). Ent√£o, as solu√ß√µes se resumem a **escalar verticalmente** (m√°quinas com mais mem√≥ria, CPU, disco r√°pido) ou **escalar horizontalmente** (dividir a carga entre v√°rios Prometheus, cada um monitorando uma parte das targets). Nas melhores pr√°ticas a seguir, daremos dicas para mitigar esses desafios de desempenho e dimensionamento.

### Native Histograms (Recurso Experimental)

O Prometheus introduziu **Native Histograms** como um recurso experimental nas vers√µes mais recentes (2.40+). Essa funcionalidade representa uma evolu√ß√£o significativa na forma como histogramas s√£o armazenados e consultados.

#### Diferen√ßas dos Histogramas Tradicionais

**Histogramas tradicionais:**
* Usam buckets predefinidos (ex: 0.1, 0.5, 1.0, 2.5, 5.0, 10.0)
* Cada bucket gera uma s√©rie separada (`_bucket`)
* Requerem m√∫ltiplas s√©ries para representar uma distribui√ß√£o
* Limitados pela granularidade dos buckets

**Native Histograms:**
* Usam buckets din√¢micos e adaptativos
* Armazenam a distribui√ß√£o completa em uma √∫nica s√©rie
* Permitem maior precis√£o nos percentis
* Reduzem significativamente o n√∫mero de s√©ries

#### Configura√ß√£o

Para habilitar native histograms, adicione a flag experimental:

```bash
prometheus --enable-feature=native-histograms
```

Ou no Docker:

```yaml
command:
  - '--enable-feature=native-histograms'
```

#### Exemplo de Uso

**Instrumenta√ß√£o com native histograms (Go):**
```go
import (
    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promauto"
)

var (
    requestDuration = promauto.NewHistogram(prometheus.HistogramOpts{
        Name: "http_request_duration_seconds",
        Help: "Duration of HTTP requests",
        NativeHistogramBucketFactor: 1.1,  // Fator de crescimento dos buckets
        NativeHistogramMaxBucketNumber: 100, // M√°ximo de buckets
    })
)
```

**Consulta de percentis:**
```promql
# Percentil 95 usando native histogram
histogram_quantile(0.95, rate(http_request_duration_seconds[5m]))

# Percentil 99
histogram_quantile(0.99, rate(http_request_duration_seconds[5m]))
```

#### Vantagens

* **Menos s√©ries**: Uma m√©trica de lat√™ncia que antes gerava 10+ s√©ries agora gera apenas 1
* **Maior precis√£o**: Buckets adaptativos capturam melhor a distribui√ß√£o real
* **Melhor performance**: Menos overhead de armazenamento e consulta
* **Compatibilidade**: Funciona com todas as fun√ß√µes PromQL existentes

#### Considera√ß√µes

* **Experimental**: Ainda em desenvolvimento, pode ter mudan√ßas na API
* **Migra√ß√£o**: Requer atualiza√ß√£o das bibliotecas cliente
* **Compatibilidade**: Funciona apenas com vers√µes recentes do Prometheus

> **Nota**: Native histograms s√£o especialmente √∫teis para m√©tricas de lat√™ncia em aplica√ß√µes de alta performance, onde a precis√£o dos percentis √© cr√≠tica.

## Melhores Pr√°ticas

Depois de entender a mec√¢nica interna do Prometheus, √© v√°lido reunir algumas recomenda√ß√µes para tirar o melhor proveito da ferramenta de forma escal√°vel e confi√°vel.

### Planejamento de Capacidade

* **Estime volume de m√©tricas e reten√ß√£o:** Antes de implantar, fa√ßa uma estimativa do n√∫mero de s√©ries que voc√™ vai coletar e defina uma reten√ß√£o condizente. Lembre que por padr√£o s√£o 15 dias. Se n√£o precisar de tudo isso para monitoramento di√°rio, reten√ß√µes menores aliviam recursos. Ao contr√°rio, se precisar de mais tempo hist√≥rico, esteja ciente do aumento de disco e possivelmente avalie armazenamento remoto.

* **Monitore o Prometheus em si:** "Quis custodiet ipsos custodes?" ‚Äì o Prometheus exp√µe suas pr√≥prias m√©tricas (no endpoint /metrics dele). Use um outro Prometheus ou a mesma inst√¢ncia para monitorar m√©tricas como `prometheus_tsdb_head_series` (n√∫mero de s√©ries no head), `prometheus_tsdb_head_samples_appended_total` (samples inseridos por segundo), `prometheus_engine_query_duration_seconds` (lat√™ncia das consultas), etc. Isso alerta para crescimento de cardinalidade inesperado ou consultas muito pesadas rodando.

* **Dimensione hardware adequadamente:** Regra emp√≠rica: 1 CPU core pode processar aproximadamente at√© 200k amostras por segundo (varia, mas √© uma ideia). Mem√≥ria, calcule \~3kB por s√©rie ativa. Disco: \~1-2 bytes por amostra armazenada comprimida (15 dias, 200 milh√µes de amostras \~ 200-300MB). Use SSDs r√°pidos ‚Äì opera√ß√µes de WAL e blocos beneficiam de I/O r√°pido.

### Organiza√ß√£o de M√©tricas e Labels

* **Consist√™ncia na nomea√ß√£o:** Siga conven√ß√µes de nomenclatura para facilitar a vida. Use nomes descritivos e padronizados (letras min√∫sculas, separadas por underscores, unidade no sufixo se aplic√°vel: `_seconds`, `_bytes`, `_total` para contadores acumulativos). Por exemplo, prefira `app_memory_usage_bytes` a algo como `MemUsed` ou outras varia√ß√µes inconsistentes. Isso ajuda todo mundo a entender do que se trata sem ambiguidade.

* **Labels estrat√©gicos:** Anexe labels que fa√ßam sentido de consulta, mas evite rotular com informa√ß√µes que tenham alta cardinalidade ou unicidade. Um bom label √© algo como `region`, `datacenter`, `instance` (desde que este n√£o seja √∫nico por m√©trica ‚Äì use instance s√≥ onde faz sentido). Maus labels incluem: ID de requisi√ß√£o, nome de usu√°rio, URL completa (em vez de caminho gen√©rico), timestamp, IP din√¢mico de cliente. Esses valores criam um n√∫mero enorme de s√©ries distintas. Lembre-se: cada combina√ß√£o diferente de labels vira **uma s√©rie separada** no TSDB. Se voc√™ tiver 1000 usu√°rios e rotular m√©tricas por usu√°rio, virou 1000 s√©ries onde antes podia ser 1 ou algumas. Leve isso em conta.

* **Explos√£o de cardinalidade:** √â um dos problemas mais comuns. Por exemplo, adicionar um label `product_id` a uma m√©trica de pedidos, onde product\_id pode assumir dezenas de milhares de valores, multiplicar√° as s√©ries. Isso pode levar o Prometheus a consumir toda mem√≥ria e travar. Portanto, s√≥ use labels cujo conjunto de valores poss√≠vel seja **limitado e relativamente pequeno**. (Regra de bolso: algumas dezenas ou poucas centenas de valores diferentes por label no m√°ximo. Mais que isso, pense duas vezes se √© necess√°rio.) Caso precise monitorar algo muito cardinal (ex: m√©tricas por usu√°rio √∫nico), talvez o Prometheus n√£o seja a ferramenta adequada ou voc√™ precisa agreg√°-las antes de expor.

* **M√©tricas altas vs baixas cardinalidades:** Prefira m√©tricas mais agregadas. Por exemplo, em vez de registrar uma m√©trica separada para cada item em fila (que n√£o faz sentido), registre o tamanho da fila como um gauge. Em vez de m√©tricas por sess√£o de usu√°rio, exponha total global ou por categoria de usu√°rio. Enfim, modele os dados de forma a minimizar detalhes desnecess√°rios.

### Consultas (PromQL) Eficientes

* **Cuidado com fun√ß√µes custosas:** Algumas fun√ß√µes PromQL podem ser muito √∫teis, por√©m custosas. `topk()` e `bottomk()`, por exemplo, obrigam o engine a ordenar muitas s√©ries para achar o top N ‚Äì pode ser caro se aplicado numa m√©trica com milhares de s√©ries. Use-as com modera√ß√£o (talvez em queries de background para dashboard, mas evite em alertas cr√≠ticos se poss√≠vel). Similar para agrega√ß√µes sem restri√ß√£o: `sum by (label)` onde label tem muitos valores, o Prometheus ter√° que materializar todas combina√ß√µes.

* **Use intervalos de tempo adequados:** Querys do tipo *\[5m]*, *\[1h]* etc. definem quanto tempo de dados v√£o considerar. Evite pedir mais do que precisa. Por exemplo, se um alerta precisa saber a taxa nos √∫ltimos 5 minutos, n√£o use 1h. Intervalos maiores = mais dados lidos e processados. Num gr√°fico, tamb√©m n√£o exagere no zoom out se n√£o for necess√°rio ‚Äì muitos dados tornam a renderiza√ß√£o e transmiss√£o pesadas.

* **Prefira `rate()` ou `increase()` para contadores ao inv√©s de `irate()` para alertas cont√≠nuos:** A fun√ß√£o `irate()` calcula instantaneamente a derivada entre os dois √∫ltimos pontos ‚Äì isso √© √∫til √†s vezes, mas tende a ser muito "barulhento" (varia√ß√£o instante a instante). Em dashboards e alertas gerais, `rate()` numa janela de pelo menos 1m ou 5m √© mais est√°vel e representativo da taxa m√©dia. Use `irate` somente quando quer realmente capturar spikes moment√¢neos e tem alta frequ√™ncia de scrape.

* **Agregue no scraping quando poss√≠vel:** Se voc√™ j√° sabe que nunca vai olhar cada inst√¢ncia individual de certa m√©trica, poderia agreg√°-la antes mesmo de enviar. Exemplo: se voc√™ tem 10 threads fazendo trabalho id√™ntico e s√≥ quer saber o total combinado, exponha uma √∫nica m√©trica total e n√£o 10 separadas. Claro que isso depende do caso de uso ‚Äì muitas vezes queremos o detalhe ‚Äì mas √© algo a pensar.

* **Limite consultas no UI:** O Prometheus permite rodar qualquer PromQL ad-hoc no UI ou via API. Em ambientes compartilhados, controle o acesso ou conscientize os usu√°rios para n√£o rodarem consultas insanas (tipo um sum sem nenhum label em milh√µes de s√©ries por 365d) que possam afetar a performance. Voc√™ pode habilitar autentica√ß√£o/TLS e at√© colocar um proxy com quotas se for necess√°rio proteger a API de uso indevido.

### Arquitetura e Escalabilidade

* **Sharding (divis√£o de carga):** Se chegar ao ponto de um √∫nico Prometheus n√£o dar conta (seja por limite de CPU/RAM ou por quest√µes organizacionais), considere dividir os alvos entre m√∫ltiplas inst√¢ncias. Por exemplo, rodar um Prometheus por cluster Kubernetes, ou por ambiente (dev/prod), ou por regi√£o geogr√°fica. Cada um monitora s√≥ seu √¢mbito. Voc√™ pode replicar as regras de alertas em todos (assim cada local alerta independentemente). Para m√©tricas globais, use federa√ß√£o ou uma camada agregadora (como Thanos) para unificar se necess√°rio.

* **Alta disponibilidade:** O Prometheus em si n√£o √© HA ‚Äì ele √© stand-alone. Se cair, fica um buraco de coleta enquanto estiver fora. Uma pr√°tica comum em produ√ß√£o √© rodar **dois Prometheus em paralelo coletando os mesmos alvos** (nas mesmas configura√ß√µes) ‚Äì assim, se um falhar, o outro continua e nenhuma m√©trica se perde. O Alertmanager pode receber alertas duplicados de ambos, mas ele deduplica automaticamente (precisa configurar ambos Prometheus com o mesmo external\_label cluster). Essa abordagem gasta mais recursos (coleta em dobro), mas √© simples e efetiva para HA de alertas.

* **Longo prazo e agrega√ß√£o global:** Conforme citado, se precisar *escalar horizontalmente* de verdade ou guardar m√©tricas por longos per√≠odos, vale integrar solu√ß√µes como **Thanos, Cortex ou Grafana Mimir**. Essas ferramentas armazenam dados em base de dados distribu√≠da (por exemplo, S3 ou BigTable no caso do Thanos/Cortex) e permitem rodar consultas PromQL que abrangem m√∫ltiplos Prometheus "como se fosse um s√≥". O Thanos, por exemplo, atua como um *sidecar* pegando os dados de cada Prometheus e enviando para o objeto storage, depois uma camada de *querier* unifica as consultas. O Grafana Mimir segue arquitetura semelhante, nascida da experi√™ncia do Cortex, permitindo **escala praticamente ilimitada (bilh√µes de s√©ries) e alta disponibilidade**, com compatibilidade total com PromQL e remote write. Claro, adicionam complexidade ‚Äì mas s√£o solu√ß√µes maduras mantidas pela CNCF/Grafana Labs.

* **Federa√ß√£o bem aplicada:** Caso use federa√ß√£o, siga a orienta√ß√£o de federar apenas m√©tricas j√° agregadas e necess√°rias globalmente. Por exemplo, federar s√≥ m√©tricas come√ßando com `job:` (indicando que s√£o resultados de recording rules j√° agregadas). N√£o federar todas as m√©tricas crus. E realize alertas localmente, deixando o global s√≥ para visualiza√ß√£o.

### Seguran√ßa

* **N√£o exponha sem prote√ß√£o em redes inseguras:** O Prometheus, por padr√£o, n√£o tem autentica√ß√£o nem TLS habilitados. Se voc√™ for disponibilizar a interface ou API em rede p√∫blica ou multi-tenant, coloque-o atr√°s de um proxy reverso que implemente TLS e autentica√ß√£o (b√°sica, OAuth, o que for). Alternativamente, rode em rede interna/VPN somente. H√° flags experimentais para TLS direto e auth no Prometheus, mas a abordagem recomendada ainda √© usar um proxy (por exemplo, Nginx, Traefik, etc).

* **Controle acesso √† API:** Considere habilitar autoriza√ß√£o se for um ambiente com v√°rios usu√°rios ou multi-time. Infelizmente, o Prometheus n√£o suporta m√∫ltiplos n√≠veis de usu√°rio nativamente. A solu√ß√£o costuma ser segregar inst√¢ncias ou novamente um proxy que filtre rotas. Por exemplo, impedir acesso direto ao `/api/v1/admin` (que possui comandos de dele√ß√£o de dados).

* **Atualiza√ß√µes e patches:** Mantenha o Prometheus atualizado ‚Äì a cada vers√£o h√° otimiza√ß√µes e corre√ß√µes, inclusive de seguran√ßa. E.g., compress√£o de WAL veio ativada por padr√£o na 2.20, reduzindo disco pela metade. Vers√µes mais novas introduziram *native histograms* (experimental) e melhorias de desempenho. Ent√£o acompanhe o changelog oficial e planeje upgrade regularmente (Prometheus √© bem compat√≠vel retroativamente em dados e configs, upgrades diretos costumam ser tranquilos).

* **Isolamento de rede para exporters:** Exporters muitas vezes exp√µem m√©tricas sens√≠veis (por exemplo, o Node Exporter exp√µe informa√ß√µes de hardware, usu√°rios logados etc.). √â boa pr√°tica deixar esses endpoints acess√≠veis s√≥ pelo Prometheus, n√£o abertos ao mundo. Use firewalls/regras de seguran√ßa nos hosts ou config de container network para limitar.

* **Naming anti-collision:** Se voc√™ usa r√≥tulos *externos* (external\_labels) para identificar inst√¢ncias em um contexto federado ou HA, garanta que cada Prometheus tenha um label √∫nico (e.g., `cluster="eu-west-1"`). Isso evita confus√£o de m√©tricas vindas de origens diferentes no caso de jun√ß√£o (Thanos, federa√ß√£o) e ajuda a filtrar.

### Backup, Recovery e Upgrade

Em ambientes de produ√ß√£o, √© fundamental ter estrat√©gias robustas para backup, recupera√ß√£o de falhas e upgrades do Prometheus. Esses aspectos s√£o frequentemente negligenciados, mas s√£o cr√≠ticos para manter a continuidade do monitoramento.

#### Backup de Dados

O Prometheus armazena dados no diret√≥rio `data/` que cont√©m os blocos de s√©ries temporais. Para fazer backup consistente:

**Backup a quente (recomendado):**
```bash
# Parar o Prometheus para garantir consist√™ncia
sudo systemctl stop prometheus

# Fazer backup do diret√≥rio data
tar -czf prometheus-backup-$(date +%Y%m%d).tar.gz /opt/prometheus/data/

# Reiniciar o Prometheus
sudo systemctl start prometheus
```

**Backup a frio (alternativa):**
```bash
# Usar promtool para verificar integridade antes do backup
promtool tsdb check /opt/prometheus/data/

# Fazer backup apenas dos blocos fechados (mais seguro)
find /opt/prometheus/data/ -name "*.json" -exec tar -czf prometheus-blocks-$(date +%Y%m%d).tar.gz {} \;
```

**Backup de configura√ß√£o:**
```bash
# Backup dos arquivos de configura√ß√£o
cp /etc/prometheus/prometheus.yml /backup/prometheus.yml.$(date +%Y%m%d)
cp /etc/prometheus/alert.rules.yml /backup/alert.rules.yml.$(date +%Y%m%d)
```

#### Recupera√ß√£o de Falhas

**Restaura√ß√£o de dados:**
```bash
# Parar o Prometheus
sudo systemctl stop prometheus

# Restaurar backup
tar -xzf prometheus-backup-20231201.tar.gz -C /

# Verificar integridade dos dados
promtool tsdb check /opt/prometheus/data/

# Reiniciar
sudo systemctl start prometheus
```

**Recupera√ß√£o de WAL corrompido:**
```bash
# Se o WAL estiver corrompido, pode ser necess√°rio recriar
rm -rf /opt/prometheus/data/wal/
rm -rf /opt/prometheus/data/chunks_head/

# Reiniciar - o Prometheus recriar√° o WAL
sudo systemctl start prometheus
```

#### Estrat√©gias de Upgrade

**Upgrade direto (mais comum):**
```bash
# Fazer backup antes do upgrade
sudo systemctl stop prometheus
tar -czf prometheus-backup-pre-upgrade.tar.gz /opt/prometheus/data/

# Baixar nova vers√£o
wget https://github.com/prometheus/prometheus/releases/download/v2.45.0/prometheus-2.45.0.linux-amd64.tar.gz
tar -xzf prometheus-2.45.0.linux-amd64.tar.gz

# Substituir bin√°rio
cp prometheus-2.45.0.linux-amd64/prometheus /opt/prometheus/
cp prometheus-2.45.0.linux-amd64/promtool /opt/prometheus/

# Verificar configura√ß√£o
/opt/prometheus/promtool check config /etc/prometheus/prometheus.yml

# Reiniciar
sudo systemctl start prometheus
```

**Upgrade com rollback:**
```bash
# Manter vers√£o anterior
cp /opt/prometheus/prometheus /opt/prometheus/prometheus.backup

# Fazer upgrade
# ... (mesmo processo acima)

# Se houver problemas, rollback
sudo systemctl stop prometheus
cp /opt/prometheus/prometheus.backup /opt/prometheus/prometheus
sudo systemctl start prometheus
```

#### Considera√ß√µes Importantes

**Compatibilidade de dados:**
* O Prometheus mant√©m compatibilidade retroativa de dados entre vers√µes menores
* Upgrades major (ex: 2.x para 3.x) podem requerer migra√ß√£o de dados
* Sempre verifique o changelog oficial antes de upgrades

**Tempo de recupera√ß√£o:**
* O Prometheus pode demorar para processar o WAL ap√≥s reinicializa√ß√£o
* Em ambientes com muitas s√©ries, a recupera√ß√£o pode levar minutos
* Monitore `prometheus_tsdb_wal_replay_duration_seconds` durante recupera√ß√£o

**Backup automatizado:**
```bash
#!/bin/bash
# Script de backup automatizado
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backup/prometheus"

# Criar backup
sudo systemctl stop prometheus
tar -czf $BACKUP_DIR/prometheus-$DATE.tar.gz /opt/prometheus/data/
sudo systemctl start prometheus

# Manter apenas √∫ltimos 7 backups
find $BACKUP_DIR -name "prometheus-*.tar.gz" -mtime +7 -delete
```

**Monitoramento de integridade:**
```yaml
# Alertas para problemas de backup/recupera√ß√£o
groups:
- name: prometheus_backup
  rules:
    - alert: PrometheusBackupFailed
      expr: time() - prometheus_build_info > 86400  # Mais de 1 dia sem restart
      for: 1h
      labels:
        severity: warning
      annotations:
        summary: "Prometheus n√£o foi reiniciado recentemente (poss√≠vel problema de backup)"
```

> **Importante**: Sempre teste backups e procedimentos de recupera√ß√£o em ambiente de desenvolvimento antes de aplicar em produ√ß√£o. A integridade dos dados de monitoramento √© t√£o cr√≠tica quanto os dados da aplica√ß√£o.

Seguindo essas pr√°ticas, voc√™ dever√° manter seu ambiente Prometheus funcionando de forma mais suave, evitando as armadilhas comuns de desempenho e garantindo que as m√©tricas coletadas realmente agreguem valor (e alertas disparem quando devem, sem falso positivos ou negativos).

## Conclus√£o

Neste artigo, exploramos em detalhes o Prometheus ‚Äì desde conceitos fundamentais at√© seu funcionamento interno e implica√ß√µes pr√°ticas de opera√ß√£o. Vimos como ele implementa um banco de dados de s√©ries temporais altamente eficiente, mantendo dados recentes em mem√≥ria para rapidez e usando compress√£o e segmenta√ß√£o em blocos para hist√≥rico em disco. Tamb√©m analisamos aspectos como modelo de coleta pull, linguagem de consulta poderosa, uso intensivo de recursos proporcionais ao volume de m√©tricas, e formas de contornar limita√ß√µes (sejam arquiteturais ou de escala) com boas pr√°ticas e ferramentas auxiliares.

O Prometheus se destaca no ecossistema de monitoramento por sua simplicidade de implanta√ß√£o e por ter sido projetado desde o in√≠cio para ambientes de microsservi√ßos e infraestrutura din√¢mica. Seu modelo multidimensional de m√©tricas com labels e o PromQL possibilitam an√°lises ricas e alertas robustos com relativamente pouco esfor√ßo de configura√ß√£o. √â not√°vel como em poucos anos ele se tornou um dos pilares da observabilidade moderna, ao lado de ferramentas complementares para logs (ELK stack) e *tracing* (Jaeger, etc.).

Por outro lado, entendemos que o Prometheus n√£o resolve tudo sozinho: reten√ß√£o de longo prazo, alta disponibilidade nativa e escalabilidade horizontal s√£o pontos fora do escopo do core do Prometheus. Em vez de tentar ser distribu√≠do, o projeto optou por interfaces (remote write/read) e pela filosofia de componibilidade ‚Äì cabendo a outras pe√ßas (como Thanos ou Mimir) suprir essas demandas quando necess√°rias. Essa decis√£o de design mant√©m o Prometheus "enxuto" e confi√°vel, mas significa que para crescer al√©m de certo limite, precisamos arquitetar bem a solu√ß√£o de monitoramento abrangendo outros componentes.

Recapitulando alguns aprendizados chave:

* Organize bem suas m√©tricas e labels para evitar sobrecarga de cardinalidade.
* Monitore o pr√≥prio Prometheus e ajuste a capacidade conforme crescimento.
* Use Alertmanager e outras integra√ß√µes para ter um uso completo (coleta, armazenamento, alerta, visualiza√ß√£o).
* Em caso de grandes escalas, parta para sharding ou ferramentas de escala distribu√≠da ‚Äì n√£o force um Prometheus √∫nico a fazer trabalho demais.
* Leve em conta seguran√ßa e isolamento, pois monitoramento tamb√©m lida com informa√ß√µes sens√≠veis do ambiente.

Esperamos que este guia tenha fornecido insights valiosos, tanto para iniciantes entenderem os conceitos do Prometheus quanto para usu√°rios experientes refinarem sua utiliza√ß√£o. Compreender o "under the hood" do Prometheus ajuda a antecipar comportamentos, otimizar configura√ß√µes e evitar armadilhas comuns na opera√ß√£o di√°ria.

O Prometheus continua em r√°pida evolu√ß√£o (com melhorias na TSDB, novos recursos como Exemplos Exemplares e Native Histograms em teste, etc.), e o ecossistema ao seu redor tamb√©m. Fique atento a atualiza√ß√µes e boas pr√°ticas emergentes ‚Äì a comunidade CNCF e blogs como o *Robust Perception* regularmente publicam conte√∫dos de alto n√≠vel a respeito. No mais, boas m√©tricas e bons alertas!

---

## Refer√™ncias

* **Documenta√ß√£o Oficial do Prometheus** ‚Äì especialmente a [Overview](https://prometheus.io/docs/introduction/overview/) , [Metric Types](https://prometheus.io/docs/concepts/metric_types/) , [Best Practices](https://prometheus.io/docs/practices/naming/) e se√ß√£o de [Storage](https://prometheus.io/docs/prometheus/latest/storage/) .
* **Blog Robust Perception (Brian Brazil)** ‚Äì v√°rias postagens aprofundadas, por exemplo: ["Federation, what is it good for?"](https://www.robustperception.io/federation-what-is-it-good-for/) , ["How much RAM does Prometheus 2.x need..."](https://www.robustperception.io/how-much-ram-does-prometheus-2-x-need-for-cardinality-and-ingestion/) , ["Using JSON file service discovery"](https://www.robustperception.io/using-json-file-service-discovery-with-prometheus) .
* **Ganesh Vernekar ‚Äì S√©rie de artigos "Prometheus TSDB"** ‚Äì *Parts 1-7* no blog do Ganesh (engenheiro Grafana Labs) detalhando a fundo a arquitetura do TSDB. Em especial, [Parte 4: Blocos persistentes e √çndice](https://ganeshvernekar.com/blog/prometheus-tsdb-persistent-block-and-its-index/) .
* **Livro "Prometheus Up & Running" (O'Reilly, 2019)** ‚Äì de Brian Brazil, √≥tima introdu√ß√£o abrangendo do b√°sico a casos avan√ßados.
* **Livro "The Prometheus Book" de James Turnbull** ‚Äì guia pr√°tico cobrindo instala√ß√£o, instrumenta√ß√£o e alertas (dispon√≠vel online).
* **Hands-On Infrastructure Monitoring with Prometheus** (Packt) ‚Äì livro focado em exemplos pr√°ticos de uso do Prometheus em cen√°rios reais.
* **Monitoring Microservices and Containerized Applications** (Apress) ‚Äì aborda Prometheus em contexto de microsservi√ßos/Kubernetes.
* **Comparativos Prometheus vs. outras ferramentas:** Artigos como *"Prometheus vs. ELK"*, *"Prometheus vs. Grafana Mimir (Cortex)"*, e posts do blog da BetterStack sobre melhores pr√°ticas.
* **Grafana Mimir** ‚Äì [P√°gina oficial](https://grafana.com/oss/mimir/)  e an√∫ncio do lan√ßamento em 2022, mostrando como escalar Prometheus para 1 bilh√£o de s√©ries.
* **Datadog e New Relic** ‚Äì documenta√ß√µes e sites oficiais para entender ofertas de monitoramento propriet√°rias integradas (APM, Logs, etc.), √∫til para ver diferen√ßas de escopo.
* **Nagios/Core e Zabbix** ‚Äì documenta√ß√£o e comunidade, para contexto hist√≥rico de monitoramento (foco em disponibilidade, sem TSDB nativo).
* **ELK Stack** ‚Äì docs Elastic e blogs de terceiros comparando com Prometheus (focando que ELK √© logs e Prometheus m√©tricas).
* **CNCF Observability Landscape** ‚Äì projetos e ferramentas relacionadas, para quem quiser explorar al√©m (OpenTelemetry, Fluentd, etc.).