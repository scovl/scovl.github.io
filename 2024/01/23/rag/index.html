<!DOCTYPE html>
<html lang="pt">
<head>
    <title>RAG Simples com Clojure e Ollama | scovl</title>
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Um prot√≥tipo funcional do zero">


<link rel="stylesheet" href="/css/styles.css">
<link rel="stylesheet" href="/css/syntax.css">


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-dark.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-clojure.min.js"></script>





<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: '\\(', right: '\\)', display: false},
                {left: '\\[', right: '\\]', display: true}
            ]
        });
    });
</script>



<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        mermaid.initialize({
            startOnLoad: true,
            theme: 'light',
            align: 'center'
        });
    });
</script>
 
</head>
<body>
    <div class="container">
        
        
        <header class="site-header">
    <div class="header-inner">
        <div class="site-branding">
            <a href="https://scovl.github.io/" class="site-title">scovl</a>
        </div>
        
        <nav class="site-nav">
            <ul>
                
                
                <li>
                    <a href="/page/about/" class="">
                        About
                    </a>
                </li>
                
                <li>
                    <a href="/page/contact/" class="">
                        Contact
                    </a>
                </li>
                
            </ul>
        </nav>
    </div>
</header> 
        
        
        
        <main>
            
<article class="post">
    <header class="post-header">
        <h1 class="post-title">RAG Simples com Clojure e Ollama</h1>
        <div class="post-meta">
            
            <time datetime="2024-01-23T19:00:00Z">
                Tue, Jan 23, 2024
            </time>
            
            
            
            <span class="post-author">por Vitor Lobo Ramos</span>
            
            
            
            <div class="post-tags">
                
                <a href="/tags/rag/" class="tag">RAG</a>
                
                <a href="/tags/llm/" class="tag">LLM</a>
                
                <a href="/tags/ai/" class="tag">AI</a>
                
                <a href="/tags/langchain/" class="tag">Langchain</a>
                
            </div>
            
            
            
            <div class="reading-time">
                Estimated reading time: 17 min
            </div>
            
            
            
            <div class="post-description">
                Um prot√≥tipo funcional do zero
            </div>
            
        </div>
    </header>
    
    <div class="post-content content-wrapper">
        <h1 id="sum√°rio">Sum√°rio</h1>
<ul>
<li><strong><a href="/2024/01/23/rag/#o-que-√©-rag-e-por-que-precisamos-dele">O que √© RAG e por que precisamos dele?</a></strong>
<ul>
<li><a href="/2024/01/23/rag/#por-que-isso-√©-um-problema">Por que isso √© um problema?</a></li>
<li><a href="/2024/01/23/rag/#e-√©-a√≠-que-entra-o-rag">E √© a√≠ que entra o RAG!</a></li>
<li><strong><a href="/2024/01/23/rag/#construindo-uma-aplica√ß√£o-rag-simples">Construindo uma aplica√ß√£o RAG simples</a></strong>
<ul>
<li><a href="/2024/01/23/rag/#preparando-o-ambiente">Preparando o ambiente</a></li>
<li><a href="/2024/01/23/rag/#estrutura-do-projeto">Estrutura do projeto</a></li>
<li><a href="/2024/01/23/rag/#usando-ollama-para-llms-locais-sem-api-keys">Usando Ollama para LLMs locais sem API keys</a></li>
<li><a href="/2024/01/23/rag/#como-usar">Como usar?</a></li>
</ul>
</li>
</ul>
</li>
<li><strong><a href="/2024/01/23/rag/#considera√ß√µes-importantes">Considera√ß√µes importantes</a></strong></li>
<li><strong><a href="/2024/01/23/rag/#pr√≥ximos-passos">Pr√≥ximos passos</a></strong></li>
<li><strong><a href="/2024/01/23/rag/#langchain4j-para-simplificar-a-cria√ß√£o-de-rag">Langchain4j para simplificar a cria√ß√£o de RAG</a></strong></li>
</ul>
<p>Ol√°, pessoal! üëã</p>
<p>Neste artigo, vamos explorar como construir uma aplica√ß√£o <a href="https://pt.wikipedia.org/wiki/Gera%C3%A7%C3%A3o_aumentada_por_recupera%C3%A7%C3%A3o">RAG (Retrieval-Augmented Generation)</a> completa do zero usando <a href="https://clojure.org/">Clojure</a>. Vamos mergulhar em uma implementa√ß√£o pr√°tica que combina processamento de texto, busca sem√¢ntica e gera√ß√£o de respostas com LLMs locais. Se voc√™ est√° interessado em melhorar a precis√£o e relev√¢ncia das respostas dos seus modelos de linguagem com informa√ß√µes atualizadas, este guia √© para voc√™!</p>
<h2 id="o-que-√©-rag-e-por-que-precisamos-dele">O que √© RAG e por que precisamos dele?</h2>
<p>Os Modelos de Linguagem de Grande Escala (LLMs), como o GPT, ChatGPT e outros, revolucionaram a forma como interagimos com a intelig√™ncia artificial. Eles s√£o capazes de gerar textos coerentes, responder perguntas complexas e at√© mesmo criar conte√∫do criativo. No entanto, esses modelos possuem uma limita√ß√£o fundamental: seu conhecimento √© &ldquo;congelado&rdquo; no tempo.</p>
<p>Quando um LLM √© treinado, ele absorve informa√ß√µes dispon√≠veis at√© um determinado momento - sua data de corte de treinamento. Ap√≥s esse ponto, o modelo n√£o tem acesso a nenhuma informa√ß√£o nova. √â como se ele tivesse uma foto do mundo quando foi treinado, e n√£o conseguisse ver nada que aconteceu depois disso. Por exemplo, se um LLM foi treinado em 2022, ele n√£o vai saber nada sobre:</p>
<ul>
<li>Eventos que aconteceram em 2023</li>
<li>Novas tecnologias que surgiram</li>
<li>Aquela s√©rie nova que todo mundo t√° assistindo</li>
<li>O √∫ltimo filme que ganhou o Oscar</li>
</ul>
<h3 id="por-que-isso-√©-um-problema">Por que isso √© um problema?</h3>
<p>Ao desenvolver aplica√ß√µes inteligentes, como assistentes financeiros que precisam de cota√ß√µes de a√ß√µes em tempo real, chatbots de suporte que devem conhecer os produtos mais recentes da empresa ou sistemas de recomenda√ß√£o que se baseiam nas √∫ltimas tend√™ncias, nos deparamos com uma limita√ß√£o crucial dos Modelos de Linguagem de Grande Escala (LLMs) tradicionais: seu conhecimento est√°tico.</p>
<p>O problema fundamental reside no fato de que esses modelos, por mais sofisticados que sejam, possuem uma base de conhecimento &ldquo;congelada&rdquo; no momento de seu treinamento. Eles carecem de acesso inerente a informa√ß√µes atualizadas, o que restringe drasticamente sua aplicabilidade em cen√°rios que exigem dados em tempo real ou conhecimento sobre eventos recentes.</p>
<blockquote>
<p>Confiar exclusivamente em um LLM &ldquo;puro&rdquo; nesses contextos resultar√° em respostas desatualizadas, potencialmente imprecisas e, consequentemente, em uma experi√™ncia do usu√°rio comprometida. A efic√°cia da aplica√ß√£o √© diretamente afetada.</p>
</blockquote>
<p>√â nesse contexto que a t√©cnica de Recupera√ß√£o de Gera√ß√£o Aumentada (RAG - Retrieval-Augmented Generation) se torna n√£o apenas vantajosa, mas essencial. RAG permite superar a limita√ß√£o temporal inerente aos LLMs, garantindo que as aplica√ß√µes inteligentes entreguem informa√ß√µes relevantes e atualizadas.</p>
<h3 id="e-√©-a√≠-que-entra-o-rag">E √© a√≠ que entra o RAG!</h3>
<p>Imagine um LLM como um erudito com vasto conhecimento enciclop√©dico, mas que viveu isolado em uma biblioteca por d√©cadas. Ele possui um conhecimento profundo de muitos assuntos, mas est√° desatualizado sobre os eventos recentes e desenvolvimentos em diversas √°reas. O RAG, nesse contexto, seria como fornecer a esse erudito um par de √≥culos de √∫ltima gera√ß√£o que n√£o apenas corrigem sua vis√£o, mas tamb√©m o conectam a um fluxo constante de informa√ß√µes atualizadas.</p>
<p>Ele agora pode ler jornais, artigos cient√≠ficos recentes, consultar bases de dados em tempo real e, assim, responder a perguntas com uma precis√£o e relev√¢ncia muito maiores.</p>
<p><strong>Os Tr√™s Pilares do RAG</strong>:</p>
<ol>
<li>
<p><strong>Conex√£o com uma base de dados atual:</strong> Em vez de depender apenas do conhecimento est√°tico adquirido durante seu treinamento (que pode se tornar obsoleto rapidamente), o LLM ganha acesso a uma fonte de informa√ß√µes din√¢mica e constantemente atualizada. Isso pode ser uma base de dados de not√≠cias, um reposit√≥rio de documentos corporativos, uma cole√ß√£o de artigos cient√≠ficos, ou qualquer outra fonte relevante para a tarefa em quest√£o.</p>
</li>
<li>
<p><strong>Pesquisa em tempo real:</strong> O LLM n√£o est√° mais limitado a &ldquo;lembrar&rdquo; de informa√ß√µes. Ele adquire a capacidade de &ldquo;procurar&rdquo; ativamente por dados relevantes para responder a uma pergunta ou gerar um texto. Isso √© semelhante a como n√≥s, humanos, usamos mecanismos de busca para encontrar informa√ß√µes que n√£o temos memorizadas. O LLM, equipado com RAG, pode formular consultas, analisar os resultados e selecionar as informa√ß√µes mais pertinentes.</p>
</li>
<li>
<p><strong>Combina√ß√£o de conhecimento base com dados novos:</strong> Este √© o ponto crucial que diferencia o RAG de uma simples busca em uma base de dados. O LLM n√£o apenas recupera informa√ß√µes, mas tamb√©m as integra ao seu conhecimento pr√©-existente. Ele usa sua capacidade de racioc√≠nio e compreens√£o para contextualizar os novos dados, identificar contradi√ß√µes, e formular respostas coerentes e informadas. O erudito agora n√£o apenas l√™ as not√≠cias, mas as interpreta √† luz de seu vasto conhecimento.</p>
</li>
</ol>
<p>Segundo um <a href="https://arxiv.org/abs/2309.01066">whitepaper recente dos pesquisadores do Google</a>, existem v√°rias t√©cnicas para turbinar o desempenho dos LLMs, e o RAG √© uma das mais promissoras. Isso ocorre porque o RAG aborda algumas das limita√ß√µes fundamentais desses modelos:</p>
<ul>
<li><strong>Redu√ß√£o de &ldquo;alucina√ß√µes&rdquo;:</strong> LLMs, sem acesso a informa√ß√µes externas, podem &ldquo;inventar&rdquo; respostas (as chamadas &ldquo;alucina√ß√µes&rdquo;) quando confrontados com perguntas sobre t√≥picos que est√£o fora de seu conhecimento base. O RAG, ao fornecer dados factuais, diminui drasticamente a probabilidade de alucina√ß√µes.</li>
<li><strong>Respostas mais atualizadas:</strong> O conhecimento de um LLM treinado em um determinado momento fica congelado no tempo. O RAG garante que as respostas reflitam as informa√ß√µes mais recentes dispon√≠veis.</li>
<li><strong>Maior transpar√™ncia e explicabilidade:</strong> Ao citar as fontes de informa√ß√£o utilizadas, o RAG torna as respostas dos LLMs mais transparentes e verific√°veis. Isso aumenta a confian√ßa no modelo, pois podemos entender de onde vieram as informa√ß√µes.</li>
<li><strong>Melhora no desempenho em tarefas especificas:</strong> O RAG aumenta a capacidade do LLM de lidar com tarefas que requerem acesso e entendimento de bases de dados ou informa√ß√µes especificas, como responder perguntas sobre documentos ou dados de uma empresa.</li>
</ul>
<blockquote>
<p>O RAG representa um avan√ßo significativo na evolu√ß√£o dos LLMs, permitindo que eles se tornem ferramentas mais confi√°veis, precisas e √∫teis para uma ampla gama de aplica√ß√µes. Ele transforma o LLM de um &ldquo;sabe-tudo&rdquo; desatualizado em um pesquisador √°gil e bem-informado, capaz de combinar conhecimento profundo com informa√ß√µes atualizadas em tempo real.</p>
</blockquote>
<p>O RAG tamb√©m √© uma maneira de voc√™ nichar seu LLM em uma √°rea espec√≠fica, seja ela um assunto, uma empresa, uma linguagem, uma tecnologia, etc. O RAG √© uma t√©cnica que combina a capacidade de gera√ß√£o de texto dos LLMs com um sistema de recupera√ß√£o de informa√ß√µes. Em vez de depender apenas do conhecimento interno do modelo, o RAG busca informa√ß√µes relevantes em uma base de dados externa antes de gerar uma resposta. A imagem abaixo mostra o fluxo de um sistema RAG:</p>


  
    
  
  <div class="mermaid">graph LR
    A[Documentos] --&gt; B[Processamento de Documentos]
    B --&gt; C[Armazenamento de Vetores]
    D[Consulta do Usu√°rio] --&gt; E[Processamento da Consulta]
    E --&gt; F[Recupera√ß√£o de Documentos Relevantes]
    C --&gt; F
    F --&gt; G[Contexto Aumentado]
    G --&gt; H[LLM]
    H --&gt; I[Resposta Final]
    
    style A fill:#f9d5e5,stroke:#333
    style B fill:#eeeeee,stroke:#333
    style C fill:#d3f8e2,stroke:#333
    style D fill:#f9d5e5,stroke:#333
    style E fill:#eeeeee,stroke:#333
    style F fill:#e3e2f9,stroke:#333
    style G fill:#d3f8e2,stroke:#333
    style H fill:#f9e2ae,stroke:#333
    style I fill:#c5e0f9,stroke:#333</div>
 <p>O diagrama acima ilustra o fluxo de um sistema RAG, desde a ingest√£o de documentos at√© a gera√ß√£o da resposta final. Primeiro, os documentos s√£o processados e armazenados como vetores. Quando um usu√°rio faz uma consulta, o sistema processa essa pergunta, recupera os documentos mais relevantes do armazenamento vetorial e cria um contexto aumentado que √© enviado ao LLM.</p>
<hr>
<h3 id="construindo-uma-aplica√ß√£o-rag-simples">Construindo uma aplica√ß√£o RAG simples</h3>
<p>Vamos construir uma aplica√ß√£o RAG simples usando Clojure! Nosso objetivo √© criar um assistente inteligente que possa responder perguntas sobre documenta√ß√£o t√©cnica de projetos open source. Vou te mostrar como fazer isso passo a passo.</p>
<h4 id="preparando-o-ambiente">Preparando o ambiente</h4>
<p>Pre-requisitos:</p>
<ul>
<li><a href="https://clojure.org/guides/getting_started">Clojure</a>: Linguagem de programa√ß√£o funcional que vamos usar para construir a aplica√ß√£o</li>
<li><a href="https://leiningen.org/">Leiningen</a>: Ferramenta de build para Clojure</li>
<li><a href="https://ollama.com/">Ollama</a>: Modelo de linguagem local</li>
</ul>
<p>Primeiro, vamos criar um novo projeto Clojure usando Leiningen:</p>


  <pre><code class="language-bash">lein new app docai
cd docai</code></pre>
 <p>Agora, vamos adicionar as depend√™ncias necess√°rias no arquivo <code>project.clj</code>:</p>


  <pre><code class="language-clojure">(defproject docai &#34;0.1.0-SNAPSHOT&#34;
  :description &#34;Um assistente RAG para consulta de documenta√ß√£o t√©cnica&#34;
  :url &#34;http://example.com/FIXME&#34;
  :license {:name &#34;EPL-2.0 OR GPL-2.0-or-later WITH Classpath-exception-2.0&#34;
            :url &#34;https://www.eclipse.org/legal/epl-2.0/&#34;}
  :dependencies [[org.clojure/clojure &#34;1.11.1&#34;]
                 [markdown-to-hiccup &#34;0.6.2&#34;]    ; Para processar Markdown
                 [hickory &#34;0.7.1&#34;]              ; Para processar HTML
                 [org.clojure/data.json &#34;2.4.0&#34;]  ; Para JSON
                 [http-kit &#34;2.6.0&#34;]             ; Para requisi√ß√µes HTTP
                 [org.clojure/tools.logging &#34;1.2.4&#34;]  ; Para logging
                 [org.clojure/tools.namespace &#34;1.4.4&#34;]  ; Para reloading
                 [org.clojure/core.async &#34;1.6.681&#34;]  ; Para opera√ß√µes ass√≠ncronas
                 [org.clojure/core.memoize &#34;1.0.257&#34;]  ; Para cache
                 [org.clojure/core.cache &#34;1.0.225&#34;]]  ; Para cache
  :main ^:skip-aot docai.core
  :target-path &#34;target/%s&#34;
  :profiles {:uberjar {:aot :all
                       :jvm-opts [&#34;-Dclojure.compiler.direct-linking=true&#34;]}})</code></pre>
 <blockquote>
<p>Curiosidade: Porque Clojure? Por ser uma linguagem funcional, facilita a implementa√ß√£o de pipelines de processamento de dados.</p>
</blockquote>
<h4 id="estrutura-do-projeto">Estrutura do projeto</h4>
<p>Nossa aplica√ß√£o ter√° tr√™s componentes principais:</p>
<ol>
<li><strong>Processamento de documenta√ß√£o (Markdown/HTML)</strong>
<ul>
<li>Extra√ß√£o de texto</li>
<li>Pr√©-processamento de texto</li>
</ul>
</li>
<li><strong>Sistema de embeddings</strong>
<ul>
<li>Cria√ß√£o de embeddings para o texto usando <a href="https://pt.wikipedia.org/wiki/TF-IDF">TF-IDF</a></li>
<li>Busca por similaridade sem√¢ntica</li>
</ul>
</li>
<li><strong>Interface com o LLM</strong>
<ul>
<li>Gera√ß√£o de resposta usando o LLM</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>Observa√ß√£o:</strong> Embora o RAG moderno utilize embeddings densos gerados por modelos de linguagem para capturar a sem√¢ntica de forma mais rica, neste artigo, usaremos uma implementa√ß√£o simplificada de <a href="https://pt.wikipedia.org/wiki/TF-IDF">TF-IDF (Term Frequency-Inverse Document Frequency)</a>.</p>
</blockquote>
<p>O TF-IDF √© uma t√©cnica que nos permite representar documentos como vetores, calculando a import√¢ncia de cada palavra com base em sua frequ√™ncia no documento e em todo o corpus. Isso nos permite realizar uma busca por similaridade, sem depender de APIs externas para a gera√ß√£o de embeddings. √â importante ressaltar que essa √© uma abordagem did√°tica e simplificada. Para um sistema RAG de produ√ß√£o, o ideal seria utilizar embeddings gerados pelo pr√≥prio modelo de linguagem (ou um modelo compat√≠vel), em conjunto com um banco de dados vetorial.</p>
<p>No entanto, para a parte de gera√ß√£o de respostas, continuaremos usando o <a href="https://ollama.com/">Ollama</a> com o modelo <a href="https://ollama.com/models/deepseek-r1">deepseek-r1</a>. Vamos criar os namespaces necess√°rios come√ßando pelo m√≥dulo de processamento de documentos:</p>


  <pre><code class="language-clojure">;; src/docai/document.clj
(ns docai.document
  (:require [markdown-to-hiccup.core :as md]
            [hickory.core :as html]
            [clojure.string :as str]))

(defn is-string? [x]
  (instance? String x))

(defn extract-text-from-markdown [content]
  (try
    (let [hiccup-result (md/md-&gt;hiccup content)
          text-nodes (filter is-string? (flatten hiccup-result))]
      text-nodes)
    (catch Exception e
      (println &#34;Erro ao processar Markdown:&#34; (.getMessage e))
      [content])))

(defn extract-text-from-html [content]
  (try
    (let [dom (html/parse content)
          hiccup-result (html/as-hiccup dom)
          text-nodes (filter is-string? (flatten hiccup-result))]
      text-nodes)
    (catch Exception e
      (println &#34;Erro ao processar HTML:&#34; (.getMessage e))
      [content])))

(defn extract-text
  &#34;Extrai texto de documenta√ß√£o (Markdown ou HTML)&#34;
  [doc-path]
  (println &#34;Extraindo texto de:&#34; doc-path)
  (let [content (slurp doc-path)
        _ (println &#34;Tamanho do conte√∫do:&#34; (count content) &#34;caracteres&#34;)
        _ (println &#34;Amostra do conte√∫do:&#34; (subs content 0 (min 100 (count content))))
        text (if (.endsWith doc-path &#34;.md&#34;)
               (extract-text-from-markdown content)
               (extract-text-from-html content))
        _ (println &#34;Quantidade de n√≥s de texto extra√≠dos:&#34; (count text))
        chunks (partition-all 512 text)]  ; 512 tokens por chunk
    (println &#34;Quantidade de chunks gerados:&#34; (count chunks))
    chunks))

(defn preprocess-chunks
  &#34;Limpa e prepara os chunks de texto&#34;
  [chunks]
  (let [processed (map #(-&gt; %
                            (str/join &#34; &#34;)
                            (str/replace #&#34;\s&#43;&#34; &#34; &#34;)
                            (str/trim))
                       chunks)]
    (println &#34;Primeiro chunk processado:&#34; (first processed))
    processed))</code></pre>
 <p>Em seguida, vamos implementar o m√≥dulo de embeddings que vai permitir procurar informa√ß√µes semanticamente relevantes. Aqui estamos usando uma implementa√ß√£o pr√≥pria de TF-IDF, que √© uma t√©cnica eficiente para representar documentos em vetores, sem depender de APIs externas:</p>


  <pre><code class="language-clojure">;; src/docai/embedding.clj
(ns docai.embedding
  (:require [clojure.string :as str]
            [clojure.core.memoize :as memo]))

;; Implementa√ß√£o de embeddings usando TF-IDF simples
;; N√£o depende de modelos externos, ao contr√°rio do Ollama que usa o deepseek-r1 para o LLM

(defn tokenize
  &#34;Divide o texto em tokens&#34;
  [text]
  (if (string? text)
    (-&gt; text
        str/lower-case
        (str/split #&#34;\s&#43;&#34;)
        (-&gt;&gt; (filter #(&gt; (count %) 2))))
    []))

(defn term-freq
  &#34;Calcula a frequ√™ncia dos termos&#34;
  [tokens]
  (frequencies tokens))

(defn string-doc? [x]
  (instance? String x))

(defn doc-freq
  &#34;Calcula a frequ√™ncia dos documentos&#34;
  [docs]
  (let [string-docs (filter string-doc? docs)  ; Use our own predicate function
        _ (println (str &#34;Processando &#34; (count string-docs) &#34; documentos v√°lidos de &#34; (count docs) &#34; total&#34;))
        doc-tokens (map tokenize string-docs)  
        all-tokens (distinct (flatten doc-tokens))
        doc-count (count string-docs)]
    (if (zero? doc-count)
      {}
      (zipmap all-tokens
              (map #(count (filter (fn [tokens] (some #{%} tokens)) doc-tokens))
                   all-tokens)))))

(defn tf-idf
  &#34;Calcula TF-IDF para um documento&#34;
  [doc doc-freq]
  (if (empty? doc-freq)
    {}
    (let [tokens (tokenize doc)
          tf (term-freq tokens)
          n-docs (count (keys doc-freq))]
      (zipmap (keys tf)
              (map #(* (get tf %) (Math/log (/ n-docs (get doc-freq % 1))))
                   (keys tf))))))

(defn vectorize
  &#34;Converte um documento em um vetor TF-IDF&#34;
  [doc doc-freq]
  (let [tf-idf-scores (tf-idf doc doc-freq)]
    (if (empty? doc-freq)
      []
      (map #(get tf-idf-scores % 0.0)
           (keys doc-freq)))))

(defn create-embeddings
  &#34;Gera embeddings para uma lista de textos usando TF-IDF&#34;
  [texts]
  (try
    (let [doc-freq (doc-freq texts)]
      (map #(vectorize % doc-freq) texts))
    (catch Exception e
      (println &#34;Erro ao criar embeddings: &#34; (.getMessage e))
      (vec (repeat (count texts) [])))))

(defn cosine-similarity
  &#34;Calcula a similaridade do cosseno entre dois vetores&#34;
  [v1 v2]
  (if (or (empty? v1) (empty? v2))
    0.0
    (let [dot-product (reduce &#43; (map * v1 v2))
          norm1 (Math/sqrt (reduce &#43; (map #(* % %) v1)))
          norm2 (Math/sqrt (reduce &#43; (map #(* % %) v2)))]
      (if (or (zero? norm1) (zero? norm2))
        0.0
        (/ dot-product (* norm1 norm2))))))

(defn similarity-search
  &#34;Encontra os N chunks mais similares&#34;
  [query-embedding doc-embeddings n]
  (if (or (empty? query-embedding) (empty? doc-embeddings))
    (take (min n (count doc-embeddings)) (range))
    (let [scores (map #(cosine-similarity query-embedding %) doc-embeddings)]
      (-&gt;&gt; (map vector scores (range))
           (sort-by first &gt;)
           (take n)
           (map second)))))</code></pre>
 <p>O c√≥digo acima implementa uma t√©cnica chamada <a href="https://pt.wikipedia.org/wiki/TF-IDF">TF-IDF (Term Frequency-Inverse Document Frequency)</a> para criar representa√ß√µes vetoriais dos textos. Esta abordagem permite calcular a similaridade entre documentos sem depender de modelos externos. No entanto, para a parte de gera√ß√£o, ainda usamos o Ollama com o modelo deepseek-r1.</p>
<h4 id="usando-ollama-para-llms-locais-sem-api-keys">Usando Ollama para LLMs locais sem API keys</h4>
<p>Uma vantagem importante dessa abordagem √© que vamos utilizar o <a href="https://ollama.com/">Ollama</a> para executar nossos modelos localmente. O Ollama √© uma ferramenta incr√≠vel que permite rodar LLMs diretamente na sua m√°quina, sem depender de servi√ßos em nuvem ou API keys. Vamos implementar a interface com o Ollama:</p>


  <pre><code class="language-clojure">;; src/docai/llm.clj
(ns docai.llm
  (:require [clojure.data.json :as json]
            [org.httpkit.client :as http]))

(def ollama-url &#34;http://localhost:11434/api/generate&#34;)
(def model-name &#34;deepseek-r1&#34;) ; Modelo DeepSeek para melhor qualidade

(defn call-ollama-api
  &#34;Chama a API do Ollama para gerar uma resposta&#34;
  [prompt]
  (let [request-body {:model model-name
                      :prompt prompt
                      :stream false}
        options {:headers {&#34;Content-Type&#34; &#34;application/json&#34;}
                 :body (json/write-str request-body)}
        response @(http/post ollama-url options)]
    (if (= (:status response) 200)
      (-&gt; response
          :body
          (json/read-str :key-fn keyword)
          :response)
      (str &#34;Erro ao chamar a API do Ollama: &#34; (:status response) &#34; - &#34; (:body response)))))

;; Fun√ß√µes de utilidade para uso futuro:
;;
;; extract-code-blocks: Extrai blocos de c√≥digo do texto usando regex
;; exemplo de uso:
;;   (extract-code-blocks &#34;```clojure\n(&#43; 1 2)\n```&#34;) =&gt; [&#34;(&#43; 1 2)&#34;]
;;
;; extract-summary: Cria um resumo de texto com tamanho m√°ximo especificado
;; exemplo de uso:
;;   (extract-summary &#34;# T√≠tulo\nConte√∫do longo...&#34; 50) =&gt; &#34;Conte√∫do longo...&#34;

(defn format-prompt
  &#34;Formata o prompt para o LLM&#34;
  [context query]
  (str &#34;Voc√™ √© um assistente especializado em documenta√ß√£o t√©cnica. Com base no seguinte contexto da documenta√ß√£o:\n\n&#34;
       context
       &#34;\n\nPergunta: &#34; query
       &#34;\n\nForne√ßa uma resposta t√©cnica precisa e, se poss√≠vel, inclua exemplos de c√≥digo. &#34;
       &#34;Se a documenta√ß√£o n√£o contiver informa√ß√µes relevantes para a pergunta, &#34;
       &#34;indique isso claramente e forne√ßa uma resposta geral com base em seu conhecimento.&#34;))

(defn generate-response
  &#34;Gera resposta usando o LLM com base no contexto&#34;
  [query context]
  (try
    (let [prompt (format-prompt context query)]
      (println &#34;DEBUG - Enviando prompt para o Ollama usando o modelo&#34; model-name)
      (call-ollama-api prompt))
    (catch Exception e
      (str &#34;Erro ao gerar resposta: &#34; (.getMessage e) 
           &#34;\n\nPor favor, verifique se o Ollama est√° em execu√ß√£o no endere√ßo &#34; 
           ollama-url 
           &#34;\n\nVoc√™ pode iniciar o Ollama com o comando: ollama serve&#34;))))</code></pre>
 <p>Note que o m√≥dulo <code>llm.clj</code> inclui fun√ß√µes utilit√°rias adicionais como <code>extract_code_blocks</code> para extrair blocos de c√≥digo das respostas e <code>extract_summary</code> para criar resumos do texto, al√©m de melhor tratamento de erros na comunica√ß√£o com o Ollama. Por fim, vamos criar o m√≥dulo principal que integra todos os componentes:</p>


  <pre><code class="language-clojure">;; src/docai/core.clj
(ns docai.core
  (:require [docai.document :as doc]
            [docai.embedding :as emb]
            [docai.llm :as llm]
            [clojure.java.io :as io]
            [clojure.string :as str])
  (:gen-class))

(def docs-path &#34;resources/docs&#34;)

(defn load-documentation
  &#34;Carrega todos os arquivos de documenta√ß√£o do diret√≥rio&#34;
  []
  (-&gt;&gt; (file-seq (io/file docs-path))
       (filter #(.isFile %))
       (map #(.getPath %))))

(defn setup-knowledge-base
  &#34;Configura a base de conhecimento inicial&#34;
  []
  (let [doc-files (load-documentation)
        _ (when (empty? doc-files)
            (println &#34;Aviso: Nenhum arquivo de documenta√ß√£o encontrado em resources/docs/&#34;))
        _ (doseq [file doc-files]
            (println &#34;Arquivo encontrado:&#34; file))
        all-chunks (mapcat doc/extract-text doc-files)
        processed-chunks (doc/preprocess-chunks all-chunks)
        _ (println (str &#34;Processando &#34; (count processed-chunks) &#34; chunks de texto...&#34;))
        _ (when (&lt; (count processed-chunks) 5)
            (println &#34;DEBUG - Primeiros chunks:&#34;)
            (doseq [chunk (take 5 processed-chunks)]
              (println (str &#34;Chunk: &#39;&#34; (subs chunk 0 (min 50 (count chunk))) &#34;...&#39;&#34;))))
        embeddings (emb/create-embeddings processed-chunks)]
    {:chunks processed-chunks
     :embeddings embeddings
     :original-files doc-files}))

(defn get-file-content
  &#34;L√™ o conte√∫do completo de um arquivo&#34;
  [file-path]
  (try
    (slurp file-path)
    (catch Exception _
      (println &#34;Erro ao ler arquivo:&#34; file-path)
      &#34;&#34;)))

(defn query-rag
  &#34;Processa uma query usando o pipeline RAG&#34;
  [knowledge-base query]
  (println &#34;DEBUG - Processando query:&#34; query)
  (if (and (seq (:chunks knowledge-base)) 
           (seq (:embeddings knowledge-base)))
    (let [query-emb (first (emb/create-embeddings [query]))
          similar-idxs (emb/similarity-search query-emb 
                                            (:embeddings knowledge-base)
                                            3)
          _ (println &#34;DEBUG - √çndices similares:&#34; similar-idxs)
          
          ;; Obter contexto relevante
          context-chunks (-&gt;&gt; similar-idxs
                              (map #(nth (:chunks knowledge-base) %))
                              (str/join &#34;\n\n&#34;))
          
          ;; Se n√£o houver chunks relevantes, use o conte√∫do original
          context (if (str/blank? context-chunks)
                    (if (seq (:original-files knowledge-base))
                      (get-file-content (first (:original-files knowledge-base)))
                      &#34;N√£o foi poss√≠vel encontrar informa√ß√µes relevantes.&#34;)
                    context-chunks)]
      
      (println &#34;DEBUG - Tamanho do contexto:&#34; (count context) &#34;caracteres&#34;)
      (println &#34;DEBUG - Amostra do contexto:&#34; (subs context 0 (min 200 (count context))) &#34;...&#34;)
      
      ;; Gerar resposta usando o LLM
      (llm/generate-response query context))
    &#34;N√£o foi poss√≠vel encontrar informa√ß√µes relevantes na base de conhecimento.&#34;))

(defn -main
  &#34;Fun√ß√£o principal que inicializa a aplica√ß√£o DocAI&#34;
  [&amp; _]
  (println &#34;Inicializando DocAI...&#34;)
  
  ;; Verificar se o Ollama est√° acess√≠vel
  (println &#34;Para usar o Ollama, certifique-se de que ele est√° em execu√ß√£o com o comando: ollama serve&#34;)
  (println &#34;Usando o modelo deepseek-r1. Se voc√™ ainda n√£o o baixou, execute: ollama pull deepseek-r1&#34;)
  
  (let [kb (setup-knowledge-base)]
    (println &#34;Base de conhecimento pronta! Fa√ßa sua pergunta:&#34;)
    (try
      (loop []
        (when-let [input (read-line)]
          (when-not (= input &#34;sair&#34;)
            (println &#34;Processando...&#34;)
            (println (query-rag kb input))
            (println &#34;\nPr√≥xima pergunta (ou &#39;sair&#39; para terminar):&#34;)
            (recur))))
      (catch Exception e
        (println &#34;Erro: &#34; (.getMessage e))
        (println &#34;Detalhes: &#34; (ex-data e))))
    (println &#34;Obrigado por usar o DocAI. At√© a pr√≥xima!&#34;)))</code></pre>
 <p>Note que o m√≥dulo principal inclui mais logs com emojis, melhor tratamento de erros e verifica√ß√µes adicionais para garantir que o sistema n√£o falhe quando n√£o h√° chunks ou embeddings dispon√≠veis.</p>
<p>A estrutura de pastas do projeto fica assim:</p>


  <pre><code class="language-bash">docai/
‚îú‚îÄ‚îÄ src/
‚îÇ ‚îú‚îÄ‚îÄ docai/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ document.clj  # Processamento de documentos
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ embedding.clj # Sistema de embeddings usando TF-IDF
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ llm.clj       # Interface com o Ollama
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ core.clj      # M√≥dulo principal
‚îú‚îÄ‚îÄ resources/
‚îÇ ‚îî‚îÄ‚îÄ docs/           # Onde ficam os documentos
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ rag.md        # Este documento sobre RAG
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ example.md    # Exemplo de documenta√ß√£o JWT
‚îú‚îÄ‚îÄ project.clj       # Configura√ß√£o do projeto
‚îî‚îÄ‚îÄ README.md</code></pre>
 <h3 id="como-usar">Como usar?</h3>
<ol>
<li>Instale o Ollama seguindo as instru√ß√µes em <a href="https://ollama.com">ollama.com</a></li>
<li>Inicie o servidor Ollama:</li>
</ol>


  <pre><code class="language-bash">ollama serve</code></pre>
 <p>√â necess√°rio rodar o servidor do Ollama antes de executar o projeto pois, caso contr√°rio, o projeto n√£o conseguir√° se conectar ao modelo.</p>
<ol start="3">
<li>Baixe o modelo DeepSeek R1 (ou qualquer outro modelo que preferir):</li>
</ol>


  <pre><code class="language-bash">ollama pull deepseek-r1</code></pre>
 <ol start="4">
<li>Coloque seus documentos na pasta <code>resources/docs/</code> (j√° inclu√≠mos dois exemplos: rag.md e example.md)</li>
<li>Execute o projeto:</li>
</ol>


  <pre><code class="language-bash">lein run</code></pre>
 <ol start="6">
<li>Fa√ßa suas perguntas! Exemplo:</li>
</ol>


  <pre><code class="language-">Inicializando DocAI...
Base de conhecimento pronta! Fa√ßa sua pergunta:
Como implementar autentica√ß√£o JWT em Clojure?
Processando...
[Resposta do modelo sobre JWT baseada no example.md]</code></pre>
 <blockquote>
<p><strong>NOTA:</strong> A prop√≥sito, o projeto docai est√° dispon√≠vel no <a href="https://github.com/scovl/docai">https://github.com/scovl/docai</a> caso voc√™ queira contribuir com o projeto ou usar em outro projeto.</p>
</blockquote>
<h3 id="considera√ß√µes-importantes">Considera√ß√µes importantes</h3>
<ol>
<li>
<p><strong>Performance</strong>: Esta implementa√ß√£o √© b√°sica e pode ser otimizada:</p>
<ul>
<li>Usando um banco de dados vetorial como <a href="https://milvus.io/">Milvus</a> ou <a href="https://github.com/facebookresearch/faiss">FAISS</a></li>
<li>Implementando cache de embeddings</li>
<li>Paralelizando o processamento de chunks</li>
</ul>
</li>
<li>
<p><strong>Mem√≥ria</strong>: Para documenta√ß√µes muito extensas, considere:</p>
<ul>
<li>Processar os chunks em lotes</li>
<li>Implementar indexa√ß√£o incremental</li>
<li>Usar streaming para arquivos grandes</li>
</ul>
</li>
<li>
<p><strong>Modelos</strong>: Diferentes modelos do Ollama t√™m diferentes caracter√≠sticas:</p>
<ul>
<li>DeepSeek R1: Bom para compreens√£o geral e gera√ß√£o de texto</li>
<li>DeepSeek Coder: Especializado em c√≥digo</li>
<li>Llama 3: Boa alternativa geral</li>
<li>Mistral: Bom para tarefas espec√≠ficas</li>
<li>Gemma: Leve e eficiente</li>
</ul>
</li>
</ol>
<hr>
<h3 id="pr√≥ximos-passos">Pr√≥ximos passos</h3>
<p>Algumas ideias para expandir o projeto:</p>
<ol>
<li><strong>Tokeniza√ß√£o Avan√ßada:</strong> Usar um tokenizador de <em>subpalavras</em> (como BPE ou WordPiece) para melhorar a busca sem√¢ntica. Idealmente, o mesmo usado no treinamento do modelo (ex: <code>deepseek-r1</code>).</li>
<li><strong>Embeddings Pr√©-treinados:</strong> Usar embeddings do pr√≥prio modelo (via Ollama) em vez de TF-IDF.  Mais simples e <em>muito</em> melhor para busca sem√¢ntica.</li>
<li><strong>Banco de Dados Vetorial:</strong> Usar um banco de dados vetorial (Milvus, FAISS, Qdrant, etc.) para lidar com <em>muitos</em> documentos de forma eficiente.</li>
<li><strong>Cache:</strong> Usar cache para os embeddings (e, opcionalmente, respostas) para acelerar o sistema.</li>
<li><strong>Erros:</strong> Tratar mais erros (Ollama offline, modelo indispon√≠vel, rede, arquivos inv√°lidos).</li>
<li><strong>Logging:</strong> Usar um framework de logging para rastreamento e depura√ß√£o.</li>
<li><strong>Testes:</strong> Adicionar testes unit√°rios e de integra√ß√£o.</li>
<li><strong>Prompt Engineering:</strong> Refinar o prompt (em <code>format-prompt</code>) para melhorar as respostas.</li>
<li><strong>Usar langchain4j:</strong> criar RAG atrav√©s do <a href="https://github.com/langchain4j/langchain4j">langchain4j</a> via interop java com o clojure.
Experimentar com:
<ul>
<li>Exemplos no prompt (few-shot learning).</li>
<li>Instru√ß√µes passo a passo (chain-of-thought).</li>
<li>Instru√ß√µes claras sobre formato, tamanho, etc.</li>
<li>Pedir a fonte da informa√ß√£o (qual chunk).</li>
</ul>
</li>
</ol>
<p>Bastante mais coisas podem ser feitas, mas essas s√£o as mais importantes.</p>
<hr>
<h2 id="langchain4j-para-simplificar-a-cria√ß√£o-de-rag">Langchain4j para simplificar a cria√ß√£o de RAG</h2>
<p>At√© aqui utilizei a abordagem TF-IDF para criar embeddings e a abordagem manual para criar o RAG com intuito de apenas demonstrar o processo. O ideal, √© usar embeddings pr√©-treinados e um banco de dados vetorial para armazenar os embeddings e realizar a busca por similaridade. Para isso, vamos usar a biblioteca <a href="https://github.com/langchain4j/langchain4j">Langchain4j</a>.</p>
<p>Langchain4j √© uma biblioteca Java que oferece uma abstra√ß√£o de alto n√≠vel para construir aplica√ß√µes de IA generativa, incluindo sistemas RAG. Ela se integra bem com Clojure atrav√©s da interoperabilidade Java. Embora a implementa√ß√£o manual que fizemos anteriormente seja um √≥timo exerc√≠cio de aprendizado, usar Langchain4j pode simplificar significativamente o desenvolvimento, especialmente para aplica√ß√µes mais complexas.</p>
<p>Vantagens de usar Langchain4j:</p>
<ul>
<li><strong>Abstra√ß√£o:</strong> Langchain4j fornece componentes pr√©-constru√≠dos para tarefas comuns como carregamento de documentos, divis√£o de texto, cria√ß√£o de embeddings, armazenamento vetorial e intera√ß√£o com LLMs.</li>
<li><strong>Modularidade:</strong> Voc√™ pode facilmente trocar diferentes implementa√ß√µes (por exemplo, usar diferentes modelos de embedding ou bancos de dados vetoriais) sem alterar o c√≥digo principal da aplica√ß√£o.</li>
<li><strong>Integra√ß√£o:</strong> Langchain4j oferece integra√ß√µes com v√°rias ferramentas e servi√ßos populares, incluindo Ollama, bancos de dados vetoriais (como Chroma, Weaviate, Qdrant), e modelos de linguagem de diferentes provedores.</li>
<li><strong>Comunidade e Suporte:</strong> Langchain4j possui uma comunidade ativa e boa documenta√ß√£o, facilitando a obten√ß√£o de ajuda e a resolu√ß√£o de problemas.</li>
</ul>
<blockquote>
<p>Em um pr√≥ximo artigo, escreverei sobre como usar <a href="https://github.com/langchain4j/langchain4j">Langchain4j</a> para criar um sistema RAG ainda neste mesmo projeto.</p>
</blockquote>
<hr>
<h2 id="refer√™ncias">Refer√™ncias</h2>
<ul>
<li><a href="https://www.pinecone.io/learn/rag/">RAG</a> - Documenta√ß√£o do Pinecone</li>
<li><a href="https://www.pinecone.io/learn/embeddings/">Embedding</a> - Documenta√ß√£o do Pinecone</li>
<li><a href="https://www.pinecone.io/learn/llms/">LLM</a> - Documenta√ß√£o do Pinecone</li>
<li><a href="https://ollama.com/">Ollama</a> - Ferramenta para rodar LLMs localmente</li>
<li><a href="https://clojure.org/">Clojure</a> - Documenta√ß√£o do Clojure</li>
<li><a href="https://github.com/http-kit/http-kit">http-kit</a> - Cliente HTTP para Clojure</li>
<li><a href="https://github.com/clojure/data.json">data.json</a> - Biblioteca JSON para Clojure</li>
<li><a href="https://clojure.github.io/clojure/clojure.test-api.html">clojure.test</a> - Documenta√ß√£o da biblioteca de testes do Clojure</li>
<li><a href="https://github.com/clj-kondo/clj-kondo">clj-kondo</a> - Linter para Clojure</li>
</ul>

    </div>
    
    
</article>

        </main>
        
        
        
        <footer class="site-footer">
    <div class="footer-inner">
        <div class="footer-content">
            <div class="copyright">
                &copy; 2025 Vitor Lobo
            </div>
            
            <div class="social-links">
                
                <a href="https://github.com/scovl" target="_blank" rel="noopener noreferrer" class="social-link">
                    GitHub
                </a>
                
                
                
                <a href="https://linkedin.com/in/vitor-lobo" target="_blank" rel="noopener noreferrer" class="social-link">
                    LinkedIn
                </a>
                
                
                
                <a href="mailto:lobocode@gmail.com" class="social-link">
                    Email
                </a>
                

                
                <a href="https://hachyderm.io/@lobocode" target="_blank" rel="noopener noreferrer" class="social-link">
                    Mastodon
                </a>
                

                
                <a href="https://scovl.github.io/index.xml" target="_blank" rel="noopener noreferrer" class="social-link">
                    RSS
                </a>
                

            </div>
        </div>
    </div>
</footer>


<script src="/js/code-escaper.js"></script>


<script>
  document.addEventListener("DOMContentLoaded", function() {
    
    if (typeof Prism !== 'undefined') {
      Prism.highlightAll();
    }
  });
</script>

</body>
</html> 
        
    </div>
    
    
    <script src="/js/main.js"></script>
    
    
    
</body>
</html> 