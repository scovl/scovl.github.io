<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/">
  <channel>
    <title>Compiladores | scovl</title>
    <link>https://scovl.github.io/tags/compiladores/</link>
    <description>scovl - Blog sobre tecnologia, programação e desenvolvimento</description>
    <language>pt</language>
    <lastBuildDate>Mon, 21 Jul 2025 12:00:00 &#43;0000</lastBuildDate>
    <sy:updatePeriod>daily</sy:updatePeriod>
    <sy:updateFrequency>1</sy:updateFrequency>
    <atom:link href="https://scovl.github.io/tags/compiladores/" rel="self" type="application/rss+xml" />
    
    
    
    
    
    
    <item>
      <title>Como o compilador do Rust funciona?</title>
      <link>https://scovl.github.io/2025/07/21/rustcomp/</link>
      <guid>https://scovl.github.io/2025/07/21/rustcomp/</guid>
      <pubDate>Mon, 21 Jul 2025 12:00:00 &#43;0000</pubDate>
      <description>&lt;![CDATA[<h1 id="introdução">Introdução</h1>
<p>O Rust é famoso por ser uma linguagem que evita muitos erros de memória sem precisar de um coletor de lixo rodando em segundo plano. Mas como ele faz isso? O segredo está no compilador, que passa seu código por várias etapas até virar um programa que o computador entende.</p>
<p>Neste artigo, explicarei de forma simples cada fase desse processo: desde a leitura do código <strong><a href="https://en.wikipedia.org/wiki/Lexical_analysis">lexing</a></strong>, passando pela análise da estrutura <strong><a href="https://en.wikipedia.org/wiki/Parsing">parsing</a></strong>, até a geração do código final pelo <strong><a href="https://llvm.org/">LLVM</a></strong>.</p>]]></description>
      <content:encoded>&lt;![CDATA[<h1 id="introdução">Introdução</h1>
<p>O Rust é famoso por ser uma linguagem que evita muitos erros de memória sem precisar de um coletor de lixo rodando em segundo plano. Mas como ele faz isso? O segredo está no compilador, que passa seu código por várias etapas até virar um programa que o computador entende.</p>
<p>Neste artigo, explicarei de forma simples cada fase desse processo: desde a leitura do código <strong><a href="https://en.wikipedia.org/wiki/Lexical_analysis">lexing</a></strong>, passando pela análise da estrutura <strong><a href="https://en.wikipedia.org/wiki/Parsing">parsing</a></strong>, até a geração do código final pelo <strong><a href="https://llvm.org/">LLVM</a></strong>.</p>
<p>Mostrarei como o <strong><a href="https://doc.rust-lang.org/reference/borrow-checker.html">borrow checker</a></strong> (aquele que reclama dos seus empréstimos de variáveis), as representações intermediárias (com nomes esquisitos como <a href="https://en.wikipedia.org/wiki/High-level_intermediate_representation">HIR</a>, <a href="https://en.wikipedia.org/wiki/Typed_high_level_intermediate_representation">THIR</a> e <a href="https://en.wikipedia.org/wiki/Mid-level_intermediate_representation">MIR</a>) e as otimizações finais trabalham juntos para impedir problemas como dois lugares mexendo na mesma memória ao mesmo tempo <strong><a href="https://en.wikipedia.org/wiki/Race_condition">data race</a></strong> ou acessar algo que já foi apagado <strong><a href="https://en.wikipedia.org/wiki/Use-after-free">use-after-free</a></strong>.</p>
<p>No fim das contas, a arquitetura em camadas do compilador do Rust permite que ele seja rápido como C, mas com muito mais garantias de que seu programa não vai dar pau por causa de bugs difíceis de achar. Tudo isso graças a essas etapas intermediárias e checagens automáticas que acontecem antes mesmo do programa rodar.</p>
<h2 id="a-ponte-entre-seu-código-e-o-computador">A ponte entre seu código e o computador</h2>
<p>Linguagens como <a href="https://en.wikipedia.org/wiki/C_%28programming_language%29">C</a>, <a href="https://en.wikipedia.org/wiki/Go_%28programming_language%29">Go</a> e <a href="https://en.wikipedia.org/wiki/Rust_%28programming_language%29">Rust</a> ficam em um ponto intermediário: elas oferecem mais controle sobre o funcionamento do computador do que linguagens como <a href="https://en.wikipedia.org/wiki/Java_%28programming_language%29">Java</a> ou <a href="https://en.wikipedia.org/wiki/C_Sharp_%28programming_language%29">C#</a>, mas não são tão próximas do hardware quanto <a href="https://en.wikipedia.org/wiki/Assembly_language">Assembly</a>.</p>
<p>O que as diferencia é a forma como lidam com a memória: em C, o programador tem liberdade total para manipular ponteiros, mas também assume todos os riscos de erros; em Go, existe um coletor de lixo que gerencia a memória automaticamente; já o Rust criou um sistema próprio de &ldquo;posse e empréstimo&rdquo; (ownership e borrowing), que previne muitos problemas de memória já na fase de compilação, antes mesmo do programa rodar.</p>
<blockquote>
<p>Quando falamos que linguagens como <a href="https://en.wikipedia.org/wiki/C_%28programming_language%29">C</a>, <a href="https://en.wikipedia.org/wiki/Go_%28programming_language%29">Go</a> e <a href="https://en.wikipedia.org/wiki/Rust_%28programming_language%29">Rust</a> são &ldquo;intermediárias&rdquo;, isso não quer dizer que existe uma escala fixa entre &ldquo;baixo nível&rdquo; (<a href="https://en.wikipedia.org/wiki/Assembly_language">Assembly</a>) e &ldquo;alto nível&rdquo; (<a href="https://en.wikipedia.org/wiki/Java_%28programming_language%29">Java</a>, <a href="https://en.wikipedia.org/wiki/Python_%28programming_language%29">Python</a>) e que elas ficam sempre no meio.</p></blockquote>
<p>Na verdade, é só uma forma de dizer que elas misturam características dos dois lados: dão bastante controle sobre o computador (como <a href="https://en.wikipedia.org/wiki/Assembly_language">Assembly</a>), mas também oferecem recursos que facilitam a vida do programador (como <a href="https://en.wikipedia.org/wiki/Java_%28programming_language%29">Java</a> ou <a href="https://en.wikipedia.org/wiki/Python_%28programming_language%29">Python</a>).</p>
<blockquote>
<p>Por exemplo, C deixa você mexer direto na memória, mas ainda é mais fácil de usar do que Assembly. Go e Rust vão além: trazem recursos modernos, ajudam a evitar erros de memória e, no caso do Rust, permitem escrever código seguro e rápido sem perder desempenho.</p></blockquote>
<p>Ou seja, &ldquo;intermediária&rdquo; é só um jeito de dizer que essas linguagens conseguem equilibrar controle e facilidade, ficando entre o mundo das linguagens super próximas do hardware e o das linguagens super abstratas.</p>
<h2 id="o-compilador-do-rust">O compilador do Rust</h2>
<p>Quando a gente fala de compilador, normalmente ele é dividido em três partes: <strong>frontend</strong> (a parte que entende o seu código e transforma em uma estrutura de árvore chamada AST), <strong>middle-end</strong> (que faz otimizações que valem pra qualquer computador) e <strong>backend</strong> (que transforma tudo em código de máquina pra rodar no seu PC). O Rust segue esse modelo, mas adiciona umas etapas extras só pra garantir que ninguém vai fazer besteira com a memória.</p>
<p><img src="https://raw.githubusercontent.com/scovl/scovl.github.io/refs/heads/main/blog/content/post/images/rustcomp01.png" alt="Rust Compiler"></p>
<p>A imagem acima mostra que o compilador do Rust funciona como uma linha de montagem em três etapas: primeiro ele lê e entende seu código (frontend), depois faz uma checagem rigorosa das regras de segurança de memória (middle, onde entra o <a href="https://doc.rust-lang.org/reference/borrow-checker.html">borrow checker</a>), e por fim transforma tudo em código de máquina que o computador entende (backend); assim, cada parte cuida de um tipo de problema e, no final, seu programa sai rápido e seguro, sem aquelas dores de cabeça típicas de bugs de memória.</p>
<p>Quando você manda o Rust compilar seu arquivo <code>.rs</code>, a primeira coisa que acontece é que o compilador lê o texto e separa tudo em &ldquo;palavrinhas&rdquo; chamadas <em>tokens</em> (nomes de variáveis, números, símbolos, etc). Isso é o trabalho do <strong><a href="https://en.wikipedia.org/wiki/Lexical_analysis">analisador léxico</a></strong>.</p>
<p><img src="https://raw.githubusercontent.com/scovl/scovl.github.io/refs/heads/main/blog/content/post/images/rustcomp02.png" alt="Lexing"></p>
<p>A imagem acima mostra, de forma bem simples, como o compilador do Rust começa a entender seu código: primeiro ele lê o texto do programa e separa tudo em &ldquo;palavrinhas&rdquo; chamadas tokens (tipo nomes de variáveis, números, símbolos), e depois organiza esses tokens em uma espécie de árvore que mostra como as partes do seu código se encaixam — como se fosse um esqueleto do programa (a <strong><a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">AST</a></strong>).</p>
<p>Ou seja, a figura mostra que o compilador transforma o texto que você escreveu em uma estrutura organizada, facilitando para as próximas etapas encontrarem erros e entenderem o que o programa realmente faz.</p>
<p><img src="https://raw.githubusercontent.com/scovl/scovl.github.io/refs/heads/main/blog/content/post/images/rustcomp03.png" alt="AST"></p>
<p>Nessa etapa, o compilador também já expande as <strong>macros</strong>. Ou seja, se você usou algum &ldquo;atalho&rdquo; ou macro, ele já troca pelo código real, pra facilitar as próximas fases. Agora vem uma etapa crucial: o compilador pega a AST (que ainda tem comandos de alto nível, tipo o <code>for</code>) e faz um &ldquo;rebaixamento&rdquo; <strong><a href="https://en.wikipedia.org/wiki/Code_lowering">lowering</a></strong>: transforma a AST numa versão mais simples chamada <strong><a href="https://en.wikipedia.org/wiki/High-level_intermediate_representation">HIR</a></strong> (High-level IR).</p>
<p>Essa transformação é fundamental porque a HIR é mais próxima do que a linguagem realmente entende — ela remove a complexidade da sintaxe e deixa tudo mais &ldquo;quadradinho&rdquo; para as próximas análises.</p>
<p>Em seguida, ele faz a análise de tipos e gera a <strong><a href="https://en.wikipedia.org/wiki/Typed_high_level_intermediate_representation">THIR</a></strong> (Typed HIR), onde cada pedacinho do código já tem um tipo definido (int, string, etc).</p>
<p>Antes de seguir, o compilador faz uma checagem de segurança chamada <strong>unsafety</strong>: ele olha a THIR pra garantir que coisas perigosas (tipo mexer direto na memória com ponteiros) só aconteçam dentro de blocos marcados como <code>unsafe</code>. Assim, ele já barra muita coisa errada antes mesmo de virar código de verdade.</p>
<p>A <strong><a href="https://en.wikipedia.org/wiki/Mid-level_intermediate_representation">MIR</a></strong> converte o programa num <strong><a href="https://en.wikipedia.org/wiki/Control-flow_graph">Grafo de Fluxo de Controle (CFG)</a></strong> explícito. Esse grafo permite ao <strong><a href="https://doc.rust-lang.org/reference/borrow-checker.html">borrow checker</a></strong> rastrear, ao longo de todos os caminhos de execução, o estado de cada valor: possuído, emprestado mutável, emprestado imutável ou movido.</p>
<p><img src="https://raw.githubusercontent.com/scovl/scovl.github.io/refs/heads/main/blog/content/post/images/rustcomp04.png" alt="CFG"></p>
<p>A imagem acima ilustra um exemplo simplificado de um <strong>Grafo de Fluxo de Controle (CFG)</strong>. Nela, o círculo azul no topo representa um ponto de decisão ou condição no seu código (como um <code>if</code> ou <code>match</code>). As setas que partem dele mostram os possíveis caminhos que o programa pode seguir: um para o caso <code>True</code> (círculo vermelho à esquerda) e outro para o caso <code>False</code> (círculo vermelho à direita).</p>
<p>Ambos os caminhos convergem para o círculo verde pontilhado na parte inferior, que simboliza a continuação do programa após a decisão. É essa representação em grafo que permite ao <strong><a href="https://doc.rust-lang.org/reference/borrow-checker.html">borrow checker</a></strong> do Rust analisar todos os fluxos possíveis do seu código e garantir a segurança da memória em cada um deles, independentemente de qual caminho o programa realmente tomar em tempo de execução.</p>
<h2 id="o-mago-da-memória-entendendo-o-borrow-checker">O Mago da Memória: Entendendo o Borrow Checker</h2>
<p>O <strong><a href="https://doc.rust-lang.org/reference/borrow-checker.html">borrow checker</a></strong> é o coração do sistema de segurança do Rust. Ele funciona como um inspetor rigoroso que analisa cada pedaço do seu código para garantir que ninguém vai mexer na memória de forma perigosa. Usando a MIR como base, o borrow checker rastreia três estados principais para cada valor:</p>
<ol>
<li><strong>Possuído (Owned)</strong>: O valor pertence exclusivamente a uma variável</li>
<li><strong>Emprestado Imutável (Borrowed Immutable)</strong>: Outras partes do código podem ler, mas não modificar</li>
<li><strong>Emprestado Mutável (Borrowed Mutable)</strong>: Apenas uma parte pode ler e modificar por vez</li>
</ol>
<p>Caso uma violação ocorra (uso de valor após movimento, criação de dados mutáveis e imutáveis simultâneos, etc.), o compilador rejeita o código. Esse mecanismo previne <strong><a href="https://en.wikipedia.org/wiki/Race_condition">data races</a></strong> e <strong><a href="https://en.wikipedia.org/wiki/Use-after-free">use‑after‑free</a></strong> sem custo em tempo de execução. O borrow checker é tão eficiente que muitos programadores Rust brincam que ele é &ldquo;o melhor professor de programação que você já teve&rdquo; — ele te ensina boas práticas de memória antes mesmo do programa rodar!</p>
<p>Após otimizações em MIR (eliminação de código morto, <strong><a href="https://en.wikipedia.org/wiki/Inline_function">inlining</a></strong> local, etc.), a IR é traduzida para <strong><a href="https://llvm.org/docs/LangRef.html">LLVM IR</a></strong>. A <strong><a href="https://llvm.org/docs/LangRef.html">LLVM IR</a></strong> (Low Level Virtual Machine Intermediate Representation) é uma linguagem intermediária de baixo nível, mas independente da arquitetura do processador.</p>
<p>É nela que o Rust traduz tudo o que foi checado e otimizado até aqui, para que a LLVM possa realizar o trabalho pesado de otimização de código. A LLVM IR não é literalmente &ldquo;entendida pelo processador&rdquo; — ela serve como uma representação intermediária que o LLVM usa para gerar o código nativo específico da arquitetura de destino (como x86-64, ARM, etc.).</p>
<p><img src="https://raw.githubusercontent.com/scovl/scovl.github.io/refs/heads/main/blog/content/post/images/rustcomp05.png" alt="LLVM IR"></p>
<p>O LLVM aplica otimizações específicas de arquitetura e, por fim, gera código objeto para a plataforma‑alvo, como x86‑64 ou AArch64. Como consequência, um binário Rust é normalmente específico à arquitetura de destino, a menos que se utilize camadas de emulação.</p>
<h2 id="por-que-tanta-etapa-intermediária">Por que tanta etapa intermediária?</h2>
<p>Pense assim: cada IR (representação intermediária) é como um filtro diferente que o compilador usa para checar seu código. Primeiro, a HIR guarda bastante informação para que o compilador possa te dar mensagens de erro detalhadas e entender o contexto do seu programa. Depois, a MIR simplifica tudo, deixando o código mais &ldquo;quadradinho&rdquo; e fácil de analisar — é nessa hora que o <strong><a href="https://doc.rust-lang.org/reference/borrow-checker.html">borrow checker</a></strong> entra em ação, garantindo que ninguém vai mexer na memória de um jeito perigoso.</p>
<p>Essa divisão em camadas faz com que cada parte do compilador só precise se preocupar com um tipo de problema por vez. Isso facilita encontrar erros antes mesmo do programa rodar, sem deixar o código final mais lento.</p>
<p>E, pra fechar com chave de ouro, o Rust entrega a última etapa (gerar o código de máquina de verdade) pro LLVM, que já é um especialista em otimização e velocidade. Assim, o Rust foca em garantir segurança e o LLVM em deixar tudo rápido.</p>
<p>No fim das contas, o compilador do Rust funciona como uma linha de montagem cheia de inspeções: cada etapa checa uma coisa diferente, pegando vários erros que em outras linguagens só apareceriam quando o programa já estivesse rodando (ou pior, em produção!). Por isso, muita gente acredita que esse modelo de &ldquo;camadas inteligentes&rdquo; vai ser cada vez mais comum nas linguagens do futuro, juntando robustez e desempenho sem dor de cabeça.</p>
<p>Por fim, vale destacar: linguagens como C e C++ não adotam esse modelo de múltiplas camadas de checagem automática durante a compilação. Nelas, o compilador faz análises mais simples e deixa a maior parte dos cuidados com memória e segurança por conta do programador.</p>
<p>Isso significa que muitos erros perigosos — como acessar memória já liberada, criar <strong><a href="https://en.wikipedia.org/wiki/Race_condition">data races</a></strong> ou sobrescrever dados sem querer — só aparecem quando o programa já está rodando, e às vezes nem são detectados. O Rust, ao contrário, pega esses problemas antes mesmo do código virar um executável, tornando o desenvolvimento mais seguro sem sacrificar desempenho.</p>
<blockquote>
<p><a href="https://www.abeacha.com/NIST_press_release_bugs_cost.html">Um estudo de 2002 e publicado em2019 da National Institute of Standards and Technology (NIST)</a> estimou que os erros de software custam à economia dos EUA mais de 59,5 bilhões de dólares anualmente, com uma parcela significativa desses custos vindo de vulnerabilidades de segurança e falhas de memória. A ausência de checagens automáticas em C/C++ contribui para que esses tipos de falhas se tornem uma preocupação constante.</p></blockquote>
<p>Para não soar como injusto, é necessário dizer que o C++23 trouxe várias novidades para tentar deixar o código mais seguro e moderno, especialmente quando o assunto é evitar bugs de memória — mas sem mudar a linguagem de cabeça pra baixo.</p>
<p>Agora, por exemplo, dá pra declarar de forma explícita quando um objeto começa a existir na memória (com o <strong><a href="https://en.cppreference.com/w/cpp/language/lifetime#start_lifetime_as">start_lifetime_as</a></strong>), o que ajuda a evitar aqueles bugs cabeludos que nem os detectores automáticos pegavam. Também ficou mais fácil e seguro conversar com APIs em C sem correr o risco de vazar memória, graças a novos adaptadores de ponteiros inteligentes.</p>
<p>Os containers ganharam versões que evitam acesso fora dos limites (tipo o <strong><a href="https://en.cppreference.com/w/cpp/container/mdspan">mdspan</a></strong> para matrizes), e ficou mais prático lidar com erros usando o <strong><a href="https://en.cppreference.com/w/cpp/utility/expected">std::expected</a></strong>, que incentiva o retorno explícito de falhas em vez de depender de códigos mágicos ou variáveis globais.</p>
<p>Até a formatação de texto ficou mais fácil, <a href="https://en.cppreference.com/w/cpp/io/c/fprintf">com funções no estilo Python</a>, e agora dá pra gerar <a href="https://en.cppreference.com/w/cpp/error/stacktrace">stacktraces portáveis sem gambiarra</a>. Apesar desses avanços, algumas proteções automáticas que o Rust já oferece — como checagem de uso de ponteiros e detecção de data races — ainda não chegaram no C++ (ficaram pra próxima versão).</p>
<blockquote>
<p>Ou seja: o C++23 está caminhando para fechar várias brechas históricas e facilitar a vida do programador, mas ainda depende bastante de disciplina e ferramentas externas, enquanto o Rust já faz muita coisa “no automático” para garantir a segurança do seu código.</p></blockquote>
<p>Enquanto o compilador do Rust atua como um inspetor de qualidade rigoroso, rejeitando qualquer código que possa violar as regras de segurança de memória, o compilador de C/C++ foca em traduzir o código de forma fiel e otimizada. Ele assume que o programador é o responsável por todas as garantias de segurança.</p>
<hr>
<h2 id="referências">REFERÊNCIAS</h2>
<ul>
<li><a href="https://doc.rust-lang.org/reference/">The Rust Reference - The Rust Compiler</a> - A referência oficial do Rust sobre o compilador e a linguagem.</li>
<li><a href="https://doc.rust-lang.org/rustc/">The Rustc Book</a> - O livro oficial do Rust sobre o compilador.</li>
<li><a href="https://rustc-dev-guide.rust-lang.org/overview.html">Rust Compiler Architecture Overview</a> - Uma visão geral da arquitetura do compilador do Rust.</li>
<li><a href="https://llvm.org/docs/LangRef.html">LLVM Language Reference Manual</a> - A referência oficial do LLVM sobre a linguagem intermediária.</li>
<li><a href="https://en.cppreference.com/w/cpp/23">C++23</a> - A referência oficial do C++23.</li>
<li><a href="https://github.com/baindlapranayraj/SolanaBlogs/blob/main/">Solana Blogs</a> - Onde o artigo se baseou.</li>
<li><a href="https://medium.com/@humble_bee/why-is-memory-safety-without-gc-a-big-deal-in-rust-41f6bdd5902f">Why is memory safety without GC a big deal in Rust?</a> - Um artigo sobre a importância da segurança de memória sem GC no Rust.</li>
<li><a href="https://rustc-dev-guide.rust-lang.org/overview.html#:~:text=Code%20generation">Overview of the compiler</a> - Uma seção do guia do Rust sobre a geração de código.</li>
<li><a href="https://aneksteind.github.io/posts/2023-06-12.html#:~:text=Exploring%20Dataflow%20Analysis%20in%20the,control%20flow%20graph%20structure">Exploring Dataflow Analysis in the Rust Compiler</a> - Um artigo sobre a análise de fluxo de dados no compilador do Rust.</li>
<li><a href="https://www.infoq.com/presentations/rust-borrow-checker/#:~:text=lowers%20it%20into%20the%20mid,also%20known%20as%20the%20MIR">Rust Borrow Checker</a> - Uma apresentação sobre o borrow checker do Rust.</li>
</ul>
]]></content:encoded>
      
      
      <category>Rust,Compiladores,LLVM,Memória,Segurança</category>
      
      
      
      <dc:creator>Vitor Lobo Ramos</dc:creator>
      
      
      
      
      
      <description>&lt;![CDATA[Uma visão geral do compilador do Rust]]></description>
      
    </item>
    
    <item>
      <title>Inferência de Tipos em Rust e C&#43;&#43;</title>
      <link>https://scovl.github.io/2025/07/18/type01/</link>
      <guid>https://scovl.github.io/2025/07/18/type01/</guid>
      <pubDate>Fri, 18 Jul 2025 23:18:18 -0300</pubDate>
      <description>&lt;![CDATA[<h2 id="introdução">Introdução</h2>
<p>Inferência de tipos é o mecanismo pelo qual o compilador descobre automaticamente o tipo de uma variável ou expressão em uma linguagem de programação. Esse recurso permite que o programador omita anotações de tipo em muitas situações sem comprometer a segurança de tipos do programa.</p>
<p>Neste artigo, discutimos como as linguagens <strong><a href="https://www.rust-lang.org/">Rust</a></strong> e <strong><a href="https://en.wikipedia.org/wiki/C%2B%2B">C++</a></strong> implementam inferência de tipos de formas fundamentalmente diferentes, analisando as consequências práticas de cada abordagem.</p>]]></description>
      <content:encoded>&lt;![CDATA[<h2 id="introdução">Introdução</h2>
<p>Inferência de tipos é o mecanismo pelo qual o compilador descobre automaticamente o tipo de uma variável ou expressão em uma linguagem de programação. Esse recurso permite que o programador omita anotações de tipo em muitas situações sem comprometer a segurança de tipos do programa.</p>
<p>Neste artigo, discutimos como as linguagens <strong><a href="https://www.rust-lang.org/">Rust</a></strong> e <strong><a href="https://en.wikipedia.org/wiki/C%2B%2B">C++</a></strong> implementam inferência de tipos de formas fundamentalmente diferentes, analisando as consequências práticas de cada abordagem.</p>
<p>Exploramos também brevemente o caso do <strong><a href="https://en.wikipedia.org/wiki/Swift_%28programming_language%29">Swift</a></strong>, comparando sua estratégia híbrida e os desafios de desempenho que ela acarreta. Ao final, refletimos sobre o impacto dessas escolhas de design na experiência de programação.</p>
<p>Em linguagens de programação <strong><a href="https://en.wikipedia.org/wiki/Type_system#Static_type_checking">estaticamente tipadas</a></strong>, como Rust e C++, cada variável e expressão possui um <strong>tipo</strong> definido em tempo de compilação. O tipo especifica que espécie de dado está sendo armazenado (por exemplo, um número inteiro, um texto, um vetor de strings etc.) e determina que operações são permitidas sobre ele.</p>
<p>Tradicionalmente, linguagens estáticas exigem que o programador declare explicitamente esses tipos, mas isso pode tornar o código verboso. A <strong>inferência de tipos</strong> veio para mitigar esse problema: trata-se da capacidade do compilador de deduzir automaticamente o tipo de uma expressão, economizando do programador o trabalho de anotá-lo manualmente em cada ocasião.</p>
<blockquote>
<p>Importante notar que os tipos continuam existindo e sendo checados – a inferência atua apenas na omissão segura das anotações redundantes.</p></blockquote>
<p><img src="https://raw.githubusercontent.com/scovl/scovl.github.io/refs/heads/main/blog/content/post/images/retropc01.png" alt=""></p>
<p>Linguagens modernas incorporaram inferência de tipos de formas variadas. A ideia remonta à pesquisa acadêmica dos anos 1960 e 1970 (trabalhos de <em><a href="https://en.wikipedia.org/wiki/Robin_Milner">Hindley</a></em> e <em><a href="https://en.wikipedia.org/wiki/Robin_Milner">Milner</a></em>, entre outros) e tornou-se um pilar em linguagens funcionais como <a href="https://en.wikipedia.org/wiki/ML_%28programming_language%29">ML</a> e <a href="https://en.wikipedia.org/wiki/Haskell_%28programming_language%29">Haskell</a>, que conseguem inferir tipos para praticamente todas as expressões sem nenhuma anotação do programador.</p>
<p>Já em linguagens de uso geral como C++, a inferência de tipos foi introduzida de forma mais limitada (por exemplo, com o keyword <code>auto</code> em <a href="https://en.wikipedia.org/wiki/C%2B%2B11">C++11</a>) para facilitar a sintaxe mantendo a compatibilidade com seu sistema de tipos complexo.</p>
<p>O Rust, por sua vez, adotou desde o início um sistema de inferência mais poderoso inspirado no algoritmo de Hindley-Milner, porém adaptado às necessidades da linguagem. A seguir, examinamos em detalhes como C++ e Rust realizam a inferência e por que essas abordagens divergem.</p>
<h2 id="inferência-de-tipos-no-c">Inferência de Tipos no C++</h2>
<p>No C++, a inferência de tipos ocorre de maneira <strong>local e unidirecional</strong>, fundamentada principalmente no uso da palavra-chave <code>auto</code> (e da construção relacionada <code>decltype</code>).</p>
<p>Quando declaramos uma variável com <code>auto</code>, estamos instruindo o compilador a <strong>deduzir o tipo daquela variável a partir apenas do valor usado na sua inicialização</strong>. Em outras palavras, o compilador olha para o lado direito da atribuição (a expressão inicializadora) e determina o tipo apropriado para a variável no lado esquerdo. Por exemplo:</p>


  <pre><code class="language-cpp">std::vector&lt;int&gt; get_vector(); // função que retorna um vetor de int

int main() {
    std::vector&lt;int&gt; v = get_vector(); // declaração explícita: v é std::vector&lt;int&gt;
    auto w = get_vector();             // inferência: w terá o tipo retornado por get_vector()
}</code></pre>
 <p>No código acima, a variável <code>w</code> será deduzida como tendo o mesmo tipo de <code>v</code> (<code>std::vector&lt;int&gt;</code>), pois <code>get_vector()</code> retorna esse tipo. A utilização de <code>auto</code> elimina a redundância de repetir <code>std::vector&lt;int&gt;</code> na declaração de <code>w</code>. Embora a economia de caracteres pareça modesta, esse recurso ganha importância em casos onde o tipo é extenso ou intrincado.</p>
<p>Um exemplo clássico é o tipo de uma <strong><a href="https://en.wikipedia.org/wiki/Lambda_calculus">lambda</a></strong> (função anônima) em C++: lambdas possuem tipos únicos gerados pelo compilador, sem um nome simples para o programador referenciar.</p>
<p>Nesse caso, <code>auto</code> se torna essencial para armazenar lambdas em variáveis, já que não existe um nome de tipo facilmente utilizável sem envolver templates ou <code>std::function</code>. De forma geral, <code>auto</code> também melhora a legibilidade quando lida com tipos muito complexos (por exemplo, iteradores de templates ou tipos dependentes de template), deixando o compilador inferir esses detalhes.</p>
<p>Além de <code>auto</code>, o C++ oferece <code>decltype</code>, que serve para extrair o tipo de uma expressão existente. Por exemplo, podemos escrever <code>decltype(x+y)</code> para obter o tipo resultante da soma de <code>x</code> e <code>y</code> e usar isso em uma declaração. Considere:</p>


  <pre><code class="language-cpp">auto x = foo(); 
auto y = bar();
// Queremos um vetor que contenha elementos do tipo de x&#43;y, sem saber exatamente qual tipo é esse
std::vector&lt;decltype(x &#43; y)&gt; v; // v terá o tipo std::vector&lt;tipo_de_x&#43;y&gt;</code></pre>
 <p>Nesse fragmento, <code>decltype(x + y)</code> produz em tempo de compilação o tipo resultante da expressão <code>x + y</code>, permitindo declarar <code>v</code> corretamente.</p>
<p>Ferramentas como <code>decltype</code> reforçam que a inferência em C++ pode ser vista como um mecanismo de <em>substituição de código</em>: o desenvolvedor diz ao compilador “insira aqui o tipo correspondente a esta expressão”. Efetivamente, o compilador resolve o tipo e <strong>substitui</strong> a palavra <code>auto</code> (ou a expressão dentro de <code>decltype(...)</code>) pelo nome do tipo deduzido.</p>
<p>Um aspecto importante é que, em C++, essa dedução <strong>não considera nenhum uso futuro da variável</strong> – ela se baseia <em>exclusivamente</em> nas informações disponíveis naquele ponto do código. Após processar uma linha de declaração, o compilador já determina e “congela” o tipo da variável para uso subsequente. Consequentemente, trechos de código como o abaixo não são permitidos em C++:</p>


  <pre><code class="language-cpp">auto x = {};   // tentativa de deduzir a partir de um inicializador vazio (ambiguo!)
foo(x);       // usar x em uma chamada posterior</code></pre>
 <p>No exemplo hipotético acima, <code>auto x = {}</code> é inválido porque <code>{}</code> (um <strong>initializer list</strong> vazio) não fornece pistas suficientes para deduzir um tipo concreto para <code>x</code>. O compilador <strong>não</strong> tentará olhar para a chamada <code>foo(x)</code> para inferir que tipo <code>x</code> deveria ter; ele simplesmente emite um erro, dizendo que não foi possível deduzir o tipo de <code>x</code>.</p>
<p>Essa filosofia de projeto está alinhada com a natureza do C++: o compilador funciona quase como um <strong>interpretador de única passada</strong> (one-pass interpreter) no que tange à inferência de tipos, determinando os tipos à medida que lê o código, sempre &ldquo;para frente&rdquo;, jamais &ldquo;para trás&rdquo; ou além do escopo local. Isso torna o comportamento mais previsível e evita que mudanças em linhas futuras alterem retrospectivamente o significado de linhas anteriores.</p>
<p>Outro impacto dessa abordagem é visto na resolução de <strong>sobrecarga de funções</strong> e instâncias de <strong>templates</strong>. Em C++, para selecionar qual versão de uma função sobrecarregada chamar, ou para deduzir parâmetros de um template, o compilador precisa conhecer os tipos dos argumentos <em>antes</em> de fazer a resolução.</p>
<p>Como o tipo de cada variável é inferido imediatamente em sua declaração, quando o compilador encontra uma chamada como <code>foo(x)</code> ele já sabe o tipo de <code>x</code> e pode resolver de forma determinística qual função <code>foo</code> (entre as possivelmente sobrecarregadas) deve ser invocada. Essa ordem de resolução (deduzir tipos primeiro, depois escolher sobrecargas) é parte integrante do modelo de compilação do C++.</p>
<p>Vale mencionar que versões modernas do C++ têm expandido modestamente as capacidades de inferência, mas sempre dentro do paradigma existente.</p>
<p>O C++17 introduziu o <strong>Class Template Argument Deduction (CTAD)</strong>, que permite ao compilador deduzir os parâmetros de template de classes a partir dos argumentos do construtor. Por exemplo, podemos escrever <code>std::pair p(2, 4.5);</code> sem especificar <code>&lt;int, double&gt;</code> explicitamente, pois o compilador deduz que <code>p</code> é <code>std::pair&lt;int, double&gt;</code> com base nos valores fornecidos. Do mesmo modo, <code>std::tuple t(4, 3, 2.5);</code> deduz <code>std::tuple&lt;int, int, double&gt;</code> automaticamente.</p>
<p>O C++20 introduziu as <em>templates abreviadas</em>, que permitem usar <code>auto</code> no lugar do tipo de um parâmetro de função, tornando a própria função uma espécie de template genérico. Assim, podemos definir:</p>


  <pre><code class="language-cpp">auto twice(auto x) {
    return x &#43; x;
}</code></pre>
 <p>A função acima aceita qualquer tipo para <code>x</code> (desde que o operador <code>+</code> esteja definido para tal tipo) e retorna um valor do mesmo tipo. Apesar da sintaxe conveniente, internamente isso é equivalente a declarar um template <code>template&lt;typename T&gt; T twice(T x) {...}</code> – ou seja, não se trata de uma inferência de tipo <strong>global</strong> ou <strong>posterior</strong>, mas apenas de um açúcar sintático para geração de funções genéricas.</p>
<p>O compilador ainda trabalha <strong>localmente</strong>: ao compilar uma chamada como <code>twice(5)</code>, ele cria uma instância da função com <code>T</code> deduzido como <code>int</code> no momento da chamada, sem tentar re-inferir nada além do escopo daquela função.</p>
<p>Em resumo, o C++ trata inferência de tipos como <strong>uma conveniência pontual</strong>. O comportamento é estritamente determinado pela expressão inicial e pelas regras locais de conversão, tornando a inferência transparente e quase mecânica.</p>
<p>Como consequência, o programador C++ às vezes precisará fornecer dicas extras ao compilador (por exemplo, especificar sufixos em literais, ou anotar tipos de template complexos) quando a dedução automática não for suficiente. Essa abordagem privilegia a <strong>previsibilidade</strong>: uma vez escrita uma linha de código, seu efeito sobre os tipos é fixo e não será alterado por código em outras partes da função.</p>
<h2 id="inferência-de-tipos-no-rust">Inferência de Tipos no Rust</h2>
<p>A linguagem Rust adota uma estratégia de inferência de tipos <strong>mais robusta e contextual</strong>, baseada no clássico algoritmo <strong><a href="https://en.wikipedia.org/wiki/Hindley%E2%80%93Milner_type_system">Hindley–Milner</a></strong> da teoria de tipos. Diferentemente do C++, em Rust não existe uma palavra-chave específica como <code>auto</code>; em vez disso, <em>todas</em> as declarações podem omitir o tipo do valor, e o compilador inferirá o tipo com base em todas as pistas disponíveis.</p>
<p>Podemos dizer que o compilador Rust age como um <strong>solucionador de restrições</strong>: ele analisa simultaneamente um bloco de código inteiro (por exemplo, o corpo de uma função), recolhendo informações sobre que tipos seriam consistentes com cada operação, e então encontra um conjunto de tipos que satisfaz todas as restrições impostas pelo código.</p>
<p>Um exemplo simples ilustra essa abordagem. Considere duas funções em Rust, uma que espera um vetor de inteiros e outra que espera um vetor de strings:</p>


  <pre><code class="language-rust">fn foo(v: Vec&lt;i32&gt;) { /*...*/ }      // aceita vetor de i32
fn bar(v: Vec&lt;String&gt;) { /*...*/ }   // aceita vetor de String

fn main() {
    let x = Vec::new(); // vetor vazio, tipo inicialmente desconhecido
    let y = Vec::new(); // outro vetor vazio, tipo inicialmente desconhecido
    foo(x);             // após esta linha, x: Vec&lt;i32&gt;
    bar(y);             // após esta linha, y: Vec&lt;String&gt;
}</code></pre>
 <p>No trecho acima, tanto <code>x</code> quanto <code>y</code> são inicializados com <code>Vec::new()</code> (um vetor vazio) sem anotação de tipo. Isoladamente, <code>Vec::new()</code> é ambíguo, pois poderia ser um <code>Vec&lt;T&gt;</code> de qualquer tipo <code>T</code>. No entanto, ao usar <code>x</code> como argumento em <code>foo(x)</code>, o compilador deduz que <code>x</code> <em>deve</em> ser <code>Vec&lt;i32&gt;</code> para satisfazer o tipo de <code>foo</code>.</p>
<p>Analogamente, <code>y</code> é deduzido como <code>Vec&lt;String&gt;</code> porque é passado para <code>bar</code>. Assim, <strong>o mesmo código de inicialização resultou em dois tipos diferentes</strong> para as variáveis, dependendo do uso posterior de cada uma. Esse comportamento seria impossível em C++ ou Go, mas em <a href="https://www.rust-lang.org/">Rust</a> ele é natural dentro do modelo de inferência global.</p>
<p><img src="https://raw.githubusercontent.com/scovl/scovl.github.io/refs/heads/main/blog/content/post/images/retropc02.png" alt=""></p>
<p>Podemos perceber que o compilador Rust efetua um <strong>raciocínio bidirecional</strong>: ele propaga informações de tipo tanto <strong>para frente</strong> (do ponto onde algo é declarado para onde é usado) quanto <strong>para trás</strong> (do contexto de uso de volta para a declaração original). Em termos práticos, o Rust consegue frequentemente inferir o tipo exato de quase todas as variáveis locais apenas olhando o contexto, sem nenhuma anotação explícita por parte do programador.</p>
<p>Tipicamente, só é necessário declarar tipos nas <strong>fronteiras</strong> – isto é, nos parâmetros e retornos de funções públicas – para que o código seja legível e para estabelecer interfaces claras entre partes do programa. Dentro de uma função, porém, é comum não ver nomes de tipos na maioria das declarações, já que o compilador pode <strong>unir os pontos</strong> de forma consistente.</p>
<p>Naturalmente, essa flexibilidade vem acompanhada de regras para garantir que o resultado da inferência seja <strong>único e coerente</strong>. O Rust exige que haja informação suficiente para determinar cada tipo de forma não-ambígua. Caso contrário, a compilação falha com um erro pedindo anotações adicionais.</p>
<p>Por exemplo, se no exemplo anterior removêssemos as chamadas <code>foo(x)</code> e <code>bar(y)</code> (ou as trocássemos acidentalmente), o compilador reclamaria que não conseguiu inferir o tipo de <code>x</code> ou <code>y</code>. Do mesmo modo, se cometemos um engano e usarmos um valor em um lugar incompatível com seu tipo inferido, o compilador detectará a contradição. Veja este cenário:</p>


  <pre><code class="language-rust">fn bar(v: Vec&lt;String&gt;) { /*...*/ }

fn main() {
    let x: Vec&lt;i32&gt; = Vec::new();
    bar(x); // ERRO: &#34;types mismatch&#34;, esperava-se Vec&lt;String&gt; mas foi fornecido Vec&lt;i32&gt;
}</code></pre>
 <p>Aqui, annotamos <code>x</code> explicitamente como <code>Vec&lt;i32&gt;</code> e, em seguida, tentamos passá-lo a <code>bar</code> que espera <code>Vec&lt;String&gt;</code>. O Rust imediatamente reporta erro de tipos incompatíveis, evitando qualquer comportamento ambíguo ou inferência incorreta.</p>
<p>Em outro caso, podemos pedir ao compilador para inferir parte de um tipo usando o curinga <code>_</code> (placeholder) em uma anotação, mas ainda assim precisamos dar informação suficiente para não ficar mais de uma possibilidade. Se nem mesmo com todas as pistas o compilador puder determinar unicamente um tipo, a inferência <strong>falhará</strong>, emitindo uma mensagem de erro solicitando uma anotação manual.</p>
<p>Em termos de filosofia, o sistema de tipos do Rust adquire uma característica mais <strong>declarativa</strong> devido à inferência robusta. O programador escreve o que deseja fazer (por exemplo, aplicar métodos, combinar valores, retornar um resultado), e o compilador trabalha nos bastidores para descobrir quais tipos tornam todas essas operações válidas simultaneamente.</p>
<p>Alguns desenvolvedores comparam essa experiência a interagir com um assistente lógico ou um provador de teoremas, já que você estabelece &ldquo;verdades&rdquo; parciais sobre os dados e o compilador verifica a consistência global dessas afirmações.</p>
<p>Uma vantagem prática disso é que cada tipo geralmente precisa ser escrito <strong>apenas uma vez</strong> em todo o programa (quando é necessário). Se uma função retorna um tipo complexo, você não precisa repetir esse tipo ao usar o valor – o compilador já sabe, e propagará a informação adiante conforme necessário. Isso reduz a redundância e o risco de discrepâncias entre declarações e usos.</p>
<p>Rust consegue oferecer essa inferência global potente em parte porque abre mão de certos recursos presentes em C++ que dificultariam o processo. Em especial, destacam-se as ausências, por design, de alguns mecanismos na linguagem Rust:</p>
<ul>
<li><strong>Sobrecarga de funções por tipo</strong>: Em Rust não é permitido definir duas funções com o mesmo nome que aceitem tipos diferentes (como se faz em C++). Cada função tem um nome único ou, se comportamentos diferentes forem necessários, usam-se <strong>traits</strong> para diferenciá-los. Isso elimina ambiguidade, pois uma chamada de função em Rust corresponde sempre a uma única definição possível (após considerado o trait/import necessário).</li>
<li><strong>Conversões implícitas de tipo</strong>: Rust não realiza conversões automáticas entre tipos numéricos ou de qualquer outro tipo (ao contrário do C++, que pode converter implicitamente, por exemplo, um <code>int</code> em <code>double</code> em certas expressões). Em Rust, ou o tipo já coincide exatamente, ou você deve convertê-lo explicitamente via métodos ou casting. Isso previne que o sistema de tipos fique tentando múltiplas vias de conversão durante a inferência – as possibilidades são restritas e claras.</li>
<li><strong>Herança de classes</strong>: Ao invés de herança tradicional (subtipos baseados em hierarquias de classes como em C++/Java), Rust utiliza <em>traits</em> (interfaces) e composição. Não havendo herança de implementação, não ocorre a situação de um objeto poder ser de múltiplos tipos numa hierarquia, o que simplifica a dedução e o despacho de métodos. A escolha de implementação de um trait para um tipo é estática e não afeta a inferência além de garantir que certos métodos estão disponíveis.</li>
<li><strong>Especialização de templates</strong>: Rust tem generics e implementações de traits para tipos genéricos, mas atualmente não permite <em>especialização</em> (isto é, fornecer implementações alternativas de um traço/genérico para um caso específico mais restrito). Em C++ templates, por exemplo, pode-se ter uma função genérica mas também uma versão especial quando <code>T</code> é um <code>int</code>. Isso pode introduzir comportamento diferente dependendo do tipo exato inferido, complicando a inferência. No Rust, cada impl de trait é única e válida para um conjunto possivelmente amplo de tipos, mas não há duas versões conflitantes do mesmo trait que o compilador precise escolher entre si durante a inferência.</li>
</ul>
<p>Essas escolhas de design do Rust limitam o espaço de busca do algoritmo de inferência. Em essência, o compilador Rust tem menos &ldquo;adivinhações&rdquo; a fazer, porque a linguagem evita construções que poderiam levar a múltiplas interpretações para uma mesma expressão.</p>
<p>A sobrecarga de funções tradicional, por exemplo, foi deliberadamente excluída porque múltiplas definições sobrecarregadas poderiam interagir mal com o sistema de inferência, complicando a resolução de tipos. Em vez disso, o Rust utiliza traits e genéricos para alcançar polimorfismo ad-hoc, mantendo a inferência mais previsível.</p>
<p>Da mesma forma, a ausência de conversões implícitas entre tipos (por exemplo, de <code>i32</code> para <code>f64</code>) evita que o compilador fique tentando adivinhar caminhos de conversão durante a inferência – qualquer conversão deve ser explícita via <code>as</code> ou métodos, eliminando ambiguidade. Essa restrição consciente de poder expressivo em algumas áreas é o que torna viável aplicar Hindley-Milner em um contexto de linguagem de sistemas com alta performance de compilação.</p>
<p>Vale notar que, embora o Rust use um sistema de inferência forte, <strong>ele não chega a inferir a assinatura completa de funções</strong>. Ou seja, diferentemente de Haskell (onde é possível escrever funções sem nenhuma anotação de tipo que o compilador deduz seu tipo genérico mais geral automaticamente), o Rust exige que os parâmetros e tipos de retorno de <strong>todas</strong> as funções sejam especificados – sejam elas públicas, privadas ou locais.</p>
<p>Essa escolha de design foi deliberada: ao não permitir inferência &ldquo;global&rdquo; entre funções, evita-se que um erro em uma função cause mensagens confusas em outro ponto distante do código.</p>
<p>Em outras palavras, a inferência do Rust ocorre apenas dentro do escopo de cada função ou bloco, e nunca ao nível de APIs entre módulos. Isso mantém as interfaces explícitas e ajuda na legibilidade e na verificação de compatibilidade entre crates (módulos compilados separadamente).</p>
<p>A inferência atua dentro dos limites dessas funções e nos tipos genéricos, mas não infere, por exemplo, que uma função <code>fn add(x, y) { x + y }</code> deve ser genérica ou qual seu tipo de retorno – tais informações devem ser anotadas (no caso, usando traits e <code>-&gt; T</code>). Essa diferença demonstra mais uma vez o equilíbrio que Rust busca: o benefício da inferência local máxima, sem sacrificar a clareza e a robustez na definição de fronteiras do código.</p>
<h2 id="comparação-com-o-swift-e-desafios-adicionais">Comparação com o Swift e Desafios Adicionais</h2>
<p>A linguagem <strong>Swift</strong>, desenvolvida pela Apple, oferece um caso interessante para compararmos com Rust e C++. Swift implementa um sistema de inferência de tipos também baseado em resolução de restrições (um tipo de <strong>unificação</strong> bidirecional semelhante ao Hindley-Milner), permitindo ao programador omitir muitos tipos.</p>
<p>Entretanto, Swift <strong>mantém recursos de linguagem que Rust evitou</strong>, como sobrecarga extensiva de funções e operadores, conversões implícitas via <strong>protocolos literais</strong>, e múltiplas conveniências sintáticas. A interação dessas características com a inferência de tipos acabou expondo desafios significativos no compilador Swift.</p>
<p>Um sintoma notório desses desafios é o famoso erro do Swift: <em>“the compiler is unable to type-check this expression in reasonable time”</em> (o compilador não consegue verificar o tipo desta expressão em tempo hábil). Esse erro ocorre quando a expressão de código é tão complexa para o mecanismo de inferência que o compilador não consegue resolver dentro de limites práticos de tempo. Por exemplo, observe a expressão abaixo:</p>


  <pre><code class="language-swift">let a: Double = -(1 &#43; 2) &#43; -(3 &#43; 4) &#43; -(5)</code></pre>
 <p>Poderia acionar esse erro no Swift (dependendo da versão do compilador), apesar de ser conceitualmente trivial. O problema de fundo é que o Swift permite que literais numéricos como <code>1</code> sejam interpretados como vários tipos diferentes (Int, Double, Float, etc., conforme contexto) e possui operadores como <code>+</code> e <code>-</code> sobrecarregados para muitas combinações de operandos (inteiros, pontos flutuantes, opcionais, strings concatenáveis, etc.).</p>
<p>Assim, ao analisar a expressão acima, o compilador Swift constrói um espaço de possibilidades combinatórias enorme: precisa considerar cada literal podendo assumir distintos tipos numéricos e cada <code>+</code> podendo invocar sobrecargas diferentes, até encontrar uma combinação consistente com o tipo declarado (<code>Double</code> neste caso).</p>
<p>Com muitas possibilidades, o problema rapidamente explode em complexidade. De fato, um caso real relatado envolveu concatenar cadeias de strings e valores numéricos numa única expressão para formar uma URL, levando o compilador Swift 42 segundos para tentar resolver os tipos antes de finalmente falhar com a mensagem de erro mencionada.</p>
<p>Nesse caso específico, nenhuma combinação de sobrecargas resolvia a expressão, pois havia uma soma entre tipos incompatíveis (Int e String), levando o solver a explorar um espaço enorme até desistir.</p>
<blockquote>
<p>Nesse período, o compilador estava explorando <strong>17 sobrecargas do operador &ldquo;+&rdquo; e 9 interpretações possíveis de literais string</strong>, resultando em um número exponencial de combinações a testar. Em contraste, um compilador C++ compilaria um programa equivalente praticamente instantaneamente, pois não realiza esse nível de busca na resolução de tipos.</p></blockquote>
<p>A equipe do Swift está ciente dessas limitações. Documentações e discussões de desenvolvimento reconhecem que o algoritmo atual de inferência pode apresentar comportamento exponencial em certos cenários, especialmente envolvendo sobrecarga de operadores e conversões implícitas de literais.</p>
<p>Chris Lattner, o criador do Swift, refletiu que a decisão de projetar um <strong>type checker</strong> muito poderoso (um “fancy bi-directional Hindley-Milner type checker”) acabou resultando em tempos de compilação ruins em expressões complexas e mensagens de erro insatisfatórias, pois um erro em uma parte distante da expressão pode invalidar o conjunto inteiro de deduções.</p>
<p>Em suas palavras, “soa ótimo [na teoria], mas na prática não funciona tão bem” dado esse comportamento.</p>
<blockquote>
<p>Em resumo, o Swift tentou combinar o “melhor dos dois mundos” – inferência ampla como a do Rust/Haskell <strong>e</strong> recursos como sobrecarga e conversões convenientes do C++ – e com isso atingiu os limites do que o algoritmo de inferência consegue suportar eficientemente.</p></blockquote>
<p>Essa comparação destaca um ponto crucial: <strong>a inferência de tipos não atua isoladamente – ela está intimamente ligada às demais features da linguagem e às escolhas de projeto do compilador</strong>.</p>
<p>No Swift, para evitar tempos de compilação excessivos, às vezes é necessário guiar o compilador inserindo anotações de tipo intermediárias ou quebrando uma expressão complexa em subexpressões menores (ajudando-o a podar o espaço de busca). Alguns desenvolvedores Swift adotam como boa prática limitar o tamanho das expressões encadeadas exatamente por causa disso.</p>
<p>Já em Rust, graças à ausência de sobrecarga arbitrária e conversões implícitas, o compilador consegue inferir tipos de forma previsível e em tempo linear na maioria dos casos, raramente exigindo intervenções manuais por desempenho. O C++ resolve o dilema evitando o problema desde o início: a inferência é tão restrita que a complexidade permanece sob controle, ao custo de requerer do programador mais especificações de tipo em cenários avançados.</p>
<h2 id="impacto-prático-e-conclusão">Impacto Prático e Conclusão</h2>
<p>As diferenças entre as abordagens de C++ e Rust na inferência de tipos têm consequências diretas no cotidiano do programador e refletem distintos equilíbrios na filosofia de design de cada linguagem. Em termos práticos:</p>
<ul>
<li><strong>Rust</strong> oferece um código mais enxuto em termos de anotações de tipo. O desenvolvedor pode focar na lógica dos dados, deixando que o compilador preencha os detalhes dos tipos. Isso agiliza a escrita de código e pode melhorar a legibilidade, já que expressões complexas não ficam poluídas com nomes de tipos longos.</li>
</ul>
<p>Por outro lado, quando o compilador não consegue deduzir algo, as mensagens de erro podem inicialmente parecer abstratas ou distantes da causa, justamente porque um erro de tipo pode surgir de uma inconsistência entre partes diferentes do código. Com a experiência, porém, os desenvolvedores Rust aprendem a interpretar essas mensagens e a ajustar o código ou inserir anotações mínimas onde necessário para guiar a inferência.</p>
<ul>
<li><strong>C++</strong>, ao exigir mais anotações em casos não triviais, proporciona uma espécie de documentação explícita dos tipos no código. Muitos erros de incompatibilidade de tipo são evidenciados imediatamente na linha onde ocorrem, e o programador tem um controle mais fino sobre como os tipos são combinados.</li>
</ul>
<p>A desvantagem é a verbosidade e a potencial duplicação de informação – frequentemente é preciso repetir um nome de tipo complexo várias vezes, o que aumenta a chance de divergência se o tipo precisar mudar durante a evolução do código.</p>
<p>As melhorias introduzidas pelo <code>auto</code> desde C++11 visam justamente reduzir essa carga, mas o desenvolvedor C++ ainda deve pensar cuidadosamente sobre tipos de template, conversões e sobrecargas, já que o compilador não tentará “adivinhar” intenções que não estejam localmente especificadas.</p>
<p>Em última análise, a escolha do sistema de inferência de tipos é um <strong>compromisso de design</strong>. <strong>Nenhuma abordagem é estritamente superior em todos os aspectos; cada linguagem define suas prioridades distintas</strong>. O C++ privilegia desempenho de compilação previsível e manutenção de compatibilidade com um ecossistema complexo (legado de décadas), por isso a inferência é propositalmente limitada. O Rust, sendo uma linguagem moderna, pôde abdicar de certos recursos para privilegiar a ergonomia do desenvolvedor com inferência abrangente.</p>
<p>O <a href="https://www.rust-lang.org/">Rust</a> valoriza a ergonomia e a segurança do desenvolvedor, usando inferência global para minimizar boilerplate, mas em troca restringe certas funcionalidades da linguagem de modo a manter a inferência decidível e eficiente. Vale notar que ferramentas modernas de IDE/LSP amenizam o custo de esconder tipos no Rust – editores exibem tipos inferidos em tempo real, então o desenvolvedor ganha o melhor dos dois mundos: código enxuto, mas informação de tipo disponível quando necessário.</p>
<p>Já o <a href="https://en.wikipedia.org/wiki/Swift_%28programming_language%29">Swift</a> ilustra os riscos de tentar estender a inferência ao máximo sem restringir funcionalidades: acaba-se encontrando limites práticos que requerem soluções (ou mudanças de arquitetura do compilador) para contornar os <em>trade-offs</em> de desempenho.</p>
<p>Para o programador, compreender essas diferenças não é apenas uma curiosidade teórica, mas algo que informa a maneira de escrever código em cada linguagem.</p>
<p>Quando alternamos entre <a href="https://en.wikipedia.org/wiki/C%2B%2B">C++</a>, <a href="https://www.rust-lang.org/">Rust</a> e <a href="https://en.wikipedia.org/wiki/Swift_%28programming_language%29">Swift</a>, devemos ajustar nossas expectativas: aquilo que o Rust faz automaticamente pode precisar ser escrito à mão em C++, e aquilo que em C++ é imediato pode levar o Swift a gastar segundos tentando resolver. Em todos os casos, a inferência de tipos serve ao propósito de garantir a correção do programa enquanto reduz a necessidade de anotações explícitas repetitivas.</p>
<p>Porém, ela vem acompanhada de um conjunto de regras e restrições que espelham a filosofia da linguagem. Assim, ao escolher uma linguagem (ou ao projetar uma), é preciso reconhecer que <em>inferência de tipos não é apenas um detalhe de implementação, mas sim um componente central que molda a experiência de programar</em> – influenciando desde a sintaxe diária até as ferramentas de depuração e o design de APIs públicas.</p>
<p>As distintas abordagens de Rust e C++ exemplificam bem esse espectro, mostrando como princípios de ciência da computação são aplicados de forma pragmática para equilibrar a conveniência do desenvolvedor com a previsibilidade e desempenho do compilador.</p>
<hr>
<p><strong>Referências</strong>:</p>
<ul>
<li>MILNER, R. <em>A Theory of Type Polymorphism in Programming.</em> Journal of Computer and System Sciences, v.17, n.3, p.348–375, 1978.</li>
<li>MATSAKIS, Niko. <em>Baby Steps in Type Inference: Unification and Type Checking in Rust.</em> <em>Small Cult Following</em> blog, 2020. Disponível em: <a href="https://smallcultfollowing.com/babysteps/">https://smallcultfollowing.com/babysteps/</a>. Acesso em 20 jul. 2025.</li>
<li>Cppreference. <em>Placeholder type specifiers (since C++11).</em> Disponível em: <a href="https://en.cppreference.com/w/cpp/language/auto">https://en.cppreference.com/w/cpp/language/auto</a>. Acesso em 20 jul. 2025.</li>
<li>HOOPER, Daniel. <em>Why Swift’s Type Checker Is So Slow.</em> Blog do autor, 12 jun. 2024. Disponível em: <a href="https://danielchasehooper.com/posts/why-swift-is-slow/">https://danielchasehooper.com/posts/why-swift-is-slow/</a>. Acesso em 20 jul. 2025.</li>
<li>Documentação do Rust. <em>Chapter 3.1: Variables and Mutability</em> e <em>Chapter 4.3: Type Inference</em>. Disponível em: <a href="https://doc.rust-lang.org/book/">https://doc.rust-lang.org/book/</a>. Acesso em 20 jul. 2025.</li>
<li><a href="https://en.cppreference.com/w/cpp/language/auto.html#:~:text=The%20type%20of%20a%20variable,initializing%20declaration%20of%20a%20variable">Placeholder type specifiers (since C++11)</a></li>
<li><a href="https://danielchasehooper.com/posts/why-swift-is-slow/#:~:text=Swift%206%20spends%2042%20seconds,No%20matter%20how">Why Swift’s Type Checker Is So Slow</a></li>
</ul>
]]></content:encoded>
      
      
      <category>Inferência de Tipos,Programação,Rust,C&#43;&#43;,Swift</category>
      
      
      
      
      
      
      
      <description>&lt;![CDATA[Por que isso pode facilitar sua vida.]]></description>
      
    </item>
    
    <item>
      <title>Try/Catch: Origem, Propósito e o Erro de Usá-lo como Fluxo Lógico</title>
      <link>https://scovl.github.io/2025/05/23/trycatch/</link>
      <guid>https://scovl.github.io/2025/05/23/trycatch/</guid>
      <pubDate>Fri, 23 May 2025 19:41:45 -0300</pubDate>
      <description>&lt;![CDATA[<p>O tratamento de exceções surgiu para separar o fluxo normal do programa do tratamento de situações inesperadas, como falhas de hardware ou erros de entrada/saída. Inicialmente, programas usavam códigos de retorno para lidar com erros, mas isso era propenso a falhas e difícil de manter.</p>
<p>O modelo <code>try/catch</code> foi evoluindo desde os anos 60, ganhando formas mais estruturadas em linguagens como <a href="https://en.wikipedia.org/wiki/PL/I">PL/I</a>, <a href="https://en.wikipedia.org/wiki/Ada_%28programming_language%29">Ada</a>, <a href="https://en.wikipedia.org/wiki/C%2B%2B">C++</a> e <a href="https://en.wikipedia.org/wiki/Java_%28programming_language%29">Java</a>, e depois sendo adotado por outras como <a href="https://en.wikipedia.org/wiki/JavaScript">JavaScript</a>.</p>
<p>O objetivo sempre foi permitir que programas lidassem de forma controlada com erros imprevisíveis, sem travar o sistema. As exceções não foram criadas para controlar o fluxo normal do programa, mas sim para tratar casos realmente excepcionais. Neste artigo, vamos ver por que usar <code>try/catch</code> como controle de fluxo é um erro e qual é o seu propósito real.</p>]]></description>
      <content:encoded>&lt;![CDATA[<p>O tratamento de exceções surgiu para separar o fluxo normal do programa do tratamento de situações inesperadas, como falhas de hardware ou erros de entrada/saída. Inicialmente, programas usavam códigos de retorno para lidar com erros, mas isso era propenso a falhas e difícil de manter.</p>
<p>O modelo <code>try/catch</code> foi evoluindo desde os anos 60, ganhando formas mais estruturadas em linguagens como <a href="https://en.wikipedia.org/wiki/PL/I">PL/I</a>, <a href="https://en.wikipedia.org/wiki/Ada_%28programming_language%29">Ada</a>, <a href="https://en.wikipedia.org/wiki/C%2B%2B">C++</a> e <a href="https://en.wikipedia.org/wiki/Java_%28programming_language%29">Java</a>, e depois sendo adotado por outras como <a href="https://en.wikipedia.org/wiki/JavaScript">JavaScript</a>.</p>
<p>O objetivo sempre foi permitir que programas lidassem de forma controlada com erros imprevisíveis, sem travar o sistema. As exceções não foram criadas para controlar o fluxo normal do programa, mas sim para tratar casos realmente excepcionais. Neste artigo, vamos ver por que usar <code>try/catch</code> como controle de fluxo é um erro e qual é o seu propósito real.</p>
<h2 id="propósito-do-trycatch">Propósito do Try/Catch</h2>
<p>A linguagem <strong><a href="https://en.wikipedia.org/wiki/SIMULA">SIMULA 67</a></strong> – precursora da programação orientada a objetos, introduzindo conceitos como classes e herança – não possuía um sistema de tratamento de exceções idêntico ao <code>try/catch</code> moderno, mas contava com um mecanismo de tratamento de condições excepcionais (<em>ON-actions</em>) para casos como falhas de operações de <strong><a href="https://en.wikipedia.org/wiki/Input/output">I/O</a></strong> (entrada/saída).</p>
<p>Esse mecanismo era conceitualmente similar aos <strong><a href="https://en.wikipedia.org/wiki/Error_handling#Error_handlers">handlers</a></strong> de erro introduzidos no <a href="https://en.wikipedia.org/wiki/PL/I">PL/I</a> em 1964 e já demonstrava a vantagem de estruturar o código para lidar separadamente com situações de erro. Em essência, no <a href="https://en.wikipedia.org/wiki/SIMULA">SIMULA</a> havia:</p>
<ul>
<li><strong>Bloco Protegido</strong> – equivalente funcional ao bloco <code>try</code>, delimitando o código onde erros poderiam ocorrer.</li>
<li><strong>Rotina de Tratamento (Handler)</strong> – definida via construções <code>ON ... DO</code>, análoga ao <code>catch</code> atual, executada caso uma condição excepcional fosse detectada.</li>
</ul>
<p>Um exemplo simplificado em pseudo-sintaxe inspirada no <a href="https://en.wikipedia.org/wiki/SIMULA">SIMULA 67</a> ilustrava essa estrutura:</p>


  <pre><code class="language-simula">BEGIN  
   ON ERROR DO BEGIN  
      ! Código de recuperação (handler).  
   END;
   ! Bloco protegido (código propenso a erro).
   ...  
END;</code></pre>
 <p>Desde cedo, a ideia de separar o código principal do tratamento de erros foi vista como um grande avanço, pois deixava os programas mais organizados e fáceis de entender. O <a href="https://en.wikipedia.org/wiki/SIMULA">SIMULA</a> trouxe esse conceito lá atrás, mas quem realmente mudou o jogo foi o <strong><a href="https://en.wikipedia.org/wiki/Lisp_%28programming_language%29">Lisp</a></strong>.</p>
<p>Embora não tivesse a estrutura <code>try/catch</code> como conhecemos, versões do <a href="https://en.wikipedia.org/wiki/Lisp_%28programming_language%29">Lisp</a> nos anos 1970 introduziram as funções <code>catch</code> e <code>throw</code> para lidar com situações inesperadas, evitando a necessidade de verificar códigos de erro manualmente o tempo todo. Isso foi revolucionário porque permitia que o programa “sinalizasse” que algo deu errado e saltasse diretamente para um ponto de tratamento pré-definido, sem <em>enrolação</em>.</p>
<p>No <a href="https://en.wikipedia.org/wiki/Lisp_%28programming_language%29">Lisp</a>, esse mecanismo de exceção era totalmente integrado à linguagem, usando as mesmas construções do restante do código – o que demonstra sua flexibilidade e quão à frente do tempo ele estava.</p>
<p>Basicamente, marcava-se um ponto de captura com uma <em>tag</em> simbólica usando <code>catch</code> e, se algo saísse errado, utilizava-se <code>throw</code> para desviar imediatamente a execução para lá, ignorando qualquer código intermediário. Essa ideia influenciou praticamente todas as linguagens modernas, embora hoje utilizemos tipos de exceção em vez de tags simbólicas como no <a href="https://en.wikipedia.org/wiki/Lisp_%28programming_language%29">Lisp</a>.</p>
<p>Apesar da sintaxe e implementação do <a href="https://en.wikipedia.org/wiki/Lisp_%28programming_language%29">Lisp</a> serem bem diferentes das linguagens atuais, sua abordagem com <code>catch/throw</code> demonstrou o paradigma fundamental de desviar o fluxo de execução sem verificações explícitas de erro – conceito que inspirou os mecanismos modernos de exceções (mesmo com implementações técnicas distintas).</p>
<p>Consolidando essa evolução histórica, linguagens como <a href="https://en.wikipedia.org/wiki/C%2B%2B">C++</a> formalizaram e refinaram esses conceitos pioneiros por meio das estruturas <strong><code>try</code></strong> e <strong><code>catch</code></strong>, introduzindo um sistema de exceções baseado em tipos. Em <a href="https://en.wikipedia.org/wiki/C%2B%2B">C++</a>, por exemplo, podemos proteger um bloco de código e tratar erros assim:</p>


  <pre><code class="language-cpp">try {
    // Código que pode lançar exceções
    throw std::runtime_error(&#34;Erro!&#34;);
} catch (const std::exception&amp; e) {
    // Tratamento da exceção
    std::cerr &lt;&lt; &#34;Exceção capturada: &#34; &lt;&lt; e.what();
}</code></pre>
 <p>O objetivo principal desse mecanismo é ajudar os programadores a lidar com problemas que ocorram durante a execução de forma organizada, separando claramente a lógica normal do tratamento de erros. Ele leva adiante – e aprimora – os princípios introduzidos por linguagens como <a href="https://en.wikipedia.org/wiki/SIMULA">SIMULA</a> e <a href="https://en.wikipedia.org/wiki/Lisp_%28programming_language%29">Lisp</a>, porém com uma implementação mais robusta e integrada à tipagem da linguagem.</p>
<blockquote>
<p>Nesse ponto, vale destacar um princípio essencial do livro <strong><a href="https://en.wikipedia.org/wiki/The_Pragmatic_Programmer">The Pragmatic Programmer</a></strong>, que recomenda: “Crash early” — ou seja, falhe cedo e com clareza quando algo realmente inesperado ocorre. Segundo os autores, &ldquo;dead programs tell no lies&rdquo; — um programa que trava rapidamente pode ser mais confiável que um inválido operando silenciosamente com dados corrompidos. Isso reforça o propósito original das exceções: detectar falhas graves imediatamente, evitando consequências imprevisíveis.</p></blockquote>
<p>Antes de explorarmos os detalhes técnicos e as melhores práticas do uso de <code>try/catch</code>, é importante entender o propósito fundamental desse mecanismo no contexto da programação moderna. O tratamento estruturado de exceções surgiu para resolver problemas clássicos de legibilidade, robustez e manutenção do código, especialmente em situações onde o fluxo normal de execução pode ser interrompido por eventos inesperados.</p>
<p>A seguir, vamos analisar como o <code>try/catch</code> evoluiu historicamente, quais problemas ele resolve em relação a abordagens mais antigas (como códigos de erro) e por que sua adoção tornou-se um marco na organização e clareza dos programas:</p>
<ol>
<li><strong>Problemas com Códigos de Erro</strong>:
Com códigos de retorno, o chamador pode simplesmente esquecer de verificar se ocorreu um erro. Quando isso acontece, o programa continua executando como se tudo estivesse normal, mesmo que tenha ocorrido um problema sério.</li>
</ol>
<p>O exemplo abaixo ilustra como isso pode levar a situações indesejadas – a função <code>read_int()</code> retorna um código indicando erro ou sucesso, mas se quem a chamou não conferir esse código, um valor inválido poderá ser usado em cálculo a seguir:</p>


  
  <div class="mermaid">graph TD
    A[&#34;read_int()&#34;] --&gt;|&#34;Pode retornar erro&#34;| B[&#34;int x = resultado&#34;]
    B --&gt; C[&#34;int y = x * 2&#34;]
    C --&gt;|&#34;Problema: x pode ser inválido&#34;| D[&#34;Operação com valor inválido&#34;]</div>
 <p>No diagrama, vê-se um fluxo onde <code>read_int()</code> pode indicar uma falha, mas esse retorno não é verificado ao atribuir o resultado à variável <code>x</code>. Em consequência, o programa segue seu curso normal, calculando <code>y = x * 2</code> mesmo que <code>x</code> possa conter um valor inválido. Isso resulta em uma operação com dado incorreto no final do fluxo, demonstrando como a falta de verificação de erros pode propagar problemas silenciosamente pelo programa.</p>
<ol start="2">
<li><strong>Separação de Preocupações</strong>:
Com exceções, a detecção de um erro (na função chamada) fica separada do tratamento do erro (na função chamadora). Isso permite um código mais limpo, em que a lógica principal não fica poluída por verificações de erro a cada passo. O tratamento pode ser centralizado em um único lugar, geralmente no nível mais alto da aplicação, enquanto o fluxo normal de execução permanece claro.</li>
</ol>
<p>O diagrama abaixo ilustra essa separação: o caminho principal (em azul) representa a execução bem-sucedida – inicia, processa dados, salva resultados, envia notificação e finaliza com sucesso. Porém, se em qualquer dessas etapas ocorrer uma exceção, o fluxo é desviado para o bloco de tratamento de erros (em vermelho), onde o erro é registrado e o programa termina de forma controlada.</p>


  
  <div class="mermaid">graph TD
    A[Início] --&gt; B[processarDados]
    B --&gt; C[salvarResultados]
    C --&gt; D[enviarNotificacao]
    D --&gt; E[Fim com Sucesso]
    
    B --&gt;|Exceção| F[catch ErroProcessamento]
    C --&gt;|Exceção| F
    D --&gt;|Exceção| F
    
    F --&gt; G[registrarErro]
    G --&gt; H[Fim com Tratamento]
    
    style B fill:#d0e0ff,stroke:#3366cc
    style C fill:#d0e0ff,stroke:#3366cc
    style D fill:#d0e0ff,stroke:#3366cc
    style F fill:#ffe0e0,stroke:#cc6666
    style G fill:#ffe0e0,stroke:#cc6666</div>
 <p>Esse diagrama destaca como o código principal pode se concentrar na lógica de negócio, enquanto o tratamento de erro fica isolado no bloco <code>catch</code>. Essa é a essência do <code>try/catch</code>: permitir que o fluxo “normal” do programa permaneça legível e que todo o código referente a erros esteja agrupado e bem definido em outro lugar. O resultado é um código mais organizado e de fácil manutenção.</p>
<ol start="3">
<li><strong>Erros Não Podem Ser Ignorados</strong>:
Se uma exceção não for capturada em lugar nenhum, o programa <strong>termina</strong> de forma controlada. Diferentemente de um código de erro que pode ser ignorado sem querer, uma exceção não tratada provoca a finalização do programa, garantindo que erros críticos não passem despercebidos.</li>
</ol>
<p>O diagrama a seguir mostra dois fluxos possíveis de um programa simples: no caminho normal, a função é executada e imprime uma mensagem (&ldquo;Esta linha&hellip;&rdquo;) antes de retornar ao <code>main</code> e encerrar normalmente; já no caminho de erro, a função lança uma exceção (<code>std::runtime_error</code>), que não é capturada em nenhuma parte do programa, resultando no encerramento imediato da aplicação. Abaixo, temos o diagrama:</p>


  
  <div class="mermaid">graph TD
    A[main] --&gt; B[funcao_que_pode_falhar]
    B --&gt;|Execução normal| C[&#34;std::cout &lt;&lt; &#39;Esta linha...&#39;&#34;]
    C --&gt; D[Retorno à main]
    D --&gt; E[return 0]
    
    B --&gt;|throw std::runtime_error| F[Exceção não capturada]
    F --&gt; G[Programa termina]
    
    style B fill:#f9f,stroke:#333
    style F fill:#f99,stroke:#900
    style G fill:#f99,stroke:#900</div>
 <p>Podemos observar, em rosa, o ponto onde “dá ruim” (onde a exceção é lançada) e, em vermelho, o caminho do erro levando ao término do programa. Esse comportamento é intencional: como o próprio <a href="https://en.wikipedia.org/wiki/Bjarne_Stroustrup">Stroustrup</a> explica, <strong>“se uma função encontrar um erro que não consiga resolver, ela lança uma exceção; alguma função acima na hierarquia de chamadas pode capturá-la, mas, se ninguém o fizer, o programa termina”</strong>.</p>
<p>Embora terminar a aplicação possa parecer drástico, isso na verdade evita consequências piores, como continuar a execução com dados corrompidos. Diferente dos códigos de erro (em que o programador <strong>precisa</strong> lembrar de verificar cada retorno), as exceções forçam uma decisão: ou você trata o problema em algum lugar, ou o programa será finalizado. Assim, falhas graves não “passam batido”.</p>
<p>Além disso, a separação clara entre lógica principal e lógica de erro torna possível garantir a liberação de recursos mesmo quando algo dá errado, graças ao comportamento do próprio mecanismo de exceções em linguagens como <a href="https://en.wikipedia.org/wiki/C%2B%2B">C++</a>.</p>
<hr>
<h2 id="principais-usos-do"><strong>Principais Usos do <code>try/catch</code> — Exemplos Práticos em Diferentes Contextos</strong></h2>
<p>O bloco <code>try/catch</code> é fundamental para lidar com eventos <strong>realmente excepcionais</strong> — aqueles que interrompem o fluxo normal e não podem ser resolvidos apenas com valores de retorno ou verificações simples. Exemplos clássicos: falta de memória, falhas de <strong><a href="https://en.wikipedia.org/wiki/Input/output">I/O</a></strong>, corrupção de dados, ou erros lógicos imprevistos.</p>
<p>A seguir, os principais cenários onde o uso do <code>try/catch</code> é apropriado, já com exemplos comentados em cada contexto:</p>
<h3 id="falhas-de-io-arquivos-rede-dispositivos">Falhas de I/O (Arquivos, Rede, Dispositivos)</h3>
<p>Situações em que o programa depende de recursos externos — um arquivo, uma conexão de rede, um socket — e o resultado pode variar a qualquer momento, independentemente da lógica do seu código.</p>


  <pre><code class="language-cpp">try {
    std::ifstream arq(&#34;dados.txt&#34;);
    if (!arq) throw std::runtime_error(&#34;Arquivo não abriu&#34;);
    std::string linha;
    while (std::getline(arq, linha))
        processar(linha);
} catch (const std::exception&amp; e) {
    logErro(&#34;Falha de I/O: &#34; &#43; std::string{e.what()});
}</code></pre>
 <ul>
<li><strong>Fluxo normal:</strong> abrir, ler, processar.</li>
<li><strong>Fluxo de erro:</strong> qualquer falha salta direto para o <code>catch</code>.</li>
</ul>
<p>Esse exemplo ilustra como o <code>try/catch</code> separa claramente o fluxo principal do tratamento de erros em operações de entrada e saída (I/O). O bloco <code>try</code> contém o código que abre e lê um arquivo linha a linha, processando cada uma delas. Caso ocorra qualquer problema — como o arquivo não existir, não abrir corretamente ou surgir uma falha durante a leitura — uma exceção é lançada e imediatamente desviada para o bloco <code>catch</code>.</p>
<p>Lá, o erro é tratado de forma centralizada, registrando a mensagem detalhada do problema. Assim, o código principal permanece limpo e focado na lógica de negócio, enquanto o tratamento de falhas fica isolado, tornando o programa mais robusto e fácil de manter.</p>
<blockquote>
<p>Conforme discutido por <a href="https://en.wikipedia.org/wiki/Scott_Meyers">Scott Meyers</a> no seu livro <strong><a href="https://en.wikipedia.org/wiki/Effective_C%2B%2B">Effective C++</a></strong>, o uso de RAII e arquiteturas seguras de exceção (exception-safe) garante que recursos sejam sempre liberados corretamente mesmo em falha, movendo o código para o nível de basic ou strong exception safety.</p></blockquote>
<hr>
<h3 id="papel-do-raii-limpeza-automática">Papel do RAII: limpeza automática</h3>
<p>Em C++ não há <code>finally</code>, porque o RAII resolve a liberação de recursos durante o “desenrolar” da pilha:</p>


  <pre><code class="language-cpp">#include &lt;fstream&gt;
#include &lt;memory&gt;

void processarArquivo(const std::string&amp; caminho) {
    std::ifstream f(caminho);                     // fecha sozinho no destrutor
    if (!f) throw std::runtime_error(&#34;Não abriu&#34;);

    auto buf = std::make_unique&lt;char[]&gt;(1024);    // libera sozinho

    f.read(buf.get(), 1024);                      // pode lançar
    // ...processa dados...
}   // Se qualquer exceção “subir”, f e buf são destruídos aqui</code></pre>
 <p>Quando uma exceção é lançada, a execução normal do programa é imediatamente interrompida. Nesse momento, todos os objetos locais têm seus destrutores chamados automaticamente, o que garante a liberação dos recursos alocados, como arquivos abertos ou blocos de memória. O controle do fluxo, então, é transferido para o bloco <code>catch</code> mais próximo que seja capaz de tratar aquela exceção.</p>
<p>Esse mecanismo faz com que, mesmo em situações em que “tudo dá errado”, o programa consiga fechar arquivos, devolver memória e encerrar de maneira previsível. Caso seja apropriado, o programa pode até continuar sua execução após o tratamento, dependendo da gravidade do erro e da lógica implementada.</p>
<p>Por outro lado, é importante não usar <code>try/catch</code> para controlar o fluxo nominal do programa. Exceções não devem ser empregadas para lidar com situações esperadas, como o fim de um arquivo durante uma leitura sequencial. Da mesma forma, se um resultado pode ser tratado por meio de valores de retorno, essa abordagem deve ser preferida. Reservar exceções para falhas realmente irrecuperáveis mantém o código mais claro e eficiente.</p>
<blockquote>
<p>Em resumo, o <code>try/catch</code> serve para isolar o código de negócio do tratamento de falhas, garantir a liberação automática de recursos (graças ao RAII) e evitar que erros críticos passem despercebidos. Essa separação contribui para a clareza, robustez e manutenibilidade do software.</p></blockquote>
<h2 id="tratamento-de-exceções-em-diferentes-contextos">Tratamento de Exceções em Diferentes Contextos</h2>
<p>Agora que vimos <strong>por que</strong> e <strong>quando</strong> usar <code>try/catch</code>, vamos explorar o mecanismo em ação em cenários do dia a dia. Os exemplos abaixo seguem o mesmo princípio apresentado na seção anterior:</p>
<blockquote>
<p><strong>separe a lógica &ldquo;feliz&rdquo; do que acontece quando algo dá errado</strong>.</p></blockquote>
<h3 id="gerenciamento-de-recursos">Gerenciamento de recursos</h3>


  <pre><code class="language-cpp">void processar() {
    auto dados = std::make_unique&lt;Buffer&gt;(1024);   // libera sozinho

    try {
        dados-&gt;carregar();
        dados-&gt;processar();
        dados-&gt;salvar();
    } catch (...) {
        logErro(&#34;Falha no processamento, propagando...&#34;);
        throw;                                     // sobe para quem souber tratar
    }                                             // `dados` é liberado aqui
}</code></pre>
 <ul>
<li>O <code>unique_ptr</code> garante liberação, dispensando <code>finally</code>.</li>
<li>O bloco <code>catch</code> adiciona contexto e re‑lança.</li>
</ul>
<p>O exemplo acima ilustra como o uso combinado de <code>try/catch</code> e RAII (através do <code>unique_ptr</code>) simplifica o gerenciamento de recursos em C++. Ao encapsular a lógica principal dentro de um bloco <code>try</code>, garantimos que qualquer exceção lançada durante o carregamento, processamento ou salvamento dos dados seja capturada no <code>catch</code>, onde podemos registrar o erro e, se necessário, propagar a exceção para níveis superiores.</p>
<p>O uso do <code>unique_ptr</code> assegura que a memória alocada para o buffer será automaticamente liberada ao final do escopo, mesmo que uma exceção ocorra — eliminando a necessidade de blocos <code>finally</code> ou liberações manuais.</p>
<blockquote>
<p>Assim, o código permanece limpo, seguro e robusto, pois separa claramente o fluxo normal do tratamento de falhas, um dos principais propósitos do mecanismo de exceções discutido neste artigo.</p></blockquote>
<h3 id="validação-de-dados">Validação de dados</h3>


  <pre><code class="language-cpp">class Usuario {
public:
    void setIdade(int idade) {
        if (idade &lt; 0 || idade &gt; 120)
            throw std::invalid_argument(&#34;Idade fora do intervalo permitido&#34;);
        idade_ = idade;
    }
private:
    int idade_{};
};

try {
    Usuario u;
    u.setIdade(valorLido);
} catch (const std::invalid_argument&amp; e) {
    logErro(&#34;Entrada inválida: &#34; &#43; std::string{e.what()});
}</code></pre>
 <ul>
<li>A regra de negócio fica <strong>dentro</strong> da classe.</li>
<li>Quem usa a API só precisa lidar com a exceção, sem checar retornos.</li>
</ul>
<p>O exemplo acima demonstra como encapsular regras de validação diretamente na classe, lançando exceções quando os dados não atendem aos critérios esperados (por exemplo, uma idade fora do intervalo permitido). Isso centraliza a lógica de negócio e simplifica o uso da API, pois quem consome a classe só precisa tratar possíveis exceções, sem se preocupar em checar retornos de erro manualmente.</p>
<blockquote>
<p>Esse padrão torna o código mais limpo, seguro e fácil de manter, além de separar claramente o fluxo normal do tratamento de falhas. Essa abordagem de propagação de exceções é especialmente útil em cenários mais complexos, como operações transacionais, que veremos a seguir.</p></blockquote>
<h3 id="transações-atômicas">Transações atômicas</h3>


  <pre><code class="language-cpp">void transferir(Conta&amp; a, Conta&amp; b, double v) {
    if (v &lt;= 0) throw std::invalid_argument(&#34;valor &lt;= 0&#34;);

    std::lock_guard lk1(a.mtx());
    std::lock_guard lk2(b.mtx());

    try {
        a.debitar(v);
        b.creditar(v);
    } catch (...) {          // qualquer erro ⇒ rollback
        a.creditar(v);
        throw;
    }
}</code></pre>
 <ul>
<li><strong>All‑or‑nothing</strong>: ou ambas as contas mudam, ou nada persiste.</li>
<li><code>lock_guard</code> solta os mutexes mesmo em caso de exceção.</li>
</ul>
<blockquote>
<p><strong>⚠️ Nota sobre deadlock</strong>: Este exemplo assume ordem consistente de aquisição de locks entre threads. Em produção, use <code>std::scoped_lock&lt;std::mutex, std::mutex&gt; lk(a.mtx(), b.mtx())</code> (C++17+) que adquire ambos os mutexes simultaneamente sem risco de deadlock, ou garanta uma ordenação determinística (ex: sempre adquirir primeiro o mutex da conta com menor endereço de memória).</p></blockquote>
<p>O exemplo acima ilustra como implementar uma operação transacional utilizando exceções para garantir a atomicidade: se qualquer etapa da transferência falhar (por exemplo, por saldo insuficiente ou erro inesperado), o código faz o rollback debitando e depois creditando novamente o valor na conta de origem, antes de propagar a exceção.</p>
<p>O uso de <code>lock_guard</code> assegura que os mutexes das contas sejam liberados automaticamente, mesmo em caso de erro, evitando deadlocks e vazamentos de recurso.</p>
<blockquote>
<p>Esse padrão é fundamental em sistemas financeiros e outros domínios críticos, pois assegura que as alterações de estado sejam consistentes e não deixem o sistema em situação intermediária caso ocorra uma falha.</p></blockquote>
<p>A seguir, veremos como enriquecer o contexto das exceções ao longo das camadas da aplicação, facilitando o diagnóstico e a rastreabilidade dos erros.</p>
<h3 id="enriquecendo-contexto-em-camadas">Enriquecendo contexto em camadas</h3>


  <pre><code class="language-cpp">void baixa()  { /* ... */ throw std::runtime_error(&#34;DB offline&#34;); }
void media()  { try { baixa(); }
                catch (const std::exception&amp; e) {
                    throw std::runtime_error(&#34;Camada média: &#34; &#43; std::string{e.what()});
                }}
void alta()   { try { media(); }
                catch (const std::exception&amp; e) {
                    throw std::runtime_error(&#34;Camada alta: &#34;  &#43; std::string{e.what()});
                }}</code></pre>
 <p>O objetivo desse padrão é fornecer uma trilha clara e detalhada do caminho percorrido pelo erro, desde sua origem até o ponto mais alto da pilha de chamadas. Ao enriquecer a mensagem de exceção em cada camada, o desenvolvedor consegue identificar rapidamente onde o problema começou e por quais etapas ele passou, facilitando o diagnóstico e a correção. Esse encadeamento de mensagens resulta em um relatório final como:</p>


  <pre><code class="language-">&#34;Camada alta: Camada média: DB offline&#34;</code></pre>
 <p>Exceções devem ser usadas exclusivamente para situações realmente excepcionais, ou seja, aquelas que impedem o fluxo normal do programa de continuar. Não utilize exceções para controlar o fluxo rotineiro da aplicação, pois isso pode tornar o código confuso, difícil de manter e impactar negativamente a performance.</p>
<blockquote>
<p>Algumas boas práticas são fundamentais para um uso correto do <code>try/catch</code>: sempre capture tipos específicos de exceção primeiro, evitando tratar tudo como erro genérico; nunca ignore exceções silenciosamente — registre o erro ou converta-o em um erro de domínio; em C++, garanta o uso de RAII (Resource Acquisition Is Initialization) para liberar recursos automaticamente, dispensando a necessidade de blocos <code>finally</code> e prevenindo vazamentos (em outras linguagens, utilize os mecanismos equivalentes, como <code>with</code> em Python ou <code>using</code> em C#).</p></blockquote>
<p>Documente claramente quais exceções sua função pode lançar, facilitando o uso e os testes; e lembre-se de que lançar exceções tem custo, então não utilize esse mecanismo para situações comuns do fluxo de controle.</p>
<hr>
<h3 id="má-prática-de-design--quando-o-trycatch-vira-gambiarra">Má prática de design — quando o <code>try/catch</code> vira gambiarra</h3>
<p>No tópico anterior, vimos quando é apropriado lançar exceções; agora, é importante abordar o outro lado: o que acontece quando utilizamos <code>try/catch</code> para tratar situações que não são realmente excepcionais. Usar exceções como substituto de verificações normais, como um simples <code>if</code>, é considerado um anti-padrão e pode trazer consequências negativas para a clareza, desempenho e manutenção do código. Observe o exemplo abaixo:</p>


  <pre><code class="language-ts">// ❌  Exceção controlando fluxo normal
function getItemPrice(item: { name: string; price?: number }): number {
  try {
    if (item.price === undefined)            // caso esperado
      throw new Error(&#34;Preço indefinido&#34;);   // força exceção
    return item.price;
  } catch {
    return 0;                                // valor padrão
  }
}</code></pre>
 <p>Usar exceções para tratar situações rotineiras, como uma simples validação de campo, é prejudicial por vários motivos. Primeiro, isso surpreende quem lê o código, pois dá a impressão de que ocorreu uma falha grave, quando na verdade é apenas um caso esperado e trivial — quebrando o <a href="https://en.wikipedia.org/wiki/Principle_of_least_astonishment">Princípio do Menor Espanto (POLA)</a>.</p>
<p>Além disso, lançar e capturar exceções é uma operação significativamente mais custosa do que um simples <code>if</code>. Embora o modelo C++ use <strong>&ldquo;zero-cost exceptions&rdquo;</strong> — que na verdade significa zero custo apenas no <em>caminho normal</em> — o custo de lançar uma exceção é extremamente alto. Como explica <a href="https://devblogs.microsoft.com/oldnewthing/20220228-00/?p=106296">Raymond Chen</a>, da Microsoft, o termo é equivocado: <em>&ldquo;Metadata-based exception handling should really be called super-expensive exceptions&rdquo;</em>.</p>
<p>O processo envolve: busca por metadados no PC (program counter), decodificação de dados DWARF compactados, chamadas ao <em>personality routine</em>, e o custoso <strong>stack unwinding</strong>. <a href="https://isocpp.org/blog/2019/09/cppcon-2019-de-fragmenting-cpp-making-exceptions-and-rtti-more-affordable-a">Herb Sutter</a> demonstra que exceções violam o <em>zero-overhead principle</em> do C++, sendo uma das únicas duas funcionalidades da linguagem (junto com RTTI) que têm opções para serem desabilitadas pelos compiladores.</p>
<p>Adicionalmente, mesmo quando não lançadas, exceções limitam otimizações do compilador: antes de qualquer operação que pode gerar exceção, o compilador deve descarregar registradores para memória e evitar reordenações que quebrariam a semântica de unwinding. As <a href="https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#e3-use-exceptions-for-error-handling-only">C++ Core Guidelines</a> enfatizam que &ldquo;exceptions are for error handling only&rdquo; e que usar exceções para controle de fluxo normal &ldquo;makes code hard to follow and maintain.&rdquo;</p>
<blockquote>
<p>Essa ideia está diretamente alinhada ao conselho do livro <strong><a href="https://en.wikipedia.org/wiki/The_Pragmatic_Programmer">The Pragmatic Programmer</a></strong>: trate apenas o que realmente é excepcional como exceção — caso contrário, você adiciona complexidade desnecessária e viola princípios como <a href="https://en.wikipedia.org/wiki/Principle_of_least_astonishment">Principle of Least Astonishment</a>.</p></blockquote>
<blockquote>
<p>Além disso, conforme <a href="https://en.wikipedia.org/wiki/Matt_Klein">Matt Klein</a> discute, verificações desnecessárias geram dívida de manutenção, e o único “erro checking” que importa são aqueles que realmente podem ocorrer no fluxo normal.</p></blockquote>
<p>Outro problema é que esse uso inadequado de exceções <a href="https://en.wikipedia.org/wiki/Stack_trace">polui o stack-trace</a>, tornando mais difícil depurar e analisar o comportamento do sistema. O excesso de exceções desnecessárias pode mascarar erros reais, dificultar o profiling e tornar o código mais difícil de manter.</p>
<p>Por isso, para validações simples e previsíveis, prefira sempre estruturas de controle explícitas, reservando as exceções apenas para situações realmente inesperadas ou graves. Vamos ver um exemplo idiomático em TypeScript:</p>


  <pre><code class="language-ts">// ✅  Fluxo explícito, sem exceção
function getItemPrice(item: { name: string; price?: number }): number {
  return item.price ?? 0;     // se undefined, usa 0
}</code></pre>
 <p>O resultado de evitar exceções para casos esperados é um código mais claro, eficiente e sem armadilhas ocultas. Em vez de usar <code>try/catch</code> para controlar fluxos normais, prefira estruturas explícitas como <code>if</code>, valores opcionais (<code>std::optional</code>, <code>std::expected</code>, <code>nullish ??</code>) ou retornos convencionais.</p>
<blockquote>
<p>Assim, situações como campo obrigatório não preenchido, busca sem resultado ou divisão por zero prevista são tratadas de forma transparente e previsível, sem sobrecarregar o sistema com o custo e a complexidade das exceções.</p></blockquote>
<p>Já para eventos realmente excepcionais — como disco cheio, queda de conexão, corrupção de dados ou necessidade de desfazer uma operação crítica — o uso de exceções (<code>throw</code>) é apropriado.</p>
<p>Nesses casos, não há como prever ou contornar o problema apenas com verificações simples, e a exceção serve para interromper o fluxo e sinalizar que algo grave aconteceu, permitindo que o erro seja tratado em um nível superior ou que o programa seja encerrado de forma segura.</p>
<p>Diversas linguagens modernas reforçam essa separação: Rust exige retornos explícitos para erros esperados e reserva exceções (<code>panic!</code>) para bugs; Clojure trata erros como valores e só lança exceção em último caso; TypeScript incentiva o uso de tipos como <code>unknown</code> e alternativas funcionais como <code>Either</code>.</p>
<p>Todas seguem o mesmo princípio: erros previsíveis devem ser tratados como dados, enquanto exceções ficam para situações realmente imprevisíveis. Assim, lançar exceção só quando necessário aproxima o erro da sua origem, evita estados inconsistentes e facilita o diagnóstico, enquanto o uso excessivo só dificulta a manutenção e a clareza do código.</p>
<h4 id="alternativa-correta-retorno-explícito-de-erro">Alternativa correta: retorno explícito de erro</h4>
<p>Para casos previsíveis como divisão por zero, a abordagem ideal é usar tipos como <code>std::optional</code> que tornam a possibilidade de falha explícita:</p>


  <pre><code class="language-cpp">std::optional&lt;int&gt; dividir_seguro(int a, int b) {
    if (b == 0) return std::nullopt;   // falha previsível
    return a / b;
}

// Uso claro e sem exceções
auto media = dividir_seguro(a, b);     // retorno explícito
if (!media) {                          // falha prevista
    log(&#34;b = 0, usando valor padrão&#34;);
} else {
    usar(*media);
}</code></pre>
 <p>No exemplo apresentado acima, vemos que tratar situações esperadas com exceções — como retornar 0 quando o preço está indefinido — prejudica a clareza e a eficiência do código. Isso ocorre porque exceções interrompem o fluxo normal e impactam significativamente otimizações do compilador: impedem marcação de funções com <code>noexcept</code> (essencial para move semântics eficientes na STL), forçam spilling de registradores para memória, e limitam reordenação de instruções que poderiam melhorar o pipeline do processador.</p>
<p>O ideal é reservar exceções para eventos realmente inesperados, como um air-bag que só deve ser acionado em caso de acidente, enquanto validações de domínio e casos previstos devem ser tratados com retornos explícitos, usando estruturas como <code>if</code>, valores opcionais ou operadores como <code>??</code>.</p>
<blockquote>
<p>Linguagens modernas reforçam essa separação ao tratar erros esperados como dados e reservar exceções para situações imprevisíveis. Seguindo essas práticas, seu código permanece limpo, eficiente e fácil de manter, pois cada ferramenta é usada para o propósito correto, evitando surpresas e facilitando o diagnóstico de problemas reais.</p></blockquote>
<hr>
<h3 id="designbycontract-e-asserções">Design by Contract e asserções</h3>
<p>Beleza, mas como decidir, de forma objetiva, o que é “inesperado”? A resposta clássica vem do <strong><a href="https://en.wikipedia.org/wiki/Design_by_contract">Design by Contract (DbC)</a></strong> de <a href="https://en.wikipedia.org/wiki/Bertrand_Meyer">Bertrand Meyer</a>.</p>
<p>O <strong>Design by Contract</strong> (DbC) é um paradigma de desenvolvimento que define um contrato explícito entre um componente e seus clientes, garantindo que ambos entendam as expectativas e as responsabilidades. O DbC estabelece três elementos essenciais:</p>
<ol>
<li><strong>Pré-condições</strong>: Obrigações que devem ser satisfeitas pelo chamador antes de invocar uma operação.</li>
<li><strong>Pós-condições</strong>: Garantias que a operação fornece quando as pré-condições são atendidas.</li>
<li><strong>Invariantes</strong>: Propriedades que devem ser mantidas ao longo do tempo.</li>
</ol>
<p>Se a pré-condição de uma função não for atendida, ou seja, se o uso já começa errado (por exemplo, tentar sacar um valor negativo ou maior que o saldo), lançar uma exceção é apropriado, pois indica um erro de uso da interface; por outro lado, se a violação da pré-condição é algo frequente e esperado, como um campo vazio em um formulário, o ideal é tratar esse caso antes mesmo de chamar a função, evitando o uso de exceções para fluxos normais.</p>
<p>O exemplo abaixo em C++ abaixo ilustra como aplicar esse princípio, diferenciando claramente quando lançar exceção por violação de contrato e quando validar previamente:</p>


  <pre><code class="language-cpp">class Conta {
    double saldo_{0};                       // invariante: ≥ 0
public:
    void sacar(double v) {
        if (v &lt;= 0)                      // pré‑condição violada → erro do usuário
            throw std::invalid_argument(&#34;valor ≤ 0&#34;);
        if (v &gt; saldo_)                  // pré‑condição violada → uso incorreto
            throw std::domain_error(&#34;saldo insuficiente&#34;);

        double antigo = saldo_;
        saldo_ -= v;

        if (saldo_ != antigo - v)        // pós‑condição falhou → erro interno
            throw std::logic_error(&#34;sacar corrompeu saldo&#34;);
        assert(saldo_ &gt;= 0);             // invariante (desligada em release)
    }
};</code></pre>
 <h4 id="programação-com-asserções">Programação com <strong>asserções</strong></h4>
<ul>
<li><strong>O que são:</strong> checagens de <em>bugs</em> de desenvolvimento, desativadas em builds release.</li>
<li><strong>Quando usar:</strong> para invariantes internas e estados “impossíveis”.</li>
<li><strong>Quando <em>não</em> usar:</strong> para validar entrada de usuário ou recursos externos (isso é papel de exceção ou valor de retorno).</li>
</ul>


  <pre><code class="language-cpp">void push(Buffer&amp; buf, int x) {
    assert(!buf.cheio());          // bug se falhar em dev
    buf.escreve(x);
}</code></pre>
 <p>O código apresentado acima ilustra como aplicar, de forma prática, os princípios do <a href="https://en.wikipedia.org/wiki/Design_by_contract">Design by Contract (DbC)</a> e o uso de asserções para garantir a robustez do software. Cada tipo de situação exige uma ferramenta adequada: bugs internos, como a quebra de invariantes, devem ser detectados com <code>assert</code> (que só dispara em modo debug); violações de pré-condições, ou seja, quando o usuário utiliza a interface de forma incorreta, são tratadas com exceções específicas como <code>invalid_argument</code> ou <code>domain_error</code>.</p>
<p>Falhas em recursos externos, como problemas de I/O ou falta de memória, são sinalizadas por exceções de runtime (<code>ios_base::failure</code>, <code>bad_alloc</code>); e, finalmente, situações esperadas e frequentes, como um campo opcional vazio, devem ser representadas por tipos como <code>std::optional</code>, <code>std::expected</code> ou códigos de status, evitando o uso de exceções para o fluxo normal.</p>
<p>O mini-checklist apresentado resume o contrato em três etapas: primeiro, garantir as pré-condições (validando entradas e lançando exceções quando necessário); segundo, executar o trabalho principal da função; e, por fim, verificar as pós-condições e usar asserções para garantir que as invariantes do objeto foram mantidas.</p>
<p>Assim, o chamador sabe exatamente o que precisa fornecer, a função garante o resultado correto ou lança uma exceção se não puder cumprir, e o objeto permanece sempre em um estado válido. Essa abordagem torna o código mais seguro, previsível e fácil de manter.</p>
<blockquote>
<p>Isso fecha, de forma formal, o ciclo começado lá atrás: <em>&ldquo;exceção para o inesperado, valor para o esperado, assert para o impossível&rdquo;</em>.</p></blockquote>
<p>Até aqui vimos que <strong>pré‑/pós‑condições e asserções</strong> deixam claro <em>o que</em> cada parte deve cumprir — e que exceções só aparecem quando o contrato é quebrado. Mas o inverso também é verdadeiro: <strong>quando usamos exceções para controlar o fluxo diário, criamos contratos escondidos</strong> que amarram módulos sem ninguém notar.</p>
<p>Quando usamos exceções para controlar o fluxo normal do programa, criamos dependências ocultas entre módulos: o cliente precisa conhecer os tipos de exceção internos do serviço, o que torna o contrato implícito — afinal, as ações a serem tomadas em caso de erro (“se der erro X faça Y”) não aparecem na assinatura da função, mas apenas nos blocos <code>catch</code> espalhados pelo código.</p>
<p>Isso gera fragilidade, pois qualquer alteração nos tipos de exceção ou nas condições que as disparam pode quebrar vários pontos do sistema, como ilustrado no exemplo em C++ abaixo, onde o serviço lança exceções específicas e o cliente é obrigado a capturar cada uma delas individualmente. Observe o exemplo abaixo:</p>


  <pre><code class="language-cpp">// Serviço lança tipos específicos  ➜  Cliente precisa capturar cada um
try {
    auto* u = auth.autenticar(user, pass);
} catch(const UsuarioNaoEncontrado&amp;) { … }
  catch(const SenhaInvalida&amp;)       { … }</code></pre>
 <p>Quando um serviço lança exceções específicas para sinalizar falhas, qualquer alteração nesses tipos de erro obriga o desenvolvedor a revisar e atualizar todos os blocos <code>catch</code> espalhados pelo código cliente.</p>
<blockquote>
<p><strong>Isso cria um acoplamento invisível entre módulos</strong>: o cliente precisa conhecer detalhes internos do serviço para capturar corretamente cada exceção, tornando a manutenção mais trabalhosa e sujeita a erros. O controle de fluxo baseado em exceções, nesse contexto, esconde contratos importantes e dificulta a evolução segura da API.</p></blockquote>
<p>Para tornar o contrato explícito e facilitar a manutenção, o C++23 introduziu o <code>std::expected&lt;T, E&gt;</code>, que incorpora o erro ao próprio tipo de retorno da função. Assim, a assinatura já deixa claro para o usuário todas as possibilidades de sucesso ou falha, como no exemplo abaixo:</p>


  <pre><code class="language-cpp">std::expected&lt;Usuario*, ErroAuth&gt;
autenticar(std::string_view user, std::string_view pass);</code></pre>
 <p>O resultado é um código menos acoplado, mais documentado e mais seguro. Pois, o usuário da função já sabe todas as possibilidades de sucesso ou falha, e o compilador força o tratamento via <code>resultado.error()</code>.</p>
<hr>
<h3 id="testabilidade-e-caminhos-de-erro">Testabilidade e caminhos de erro</h3>
<p>Na seção anterior, vimos que tornar os contratos explícitos e como é possível reduzir o acoplamento entre módulos e deixa o código mais robusto. Um benefício imediato dessa abordagem é a facilidade de testar: quando o erro é representado como valor de retorno, fica muito mais simples cobrir todos os caminhos possíveis em testes unitários, sem depender de manipulação de exceções.</p>
<blockquote>
<p>Isso está totalmente alinhado com o conselho do livro <strong>Pragmatic Programmer</strong> que diz: “<strong>Test your software, or your users will</strong>”. Ou seja, &ldquo;se você não testar seu software, seus usuários vão testar&rdquo;.</p></blockquote>
<p>Cenários de erro merecem atenção especial nos testes, pois é justamente no tratamento de falhas que costumam aparecer os bugs mais críticos. Testar apenas o “caminho feliz” não garante a qualidade do sistema. Além disso, garantir que mudanças internas não quebrem o contrato de erro é fundamental para evitar regressões. Testes bem escritos também funcionam como documentação viva, mostrando claramente como o sistema reage a cada tipo de problema.</p>
<p>Por outro lado, quando o tratamento de falhas depende de exceções, surgem desafios práticos. Simular e capturar exceções em testes exige o uso de mocks que lançam erros, além de poluir o código de teste com blocos <code>try-catch</code> ou macros como <code>EXPECT_THROW</code>. Isso pode prejudicar a legibilidade e facilitar a omissão de casos importantes, já que é fácil esquecer de testar um <code>catch</code> específico. O resultado é uma cobertura de testes parcial e menos confiável.</p>
<p>Ao adotar contratos explícitos, como no exemplo em C++ abaixo usando <code>std::expected</code>, o teste se torna mais direto: basta verificar o valor retornado, sem precisar capturar exceções. Isso simplifica o código de teste, aumenta a clareza e garante que todos os ramos — inclusive os de erro — sejam exercitados de forma sistemática.</p>
<p>Assim, além de reduzir o acoplamento, esse padrão melhora a testabilidade e contribui para a manutenção segura do software. Abaixo, vamos ver como testar o código com contratos explícitos e como testar o código com exceções:</p>
<ol>
<li>
<p><strong>Mocks que simulam falha</strong></p>


  <pre><code class="language-cpp">class IServico { public: virtual Dados get(std::string) = 0; };
class MockFalho : public IServico {
    std::exception_ptr ex_;
public:
    void setFalha(const std::string&amp; msg) {
        ex_ = std::make_exception_ptr(std::runtime_error(msg));
    }
    Dados get(std::string) override { std::rethrow_exception(ex_); }
};</code></pre>
 <p>Teste foca em como o <em>consumidor</em> reage, sem depender do serviço real.</p>
</li>
<li>
<p><strong>Retorno explícito para casos esperados</strong></p>


  <pre><code class="language-cpp">struct Resultado { bool ok; std::string erro; Dados dados; };

Resultado processar(const Entrada&amp; in) {
    if (!valido(in)) return {false,&#34;Entrada inválida&#34;,{}};
    // ...
    return {true,&#34;&#34;,dados};
}
// Teste
EXPECT_FALSE(processar(invalido).ok);</code></pre>
 </li>
<li>
<p><strong><code>std::expected</code> (C++23)</strong> – contrato de erro no tipo</p>


  <pre><code class="language-cpp">std::expected&lt;Dados,ErroIO&gt; lerArquivo(...);
ASSERT_FALSE(lerArquivo(path).has_value());</code></pre>
 </li>
<li>
<p><strong>Testes de propriedade</strong> – use frameworks como <em>rapidcheck</em> ou <em>Catch2 generators</em> para iterar entradas aleatórias e garantir:</p>
<ul>
<li>“Nenhum input válido gera exceção”.</li>
<li>“Toda falha retorna erro não‑vazio”.</li>
</ul>
</li>
<li>
<p><strong>Ambiente de integração controlado</strong> – docker de DB que cai, servidor fake que devolve <em>timeouts</em>; reproduz falhas reais sem mexer no prod.</p>
</li>
</ol>
<hr>
<h2 id="referências">Referências</h2>
<ol>
<li><a href="https://a.co/d/8ZBw0ix"><strong>&ldquo;The Pragmatic Programmer: Your Journey to Mastery&rdquo;</strong> - David Thomas &amp; Andrew Hunt</a><br>
*Apresenta o princípio &ldquo;Crash Early&rdquo; e outras práticas essenciais para programação profissional, incluindo tratamento de erros e resiliência em sistemas.</li>
<li><a href="https://a.co/d/1L2Bwz4"><strong>&ldquo;Effective Modern C++: 42 Specific Ways to Improve Your Use of C++11 and C++14&rdquo;</strong> - Scott Meyers</a><br>
<em>Discute técnicas modernas de C++, incluindo o uso correto de exceções e alternativas como <code>std::optional</code>.</em></li>
<li><a href="https://a.co/d/3Wy2dFE"><strong>&ldquo;Programming: Principles and Practice Using C++&rdquo;</strong> - Bjarne Stroustrup</a><br>
<em>O criador do C++ explica fundamentos da linguagem, incluindo tratamento de erros e quando usar exceções.</em></li>
<li><a href="https://a.co/d/a4zoUcs"><strong>&ldquo;The Rust Programming Language&rdquo; (Livro Oficial)</strong> - Steve Klabnik &amp; Carol Nichols</a><br>
<em>Explica o sistema de <code>Result</code> e <code>Option</code> do Rust, que evita exceções.</em></li>
<li><a href="https://a.co/d/4geTFbr"><strong>&ldquo;Clojure for the Brave and True&rdquo;</strong> - Daniel Higginbotham</a><br>
<em>Aborda a filosofia de tratamento de erros em Clojure usando valores e mapas.</em></li>
<li><a href="https://a.co/d/8oEH9z4"><strong>&ldquo;Designing Data-Intensive Applications&rdquo;</strong> - Martin Kleppmann</a><br>
<em>Discute tolerância a falhas em sistemas distribuídos, complementando o conceito de &ldquo;graceful failure&rdquo;.</em></li>
<li><a href="https://a.co/d/66ya4UP"><strong>&ldquo;Release It!: Design and Deploy Production-Ready Software&rdquo;</strong> - Michael T. Nygard</a><br>
<em>Ensina padrões como &ldquo;Circuit Breaker&rdquo; para lidar com erros em produção.</em></li>
<li><a href="https://a.co/d/5bg0IIB"><strong>&ldquo;Functional Light JavaScript&rdquo;</strong> - Kyle Simpson</a><br>
<em>Mostra como aplicar conceitos funcionais (incluindo tratamento de erros sem exceções) em JavaScript.</em></li>
<li><a href="https://a.co/d/9S37n8W"><strong>&ldquo;Domain Modeling Made Functional&rdquo;</strong> - Scott Wlaschin</a><br>
<em>Usa F# para demonstrar como tipos como <code>Result</code> podem modelar erros de forma explícita.</em></li>
</ol>
]]></content:encoded>
      
      
      <category>tratamento de erros,exceções,javascript,typescript,rust,clojure,boas práticas,c&#43;&#43;</category>
      
      
      
      <dc:creator>Vitor Lobo Ramos</dc:creator>
      
      
      
      
      
      <description>&lt;![CDATA[Entenda quando, por que e para que o try/catch foi criado, e por que ele não deve ser usado como controle de fluxo lógico.]]></description>
      
    </item>
    
    <item>
      <title>Tratamento Funcional de Erros em TypeScript</title>
      <link>https://scovl.github.io/2025/05/12/tserr/</link>
      <guid>https://scovl.github.io/2025/05/12/tserr/</guid>
      <pubDate>Mon, 12 May 2025 18:31:45 -0300</pubDate>
      <description>&lt;![CDATA[<p>Neste artigo, vamos explorar uma abordagem mais estruturada para o tratamento de erros em TypeScript usando conceitos de programação funcional. A biblioteca <a href="https://gcanti.github.io/fp-ts/">fp-ts</a> (Functional Programming em TypeScript) oferece ferramentas que permitem lidar com falhas de forma explícita e type-safe, melhorando a robustez e legibilidade do código. Existem também outras bibliotecas com abordagens semelhantes ou complementares no ecossistema TypeScript:</p>
<ul>
<li><a href="https://github.com/purify-ts/purify-ts"><strong>Purify-ts</strong></a>: Uma alternativa mais leve ao fp-ts, focada em tipos como Maybe e Either</li>
<li><a href="https://github.com/microsoft/neverthrow"><strong>Neverthrow</strong></a>: Biblioteca especializada em tratamento de erros com Result/Either</li>
<li><a href="https://github.com/Effect-TS/effect"><strong>Effect</strong></a>: Uma biblioteca mais recente que expande os conceitos do fp-ts com foco em STM (Software Transactional Memory), concorrência e gerenciamento de recursos</li>
<li><a href="https://github.com/zio/zio-ts"><strong>Zio-ts</strong></a>: Inspirada na biblioteca ZIO de Scala, oferecendo abstrações para IO, concorrência e recursos</li>
</ul>
<p>Cada uma dessas bibliotecas tem seus pontos fortes, mas o <a href="https://gcanti.github.io/fp-ts/">fp-ts</a> se destaca pela sua maturidade, documentação abrangente e ecossistema rico de bibliotecas complementares. Neste artigo, focaremos no fp-ts e em como ele aborda o tratamento de erros de forma funcional.</p>]]></description>
      <content:encoded>&lt;![CDATA[<p>Neste artigo, vamos explorar uma abordagem mais estruturada para o tratamento de erros em TypeScript usando conceitos de programação funcional. A biblioteca <a href="https://gcanti.github.io/fp-ts/">fp-ts</a> (Functional Programming em TypeScript) oferece ferramentas que permitem lidar com falhas de forma explícita e type-safe, melhorando a robustez e legibilidade do código. Existem também outras bibliotecas com abordagens semelhantes ou complementares no ecossistema TypeScript:</p>
<ul>
<li><a href="https://github.com/purify-ts/purify-ts"><strong>Purify-ts</strong></a>: Uma alternativa mais leve ao fp-ts, focada em tipos como Maybe e Either</li>
<li><a href="https://github.com/microsoft/neverthrow"><strong>Neverthrow</strong></a>: Biblioteca especializada em tratamento de erros com Result/Either</li>
<li><a href="https://github.com/Effect-TS/effect"><strong>Effect</strong></a>: Uma biblioteca mais recente que expande os conceitos do fp-ts com foco em STM (Software Transactional Memory), concorrência e gerenciamento de recursos</li>
<li><a href="https://github.com/zio/zio-ts"><strong>Zio-ts</strong></a>: Inspirada na biblioteca ZIO de Scala, oferecendo abstrações para IO, concorrência e recursos</li>
</ul>
<p>Cada uma dessas bibliotecas tem seus pontos fortes, mas o <a href="https://gcanti.github.io/fp-ts/">fp-ts</a> se destaca pela sua maturidade, documentação abrangente e ecossistema rico de bibliotecas complementares. Neste artigo, focaremos no fp-ts e em como ele aborda o tratamento de erros de forma funcional.</p>
<blockquote>
<p><strong>Nota sobre versões:</strong> Este artigo utiliza a sintaxe do fp-ts 3.x (lançada em abril de 2025), que introduziu mudanças significativas na API, incluindo <code>fold</code> → <code>match</code>, <code>mapLeft</code> → <code>mapError</code>, e outros renomeamentos para maior clareza. Se você estiver usando fp-ts 2.x, consulte a <a href="https://gcanti.github.io/fp-ts/guides/migration-v3.html">documentação de migração</a> para detalhes sobre as mudanças.</p></blockquote>
<hr>
<h2 id="o-dilema-do-tratamento-de-erros-convencional">O Dilema do Tratamento de Erros Convencional</h2>
<p>No ecossistema JavaScript/TypeScript, historicamente, recorremos a duas abordagens principais para lidar com erros, cada uma com suas armadilhas. A forma mais comum de sinalizar e capturar erros é através de exceções, usando <code>throw</code> e <code>try/catch</code>. Porém, essa abordagem tem alguns problemas:</p>


  <pre><code class="language-typescript">function dividirLegado(a: number, b: number): number {
  if (b === 0) {
    throw new Error(&#34;Divisão por zero não é permitida!&#34;);
  }
  return a / b;
}

try {
  const resultado = dividirLegado(10, 0);
  console.log(&#34;Resultado:&#34;, resultado);
} catch (error: any) { // Note o &#39;any&#39;, um ponto fraco comum
  console.error(&#34;Ops, algo deu errado:&#34;, error.message);
  // Saída: &#34;Ops, algo deu errado: Divisão por zero não é permitida!&#34;
}</code></pre>
 <blockquote>
<p><strong>Nota:</strong> O uso de <code>any</code> para o tipo do erro é uma prática comum, mas não é a melhor opção. Em um sistema mais complexo, isso pode levar a erros de tipo que são difíceis de detectar.</p></blockquote>
<p>O uso de exceções apresenta sérios problemas de design: a assinatura da função <code>dividirLegado</code> não revela ao compilador a possibilidade de exceções, criando um contrato implícito onde o chamador precisa adivinhar a necessidade de um <code>try/catch</code>. Além disso, o <code>throw</code> interrompe abruptamente o fluxo de execução, dificultando o rastreamento e comprometendo a pureza funcional, enquanto a facilidade de esquecer blocos <code>try/catch</code> pode resultar em erros não capturados que derrubam aplicações inteiras. Uma alternativa comum é retornar valores especiais como <code>null</code>, <code>undefined</code> ou objetos de erro para sinalizar falhas, embora essa abordagem também apresente suas próprias limitações. Por exemplo:</p>


  <pre><code class="language-typescript">interface ResultadoDivisao {
  valor?: number;
  erro?: string;
}

function dividirComObjeto(a: number, b: number): ResultadoDivisao {
  if (b === 0) {
    return { erro: &#34;Divisão por zero!&#34; };
  }
  return { valor: a / b };
}

const resultadoObj = dividirComObjeto(10, 0);
if (resultadoObj.erro) {
  console.error(&#34;Falha:&#34;, resultadoObj.erro);
} else {
  console.log(&#34;Sucesso:&#34;, resultadoObj.valor);
}

// Ou com null:
function dividirComNull(a: number, b: number): number | null {
    if (b === 0) return null;
    return a / b;
}
const resultadoNull = dividirComNull(10, 0);
if (resultadoNull === null) console.error(&#34;Divisão por zero!&#34;);</code></pre>
 <p>Essa abordagem infelizmente também apresenta problemas significativos de usabilidade e segurança. O código se torna verboso e menos legível devido às constantes verificações manuais como <code>if (resultado.erro)</code> ou <code>if (resultado === null)</code>, enquanto a perda de contexto é inevitável, especialmente com valores <code>null</code> que não informam o motivo da falha - mesmo objetos de erro exigem disciplina manual consistente. Além disso, há um risco constante de erros silenciosos no sistema, pois esquecer de verificar o <code>null</code> ou a propriedade <code>erro</code> pode facilmente resultar em erros do tipo <code>TypeError: Cannot read property '...' of null</code> em partes subsequentes do código, comprometendo a robustez da aplicação como um todo.</p>
<hr>
<h2 id="erros-como-cidadãos-de-primeira-classe">Erros Como Cidadãos de Primeira Classe</h2>
<p>A Programação Funcional (FP) encara os erros de uma maneira fundamentalmente diferente: <strong>erros são simplesmente valores</strong>. Em vez de lançar exceções que quebram o fluxo, as funções retornam tipos de dados explícitos que representam tanto o sucesso quanto a falha. <code>fp-ts</code> nos fornece estruturas de dados poderosas para isso, como <code>Option</code> e <code>Either</code>. Antes de <code>Either</code>, vamos entender <code>Option</code>. Ele é usado para representar um valor que pode ou não estar presente. Pense nele como um substituto type-safe para <code>null</code> ou <code>undefined</code>.</p>
<ul>
<li><strong><code>Some&lt;A&gt;</code></strong>: Contém um valor do tipo <code>A</code>.</li>
<li><strong><code>None</code></strong>: Representa a ausência de um valor.</li>
</ul>
<p>O código abaixo mostra como usar <code>Option</code> para lidar com a ausência de valor. Vejamos:</p>


  <pre><code class="language-typescript">import * as O from &#39;fp-ts/Option&#39;;
import { pipe } from &#39;fp-ts/function&#39;;

interface User {
  id: number;
  name: string;
}
const users: User[] = [{ id: 1, name: &#34;Alice&#34; }, { id: 2, name: &#34;Bob&#34; }];

function findUserById(id: number): O.Option&lt;User&gt; {
  const user = users.find(u =&gt; u.id === id);
  return user ? O.some(user) : O.none; // Explicita a possibilidade de não encontrar
}

// Usando Option
const user1 = findUserById(1); // Some({ id: 1, name: &#34;Alice&#34; })
const user3 = findUserById(3); // None

pipe(
  user1,
  O.map(user =&gt; user.name.toUpperCase()), // Só executa se for Some
  O.match(
    () =&gt; console.log(&#34;Usuário não encontrado.&#34;), // Caso None
    (name) =&gt; console.log(&#34;Nome em maiúsculas:&#34;, name) // Caso Some
  )
); // Saída: Nome em maiúsculas: ALICE

pipe(
  user3,
  O.map(user =&gt; user.name.toUpperCase()),
  O.match(
    () =&gt; console.log(&#34;Usuário não encontrado.&#34;),
    (name) =&gt; console.log(&#34;Nome em maiúsculas:&#34;, name)
  )
); // Saída: Usuário não encontrado.</code></pre>
 <p>Note que <code>Option</code> é perfeito para casos onde a ausência não é necessariamente um &ldquo;erro&rdquo;, mas um estado esperado. Ele nos permite modelar de forma elegante situações como buscas que podem não retornar resultados, valores opcionais em formulários, ou acessos a propriedades que podem não existir.</p>
<p>Ao usar <code>Option</code>, tornamos explícito no sistema de tipos que um valor pode estar ausente, forçando o desenvolvedor a lidar com ambos os casos. Isso elimina erros comuns como referências nulas inesperadas e torna o código mais robusto, previsível e auto-documentado, sem a necessidade de verificações defensivas espalhadas pelo código.</p>
<blockquote>
<p><strong>Nota:</strong> O uso de <code>Option</code> é uma abordagem mais moderna e elegante para lidar com valores que podem estar ausentes. Ele é preferível ao uso de <code>null</code> ou <code>undefined</code> em muitos casos, pois fornece um tipo mais explícito e seguro para representar a ausência de valor.</p></blockquote>
<p>Para ilustrar ainda mais a utilidade de <code>Option</code>, especialmente em cenários do mundo real, vamos considerar uma operação assíncrona, como buscar dados de uma API. Muitas vezes, uma API pode não encontrar o recurso solicitado, e <code>Option</code> é uma excelente forma de modelar essa possibilidade sem recorrer a <code>null</code> ou exceções para um &ldquo;não encontrado&rdquo; esperado. Imagine que estamos buscando uma notícia por ID:</p>


  <pre><code class="language-typescript">interface Noticia {
  id: number;
  titulo: string;
  conteudo: string;
}

const buscarNoticia = async (id: number): Promise&lt;O.Option&lt;Noticia&gt;&gt; =&gt; {
  const noticia = await fetch(`https://api.exemplo.com/noticias/${id}`);
  if (noticia.status === 404) {
    return O.none;
  }
  return O.some(await noticia.json());
}

const noticia = await buscarNoticia(1);
pipe(
  noticia,
  O.match(
    () =&gt; console.log(&#34;Notícia não encontrada&#34;),
    (noticia) =&gt; console.log(noticia.titulo)
  )
);</code></pre>
 <p>Neste exemplo, <code>buscarNoticia</code> retorna um <code>Option&lt;Noticia&gt;</code>, que pode ser <code>Some</code> (com a notícia encontrada) ou <code>None</code> (quando a notícia não é encontrada). Isso torna o código mais claro e seguro, pois não precisamos verificar o status da resposta ou lidar com <code>null</code>/<code>undefined</code>.</p>
<h2 id="sucesso-explícito-ou-falha-detalhada">Sucesso Explícito ou Falha Detalhada</h2>
<p>Já o <code>Either</code> é o tipo protagonista no paradigma funcional quando precisamos modelar operações que podem falhar, oferecendo uma estrutura elegante que não apenas sinaliza o erro, mas também fornece detalhes específicos sobre a falha. Diferente de exceções tradicionais que interrompem o fluxo de execução, <code>Either</code> encapsula tanto o sucesso quanto o erro como valores de primeira classe, permitindo composição e transformação de operações falíveis de forma segura e previsível. Basicamente, <code>Either</code> é uma união de dois tipos: <code>Right</code> e <code>Left</code>:</p>
<ul>
<li><strong><code>Right&lt;A&gt;</code></strong>: Representa um resultado de sucesso, contendo um valor do tipo <code>A</code>. (Pense &ldquo;Right&rdquo; como &ldquo;correto&rdquo;).</li>
<li><strong><code>Left&lt;E&gt;</code></strong>: Representa uma falha, contendo um erro do tipo <code>E</code>. (Pense &ldquo;Left&rdquo; como o que sobrou, o erro).</li>
</ul>
<p>Para ficar mais claro, veja o gráfico abaixo:</p>


  
  <div class="mermaid">graph TD
    A[Sucesso] --&gt; B[Right&lt;A&gt;]
    C[Falha] --&gt; D[Left&lt;E&gt;]</div>
 <p>Esta estrutura nos permite representar de forma explícita tanto o caminho feliz quanto o caminho de erro em nossas operações, sem recorrer a exceções ou valores nulos. O tipo <code>Either</code> força o programador a considerar ambos os casos, tornando o código mais robusto e previsível. Vamos ver um exemplo prático em código:</p>


  <pre><code class="language-typescript">import * as E from &#34;fp-ts/Either&#34;;
import { pipe } from &#34;fp-ts/function&#34;; // pipe é essencial!

// Nosso divisor, agora funcional e type-safe!
function dividir(a: number, b: number): E.Either&lt;string, number&gt; {
  if (b === 0) {
    return E.left(&#34;Divisão por zero!&#34;); // Falha explícita com uma mensagem
  }
  return E.right(a / b); // Sucesso explícito com o valor
}

const resultado1 = dividir(10, 2); // Right(5)
const resultado2 = dividir(10, 0); // Left(&#34;Divisão por zero!&#34;)

console.log(resultado1);
console.log(resultado2);</code></pre>
 <p>O tipo de retorno <code>E.Either&lt;string, number&gt;</code> diz claramente: &ldquo;esta função retorna um número em caso de sucesso, OU uma string de erro em caso de falha.&rdquo; O compilador TypeScript agora <em>sabe</em> dos possíveis resultados. Nunca acessamos diretamente <code>Left</code> ou <code>Right</code> (ou <code>Some</code>/<code>None</code>). Em vez disso, usamos funções de alta ordem que operam sobre esses &ldquo;containers&rdquo;. A função <code>pipe</code> de <code>fp-ts/function</code> é crucial aqui para compor essas operações de forma legível.</p>
<p>A função <code>pipe(valorInicial, fn1, fn2, fn3)</code> é equivalente a <code>fn3(fn2(fn1(valorInicial)))</code>, simplificando a composição de funções. Ela recebe um valor inicial e o encaminha através de uma sequência de transformações, criando um fluxo de dados da esquerda para a direita que é intuitivo e fácil de acompanhar, melhorando significativamente a legibilidade do código em comparação com as chamadas aninhadas tradicionais. Veja o gráfico abaixo:</p>


  
  <div class="mermaid">graph LR
    A[valorInicial] --&gt; B[fn1]
    B --&gt; C[fn2]
    C --&gt; D[fn3]
    
    subgraph &#34;pipe(valorInicial, fn1, fn2, fn3)&#34;
    A
    B
    C
    D
    end
    
    style A fill:#f9f9f9,stroke:#666
    style D fill:#d5f5e3,stroke:#2ecc71,stroke-width:2px</div>
 <p>O diagrama acima mostra como funciona a função <code>pipe</code> de uma forma simples. Em vez de escrever código aninhado como <code>fn3(fn2(fn1(valorInicial)))</code>, que é difícil de ler, usamos <code>pipe(valorInicial, fn1, fn2, fn3)</code>, que é como ler uma receita: primeiro faça isso, depois aquilo&hellip;por exemplo:</p>


  <pre><code class="language-typescript">// Sem pipe (difícil de ler):
const resultado = multiplicarPorDois(somarCinco(converterParaNumero(&#34;10&#34;)));

// Com pipe (fácil de seguir):
const resultado = pipe(
  &#34;10&#34;,               // Valor inicial
  converterParaNumero, // Primeira transformação
  somarCinco,         // Segunda transformação
  multiplicarPorDois  // Terceira transformação
);</code></pre>
 <p>Pense no <code>pipe</code> como uma linha de montagem: o valor inicial entra por um lado, passa por várias estações de trabalho (funções), e sai transformado do outro lado!</p>
<h2 id="propriedades-avançadas-do-either">Propriedades Avançadas do Either</h2>
<p>O <code>Either</code> vai muito além de ser apenas um container para sucesso ou erro - ele é um conceito fundamental da programação funcional que implementa padrões poderosos que nos permitem compor operações de forma elegante e segura. Na programação funcional, o <code>Either</code> é classificado como um tipo algebráico que implementa interfaces importantes como <a href="https://en.wikipedia.org/wiki/Functor">Functor</a> e <a href="https://en.wikipedia.org/wiki/Monad_%28category_theory%29">Monad</a>. Vamos entender o que isso significa na prática e como isso nos ajuda a escrever código mais robusto:</p>
<ol>
<li><strong>Functor</strong>: O <code>Either</code> é um functor porque implementa a operação <code>map</code>, que permite transformar o valor dentro de um <code>Right</code> sem alterar a estrutura do container. Se for um <code>Left</code>, o erro é simplesmente propagado sem alteração.</li>
</ol>


  <pre><code class="language-typescript">// map transforma apenas o lado Right
const resultado = pipe(
  dividir(10, 2), // Right(5)
  E.map(valor =&gt; valor * 2) // Right(10)
);

// Se for Left, map não faz nada
const resultadoErro = pipe(
  dividir(10, 0), // Left(&#34;Divisão por zero!&#34;)
  E.map(valor =&gt; valor * 2) // Continua Left(&#34;Divisão por zero!&#34;)
);</code></pre>
 <ol start="2">
<li><strong>Monad</strong>: O <code>Either</code> também é uma monad porque implementa a operação <code>chain</code> (também chamada de <code>flatMap</code> ou <code>bind</code> em outras linguagens). Isso permite compor operações que também podem falhar, evitando o aninhamento de <code>E.Either&lt;E, E.Either&lt;E, A&gt;&gt;</code>.</li>
</ol>


  <pre><code class="language-typescript">// Outra função que pode falhar
const raizQuadrada = (n: number): E.Either&lt;string, number&gt; =&gt;
n &lt; 0 ? E.left(&#34;Não existe raiz de número negativo&#34;) : E.right(Math.sqrt(n));

// Usando chain para compor operações falíveis
const calcularRaizDaDivisao = (a: number, b: number) =&gt; pipe(
  dividir(a, b),        // E.Either&lt;string, number&gt;
E.chain(raizQuadrada) // E.Either&lt;string, number&gt;
);

console.log(calcularRaizDaDivisao(16, 4));  // Right(2)
console.log(calcularRaizDaDivisao(16, 0));  // Left(&#34;Divisão por zero!&#34;)
console.log(calcularRaizDaDivisao(-16, 4)); // Left(&#34;Não existe raiz de número negativo&#34;)</code></pre>
 <p>Estas propriedades tornam o <code>Either</code> extremamente poderoso para composição de operações, permitindo criar fluxos complexos de tratamento de erros de forma elegante e type-safe. O <code>map</code> nos permite transformar valores de sucesso, enquanto o <code>chain</code> nos permite sequenciar operações que podem falhar, com propagação automática de erros.</p>
<h2 id="usando-match-para-extrair-valores-de-either">Usando <code>match</code> para Extrair Valores de <code>Either</code></h2>
<p>Agora que entendemos o conceito de <code>pipe</code>, vamos explorar a função <code>match</code>, que é fundamental para extrair valores de um <code>Either</code>. Esta função permite definir duas funções: uma para o caso <code>Left</code> (erro) e outra para o caso <code>Right</code> (sucesso), funcionando essencialmente como um <code>if/else</code> especializado para o tipo <code>Either</code>. Com <code>match</code>, podemos transformar nosso <code>Either</code> em qualquer outro tipo, garantindo que ambos os casos sejam tratados explicitamente.</p>
<p>O <code>match</code> é uma forma de &ldquo;pattern matching&rdquo; funcional - um conceito poderoso de linguagens funcionais que permite lidar com diferentes &ldquo;casos&rdquo; ou &ldquo;formas&rdquo; que um valor pode ter. No caso do <code>Either</code>, temos dois padrões possíveis: <code>Left</code> e <code>Right</code>. O pattern matching nos força a tratar todos os casos possíveis de forma explícita, eliminando a possibilidade de esquecermos algum caminho. Isso é especialmente valioso em TypeScript, onde o sistema de tipos garanta que não podemos acessar o valor interno de um <code>Either</code> sem primeiro &ldquo;desempacotá-lo&rdquo; usando <code>match</code> ou funções similares. Agora que você já entendeu o conceito de <code>pipe</code>, vamos ver como usar <code>match</code> para extrair valores de um <code>Either</code> acompanhando o gráfico abaixo:</p>


  
  <div class="mermaid">graph LR
    A[Either&lt;E, A&gt;] --&gt; B{É Right?}
    B --&gt;|Sim| C[Right&lt;A&gt;]
    B --&gt;|Não| D[Left&lt;E&gt;]
    C --&gt; E[fnSucesso: A → B]
    D --&gt; F[fnErro: E → B]
    E --&gt; G[Resultado Final: B]
    F --&gt; G

    subgraph &#34;pipe &#43; match&#34;
    A
    B
    C
    D
    E
    F
    G
    end

    style A fill:#f9f9f9,stroke:#666
    style C fill:#d5f5e3,stroke:#2ecc71
    style D fill:#ffdddd,stroke:#e74c3c
    style G fill:#d6eaf8,stroke:#3498db,stroke-width:2px</div>
 <p>O processo começa com uma entrada <code>E.Either&lt;E, A&gt;</code>, que representa um valor que pode ser um sucesso (<code>Right&lt;A&gt;</code>) ou um erro (<code>Left&lt;E&gt;</code>). Quando aplicamos a função <code>match</code>, ela toma uma decisão baseada no tipo do <code>Either</code>: se for um <code>Right</code>, aplica a função de sucesso (<code>fnSucesso</code>) ao valor interno, transformando <code>A</code> em <code>B</code>; se for um <code>Left</code>, aplica a função de erro (<code>fnErro</code>) ao erro interno, transformando <code>E</code> também em <code>B</code>. O resultado final deste processo é sempre um valor do tipo <code>B</code>, independentemente do caminho seguido. Esta é a beleza do <code>match</code>: ele unifica os dois caminhos possíveis (sucesso e erro) em um único tipo de saída, permitindo que o código subsequente trabalhe com um valor concreto sem precisar verificar constantemente se estamos lidando com um sucesso ou um erro. Vamos ver um exemplo prático em código:</p>


  <pre><code class="language-typescript">import * as E from &#34;fp-ts/Either&#34;;
import { pipe } from &#34;fp-ts/function&#34;;

// Função que pode falhar
const divide = (a: number, b: number): E.Either&lt;string, number&gt; =&gt;
  b === 0 ? E.left(&#34;Divisão por zero!&#34;) : E.right(a / b);

// Tratamento com match
const result = pipe(
  divide(10, 0),
  E.match(
    (error) =&gt; `Erro: ${error}`, // fnErro
    (value) =&gt; `Resultado: ${value}` // fnSucesso
  )
);

console.log(result); // &#34;Erro: Divisão por zero!&#34;</code></pre>
 <p>O método <code>match</code> é particularmente útil quando você precisa <strong>transformar</strong> o resultado final de uma operação em um formato específico, como preparar dados para exibição na interface do usuário ou formatar mensagens para logging. Esta função é essencial para unificar os caminhos de sucesso e erro em um único tipo de retorno. Além disso, <code>match</code> serve como uma excelente maneira de <strong>encerrar</strong> uma cadeia de operações com um valor concreto, permitindo que você conclua o processamento de um <code>Either</code> e obtenha um resultado final que não é mais um tipo monádico.</p>
<h2 id="usando-map-para-transformar-o-valor-de-sucesso">Usando <code>map</code> para Transformar o Valor de Sucesso</h2>
<p>Enquanto <code>match</code> nos permite encerrar uma cadeia de operações unificando os caminhos de sucesso e erro, muitas vezes precisamos apenas transformar o valor de sucesso sem alterar o fluxo de tratamento de erros. É aqui que o operador <code>map</code> se torna valioso. Esta função aplica uma transformação apenas ao valor contido em um <code>Right</code>, deixando qualquer <code>Left</code> intacto e propagando o erro original sem modificação. O gráfico abaixo mostra como funciona o <code>map</code> em um <code>Either</code>:</p>


  
  <div class="mermaid">graph LR
    A[&#34;parseNumber(&#39;42&#39;)&#34;] --&gt; B[Right&lt;42&gt;]
    B --&gt; C[map: n → n * 2]
    C --&gt; D[Right&lt;84&gt;]
    D --&gt; E[fold: exibe resultado]
    
    A2[&#34;parseNumber(&#39;abc&#39;)&#34;] --&gt; B2[Left&lt;&#39;Erro&#39;&gt;]
    B2 --&gt; C2[map: ignorado]
    C2 --&gt; D2[Left&lt;&#39;Erro&#39;&gt;]
    D2 --&gt; E2[fold: exibe erro]

    subgraph &#34;Exemplo Completo&#34;
    A --&gt; E
    A2 --&gt; E2
    end

    style D fill:#d5f5e3,stroke:#2ecc71
    style D2 fill:#ffdddd,stroke:#e74c3c</div>
 <p>Vamos entender o diagrama acima: ele ilustra como o operador <code>map</code> funciona com o tipo <code>Either</code>. No caminho superior, quando <code>parseNumber('42')</code> retorna um <code>Right&lt;42&gt;</code> (sucesso), o <code>map</code> aplica a função de transformação (multiplicação por 2), resultando em <code>Right&lt;84&gt;</code>. No caminho inferior, quando <code>parseNumber('abc')</code> retorna um <code>Left&lt;'Erro'&gt;</code> (falha), o <code>map</code> ignora completamente a função de transformação, propagando o erro original sem modificação. Este comportamento é fundamental para a programação funcional, pois permite transformar valores de sucesso enquanto preserva automaticamente os erros, criando um fluxo de dados seguro e previsível. Vejamos um exemplo prático de como usar <code>map</code> com <code>Either</code>:</p>


  <pre><code class="language-typescript">const resultadoDobrado = pipe(
  dividir(20, 2),         // Right(10)
  E.map(valor =&gt; valor * 2) // Aplica valor * 2 somente se for Right
); // resultadoDobrado é Right(20)

const falhaDobrada = pipe(
  dividir(20, 0),         // Left(&#34;Divisão por zero!&#34;)
  E.map(valor =&gt; valor * 2) // Não é executado
); // falhaDobrada é Left(&#34;Divisão por zero!&#34;)</code></pre>
 <h2 id="usando-chain-para-encadear-operações-falíveis">Usando <code>chain</code> para Encadear Operações Falíveis</h2>
<p>Enquanto <code>map</code> é perfeito para transformações simples de valores de sucesso, ele não é suficiente quando a própria transformação pode falhar. É aqui que <code>chain</code> se torna essencial. Esta função permite compor operações sequenciais onde cada etapa depende do resultado bem-sucedido da anterior e pode, por si só, produzir um erro. Diferente do <code>map</code>, que sempre envolve o resultado da transformação em um novo <code>Right</code>, o <code>chain</code> espera que a função de transformação já retorne um <code>Either</code>, evitando o aninhamento desnecessário de estruturas.</p>
<p>Na prática, <code>chain</code> é fundamental para construir fluxos de validação e processamento robustos. Por exemplo, ao processar dados de usuário, podemos encadear várias validações (verificar formato de email, checar comprimento de senha, validar idade) onde cada etapa só é executada se a anterior for bem-sucedida. Se qualquer validação falhar, o erro é propagado automaticamente até o final da cadeia, eliminando a necessidade de verificações condicionais repetitivas e tornando o código mais declarativo e menos propenso a erros. Vejamos um exemplo prático de como usar <code>chain</code> com <code>Either</code> no gráfico abaixo:</p>


  
  <div class="mermaid">graph TD
    A[Either&lt;E, A&gt;] --&gt; B{É Right?}
    B --&gt;|Sim| C[Right&lt;A&gt;]
    B --&gt;|Não| D[Left&lt;E&gt;]
    C --&gt; E[chain: A → Either&lt;E, B&gt;]
    D --&gt; F[Left&lt;E&gt;]
    E --&gt; G{É Right?}
    G --&gt;|Sim| H[Right&lt;B&gt;]
    G --&gt;|Não| I[Left&lt;E&gt;]
    H --&gt; J[map: B → C]
    I --&gt; K[Left&lt;E&gt;]
    J --&gt; L[Right&lt;C&gt;]
    K --&gt; M[Left&lt;E&gt;]
        L --&gt; N[match: C → D]
    M --&gt; N[match: E → D]
    
    subgraph &#34;pipe &#43; chain &#43; map &#43; match&#34;
    A
    B
    C
    D
    E
    F
    G
    H
    I
    J
    K
    L
    M
    N
    end

    style A fill:#f9f9f9,stroke:#666
    style C fill:#d5f5e3,stroke:#2ecc71
    style D fill:#ffdddd,stroke:#e74c3c
    style H fill:#d5f5e3,stroke:#2ecc71
    style I fill:#ffdddd,stroke:#e74c3c
    style L fill:#d5f5e3,stroke:#2ecc71
    style M fill:#ffdddd,stroke:#e74c3c
    style N fill:#d6eaf8,stroke:#3498db,stroke-width:2px</div>
 <p>O diagrama acima ilustra o fluxo de processamento usando a combinação de operadores <code>pipe</code>, <code>chain</code>, <code>map</code> e <code>match</code> com o tipo <code>Either</code>. Ele demonstra como um valor inicial <code>E.Either&lt;E, A&gt;</code> é processado através de uma série de transformações condicionais. Se o valor for um <code>Right&lt;A&gt;</code>, ele passa pela função <code>chain</code> que pode produzir um novo <code>E.Either&lt;E, B&gt;</code>. Se esse resultado for um <code>Right&lt;B&gt;</code>, ele é transformado pela função <code>map</code> em um <code>Right&lt;C&gt;</code>. Em qualquer ponto onde um <code>Left&lt;E&gt;</code> é encontrado, o fluxo de transformações é curto-circuitado, propagando o erro até o final.</p>
<p>Finalmente, a função <code>match</code> é aplicada para extrair o valor final, seja ele um sucesso (<code>C</code>) ou um erro (<code>E</code>), convertendo-os para um tipo comum <code>D</code>. Este padrão de composição permite criar pipelines de processamento robustos onde os erros são tratados de forma elegante e explícita. Vamos ver um exemplo prático de como usar <code>chain</code> com <code>Either</code> no código abaixo:</p>


  <pre><code class="language-typescript">// Função que valida se um número é positivo
const garantirPositivo = (n: number): E.Either&lt;string, number&gt; =&gt;
  n &gt; 0 ? E.right(n) : E.left(&#34;Número deve ser positivo!&#34;);

// Função que calcula a raiz quadrada (apenas para positivos)
const raizQuadradaSegura = (n: number): E.Either&lt;string, number&gt; =&gt;
  n &lt; 0 ? E.left(&#34;Não é possível calcular raiz de número negativo!&#34;) : E.right(Math.sqrt(n));

// Cenário 1: Sucesso em tudo
const computacaoSucesso = pipe(
  dividir(32, 2),           // Right(16)
  E.chain(garantirPositivo),  // Right(16) -&gt; garantirPositivo(16) -&gt; Right(16)
  E.chain(raizQuadradaSegura) // Right(16) -&gt; raizQuadradaSegura(16) -&gt; Right(4)
);
console.log(pipe(computacaoSucesso, E.match(e =&gt; e, v =&gt; v.toString()))); // &#34;4&#34;

// Cenário 2: Falha na divisão
const computacaoFalhaDivisao = pipe(
  dividir(32, 0),           // Left(&#34;Divisão por zero!&#34;)
  E.chain(garantirPositivo),  // Ignorado, propaga Left(&#34;Divisão por zero!&#34;)
  E.chain(raizQuadradaSegura) // Ignorado, propaga Left(&#34;Divisão por zero!&#34;)
);
console.log(pipe(computacaoFalhaDivisao, E.match(e =&gt; e, v =&gt; v.toString()))); // &#34;Divisão por zero!&#34;

// Cenário 3: Falha na validação de positivo
const computacaoFalhaPositivo = pipe(
  E.right(-10),               // Começamos com um Right(-10) para este exemplo
  E.chain(garantirPositivo),  // Right(-10) -&gt; garantirPositivo(-10) -&gt; Left(&#34;Número deve ser positivo!&#34;)
  E.chain(raizQuadradaSegura) // Ignorado, propaga Left(&#34;Número deve ser positivo!&#34;)
);
console.log(pipe(computacaoFalhaPositivo, E.match(e =&gt; e, v =&gt; v.toString()))); // &#34;Número deve ser positivo!&#34;</code></pre>
 <p>Note como o primeiro <code>Left</code> encontrado interrompe a cadeia e é propagado até o final.</p>
<hr>
<h2 id="usando-taskeithere-a-o-poder-de-either-no-mundo-assíncrono">Usando <code>TaskEither&lt;E, A&gt;</code>: O Poder de <code>Either</code> no Mundo Assíncrono</h2>
<p>E quando nossas operações são assíncronas, como chamadas de API ou interações com banco de dados? Para entender o <code>TaskEither&lt;E, A&gt;</code>, vamos construir o conceito passo a passo:</p>
<ol>
<li>
<p>Uma <code>Promise&lt;A&gt;</code> no JavaScript representa uma operação assíncrona que eventualmente produzirá um valor do tipo <code>A</code> ou será rejeitada com um erro.</p>
</li>
<li>
<p>Na biblioteca fp-ts, o tipo <code>Task&lt;A&gt;</code> é essencialmente uma função que retorna uma <code>Promise&lt;A&gt;</code>, mas com uma abordagem mais funcional. É definido como <code>() =&gt; Promise&lt;A&gt;</code>.</p>
</li>
<li>
<p>O <code>TaskEither&lt;E, A&gt;</code> combina o conceito de <code>Task</code> com <code>Either</code>. Formalmente, é um <code>Task&lt;Either&lt;E, A&gt;&gt;</code>, ou seja, uma função que retorna uma promessa que resolverá para um <code>Either&lt;E, A&gt;</code>.</p>
</li>
</ol>
<p>Isso nos dá o melhor dos dois mundos: a capacidade de lidar com operações assíncronas (como o <code>Promise</code>) e um tratamento de erros explícito e tipado (como o <code>Either</code>). Na prática, o <code>TaskEither</code> é perfeito para operações que demoram para completar e podem falhar, como buscar dados de um servidor ou ler um arquivo. Em vez de usar <code>try/catch</code> espalhados pelo código ou verificar erros manualmente, você encadeia operações de forma elegante e o TypeScript garanta que você não esqueça de tratar os erros.</p>
<p>A grande vantagem é que, diferente de uma <code>Promise</code> comum que mistura o fluxo de sucesso e erro em callbacks separados (<code>.then()</code> e <code>.catch()</code>), o <code>TaskEither</code> mantém ambos os caminhos dentro do mesmo tipo, permitindo composição mais segura e previsível de operações assíncronas que podem falhar. Vamos ver um exemplo prático de como usar <code>TaskEither</code> no código abaixo:</p>


  <pre><code class="language-typescript">import * as TE from &#34;fp-ts/TaskEither&#34;;
// &#39;pipe&#39; já foi importado de &#39;fp-ts/function&#39;

interface UserData {
  id: number;
  name: string;
  email: string;
}

// Erro customizado para nossa API
class NetworkError extends Error {
  constructor(message: string, public status?: number) {
    super(message);
    this.name = &#34;NetworkError&#34;;
  }
}

const fetchUser = (userId: number): TE.TaskEither&lt;NetworkError, UserData&gt; =&gt;
  TE.tryCatch&lt;NetworkError, UserData&gt;(
    // A função que retorna uma Promise (o &#34;try&#34; do tryCatch)
    async () =&gt; {
      const response = await fetch(`https://jsonplaceholder.typicode.com/users/${userId}`);
      if (!response.ok) {
        // Lançamos um erro customizado para ser capturado pelo &#39;onRejected&#39;
        throw new NetworkError(`Falha na requisição: ${response.statusText}`, response.status);
      }
      return response.json() as Promise&lt;UserData&gt;; // Garantimos o tipo
    },
    // A função que converte o erro/rejeição da Promise em um Left&lt;E&gt;
    (motivoDesconhecido: unknown): NetworkError =&gt; {
      if (motivoDesconhecido instanceof NetworkError) {
        return motivoDesconhecido;
      }
      // Para outros tipos de erros (ex: falha de rede antes da resposta HTTP)
      return new NetworkError(String(motivoDesconhecido));
    }
  );

// Como usar:
async function exibirNomeUsuario(id: number): Promise&lt;void&gt; {
  const programa = pipe(
    fetchUser(id), // Retorna TaskEither&lt;NetworkError, UserData&gt;
    TE.map(user =&gt; `Nome do usuário: ${user.name}`), // Transforma o sucesso
    TE.matchE(
      // Função para o caso de falha (Left)
      (erro) =&gt; async () =&gt; console.error(`Erro ao buscar usuário: ${erro.message}${erro.status ? ` (Status: ${erro.status})` : &#39;&#39;}`),
      // Função para o caso de sucesso (Right)
      (nomeFormatado) =&gt; async () =&gt; console.log(nomeFormatado)
    )
  );
  // Para executar o TaskEither e obter o resultado (ou efeito colateral), chamamos a função retornada por matchE:
  await programa();
}

// Pattern alternativo: TE.mapError para logging de erros no pipeline
const programaComLog = pipe(
  fetchUser(id),
  TE.mapError((erro) =&gt; {
    console.error(`Falha na operação: ${erro.message}`);
    // Aqui você pode adicionar logging estruturado, métricas, etc.
    return erro; // Retorna o erro para continuar o pipeline
  }),
  TE.matchE(
    (erro) =&gt; async () =&gt; console.error(`Erro final: ${erro.message}`),
    (user) =&gt; async () =&gt; console.log(`Sucesso: ${user.name}`)
  )
);

// Testando:
// exibirNomeUsuario(1); // Deve imprimir &#34;Nome do usuário: Leanne Graham&#34; (ou similar)
// exibirNomeUsuario(999); // Deve imprimir o erro de &#34;Falha na requisição: Not Found (Status: 404)&#34;</code></pre>
 <p>O <code>tryCatch</code> é um construtor muito útil para envolver código baseado em Promises que pode rejeitar. Ele transforma o modelo tradicional de tratamento de erros com <code>try/catch</code> em uma estrutura funcional, encapsulando tanto o caminho feliz quanto o de erro em um único tipo de dados <code>TaskEither</code>. Isso permite que o código cliente trabalhe com um valor que representa explicitamente a possibilidade de falha, em vez de depender de exceções implícitas.</p>
<p>A principal vantagem desse construtor é a separação clara entre a lógica de negócio e o tratamento de erros. Ao usar <code>tryCatch</code>, você define duas funções: uma que executa a operação principal (retornando uma Promise) e outra que converte qualquer erro em um tipo específico. Isso torna o código mais previsível e facilita o rastreamento de todos os possíveis caminhos de erro através do sistema de tipos.</p>
<p>Além disso, <code>tryCatch</code> se integra perfeitamente com outras funções do ecossistema fp-ts, permitindo compor operações assíncronas que podem falhar de maneira elegante e segura. Em vez de aninhamentos complexos de try/catch ou promessas encadeadas com .catch(), você pode usar operadores como pipe, map e chain para expressar fluxos de dados complexos de forma declarativa, mantendo o tratamento de erros consistente em toda a aplicação.</p>
<p>A tabela abaixo compara as diferentes estratégias de tratamento de erros em TypeScript, destacando os pontos fortes e fracos de cada uma:</p>
<table>
  <thead>
      <tr>
          <th>Abordagem</th>
          <th>Prós</th>
          <th>Contras</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>try/catch com exceções</strong></td>
          <td>• Sintaxe familiar e padrão da linguagem<br>• Separação visual entre código normal e tratamento de erro<br>• Captura erros em qualquer nível da pilha de chamadas</td>
          <td>• Contrato implícito (assinatura da função não indica possibilidade de erro)<br>• Interrompe abruptamente o fluxo de execução<br>• Fácil esquecer de usar try/catch<br>• Difícil composição de funções que podem lançar exceções<br>• Tipagem de erros geralmente fraca (any)</td>
      </tr>
      <tr>
          <td><strong>Retorno de null/undefined</strong></td>
          <td>• Simplicidade de implementação<br>• Não interrompe o fluxo de execução</td>
          <td>• Perda completa de contexto do erro<br>• Verificações constantes de null/undefined<br>• Fácil esquecer verificações, causando erros em runtime<br>• Não escala bem para operações compostas</td>
      </tr>
      <tr>
          <td><strong>Objetos de resultado/erro</strong></td>
          <td>• Contrato explícito<br>• Preserva algum contexto de erro<br>• Não interrompe o fluxo de execução</td>
          <td>• Código verboso com muitas verificações manuais<br>• Disciplina manual para manter consistência<br>• Composição de operações torna-se complexa<br>• Tipagem pode ser ambígua (propriedades opcionais)</td>
      </tr>
      <tr>
          <td><strong>Either/TaskEither</strong></td>
          <td>• Contrato totalmente explícito via sistema de tipos<br>• Composição elegante de operações<br>• Tratamento de erro obrigatório (impossível &ldquo;esquecer&rdquo;)<br>• Preservação completa do contexto de erro<br>• Fluxo de execução previsível<br>• Facilita testes unitários</td>
          <td>• Curva de aprendizado inicial<br>• Requer familiaridade com conceitos funcionais<br>• Verbosidade em casos simples<br>• Dependência de biblioteca externa (fp-ts)</td>
      </tr>
  </tbody>
</table>
<p>A abordagem com <code>Either</code> e <code>TaskEither</code> oferece o melhor equilíbrio entre segurança de tipos, composição e manutenibilidade para sistemas complexos, embora exija um investimento inicial em aprendizado dos conceitos de programação funcional.</p>
<h3 id="taskeither-computação-assíncrona-com-tratamento-explícito-de-erros">TaskEither: Computação Assíncrona com Tratamento Explícito de Erros</h3>
<p>Um aspecto fundamental a ser compreendido sobre <code>TaskEither</code> é que ele representa uma <em>descrição</em> de uma computação assíncrona que pode falhar, não a execução imediata dessa computação. Quando você cria um <code>TaskEither</code>, está apenas definindo o que deve acontecer, sem executar nenhum código assíncrono naquele momento. Esta é uma característica poderosa da programação funcional: a separação entre a definição de uma computação e sua execução.</p>
<p>Formalmente, um <code>TaskEither&lt;E, A&gt;</code> é definido como <code>() =&gt; Promise&lt;E.Either&lt;E, A&gt;&gt;</code> - uma função que retorna uma Promise que resolverá para um <code>E.Either&lt;E, A&gt;</code>. Esta definição torna explícito que <code>TaskEither</code> é lazy: a computação só é executada quando a função é chamada.</p>
<p>Esta separação oferece benefícios significativos. Primeiro, permite compor operações complexas de forma declarativa, construindo um pipeline de transformações antes de qualquer execução. Segundo, facilita o teste unitário, já que você pode inspecionar e manipular a descrição da computação sem disparar efeitos colaterais. Terceiro, proporciona otimizações como <a href="https://en.wikipedia.org/wiki/Lazy_evaluation">lazy evaluation (avaliação preguiçosa)</a>, onde computações são executadas apenas quando realmente necessárias.</p>
<p>A execução real só ocorre no que chamamos de &ldquo;fim do mundo&rdquo; - o momento em que você efetivamente precisa do resultado ou do efeito colateral. Isso acontece em duas etapas: primeiro, quando usamos <code>matchE</code> (ou outros combinadores finais como <code>getOrElseEW</code>) para &ldquo;consumir&rdquo; o <code>TaskEither</code> e transformá-lo em uma <code>Task</code> (que é essencialmente uma função <code>() =&gt; Promise&lt;A&gt;</code>); e depois, quando chamamos <code>await programa()</code> para executar essa <code>Task</code> e obter o resultado final. Este modelo de execução adiada dá ao desenvolvedor controle preciso sobre quando e como os efeitos ocorrem, tornando o código mais previsível e facilitando o raciocínio sobre o fluxo de dados, especialmente em aplicações complexas com múltiplas operações assíncronas interdependentes.</p>
<h2 id="taskoption-quando-a-ausência-é-esperada">TaskOption: Quando a Ausência é Esperada</h2>
<p>Além do <code>TaskEither</code>, o fp-ts oferece <code>TaskOption</code> para cenários onde a ausência de valor é um resultado esperado em operações assíncronas. Diferente do <code>TaskEither</code>, que modela falhas como erros, o <code>TaskOption</code> é ideal quando &ldquo;não encontrado&rdquo; é um estado válido da aplicação.</p>


  <pre><code class="language-typescript">import * as TO from &#39;fp-ts/TaskOption&#39;;

// Buscar usuário que pode não existir
const buscarUsuarioOpcional = (id: number): TO.TaskOption&lt;UserData&gt; =&gt;
  TO.tryCatch(async () =&gt; {
    const response = await fetch(`/api/users/${id}`);
    if (response.status === 404) {
      return TO.none; // Usuário não encontrado - estado válido
    }
    if (!response.ok) {
      throw new Error(`Erro de rede: ${response.status}`);
    }
    return TO.some(await response.json());
  }, (e: unknown) =&gt; new Error(String(e)));

// Uso: a ausência é tratada como um caso normal
const programa = pipe(
  buscarUsuarioOpcional(123),
  TO.match(
    () =&gt; console.log(&#34;Usuário não encontrado&#34;), // Caso normal
    (user) =&gt; console.log(`Usuário: ${user.name}`)
  )
);</code></pre>
 <p>Use <code>TaskOption</code> quando a ausência de valor é um resultado esperado (como buscas que podem não retornar resultados), e <code>TaskEither</code> quando a ausência representa uma falha real da operação.</p>
<h2 id="interoperabilidade-com-promises">Interoperabilidade com Promises</h2>
<p>O fp-ts 3.x oferece funções especializadas para interoperar com código baseado em Promises existente. Em vez de usar <code>tryCatch</code> manualmente para envolver funções que já retornam <code>Promise</code>, prefira <code>tryCatchK</code> ou <code>fromPromise</code>:</p>


  <pre><code class="language-typescript">import * as TE from &#39;fp-ts/TaskEither&#39;;

// ❌ Evite: tryCatch manual para funções que já retornam Promise
const fetchUserManual = (id: number): TE.TaskEither&lt;Error, UserData&gt; =&gt;
  TE.tryCatch(
    () =&gt; fetchUserPromise(id), // Função que já retorna Promise&lt;UserData&gt;
    (e) =&gt; new Error(String(e))
  );

// ✅ Prefira: tryCatchK para funções que já retornam Promise
const fetchUserK = (id: number): TE.TaskEither&lt;Error, UserData&gt; =&gt;
  TE.tryCatchK(
    fetchUserPromise, // Passa a função diretamente
    (e: unknown) =&gt; new Error(String(e))
  )(id);

// ✅ Ou use fromPromise para conversão direta
const fetchUserFromPromise = (id: number): TE.TaskEither&lt;Error, UserData&gt; =&gt;
  TE.fromPromise(
    fetchUserPromise(id),
    (e: unknown) =&gt; new Error(String(e))
  );</code></pre>
 <p>Estas funções são mais idiomáticas e type-safe, especialmente quando você está integrando bibliotecas existentes que já trabalham com Promises.</p>
<blockquote>
<p><strong>Nota:</strong> A abordagem lazy evaluation é uma técnica que adia a execução de uma computação até que seu resultado seja realmente necessário. Em outras palavras, a computação não é executada imediatamente, mas apenas quando realmente precisamos do resultado. Isso pode ser benéfico em situações onde a computação é cara (em termos de tempo ou recursos) e não é necessária imediatamente.</p></blockquote>
<p>Um dos benefícios mais profundos de <code>Either</code> e <code>TaskEither</code> é como eles tornam os efeitos colaterais explícitos no sistema de tipos. Em programação funcional, um &ldquo;efeito colateral&rdquo; é qualquer interação com o mundo externo: leitura/escrita de arquivos, chamadas de rede, acesso a banco de dados, ou qualquer operação que possa falhar por razões fora do controle do programa.</p>
<p>Tradicionalmente, esses efeitos são &ldquo;invisíveis&rdquo; na assinatura das funções em TypeScript/JavaScript. Uma função que faz uma chamada HTTP não indica isso em seu tipo de retorno, e as exceções que podem ocorrer não são capturadas pelo sistema de tipos. Isso cria um &ldquo;vazamento&rdquo; onde efeitos potencialmente perigosos escapam da análise estática.</p>
<h2 id="compondo-múltiplas-requisições-assíncronas">Compondo Múltiplas Requisições Assíncronas</h2>
<p>O <code>fp-ts</code> brilha na composição. Se precisarmos de dados de múltiplas fontes, podemos encadear operações de forma elegante e segura, como demonstrado nos exemplos acima. Por fim, a biblioteca fp-ts oferece uma abordagem robusta para lidar com erros em TypeScript, transformando o tratamento de exceções tradicional em um fluxo de dados previsível e tipado.</p>
<p>Ao adotar esses padrões funcionais, conseguimos criar código mais confiável, testável e manutenível, onde os erros são tratados como cidadãos de primeira classe em vez de casos excepcionais. Essa mudança de paradigma não apenas melhora a qualidade do código, mas também proporciona uma experiência de desenvolvimento mais agradável, onde a composição de operações complexas se torna natural e o sistema de tipos trabalha a nosso favor para garantir que todos os casos de erro sejam devidamente considerados.</p>
<p>Vamos ver um exemplo prático de como usar <code>TaskEither</code> para buscar um post e depois seus comentários:</p>


  <pre><code class="language-typescript">import * as TE from &#39;fp-ts/TaskEither&#39;;
import * as A from &#39;fp-ts/Array&#39;;
// pipe já importado de &#39;fp-ts/function&#39;

// Suponha que fetchPost retorne TaskEither&lt;NetworkError, PostData&gt;
// e fetchComments retorne TaskEither&lt;NetworkError, CommentData[]&gt; para um postId

// Exemplo fictício:
interface PostData { id: number; title: string; userId: number; }
interface CommentData { id: number; body: string; postId: number; }

const fetchPost = (postId: number): TE.TaskEither&lt;NetworkError, PostData&gt; =&gt;
  TE.tryCatch(
    async () =&gt; {
      const res = await fetch(`https://jsonplaceholder.typicode.com/posts/${postId}`);
      if (!res.ok) throw new NetworkError(`Post não encontrado: ${res.status}`, res.status);
      return res.json();
    },
    (r: unknown): NetworkError =&gt; new NetworkError(String(r))
  );

const fetchCommentsForPost = (postId: number): TE.TaskEither&lt;NetworkError, CommentData[]&gt; =&gt;
  TE.tryCatch(
    async () =&gt; {
      const res = await fetch(`https://jsonplaceholder.typicode.com/posts/${postId}/comments`);
      if (!res.ok) throw new NetworkError(`Comentários não encontrados: ${res.status}`, res.status);
      return res.json();
    },
    (r: unknown): NetworkError =&gt; new NetworkError(String(r))
  );


// Objetivo: buscar um post e depois seus comentários
const getPostWithComments = (postId: number): TE.TaskEither&lt;NetworkError, { post: PostData; comments: CommentData[] }&gt; =&gt;
  pipe(
    fetchPost(postId), // TaskEither&lt;NetworkError, PostData&gt;
    TE.chain(post =&gt; // Se fetchPost deu certo, &#39;post&#39; é PostData
      pipe(
        fetchCommentsForPost(post.id), // TaskEither&lt;NetworkError, CommentData[]&gt;
        TE.map(comments =&gt; ({ post, comments })) // Se fetchComments deu certo, combina os resultados
      )
    )
  );

// Para buscar vários posts e seus comentários em paralelo (cuidado com limites de API):
const getUserIds = [1, 2, 3]; // IDs de posts, por exemplo

// Criamos um array de TaskEithers, cada um buscando um usuário
const fetchAllUsersPrograms: Array&lt;TE.TaskEither&lt;NetworkError, UserData&gt;&gt; = getUserIds.map(fetchUser);

// TE.sequenceArray transforma Array&lt;TaskEither&lt;E, A&gt;&gt; em TaskEither&lt;E, Array&lt;A&gt;&gt;
// Ele executa todas as Tasks em paralelo. Se qualquer uma falhar, o resultado é o primeiro Left.
// ⚠️ Importante: sequenceArray só funciona com tipos homogêneos (todos retornam o mesmo tipo A).
// Se os tipos de retorno diferem, use sequenceT ou mapeie para uma união A | B.
// Nota: Em fp-ts 3.x, sequenceArray pode requerer import específico
const allUsersProgram: TE.TaskEither&lt;NetworkError, UserData[]&gt; = pipe(
  fetchAllUsersPrograms,
  TE.sequenceArray
);

async function processarUsuarios() {
  const resultado = await pipe(
    allUsersProgram,
    TE.map(users =&gt; users.map(u =&gt; u.name)), // Extrai apenas os nomes se tudo der certo
    TE.matchE(
      (erro) =&gt; async () =&gt; `Falha ao buscar usuários: ${erro.message}`,
      (nomes) =&gt; async () =&gt; `Nomes dos usuários: ${nomes.join(&#39;, &#39;)}`
    )
  )(); // Executa e obtém a string final
  console.log(resultado);
}

// processarUsuarios();</code></pre>
 <p>Note que o <code>TE.sequenceArray</code> (e seu análogo <code>A.sequence(TE.ApplicativePar)</code>) é poderoso para paralelizar operações falíveis. Adotar <code>Either</code>, <code>Option</code> e <code>TaskEither</code> traz benefícios significativos como type safety explícito e previsibilidade. O compilador se torna seu aliado, forçando você a lidar com todos os caminhos possíveis, enquanto as falhas são tratadas como valores esperados no fluxo de dados, eliminando surpresas como <code>Uncaught Error</code> que interrompem sua aplicação.</p>
<p>A adoção desses tipos funcionais resulta em código mais limpo e declarativo, reduzindo drasticamente a necessidade de <code>try/catch</code> aninhados e condicionais. O fluxo de dados se torna mais claro com operações como <code>pipe</code>, <code>map</code> e <code>chain</code>, enquanto funções que retornam <code>Either</code> ou <code>TaskEither</code> podem ser encadeadas de forma segura e elegante, com propagação automática de erros que simplifica lógicas complexas.</p>
<p>Além disso, a manutenção e evolução do código se tornam mais robustas, pois alterar ou adicionar etapas em um fluxo de processamento é mais seguro quando o sistema de tipos garanta que todos os casos de erro sejam considerados. A testabilidade também é aprimorada, já que funções puras que retornam <code>Either</code> são mais fáceis de testar unitariamente por não dependerem de mecanismos de exceção globais, contribuindo para uma base de código mais confiável e sustentável.</p>
<h2 id="trade-offs">Trade-offs</h2>
<p>Embora as abstrações funcionais como <code>Either</code> e <code>TaskEither</code> ofereçam benefícios significativos para o tratamento de erros, é importante considerar alguns trade-offs, especialmente em relação à performance:</p>
<ul>
<li>
<p><strong>Overhead de Alocação:</strong> cada <code>Either</code>, <code>TaskEither</code> ou <code>Option</code> cria estruturas de dados adicionais na memória. Em <a href="https://en.wikipedia.org/wiki/Hot_path">hot paths</a> de aplicações que processam grandes volumes de dados, esse overhead de alocação pode se tornar perceptível. Comparado com abordagens mais diretas como verificações de <code>null</code> ou <code>try/catch</code>, há um custo adicional de memória.</p>
</li>
<li>
<p><strong>Micro-overhead em Operações Assíncronas:</strong> Benchmarks mostram que <code>TaskEither</code> adiciona aproximadamente <strong>3-5 µs por operação</strong> em comparação com Promises nativas. Este overhead é geralmente insignificante para a maioria das aplicações (uma operação de rede típica leva 50-200ms), mas pode ser relevante em sistemas com milhares de operações por segundo ou requisitos extremos de baixa latência.</p>
</li>
<li>
<p><strong>Curva de Aprendizado:</strong> a programação funcional e seus tipos algebráicos têm uma curva de aprendizado significativa para equipes acostumadas com paradigmas imperativos. Isso pode reduzir temporariamente a produtividade até que a equipe esteja confortável com conceitos como functors, monads e composição de funções.</p>
</li>
<li>
<p><strong>Pilha de Chamadas e Debugging:</strong> em cadeias longas de operações com <code>pipe</code> e <code>chain</code>, os stacktraces podem se tornar mais difíceis de interpretar quando ocorrem erros. Isso pode complicar o debugging em comparação com código imperativo mais direto. Para mitigar esse problema, é recomendável usar <code>mapError</code> ou <code>bimap</code> para enriquecer erros com contexto adicional em pontos estratégicos da cadeia.</p>
</li>
<li>
<p><strong>Tamanho do Bundle:</strong> a inclusão da biblioteca <code>fp-ts</code> adiciona peso ao bundle final da aplicação. Embora técnicas de <a href="https://en.wikipedia.org/wiki/Tree_shaking">tree-shaking</a> possam mitigar isso, aplicações que priorizam tamanho mínimo de bundle (como PWAs ou aplicações móveis) precisam considerar esse impacto.</p>
</li>
</ul>
<hr>
<h2 id="referências">Referências</h2>
<ul>
<li><a href="https://github.com/gcanti/fp-ts">fp-ts</a> - Documentação oficial</li>
<li><a href="https://a.co/d/3LxV0CO">Hands-On Functional Programming with Typescript</a> - Livro do Remo H. Jansen publicado pela Packt. O livro aborda conceitos fundamentais para o tratamento funcional de erros, discutindo na seção &ldquo;side-effects&rdquo; como podemos usar técnicas de programação funcional para tornar explícitos os efeitos colaterais em TypeScript.</li>
</ul>
]]></content:encoded>
      
      
      <category>javascript,typescript,fp-ts,programação funcional,tratamento de erros,desenvolvimento</category>
      
      
      
      <dc:creator>Vitor Lobo Ramos</dc:creator>
      
      
      
      
      
      <description>&lt;![CDATA[Usando fp-ts para gerenciar erros de forma robusta e tipada]]></description>
      
    </item>
    
    <item>
      <title>01 - RAG Simples com Clojure e Ollama</title>
      <link>https://scovl.github.io/2025/03/23/rag/</link>
      <guid>https://scovl.github.io/2025/03/23/rag/</guid>
      <pubDate>Sun, 23 Mar 2025 19:00:00 &#43;0000</pubDate>
      <description>&lt;![CDATA[<h2 id="introdução">Introdução</h2>
<p>Olá, pessoal! 👋</p>
<p>Neste artigo, vamos explorar como construir uma aplicação <a href="https://pt.wikipedia.org/wiki/Gera%C3%A7%C3%A3o_aumentada_por_recupera%C3%A7%C3%A3o">RAG (Retrieval-Augmented Generation)</a> completa do zero usando <a href="https://clojure.org/">Clojure</a>. Vamos mergulhar em uma implementação prática que combina processamento de texto, busca semântica e geração de respostas com LLMs locais. Se você está interessado em melhorar a precisão e relevância das respostas dos seus modelos de linguagem com informações atualizadas, este guia é para você!</p>
<h2 id="fundamentos-do-rag">Fundamentos do RAG</h2>
<h3 id="o-que-é-rag">O que é RAG?</h3>
<p>Os Modelos de Linguagem de Grande Escala (<a href="https://en.wikipedia.org/wiki/Large_language_model">LLMs</a>), como o <a href="https://openai.com/api/">GPT</a>, <a href="https://openai.com/api/">ChatGPT</a> e outros, revolucionaram a forma como interagimos com a inteligência artificial. Eles são capazes de gerar textos coerentes, responder perguntas complexas e até mesmo criar conteúdo criativo. No entanto, esses modelos possuem uma limitação fundamental: seu conhecimento é &ldquo;congelado&rdquo; no tempo.</p>]]></description>
      <content:encoded>&lt;![CDATA[<h2 id="introdução">Introdução</h2>
<p>Olá, pessoal! 👋</p>
<p>Neste artigo, vamos explorar como construir uma aplicação <a href="https://pt.wikipedia.org/wiki/Gera%C3%A7%C3%A3o_aumentada_por_recupera%C3%A7%C3%A3o">RAG (Retrieval-Augmented Generation)</a> completa do zero usando <a href="https://clojure.org/">Clojure</a>. Vamos mergulhar em uma implementação prática que combina processamento de texto, busca semântica e geração de respostas com LLMs locais. Se você está interessado em melhorar a precisão e relevância das respostas dos seus modelos de linguagem com informações atualizadas, este guia é para você!</p>
<h2 id="fundamentos-do-rag">Fundamentos do RAG</h2>
<h3 id="o-que-é-rag">O que é RAG?</h3>
<p>Os Modelos de Linguagem de Grande Escala (<a href="https://en.wikipedia.org/wiki/Large_language_model">LLMs</a>), como o <a href="https://openai.com/api/">GPT</a>, <a href="https://openai.com/api/">ChatGPT</a> e outros, revolucionaram a forma como interagimos com a inteligência artificial. Eles são capazes de gerar textos coerentes, responder perguntas complexas e até mesmo criar conteúdo criativo. No entanto, esses modelos possuem uma limitação fundamental: seu conhecimento é &ldquo;congelado&rdquo; no tempo.</p>


  
  <div class="mermaid">graph TD
    A[LLM Treinado] --&gt; B[Data de Corte]
    B --&gt; C[Conhecimento Congelado]
    C --&gt; D[Limitações]
    D --&gt; E[Não sabe eventos recentes]
    D --&gt; F[Não tem dados atualizados]
    D --&gt; G[Não conhece novas tecnologias]</div>
 <h3 id="por-que-precisamos-do-rag">Por que precisamos do RAG?</h3>
<p>Ao desenvolver aplicações inteligentes, como assistentes financeiros que precisam de cotações de ações em tempo real, chatbots de suporte que devem conhecer os produtos mais recentes da empresa ou sistemas de recomendação que se baseiam nas últimas tendências, nos deparamos com uma limitação crucial dos Modelos de Linguagem de Grande Escala (<a href="https://en.wikipedia.org/wiki/Large_language_model">LLMs</a>) tradicionais: seu conhecimento estático.</p>
<p>O problema fundamental reside no fato de que esses modelos, por mais sofisticados que sejam, possuem uma base de conhecimento &ldquo;congelada&rdquo; no momento de seu treinamento. Eles carecem de acesso inerente a informações atualizadas, o que restringe drasticamente sua aplicabilidade em cenários que exigem dados em tempo real ou conhecimento sobre eventos recentes.</p>
<blockquote>
<p>Confiar exclusivamente em um <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM &ldquo;puro&rdquo;</a> nesses contextos resultará em respostas desatualizadas, potencialmente imprecisas e, consequentemente, em uma experiência do usuário comprometida. A eficácia da aplicação é diretamente afetada.</p></blockquote>
<h3 id="os-três-pilares-do-rag">Os Três Pilares do RAG</h3>


  
  <div class="mermaid">graph LR
    A[RAG] --&gt; B[Base de Dados Atual]
    A --&gt; C[Pesquisa em Tempo Real]
    A --&gt; D[Combinação de Conhecimento]
    
    B --&gt; E[Documentos Atualizados]
    B --&gt; F[Dados em Tempo Real]
    
    C --&gt; G[Busca Ativa]
    C --&gt; H[Seleção de Informações]
    
    D --&gt; I[Integração com LLM]
    D --&gt; J[Contextualização]</div>
 <ol>
<li><strong>Conexão com uma base de dados atual:</strong> Em vez de depender apenas do conhecimento estático adquirido durante seu treinamento (que pode se tornar obsoleto rapidamente), o <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a> ganha acesso a uma fonte de informações dinâmica e constantemente atualizada. Isso pode ser uma base de dados de notícias, um repositório de documentos corporativos, uma coleção de artigos científicos, ou qualquer outra fonte relevante para a tarefa em questão.</li>
<li><strong>Pesquisa em tempo real:</strong> O <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a> não está mais limitado a &ldquo;lembrar&rdquo; de informações. Ele adquire a capacidade de &ldquo;procurar&rdquo; ativamente por dados relevantes para responder a uma pergunta ou gerar um texto. Isso é semelhante a como nós, humanos, usamos mecanismos de busca para encontrar informações que não temos memorizadas. O <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a>, equipado com RAG, pode formular consultas, analisar os resultados e selecionar as informações mais pertinentes.</li>
<li><strong>Combinação de conhecimento base com dados novos:</strong> Este é o ponto crucial que diferencia o <a href="https://pt.wikipedia.org/wiki/Gera%C3%A7%C3%A3o_aumentada_por_recupera%C3%A7%C3%A3o">RAG</a> de uma simples busca em uma base de dados. O <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a> não apenas recupera informações, mas também as integra ao seu conhecimento pré-existente. Ele usa sua capacidade de raciocínio e compreensão para contextualizar os novos dados, identificar contradições, e formular respostas coerentes e informadas.</li>
</ol>
<h3 id="rag-em-produção">RAG em Produção</h3>
<p>Sistemas RAG em produção frequentemente incluem etapas adicionais para melhorar a precisão: <strong>re-ranking</strong> (onde um modelo especializado re-avalia a relevância dos documentos recuperados) e <strong>merge-rerank</strong> (que combina resultados de múltiplas estratégias de busca como semântica, lexical e híbrida). Essas técnicas aumentam significativamente a qualidade das respostas, mas adicionam complexidade ao sistema.</p>
<blockquote>
<p><strong>Nota</strong>: Nossa implementação atual usa apenas busca semântica simples com TF-IDF, focando na compreensão dos fundamentos do RAG. Para aplicações em produção, considere implementar essas técnicas avançadas.</p></blockquote>
<p>Segundo um <a href="https://arxiv.org/abs/2309.01066">whitepaper recente dos pesquisadores do Google</a>, existem várias técnicas para turbinar o desempenho dos <a href="https://en.wikipedia.org/wiki/Large_language_model">LLMs</a>, e o RAG é uma das mais promissoras. Isso ocorre porque o RAG aborda algumas das limitações fundamentais desses modelos:</p>
<p>O RAG resolve vários problemas de uma vez só: diminui aquelas &ldquo;viagens&rdquo; dos <a href="https://en.wikipedia.org/wiki/Large_language_model">LLMs</a> quando inventam respostas (as famosas alucinações), mantém tudo atualizado em vez de ficar preso no passado, deixa as respostas mais transparentes porque você sabe de onde veio a informação, e ainda melhora o desempenho do modelo quando ele precisa lidar com documentos ou dados específicos da sua empresa. É como dar ao modelo um Google particular que ele pode consultar antes de responder!</p>
<blockquote>
<p>O RAG representa um avanço significativo na evolução dos <a href="https://en.wikipedia.org/wiki/Large_language_model">LLMs</a>, permitindo que eles se tornem ferramentas mais confiáveis, precisas e úteis para uma ampla gama de aplicações. Ele transforma o <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a> de um &ldquo;sabe-tudo&rdquo; desatualizado em um pesquisador ágil e bem-informado, capaz de combinar conhecimento profundo com informações atualizadas em tempo real.</p></blockquote>
<h3 id="por-que-o-deepseek-r1">Por que o DeepSeek R1?</h3>
<p>Ele trabalha muito bem com documentação técnica, o que é perfeito para nosso sistema <a href="https://pt.wikipedia.org/wiki/Gera%C3%A7%C3%A3o_aumentada_por_recupera%C3%A7%C3%A3o">RAG</a> focado em docs técnicas. O DeepSeek R1 consegue equilibrar qualidade e velocidade melhor que outros modelos do Ollama, rodando na sua máquina sem ficar alucinando com respostas que não fazem sentido.</p>
<p>O modelo também se dá super bem com várias linguagens de programação, incluindo <a href="https://clojure.org/">Clojure</a>, então ele responde numa boa sobre implementações técnicas e documentação de código. E o melhor: mesmo quando você joga informações pela metade ou todas bagunçadas, ele ainda consegue manter o contexto e dar respostas que fazem sentido. Por isso ele é perfeito para o que estamos construindo!</p>
<h2 id="implementação-prática">Implementação Prática</h2>
<h3 id="preparando-o-ambiente">Preparando o Ambiente</h3>
<p>Pre-requisitos:</p>
<ul>
<li><a href="https://clojure.org/guides/getting_started">Clojure</a>: Linguagem de programação funcional que vamos usar para construir a aplicação</li>
<li><a href="https://leiningen.org/">Leiningen</a>: Ferramenta de build para Clojure</li>
<li><a href="https://ollama.com/">Ollama</a>: Modelo de linguagem local</li>
</ul>
<h3 id="estrutura-do-projeto">Estrutura do Projeto</h3>
<p>Nossa aplicação terá três componentes principais:</p>
<ol>
<li><strong>Processamento de documentação (Markdown/HTML)</strong>
<ul>
<li>Extração de texto</li>
<li>Pré-processamento de texto</li>
</ul>
</li>
<li><strong>Sistema de embeddings</strong>
<ul>
<li>Criação de embeddings para o texto usando <a href="https://pt.wikipedia.org/wiki/TF-IDF">TF-IDF</a></li>
<li>Busca por similaridade semântica</li>
</ul>
</li>
<li><strong>Interface com o LLM</strong>
<ul>
<li>Geração de resposta usando o LLM</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>Observação:</strong> Embora o RAG moderno utilize embeddings densos gerados por modelos de linguagem para capturar a semântica de forma mais rica, neste artigo, usaremos uma implementação simplificada de <a href="https://pt.wikipedia.org/wiki/TF-IDF">TF-IDF (Term Frequency-Inverse Document Frequency)</a> como <strong>prova de conceito</strong>. Para aplicações em produção, recomendamos fortemente o uso de embeddings densos.</p></blockquote>
<h3 id="tf-idf">TF-IDF</h3>
<p>O <a href="https://pt.wikipedia.org/wiki/TF-IDF">TF-IDF</a> (Term Frequency-Inverse Document Frequency) é uma técnica estatística usada para avaliar a importância de uma palavra em um documento, em relação a uma coleção de documentos. Vamos entender como funciona:</p>
<ol>
<li>
<p><strong>Term Frequency (TF)</strong>: Mede a frequência de uma palavra em um documento.</p>


  <pre><code class="language-">TF(termo) = (Número de vezes que o termo aparece no documento) / (Total de termos no documento)</code></pre>
 </li>
<li>
<p><strong>Inverse Document Frequency (IDF)</strong>: Mede a raridade de um termo na coleção de documentos.</p>


  <pre><code class="language-">IDF(termo) = log(Número total de documentos / Número de documentos contendo o termo)</code></pre>
 </li>
<li>
<p><strong>TF-IDF</strong>: É o produto desses dois valores.</p>


  <pre><code class="language-">TF-IDF(termo) = TF(termo) × IDF(termo)</code></pre>
 </li>
</ol>
<p>Vamos imaginar um cenário prático com três documentos técnicos:</p>
<ul>
<li>Doc1: &ldquo;Clojure é uma linguagem funcional baseada em <a href="https://en.wikipedia.org/wiki/Lisp_%28programming_language%29">Lisp</a>&rdquo;</li>
<li>Doc2: &ldquo;Python é uma linguagem de programação versátil&rdquo;</li>
<li>Doc3: &ldquo;Clojure e Python são linguagens de programação populares&rdquo;</li>
</ul>
<p>O TF-IDF é uma técnica que nos ajuda a identificar quais palavras são mais importantes em cada documento, comparando a frequência de um termo no documento (TF) com a raridade desse termo em toda a coleção (IDF). Por exemplo, se &ldquo;Clojure&rdquo; aparece uma vez em um documento de oito palavras, seu TF é 0,125; como está presente em dois de três documentos, seu IDF é log(3/2) ≈ 0,176, resultando em um TF-IDF de aproximadamente 0,022. Já termos muito comuns, como &ldquo;linguagem&rdquo;, acabam com TF-IDF zero, pois não ajudam a diferenciar os documentos.</p>
<p>Esse método é fundamental em sistemas de busca, pois destaca os termos que realmente caracterizam cada texto. No contexto do RAG, o TF-IDF permite indexar e encontrar rapidamente os documentos mais relevantes para uma consulta, servindo como uma base simples e eficiente para recuperação de informações, que pode ser aprimorada com técnicas mais avançadas como embeddings densos.</p>
<h3 id="requisitos-mínimos">Requisitos Mínimos</h3>
<p>Este experimento funciona com hardware básico: <strong>4 cores de CPU e 8GB de RAM</strong> são suficientes. Para máquinas mais lentas, use <code>ollama pull deepseek-r1:3b</code> (versão otimizada).</p>
<blockquote>
<p>Para requisitos detalhados de produção e otimizações avançadas, consulte o <a href="/2025/03/23/rag/#requisitos-de-hardware-detalhados">apêndice de hardware</a> ao final do artigo.</p></blockquote>
<h3 id="configuração-do-projeto">Configuração do Projeto</h3>
<ol>
<li>Crie um novo projeto Clojure:</li>
</ol>


  <pre><code class="language-bash">lein new app docai
cd docai</code></pre>
 <ol start="2">
<li>Configure o <code>project.clj</code>:</li>
</ol>


  <pre><code class="language-clojure">(defproject docai &#34;0.1.0-SNAPSHOT&#34;
  :description &#34;Um assistente RAG para consulta de documentação técnica&#34;
  :url &#34;http://example.com/FIXME&#34;
  :license {:name &#34;EPL-2.0 OR GPL-2.0-or-later WITH Classpath-exception-2.0&#34;
            :url &#34;https://www.eclipse.org/legal/epl-2.0/&#34;}
  :dependencies [[org.clojure/clojure &#34;1.11.1&#34;]
                 [markdown-to-hiccup &#34;0.6.2&#34;]
                 [hickory &#34;0.7.1&#34;]
                 [org.clojure/data.json &#34;2.4.0&#34;]
                 [http-kit &#34;2.6.0&#34;]
                 [org.clojure/tools.logging &#34;1.2.4&#34;]
                 [org.clojure/tools.namespace &#34;1.4.4&#34;]
                 [org.clojure/core.async &#34;1.6.681&#34;]
                 [org.clojure/core.memoize &#34;1.0.257&#34;]
                 [org.clojure/core.cache &#34;1.0.225&#34;]]
  :main ^:skip-aot docai.core
  :target-path &#34;target/%s&#34;
  :profiles {:uberjar {:aot :all
                       :jvm-opts [&#34;-Dclojure.compiler.direct-linking=true&#34;]}})</code></pre>
 <p>A estrutura do projeto acima define um aplicativo Clojure para RAG (Retrieval-Augmented Generation) com várias dependências essenciais. Entre elas, <code>markdown-to-hiccup</code> e <code>hickory</code> são usadas para processar documentos em Markdown e HTML, enquanto <code>data.json</code> e <code>http-kit</code> facilitam a comunicação com APIs externas, como a do Ollama. Além disso, <code>tools.logging</code> é responsável pelo registro de eventos e logs, e <code>tools.namespace</code> auxilia no gerenciamento de namespaces do projeto.</p>
<p>Já <code>core.async</code> permite operações assíncronas, o que é especialmente útil ao lidar com o processamento de documentos grandes. Por fim, <code>core.memoize</code> e <code>core.cache</code> são utilizados para implementar cache de resultados, como embeddings ou respostas do LLM, melhorando significativamente a performance ao evitar recálculos desnecessários, principalmente em consultas repetidas ou similares.</p>
<h3 id="implementação-dos-componentes">Implementação dos Componentes</h3>
<p>Agora vamos implementar os três componentes principais do nosso sistema RAG e vamos começar com o processamento de documentos. Pois, ele é o ponto de entrada para o RAG onde vamos processar os documentos e extrair o texto para ser usado nos outros componentes.</p>
<h4 id="processamento-de-documentos">Processamento de Documentos</h4>


  <pre><code class="language-clojure">;; src/docai/document.clj
(ns docai.document
  (:require [markdown-to-hiccup.core :as md]
            [hickory.core :as html]
            [clojure.string :as str]))

(defn extract-text-from-markdown [content]
  (try
    (let [hiccup-result (md/md-&gt;hiccup content)
          text-nodes (filter string? (flatten hiccup-result))]
      text-nodes)
    (catch Exception e
      (println &#34;Erro ao processar Markdown:&#34; (.getMessage e))
      [content])))

(defn extract-text-from-html [content]
  (try
    (let [dom (html/parse content)
          hiccup-result (html/as-hiccup dom)
          text-nodes (filter string? (flatten hiccup-result))]
      text-nodes)
    (catch Exception e
      (println &#34;Erro ao processar HTML:&#34; (.getMessage e))
      [content])))

;; Declare functions that will be defined later
(declare create-token-aware-chunks)

(defn extract-text
  &#34;Extrai texto de documentação (Markdown ou HTML)&#34;
  [doc-path]
  (println &#34;Extraindo texto de:&#34; doc-path)
  (let [content (slurp doc-path)
        _ (println &#34;Tamanho do conteúdo:&#34; (count content) &#34;caracteres&#34;)
        _ (println &#34;Amostra do conteúdo:&#34; (subs content 0 (min 100 (count content))))
        text (if (.endsWith doc-path &#34;.md&#34;)
               (extract-text-from-markdown content)
               (extract-text-from-html content))
        _ (println &#34;Quantidade de nós de texto extraídos:&#34; (count text))
        ;; Usar tokens reais em vez de caracteres para chunking preciso
        chunks (create-token-aware-chunks text 512)]
    (println &#34;Quantidade de chunks gerados:&#34; (count chunks))
    chunks))

(defn count-tokens
  &#34;Conta tokens usando heurística (para desenvolvimento)&#34;
  [text]
  ;; ⚠️ ATENÇÃO: Esta é uma heurística aproximada
  ;; Para produção, use [clojure-tiktoken](https://github.com/justone/clojure-tiktoken)
  ;; ou API do Ollama para contagem precisa
  (try
    (let [words (str/split text #&#34;\s&#43;&#34;)
          ;; Estimativa melhorada para português brasileiro
          ;; Ainda pode errar 2x em textos muito curtos/longos
          estimated-tokens (reduce &#43; 
                                 (map (fn [word]
                                        (cond
                                          ;; Palavras muito longas (composição)
                                          (&gt; (count word) 15) (* (count word) 0.8)
                                          ;; Palavras longas (derivação)
                                          (&gt; (count word) 10) (* (count word) 0.6)
                                          ;; Palavras médias
                                          (&gt; (count word) 5) (* (count word) 0.4)
                                          ;; Palavras curtas
                                          :else 1.0))
                                      words))]
      (int estimated-tokens))
    (catch Exception e
      (println &#34;Erro ao contar tokens:&#34; (.getMessage e))
      ;; Fallback conservador: 1 token por caractere
      (count text))))

(defn create-token-aware-chunks
  &#34;Cria chunks baseados em tokens reais, não caracteres&#34;
  [text-nodes max-tokens]
  (loop [nodes text-nodes
         current-chunk []
         current-tokens 0
         all-chunks []]
    (if (empty? nodes)
      (if (seq current-chunk)
        (conj all-chunks (str/join &#34; &#34; current-chunk))
        all-chunks)
      (let [node (first nodes)
            node-tokens (count-tokens node)
            new-total (&#43; current-tokens node-tokens)]
        (if (and (&gt; new-total max-tokens) (seq current-chunk))
          ;; Chunk cheio, salva e inicia novo
          (recur (rest nodes)
                 [node]
                 node-tokens
                 (conj all-chunks (str/join &#34; &#34; current-chunk)))
          ;; Adiciona ao chunk atual
          (recur (rest nodes)
                 (conj current-chunk node)
                 new-total
                 all-chunks))))))

(defn preprocess-chunks
  &#34;Limpa e prepara os chunks de texto&#34;
  [chunks]
  (let [processed (map #(-&gt; %
                            (str/replace #&#34;\s&#43;&#34; &#34; &#34;)
                            (str/trim))
                       chunks)]
    (println &#34;Primeiro chunk processado:&#34; (first processed))
    processed))</code></pre>
 <p>Este trecho de código implementa a parte de processamento de documentos do nosso sistema RAG. Basicamente, ele pega arquivos Markdown ou HTML e extrai o texto puro deles para que possamos usar depois na busca semântica. O código usa bibliotecas como <code>markdown-to-hiccup</code> e <code>hickory</code> para converter os documentos em estruturas de dados que facilitam a extração do texto.</p>


  
  <div class="mermaid">graph TD
    A[Documento] --&gt; B{É Markdown?}
    B --&gt;|Sim| C[Processa Markdown]
    B --&gt;|Não| D[Processa HTML]
    C --&gt; E[Extrai Texto]
    D --&gt; E
    E --&gt; F[Divide em Chunks]
    F --&gt; G[Limpa e Formata]
    G --&gt; H[Chunks Prontos]</div>
 <p>O fluxo é bem direto: primeiro verificamos se estamos lidando com Markdown ou HTML, depois extraímos o texto usando a função apropriada, dividimos em pedaços menores (chunks) baseados em tokens reais (não caracteres), e finalmente limpamos esses chunks removendo espaços extras e formatando tudo direitinho.</p>
<p>O código também inclui bastante logging para ajudar a depurar o processo, mostrando informações como o tamanho do documento, quantidade de texto extraído e número de chunks gerados.</p>
<p>Essa abordagem de dividir o texto em pedaços menores é crucial para o RAG, já que permite processar documentos grandes sem sobrecarregar o modelo de linguagem.</p>
<blockquote>
<p><strong>Importante</strong>: Dividimos o texto em chunks usando tokens (não caracteres) para não ultrapassar o limite do modelo. A contagem de tokens é aproximada. Para produção, use uma biblioteca como <a href="https://github.com/justone/clojure-tiktoken">clojure-tiktoken</a> para maior precisão.</p></blockquote>
<h4 id="sistema-de-embeddings">Sistema de Embeddings</h4>
<p>Agora vamos implementar o sistema de embeddings. Ele é responsável por criar embeddings para o texto para que possamos usar na busca semântica.</p>


  <pre><code class="language-clojure">;; src/docai/embedding.clj
(ns docai.embedding
  (:require [clojure.string :as str]
            [clojure.core.memoize :as memo]))

;; Implementação de embeddings usando TF-IDF simples
;; Não depende de modelos externos, ao contrário do Ollama que usa o deepseek-r1 para o LLM

(defn tokenize
  &#34;Divide o texto em tokens&#34;
  [text]
  (if (string? text)
    (-&gt; text
        str/lower-case
        (str/split #&#34;\s&#43;&#34;)
        (-&gt;&gt; (filter #(&gt; (count %) 2))))
    []))

(defn term-freq
  &#34;Calcula a frequência dos termos&#34;
  [tokens]
  (frequencies tokens))



(defn doc-freq
  &#34;Calcula a frequência dos documentos&#34;
  [docs]
  (let [string-docs (filter string? docs)  ; Use Clojure&#39;s built-in string? function
        _ (println (str &#34;Processando &#34; (count string-docs) &#34; documentos válidos de &#34; (count docs) &#34; total&#34;))
        doc-tokens (map tokenize string-docs)  
        all-tokens (distinct (flatten doc-tokens))
        doc-count (count string-docs)]
    (if (zero? doc-count)
      {}
      (zipmap all-tokens
              (map #(count (filter (fn [tokens] (some #{%} tokens)) doc-tokens))
                   all-tokens)))))

(defn tf-idf
  &#34;Calcula TF-IDF para um documento&#34;
  [doc doc-freq doc-count]
  (if (empty? doc-freq)
    {}
    (let [tokens (tokenize doc)
          tf (term-freq tokens)]
      (zipmap (keys tf)
              (map #(* (get tf %) (Math/log (/ doc-count (get doc-freq % 1))))
                   (keys tf))))))

(defn vectorize
  &#34;Converte um documento em um vetor TF-IDF&#34;
  [doc doc-freq doc-count vocab]
  (let [tf-idf-scores (tf-idf doc doc-freq doc-count)]
    (if (empty? vocab)
      []
      (map #(get tf-idf-scores % 0.0) vocab))))

(defn create-embeddings
  &#34;Gera embeddings para uma lista de textos usando TF-IDF&#34;
  [texts]
  (try
    (let [doc-freq (doc-freq texts)
          doc-count (count (filter string? texts))
          ;; Vocabulário ordenado para garantir ordem estável
          vocab (sort (keys doc-freq))]
      (map #(vectorize % doc-freq doc-count vocab) texts))
    (catch Exception e
      (println &#34;Erro ao criar embeddings: &#34; (.getMessage e))
      (vec (repeat (count texts) [])))))

(defn cosine-similarity
  &#34;Calcula a similaridade do cosseno entre dois vetores&#34;
  [v1 v2]
  (if (or (empty? v1) (empty? v2))
    0.0
    (let [dot-product (reduce &#43; (map * v1 v2))
          norm1 (Math/sqrt (reduce &#43; (map #(* % %) v1)))
          norm2 (Math/sqrt (reduce &#43; (map #(* % %) v2)))]
      (if (or (zero? norm1) (zero? norm2))
        0.0
        (/ dot-product (* norm1 norm2))))))

(defn similarity-search
  &#34;Encontra os N chunks mais similares&#34;
  [query-embedding doc-embeddings n]
  (if (or (empty? query-embedding) (empty? doc-embeddings))
    (take (min n (count doc-embeddings)) (range))
    (let [scores (map #(cosine-similarity query-embedding %) doc-embeddings)]
      (-&gt;&gt; (map vector scores (range))
           (sort-by first &gt;)
           (take n)
           (map second)))))</code></pre>
 <p>O código acima implementa um sistema simples de embeddings usando TF-IDF (Term Frequency-Inverse Document Frequency) para transformar textos em vetores numéricos.</p>
<p>Basicamente, ele pega documentos de texto, quebra em palavras (tokens), calcula a importância de cada palavra considerando tanto sua frequência no documento quanto sua raridade na coleção inteira, e cria vetores que representam cada documento. É como transformar textos em coordenadas matemáticas para que o computador possa entender a &ldquo;semelhança&rdquo; entre eles.</p>


  
  <div class="mermaid">graph TD
    A[Documentos] --&gt;|Tokenização| B[Tokens]
    B --&gt;|TF-IDF| C[Vetores Numéricos]
    C --&gt;|Similaridade do Cosseno| D[Documentos Similares]</div>
 <p>A parte mais legal é a função <code>similarity_search</code>, que usa a similaridade do cosseno para encontrar documentos parecidos com uma consulta. Imagine que cada documento é um ponto num espaço multidimensional – quanto menor o ângulo entre dois pontos, mais similares eles são.</p>
<p>O código não usa nenhum modelo de IA sofisticado para isso, apenas matemática básica, o que o torna leve e rápido, embora menos poderoso que embeddings modernos baseados em redes neurais. É como um GPS simples que te leva ao destino sem todos os recursos de um Google Maps.</p>
<p>O TF-IDF transforma textos em vetores numéricos ao combinar a frequência de cada palavra em um documento (TF) com o quanto essa palavra é rara em toda a coleção (IDF): palavras comuns como &ldquo;linguagem&rdquo; têm peso baixo, enquanto termos mais exclusivos como &ldquo;Clojure&rdquo; ganham peso alto, permitindo que o computador compare documentos de forma eficiente e encontre os mais relevantes para cada consulta.</p>
<p>Outra abordagem, é por meio da similaridade do cosseno, que compara dois vetores TF-IDF calculando o ângulo entre eles: quanto menor o ângulo, mais parecidos são os textos, usando a fórmula cos(θ) = (A·B) / (||A|| ||B||), onde A·B é o produto escalar e ||A|| e ||B|| são os tamanhos dos vetores; porém, o TF-IDF tem limitações, pois não entende sinônimos, contexto ou ordem das palavras, tratando termos como &ldquo;carro&rdquo; e &ldquo;automóvel&rdquo; como diferentes e podendo gerar vetores grandes.</p>
<blockquote>
<p><strong>Importante</strong>: Esta implementação TF-IDF é uma <strong>prova de conceito</strong> para demonstrar os fundamentos do RAG. Em aplicações reais, embeddings densos modernos como <a href="https://www.sbert.net/">SBERT</a>, <a href="https://huggingface.co/intfloat/e5-large">E5</a>, <a href="https://huggingface.co/BAAI/bge-large-en">BGE</a> ou modelos via Ollama superam significativamente o TF-IDF em tarefas de busca semântica e question-answering.</p></blockquote>
<h4 id="interface-com-ollama">Interface com Ollama</h4>
<p>Agora vamos implementar a interface com o Ollama. Ele é responsável por gerar a resposta para a query do usuário (essa parte aqui é super divertida, pois é onde vamos usar o LLM).</p>


  <pre><code class="language-clojure">;; src/docai/llm.clj
(ns docai.llm
  (:require [clojure.data.json :as json]
            [org.httpkit.client :as http]))

(def ollama-url &#34;http://localhost:11434/api/generate&#34;)
(def model-name &#34;deepseek-r1&#34;) ; Modelo DeepSeek para melhor qualidade

(defn call-ollama-api
  &#34;Chama a API do Ollama para gerar uma resposta&#34;
  [prompt]
  (let [request-body {:model model-name
                      :prompt prompt
                      :stream false}
        options {:headers {&#34;Content-Type&#34; &#34;application/json&#34;}
                 :body (json/write-str request-body)}
        response @(http/post ollama-url options)]
    (if (= (:status response) 200)
      (-&gt; response
          :body
          (json/read-str :key-fn keyword)
          ;; Compatível com versões antigas (:response) e novas (:message) do Ollama
          (#(or (:response %) (:message %))))
      (str &#34;Erro ao chamar a API do Ollama: &#34; (:status response) &#34; - &#34; (:body response)))))

;; Funções de utilidade para uso futuro:
;;
;; extract-code-blocks: Extrai blocos de código do texto usando regex
;; exemplo de uso:
;;   (extract-code-blocks &#34;```clojure\n(&#43; 1 2)\n```&#34;) =&gt; [&#34;(&#43; 1 2)&#34;]
;;
;; extract-summary: Cria um resumo de texto com tamanho máximo especificado
;; exemplo de uso:
;;   (extract-summary &#34;# Título\nConteúdo longo...&#34; 50) =&gt; &#34;Conteúdo longo...&#34;

(defn format-prompt
  &#34;Formata o prompt para o LLM com delimitação segura do contexto&#34;
  [context query]
  (str &#34;Você é um assistente especializado em documentação técnica. &#34;
       &#34;Use APENAS as informações do contexto fornecido para responder.\n\n&#34;
       &#34;DOCUMENTO:\n&#34;
       &#34;```\n&#34;
       context
       &#34;\n```\n\n&#34;
       &#34;Pergunta: &#34; query
       &#34;\n\n&#34;
       &#34;Instruções:\n&#34;
       &#34;- Responda baseado APENAS no contexto fornecido\n&#34;
       &#34;- Se a informação não estiver no contexto, indique claramente\n&#34;
       &#34;- Forneça exemplos de código quando relevante\n&#34;
       &#34;- Se o contexto for limitado, mencione essa limitação\n&#34;
       &#34;- NÃO invente informações que não estão no contexto&#34;))

(defn generate-response
  &#34;Gera resposta usando o LLM com base no contexto&#34;
  [query context]
  (try
    (let [prompt (format-prompt context query)]
      (println &#34;DEBUG - Enviando prompt para o Ollama usando o modelo&#34; model-name)
      (call-ollama-api prompt))
    (catch Exception e
      (str &#34;Erro ao gerar resposta: &#34; (.getMessage e) 
           &#34;\n\nPor favor, verifique se o Ollama está em execução no endereço &#34; 
           ollama-url 
           &#34;\n\nVocê pode iniciar o Ollama com o comando: ollama serve&#34;))))

;; Exemplo de prompt seguro gerado:
;; Você é um assistente especializado em documentação técnica. 
;; Use APENAS as informações do contexto fornecido para responder.
;;
;; DOCUMENTO:
;; ```
;; [contexto aqui]
;; ```
;;
;; Pergunta: [pergunta do usuário]
;;
;; Instruções:
;; - Responda baseado APENAS no contexto fornecido
;; - Se a informação não estiver no contexto, indique claramente
;; - Forneça exemplos de código quando relevante
;; - Se o contexto for limitado, mencione essa limitação
;; - NÃO invente informações que não estão no contexto</code></pre>
 <p>A parte mais importante aqui é a função <code>call-ollama-api</code>, que faz uma requisição HTTP para o servidor Ollama rodando na máquina local. Ela envia um prompt de texto e recebe de volta a resposta gerada pelo modelo DeepSeek R1. O código também inclui uma função <code>format-prompt</code> super importante, que estrutura a mensagem enviada ao modelo.</p>
<p>Ela combina o contexto (os trechos de documentação relevantes que encontramos) com a pergunta do usuário, e adiciona instruções específicas para o modelo se comportar como um assistente técnico. Essa &ldquo;engenharia de prompt&rdquo; é crucial para obter respostas de qualidade - estamos essencialmente ensinando o modelo a responder no formato que queremos.</p>
<p>A função <code>generate-response</code> amarra tudo isso, pegando a pergunta e o contexto, formatando o prompt, enviando para o Ollama e tratando possíveis erros. Tem até uma mensagem amigável caso o Ollama não esteja rodando, sugerindo como iniciar o serviço. É um exemplo clássico de como interfaces com LLMs funcionam: você prepara um prompt bem estruturado, envia para o modelo, e recebe de volta texto gerado que (esperamos!) responda à pergunta original com base no contexto fornecido.</p>
<h4 id="módulo-principal">Módulo Principal</h4>
<p>Agora vamos implementar o módulo principal que vai ser o ponto de entrada para o RAG. Ele vai ser responsável por carregar os documentos, processar os chunks, criar os embeddings e gerar a resposta para a query do usuário.</p>


  <pre><code class="language-clojure">;; src/docai/core.clj
(ns docai.core
  (:require [docai.document :as doc]
            [docai.embedding :as emb]
            [docai.llm :as llm]
            [clojure.java.io :as io]
            [clojure.string :as str])
  (:gen-class))

(def docs-path &#34;resources/docs&#34;)

(defn load-documentation
  &#34;Carrega todos os arquivos de documentação do diretório&#34;
  []
  (-&gt;&gt; (file-seq (io/file docs-path))
       (filter #(.isFile %))
       (map #(.getPath %))))

(defn setup-knowledge-base
  &#34;Configura a base de conhecimento inicial&#34;
  []
  (let [doc-files (load-documentation)
        _ (when (empty? doc-files)
            (println &#34;Aviso: Nenhum arquivo de documentação encontrado em resources/docs/&#34;))
        _ (doseq [file doc-files]
            (println &#34;Arquivo encontrado:&#34; file))
        all-chunks (mapcat doc/extract-text doc-files)
        processed-chunks (doc/preprocess-chunks all-chunks)
        _ (println (str &#34;Processando &#34; (count processed-chunks) &#34; chunks de texto...&#34;))
        _ (when (&lt; (count processed-chunks) 5)
            (println &#34;DEBUG - Primeiros chunks:&#34;)
            (doseq [chunk (take 5 processed-chunks)]
              (println (str &#34;Chunk: &#39;&#34; (subs chunk 0 (min 50 (count chunk))) &#34;...&#39;&#34;))))
        doc-freq (emb/doc-freq processed-chunks)
        doc-count (count (filter string? processed-chunks))
        ;; Vocabulário ordenado para garantir ordem estável entre execuções
        vocab (sort (keys doc-freq))
        embeddings (map #(emb/vectorize % doc-freq doc-count vocab) processed-chunks)]
          {:chunks processed-chunks
       :embeddings embeddings
       :doc-freq doc-freq
       :doc-count doc-count
            :vocab vocab  ; Persistir vocabulário ordenado
     :original-files doc-files}))

;; Função para forçar recálculo (útil para desenvolvimento)
(defn force-recalculate-kb []
  (let [kb-file &#34;resources/knowledge-base.json&#34;]
    (when (.exists (io/file kb-file))
      (.delete (io/file kb-file)))
  (setup-knowledge-base))

(defn get-file-content
  &#34;Lê o conteúdo completo de um arquivo&#34;
  [file-path]
  (try
    (slurp file-path)
    (catch Exception _
      (println &#34;Erro ao ler arquivo:&#34; file-path)
      &#34;&#34;)))

(defn get-limited-fallback-content
  &#34;Obtém conteúdo limitado para fallback (evita estourar contexto)&#34;
  [file-path]
  (try
    (let [content (slurp file-path)
          max-chars 8000  ; Limite de ~8KB para evitar estourar contexto
          limited-content (if (&gt; (count content) max-chars)
                           (str (subs content 0 max-chars) 
                                &#34;\n\n[Conteúdo truncado - arquivo muito grande]&#34;)
                           content)]
      (str &#34;Informações limitadas da documentação:\n\n&#34; limited-content))
    (catch Exception _
      (println &#34;Erro ao ler arquivo para fallback:&#34; file-path)
      &#34;Não foi possível acessar a documentação.&#34;)))

(defn query-rag
  &#34;Processa uma query usando o pipeline RAG&#34;
  [knowledge-base query]
  (cond
    (str/blank? query)
    &#34;Por favor, digite uma pergunta válida.&#34;
    
    (and (seq (:chunks knowledge-base)) 
         (seq (:embeddings knowledge-base)))
    (let [query-emb (emb/vectorize query (:doc-freq knowledge-base) (:doc-count knowledge-base) (:vocab knowledge-base))
          similar-idxs (emb/similarity-search query-emb 
                                            (:embeddings knowledge-base)
                                            3)
          _ (println &#34;DEBUG - Índices similares:&#34; similar-idxs)
          
          ;; Obter contexto relevante
          context-chunks (-&gt;&gt; similar-idxs
                              (map #(nth (:chunks knowledge-base) %))
                              (str/join &#34;\n\n&#34;))
          
          ;; Se não houver chunks relevantes, use fallback inteligente
          context (if (str/blank? context-chunks)
                    (if (seq (:original-files knowledge-base))
                      (get-limited-fallback-content (first (:original-files knowledge-base)))
                      &#34;Não foi possível encontrar informações relevantes.&#34;)
                    context-chunks)]
      
      (println &#34;DEBUG - Tamanho do contexto:&#34; (count context) &#34;caracteres&#34;)
      (println &#34;DEBUG - Amostra do contexto:&#34; (subs context 0 (min 200 (count context))) &#34;...&#34;)
      
      ;; Gerar resposta usando o LLM
      (llm/generate-response query context))
    
    :else
    &#34;Não foi possível encontrar informações relevantes na base de conhecimento.&#34;))

(defn -main
  &#34;Função principal que inicializa a aplicação DocAI&#34;
  [&amp; _]
  (println &#34;Inicializando DocAI...&#34;)
  
  ;; Verificar se o Ollama está acessível
  (println &#34;Para usar o Ollama, certifique-se de que ele está em execução com o comando: ollama serve&#34;)
  (println &#34;Usando o modelo deepseek-r1. Se você ainda não o baixou, execute: ollama pull deepseek-r1&#34;)
  
  (let [kb (setup-knowledge-base)]
    (println &#34;Base de conhecimento pronta! Faça sua pergunta:&#34;)
    (try
      (loop []
        (when-let [input (read-line)]
          (cond
            (= input &#34;sair&#34;) 
            (println &#34;Obrigado por usar o DocAI. Até a próxima!&#34;)
            
            (str/blank? input)
            (do
              (println &#34;Digite uma pergunta ou &#39;sair&#39; para terminar.&#34;)
              (recur))
            
            :else
            (do
              (println &#34;Processando...&#34;)
              (println (query-rag kb input))
              (println &#34;\nPróxima pergunta (ou &#39;sair&#39; para terminar):&#34;)
              (recur)))))
      (catch Exception e
        (println &#34;Erro: &#34; (.getMessage e))
        (println &#34;Detalhes: &#34; (ex-data e))))
    (println &#34;Obrigado por usar o DocAI. Até a próxima!&#34;)))</code></pre>
 <p>Basicamente, quando você faz uma pergunta, o sistema primeiro transforma sua pergunta em números (embeddings) e depois procura nos documentos quais partes são mais parecidas com o que você perguntou.</p>
<p>É como se ele estivesse destacando os trechos mais relevantes de um livro para responder sua dúvida. Você pode ver isso acontecendo quando ele imprime os &ldquo;índices similares&rdquo; no console - são as posições dos pedaços de texto que ele achou mais úteis.</p>
<p>Depois de encontrar os trechos relevantes, o sistema junta tudo em um &ldquo;contexto&rdquo; - que é basicamente um resumo das informações importantes. Se ele não achar nada parecido com sua pergunta, ele tenta usar o documento inteiro ou avisa que não tem informação suficiente.</p>
<p>Dá para ver que ele é bem transparente, mostrando no console o tamanho do contexto e até uma amostra do que encontrou, para você entender o que está acontecendo nos bastidores.</p>
<p>Por fim, ele passa sua pergunta original junto com o contexto encontrado para o modelo de linguagem (LLM) gerar uma resposta personalizada. É como dar a um especialista tanto a sua pergunta quanto as páginas relevantes de um manual técnico - assim ele pode dar uma resposta muito mais precisa e fundamentada.</p>
<p>Todo esse processo acontece em segundos, permitindo que você tenha uma conversa fluida com seus documentos, como se estivesse conversando com alguém que leu tudo e está pronto para responder suas dúvidas específicas.</p>
<hr>
<h2 id="como-usar">Como Usar</h2>
<p>Abaixo um guia para você instalar e usar o DocAI (e ver o processo em ação).</p>
<h3 id="instalação-do-ollama">Instalação do Ollama</h3>
<ol>
<li>
<p><strong>Instalação</strong>:</p>
<ul>
<li><strong>Windows</strong>: Baixe o instalador do <a href="https://ollama.com/download">site oficial do Ollama</a> e execute-o</li>
<li><strong>Linux</strong>: Execute o comando:


  <pre><code class="language-bash">curl https://ollama.ai/install.sh | sh</code></pre>
 </li>
<li><strong>macOS</strong>: Use o Homebrew:


  <pre><code class="language-bash">brew install ollama</code></pre>
 </li>
</ul>
</li>
<li>
<p><strong>Iniciando o Servidor</strong>:</p>


  <pre><code class="language-bash">ollama serve</code></pre>
 </li>
<li>
<p><strong>Baixando o Modelo</strong>:</p>


  <pre><code class="language-bash">ollama pull deepseek-r1</code></pre>
 </li>
<li>
<p><strong>Verificando a Instalação</strong>:</p>
<ul>
<li>Execute um teste simples:


  <pre><code class="language-bash">ollama run deepseek-r1 &#34;Olá! Como você está?&#34;</code></pre>
 </li>
<li>Se tudo estiver funcionando, você receberá uma resposta do modelo</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>Dica</strong>: O Ollama mantém os modelos em cache local. Se você precisar liberar espaço, pode usar <code>ollama rm deepseek-r1</code> para remover o modelo.</p></blockquote>
<h3 id="executando-a-aplicação">Executando a Aplicação</h3>
<ol>
<li>Coloque seus documentos na pasta <code>resources/docs/</code> (já incluímos dois exemplos: <code>example.md</code>)</li>
<li>Execute o projeto:</li>
</ol>


  <pre><code class="language-bash">lein run</code></pre>
 <ol start="3">
<li>Faça suas perguntas! Exemplo:</li>
</ol>


  <pre><code class="language-bash">Como implementar autenticação JWT em Clojure?
Como implementar auth saml em python?
Como integrar o auth0 em uma aplicação Clojure?
etc...</code></pre>
 <p>O DocAI processa sua pergunta em várias etapas:</p>


  
  <div class="mermaid">flowchart TD
    A[Início] --&gt; B[Carrega Documentação]
    B --&gt; C[Processa Documentos]
    C --&gt; D[Gera Embeddings]
    D --&gt; E[Base de Conhecimento]
    
    F[Consulta do Usuário] --&gt; G[Processa Consulta]
    G --&gt; H[Gera Embedding da Consulta]
    H --&gt; I[Busca Similaridade]
    I --&gt; J[Seleciona Chunks Relevantes]
    J --&gt; K[Combina Contexto]
    K --&gt; L[Gera Prompt]
    L --&gt; M[LLM DeepSeek R1]
    M --&gt; N[Resposta Final]
    
    E --&gt; I
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style N fill:#9ff,stroke:#333,stroke-width:2px</div>
 <ol>
<li><strong>Processamento da Consulta</strong>: A pergunta é convertida em um vetor TF-IDF</li>
<li><strong>Busca por Similaridade</strong>: O sistema encontra os chunks mais relevantes</li>
<li><strong>Geração de Contexto</strong>: Os chunks são combinados em um contexto coeso</li>
<li><strong>Geração de Resposta</strong>: O LLM gera uma resposta baseada no contexto</li>
</ol>
<p>Você pode ver o processo em ação nos logs:</p>


  <pre><code class="language-bash">DEBUG - Processando query: Como implementar autenticação JWT em Clojure?
DEBUG - Índices similares: [2, 5, 8]
DEBUG - Tamanho do contexto: 1234 caracteres
DEBUG - Amostra do contexto: &#34;Para implementar autenticação JWT em Clojure...&#34;</code></pre>
 <blockquote>
<p><strong>NOTA:</strong> A propósito, o projeto docai está disponível no <a href="https://github.com/scovl/docai">https://github.com/scovl/docai</a> caso você queira contribuir com o projeto ou usar em outro projeto.</p></blockquote>
<hr>
<h2 id="considerações-técnicas">Considerações Técnicas</h2>
<h3 id="performance-e-otimizações">Performance e Otimizações</h3>
<p>Nossa implementação atual oferece uma base funcional, mas pode ser significativamente otimizada em termos de performance através da adoção de bancos de dados vetoriais como <a href="https://milvus.io/">Milvus</a> ou <a href="https://github.com/facebookresearch/faiss">FAISS</a>, implementação de cache de embeddings e paralelização do processamento de chunks, permitindo consultas mais rápidas mesmo com grandes volumes de dados.</p>
<p>Para lidar com documentações extensas, recomendo estratégias específicas de gerenciamento de memória, como o processamento de chunks em lotes menores, implementação de indexação incremental que constrói a base de conhecimento gradualmente, e utilização de técnicas de streaming para processar arquivos grandes sem sobrecarregar a memória disponível.</p>
<p>Quanto à escolha de modelos no ecossistema Ollama, cada um apresenta características distintas que podem ser exploradas conforme a necessidade: o <a href="https://ollama.com/models/deepseek-r1">DeepSeek R1</a> destaca-se na compreensão geral e geração de texto, o <a href="https://ollama.com/models/deepseek-coder">DeepSeek Coder</a> é especializado em código, o <a href="https://ollama.com/models/llama3">Llama 3</a> serve como excelente alternativa geral, o <a href="https://ollama.com/models/mistral">Mistral</a> demonstra eficácia em tarefas específicas, enquanto o <a href="https://ollama.com/models/gemma">Gemma</a> oferece uma solução leve e eficiente para ambientes com recursos limitados.</p>
<p>Outra questão importante é como estou tratando os erros. O sistema implementa várias camadas de tratamento de erros para lidar com diferentes cenários:</p>
<ol>
<li>
<p><strong>Ollama Offline</strong></p>
<ul>
<li><strong>Sintoma</strong>: O sistema não consegue se conectar ao servidor Ollama</li>
<li><strong>Tratamento</strong>: O código verifica a disponibilidade do servidor e fornece mensagens claras de erro:</li>
</ul>


  <pre><code class="language-clojure">(catch Exception e
  (str &#34;Erro ao gerar resposta: &#34; (.getMessage e) 
       &#34;\n\nPor favor, verifique se o Ollama está em execução no endereço &#34; 
       ollama-url 
       &#34;\n\nVocê pode iniciar o Ollama com o comando: ollama serve&#34;))</code></pre>
 </li>
<li>
<p><strong>Documentação Muito Grande</strong></p>
<ul>
<li><strong>Sintoma</strong>: Arquivos de documentação que excedem a memória disponível</li>
<li><strong>Tratamento</strong>: O sistema implementa:
<ul>
<li>Chunking de documentos (512 tokens por chunk)</li>
<li>Processamento em lotes</li>
<li>Logs de progresso para monitoramento</li>
</ul>
</li>
</ul>


  <pre><code class="language-clojure">(let [content (slurp doc-path)
      chunks (partition-all 512 text)]
  (println &#34;Quantidade de chunks gerados:&#34; (count chunks)))</code></pre>
 </li>
<li>
<p><strong>Consultas sem Relação com a Documentação</strong></p>
<ul>
<li><strong>Sintoma</strong>: Nenhum chunk relevante é encontrado para a consulta</li>
<li><strong>Tratamento</strong>: O sistema:
<ul>
<li>Verifica se há chunks disponíveis</li>
<li>Usa fallback para conteúdo original se necessário</li>
<li>Fornece feedback claro ao usuário</li>
</ul>
</li>
</ul>


  <pre><code class="language-clojure">(if (str/blank? context-chunks)
  (if (seq (:original-files knowledge-base))
    (get-file-content (first (:original-files knowledge-base)))
    &#34;Não foi possível encontrar informações relevantes.&#34;)
  context-chunks)</code></pre>
 </li>
<li>
<p><strong>Melhorias Futuras</strong> - Implementar <a href="https://en.wikipedia.org/wiki/Exponential_backoff">retry com backoff exponencial</a> para falhas de conexão, adicionar <a href="https://en.wikipedia.org/wiki/Cache_%28computing%29">cache de embeddings</a> para melhor performance, implementar <a href="https://en.wikipedia.org/wiki/Streaming_media">streaming</a> para arquivos muito grandes, adicionar <a href="https://en.wikipedia.org/wiki/Document_validation">validação de formato de documentos</a> e implementar <a href="https://en.wikipedia.org/wiki/Rate_limiting">rate limiting</a> para evitar sobrecarga do Ollama.</p>
</li>
</ol>
<hr>
<h3 id="melhorando-os-prompts">Melhorando os Prompts</h3>
<p>Para obter melhores respostas do sistema RAG, você pode usar prompts mais estruturados:</p>


  <pre><code class="language-clojure">(defn format-advanced-prompt
  &#34;Prompt otimizado com diretrizes claras&#34;
  [context query]
  (str &#34;Você é um especialista em documentação técnica de software.\n\n&#34;
       &#34;DOCUMENTO:\n```\n&#34; context &#34;\n```\n\n&#34;
       &#34;Pergunta: &#34; query &#34;\n\n&#34;
       &#34;Diretrizes:\n&#34;
       &#34;1. Use APENAS informações do contexto fornecido\n&#34;
       &#34;2. Seja preciso e técnico\n&#34;
       &#34;3. Inclua exemplos de código quando relevante\n&#34;
       &#34;4. Se a informação não estiver no contexto, indique claramente\n&#34;
       &#34;5. Use formatação Markdown para melhor legibilidade&#34;))</code></pre>
 <blockquote>
<p>Para técnicas avançadas de prompt engineering, consulte o <a href="https://www.promptingguide.ai/">Guia Completo de Prompt Engineering</a>.</p></blockquote>
<h2 id="próximos-passos">Próximos Passos</h2>
<p>Abaixo uma lista de melhorias que podem ser feitas no projeto atual.</p>
<h3 id="melhorias-rápidas-implementação-imediata">Melhorias Rápidas (Implementação Imediata)</h3>
<h4 id="1-persistência-da-base-de-conhecimento"><strong>1. Persistência da Base de Conhecimento</strong></h4>


  <pre><code class="language-clojure">;; src/docai/persistence.clj
(ns docai.persistence
  (:require [clojure.data.json :as json]
            [clojure.edn :as edn]))

(defn calculate-checksum
  &#34;Calcula checksum dos arquivos de documentação&#34;
  [doc-files]
  (let [checksums (map #(hash (slurp %)) doc-files)]
    (hash checksums)))

(defn save-knowledge-base
  &#34;Salva a base de conhecimento em disco com checksum&#34;
  [kb filename]
  (let [doc-files (:original-files kb)
        checksum (calculate-checksum doc-files)
        serializable-kb (-&gt; kb
                           (select-keys [:chunks :embeddings :doc-freq :doc-count :vocab])
                           (assoc :checksum checksum :doc-files doc-files))]
    (spit filename (json/write-str serializable-kb))))

(defn load-knowledge-base
  &#34;Carrega a base de conhecimento do disco com verificação de mudanças&#34;
  [filename doc-files]
  (try
    (let [content (slurp filename)
          data (json/read-str content :key-fn keyword)
          cached-checksum (:checksum data)
          current-checksum (calculate-checksum doc-files)]
      (if (= cached-checksum current-checksum)
        (do
          (println &#34;Cache válido - carregando embeddings...&#34;)
          (assoc data :original-files doc-files))
        (do
          (println &#34;Arquivos modificados - recalculando embeddings...&#34;)
          nil)))
    (catch Exception e
      (println &#34;Erro ao carregar KB:&#34; (.getMessage e))
      nil)))

;; Uso no core.clj
(defn setup-knowledge-base
  &#34;Configura a base de conhecimento (com cache inteligente)&#34;
  []
  (let [kb-file &#34;resources/knowledge-base.json&#34;
        doc-files (load-documentation)]
    (if (.exists (io/file kb-file))
      (if-let [cached-kb (load-knowledge-base kb-file doc-files)]
        cached-kb
        (do
          (println &#34;Recriando KB devido a mudanças nos arquivos...&#34;)
          (let [kb (create-knowledge-base)]
            (save-knowledge-base kb kb-file)
            kb)))
      (do
        (println &#34;Criando nova KB...&#34;)
        (let [kb (create-knowledge-base)]
          (save-knowledge-base kb kb-file)
          kb)))))</code></pre>
 <h4 id="2-testes-unitários"><strong>2. Testes Unitários</strong></h4>


  <pre><code class="language-clojure">;; test/docai/embedding_test.clj
(ns docai.embedding-test
  (:require [clojure.test :refer :all]
            [docai.embedding :as emb]))

(deftest test-tokenize
  (testing &#34;Tokenização básica&#34;
    (is (= [&#34;hello&#34; &#34;world&#34;] (emb/tokenize &#34;Hello World!&#34;)))
    (testing &#34;Filtra palavras curtas&#34;
      (is (= [] (emb/tokenize &#34;a b c&#34;)))))

(deftest test-tf-idf
  (testing &#34;Cálculo TF-IDF&#34;
    (let [doc &#34;hello world hello&#34;
          doc-freq {&#34;hello&#34; 2 &#34;world&#34; 1}
          doc-count 2
          result (emb/tf-idf doc doc-freq doc-count)]
      (is (contains? result &#34;hello&#34;))
      (is (contains? result &#34;world&#34;)))))

(deftest test-cosine-similarity
  (testing &#34;Similaridade do cosseno&#34;
    (is (= 1.0 (emb/cosine-similarity [1 0] [1 0])))
    (is (= 0.0 (emb/cosine-similarity [1 0] [0 1])))
    (is (= 0.707 (emb/cosine-similarity [1 1] [1 0]) :delta 0.001))))</code></pre>
 <h4 id="3-streaming-de-respostas"><strong>3. Streaming de Respostas</strong></h4>


  <pre><code class="language-clojure">;; src/docai/streaming.clj
(ns docai.streaming
  (:require [clojure.data.json :as json]
            [org.httpkit.client :as http]))

(defn stream-ollama-response
  &#34;Streaming de resposta do Ollama&#34;
  [prompt]
  (let [url &#34;http://localhost:11434/api/generate&#34;
        request-body {:model &#34;deepseek-r1&#34;
                     :prompt prompt
                     :stream true}]
    (with-open [conn @(http/post url {:body (json/write-str request-body)
                                      :as :stream})]
      (doseq [line (line-seq (:body conn))]
        (when-not (str/blank? line)
          (let [data (json/read-str line :key-fn keyword)]
            ;; Compatível com versões antigas (:response) e novas (:message) do Ollama
            (when-let [content (or (:response data) (:message data))]
              (print content)
              (flush))))))))</code></pre>
 <h4 id="4-cache-de-embeddings"><strong>4. Cache de Embeddings</strong></h4>


  <pre><code class="language-clojure">;; src/docai/cache.clj
(ns docai.cache
  (:require [clojure.core.cache :as cache]))

;; Cache LRU com limite de memória (evita vazamentos)
(def embedding-cache (atom (cache/lru-cache-factory {} :threshold 1000))) ; Máximo 1000 embeddings

(defn cached-embedding
  &#34;Embedding com cache LRU&#34;
  [text doc-freq doc-count vocab]
  (if-let [cached (cache/lookup @embedding-cache text)]
    cached
    (let [embedding (emb/vectorize text doc-freq doc-count vocab)]
      (swap! embedding-cache cache/miss text embedding)
      embedding)))

;; Cache para respostas do LLM (também LRU)
(def response-cache (atom (cache/lru-cache-factory {} :threshold 500))) ; Máximo 500 respostas

(defn cached-llm-response
  &#34;Resposta do LLM com cache LRU&#34;
  [prompt]
  (if-let [cached (cache/lookup @response-cache prompt)]
    cached
    (let [response (llm/call-ollama-api prompt)]
      (swap! response-cache cache/miss prompt response)
      response)))

;; Função para limpar cache manualmente se necessário
(defn clear-caches []
  (reset! embedding-cache (cache/lru-cache-factory {} :threshold 1000))
  (reset! response-cache (cache/lru-cache-factory {} :threshold 500))
  (println &#34;Caches limpos!&#34;))

;; Monitoramento de cache
(defn cache-stats []
  (let [embedding-size (count @embedding-cache)
        response-size (count @response-cache)]
    (println (str &#34;Embedding cache: &#34; embedding-size &#34;/1000&#34;))
    (println (str &#34;Response cache: &#34; response-size &#34;/500&#34;))))</code></pre>
 <p><strong>Vantagens do Cache LRU:</strong></p>
<ul>
<li><strong>🔄 Auto-limpeza</strong>: Remove itens menos usados automaticamente</li>
<li><strong>💾 Controle de memória</strong>: Limite máximo de itens</li>
<li><strong>⚡ Performance</strong>: Acesso rápido a dados frequentes</li>
<li><strong>🛡️ Estabilidade</strong>: Evita vazamentos de memória</li>
</ul>
<p><strong>Cache Inteligente de Embeddings:</strong></p>
<ul>
<li><strong>📁 Persistência</strong>: Embeddings salvos em disco</li>
<li><strong>🔍 Verificação de Mudanças</strong>: Checksum dos arquivos</li>
<li><strong>⚡ Recarregamento Rápido</strong>: Só recalcula se necessário</li>
<li><strong>🔄 Invalidação Automática</strong>: Detecta modificações nos arquivos</li>
</ul>
<h4 id="5-banco-vetorial-simples-bm25-manual"><strong>5. Banco Vetorial Simples (BM25 Manual)</strong></h4>


  <pre><code class="language-clojure">;; src/docai/vector_store.clj
(ns docai.vector-store
  (:require [clojure.string :as str]))

(defn create-simple-vector-store
  &#34;Store vetorial simples com BM25 (implementação manual)&#34;
  [documents]
  (let [index (atom {})
        doc-freq (emb/doc-freq documents)
        vocab (sort (keys doc-freq))]  ; Vocabulário ordenado
    (doseq [[idx doc] (map-indexed vector documents)]
      (let [tokens (emb/tokenize doc)
            tf (emb/term-freq tokens)]
        (swap! index assoc idx {:doc doc :tf tf})))
    {:index index :doc-freq doc-freq :vocab vocab}))

(defn calculate-bm25
  &#34;Calcula score BM25 para um documento&#34;
  [query-tokens doc-tf doc-freq]
  (let [k1 1.2  ; Parâmetro de saturação de termo
        b 0.75   ; Parâmetro de normalização de comprimento
        avg-doc-len 100  ; Comprimento médio do documento (aproximação)
        doc-len (reduce &#43; (vals doc-tf))
        
        ;; IDF para cada termo da query
        idf-scores (map (fn [term]
                          (let [df (get doc-freq term 0)
                                n (count doc-freq)]
                            (if (zero? df)
                              0
                              (Math/log (/ (- n df 0.5) (&#43; df 0.5)))))
                        query-tokens)
        
        ;; TF para cada termo da query no documento
        tf-scores (map (fn [term]
                         (let [tf (get doc-tf term 0)]
                           (/ (* tf (&#43; k1 1))
                              (&#43; tf (* k1 (- 1 b (* b (/ doc-len avg-doc-len)))))))
                       query-tokens)]
    
    ;; Soma ponderada de IDF * TF
    (reduce &#43; (map * idf-scores tf-scores))))

(defn search-bm25
  &#34;Busca híbrida: BM25 &#43; similaridade semântica&#34;
  [query vector-store top-k]
  (let [query-tokens (emb/tokenize query)
        query-embedding (emb/vectorize query (:doc-freq vector-store) (:doc-count vector-store) (:vocab vector-store))
        
        ;; BM25 scores
        bm25-scores (map-indexed 
                      (fn [idx {:keys [tf]}]
                        [idx (calculate-bm25 query-tokens tf (:doc-freq vector-store))])
                      (vals @(:index vector-store)))
        
        ;; Semantic scores
        semantic-scores (map-indexed
                          (fn [idx _]
                            [idx (emb/cosine-similarity query-embedding 
                                                       (emb/vectorize (get-in @(:index vector-store) [idx :doc])
                                                                      (:doc-freq vector-store)
                                                                      (:doc-count vector-store)
                                                                      (:vocab vector-store)))])
                          (vals @(:index vector-store)))
        
        ;; Combine scores (weighted average)
        combined-scores (map (fn [[idx bm25] [idx2 semantic]]
                              [idx (&#43; (* 0.3 bm25) (* 0.7 semantic))])
                            bm25-scores semantic-scores)]
    
    (-&gt;&gt; combined-scores
         (sort-by second &gt;)
         (take top-k)
         (map first))))</code></pre>
 <p><strong>Sobre o Algoritmo BM25:</strong></p>
<ul>
<li><strong>k1 = 1.2</strong>: Controla saturação de frequência de termos</li>
<li><strong>b = 0.75</strong>: Normaliza pelo comprimento do documento</li>
<li><strong>IDF</strong>: Mede raridade dos termos na coleção</li>
<li><strong>TF</strong>: Frequência dos termos no documento</li>
<li><strong>Combinação</strong>: 30% BM25 + 70% similaridade semântica</li>
</ul>
<p><strong>Nota</strong>: Esta é uma implementação manual do BM25. Para produção, considere usar Apache Lucene (veja dependências acima) que oferece BM25 nativo e otimizado.</p>
<h3 id="melhorias-avançadas">Melhorias Avançadas</h3>


  
  <div class="mermaid">mindmap
  root((Melhorias))
    Tokenização
      BPE
      WordPiece
      Tokenizador do Modelo
    Embeddings
      Pré-treinados
      Via Ollama
      Cache
    Banco de Dados
      Milvus
      FAISS
      Qdrant
    Cache
      Embeddings
      Respostas
    Erros
      Conexão
      Modelo
      Rede
    Logging
      Framework
      Rastreamento
    Testes
      Unitários
      Integração
    Prompt
      Few-shot
      Chain-of-thought
      Formato</div>
 <h3 id="dependências-e-próximos-passos">Dependências e Próximos Passos</h3>
<h4 id="dependências-recomendadas"><strong>Dependências Recomendadas</strong></h4>
<p>Para implementar as funcionalidades avançadas mencionadas no artigo, adicione estas dependências ao <code>project.clj</code>:</p>


  <pre><code class="language-clojure">;; Dependências para produção
[com.github.justone/clojure-tiktoken &#34;0.1.0&#34;]  ; Contagem precisa de tokens
[org.apache.lucene/lucene-core &#34;9.10.0&#34;]       ; Busca textual avançada
[org.apache.lucene/lucene-analyzers-common &#34;9.10.0&#34;]  ; Analisadores de texto
[org.apache.lucene/lucene-queryparser &#34;9.10.0&#34;] ; Parser de queries
[com.github.clojure-lsp/clojure-lsp &#34;2024.01.15-20.32.45&#34;]  ; LSP para IDE</code></pre>
 <h4 id="implementação-com-lucene"><strong>Implementação com Lucene</strong></h4>


  <pre><code class="language-clojure">;; src/docai/lucene_store.clj
(ns docai.lucene-store
  (:import [org.apache.lucene.analysis.standard StandardAnalyzer]
           [org.apache.lucene.document Document Field Field$Store]
           [org.apache.lucene.index IndexWriter IndexWriterConfig DirectoryReader]
           [org.apache.lucene.search IndexSearcher QueryParser]
           [org.apache.lucene.store RAMDirectory]))

(defn create-lucene-index
  &#34;Cria índice Lucene para busca textual&#34;
  [documents]
  (let [analyzer (StandardAnalyzer.)
        directory (RAMDirectory.)
        config (IndexWriterConfig. analyzer)
        writer (IndexWriter. directory config)]
    
    ;; Adiciona documentos ao índice
    (doseq [[idx doc] (map-indexed vector documents)]
      (let [document (Document.)]
        (.add document (Field. &#34;content&#34; doc Field$Store/YES))
        (.add document (Field. &#34;id&#34; (str idx) Field$Store/YES))
        (.addDocument writer document)))
    
    (.close writer)
    
    {:directory directory
     :analyzer analyzer
     :reader (DirectoryReader/open directory)
     :searcher (IndexSearcher. (DirectoryReader/open directory))}))

(defn search-lucene
  &#34;Busca textual usando Lucene&#34;
  [index query top-k]
  (let [parser (QueryParser. &#34;content&#34; (:analyzer index))
        query-obj (.parse parser query)
        hits (.search (:searcher index) query-obj top-k)]
    (map #(.doc (:searcher index) %) (.scoreDocs hits))))</code></pre>
 <h3 id="upgrade-para-embeddings-densos">Upgrade para Embeddings Densos</h3>
<p>Para evoluir de TF-IDF para embeddings densos modernos, considere estas opções:</p>
<h4 id="1-via-ollama-embeddings-api">1. <strong>Via Ollama Embeddings API</strong></h4>


  <pre><code class="language-clojure">;; Exemplo de upgrade usando Ollama embeddings
(defn create-dense-embeddings [texts]
  (let [embeddings-url &#34;http://localhost:11434/api/embeddings&#34;]
    (map #(call-ollama-embeddings embeddings-url %) texts)))

(defn call-ollama-embeddings [url text]
  (let [request-body {:model &#34;deepseek-r1&#34; :prompt text}
        response @(http/post url {:body (json/write-str request-body)})]
    (if (= (:status response) 200)
      (-&gt; response :body (json/read-str :key-fn keyword) :embedding)
      (throw (ex-info &#34;Erro ao gerar embedding&#34; {:status (:status response)})))))

;; Token counting preciso via Ollama API
(defn count-tokens-ollama [text]
  (let [url &#34;http://localhost:11434/api/generate&#34;
        request-body {:model &#34;deepseek-r1&#34; 
                     :prompt text 
                     :stream false
                     :options {:num_predict 0}}]
    (try
      (let [response @(http/post url {:body (json/write-str request-body)})]
        (if (= (:status response) 200)
          (-&gt; response :body (json/read-str :key-fn keyword) :eval_count)
          0))
      (catch Exception _ 0))))

;; Implementação com clojure-tiktoken (recomendado para produção)
(defn count-tokens-precise [text]
  (try
    ;; Requer: [com.github.justone/clojure-tiktoken &#34;0.1.0&#34;]
    ;; (require &#39;[com.github.justone.clojure-tiktoken :as tiktoken])
    ;; (tiktoken/count-tokens text &#34;cl100k_base&#34;)
    (count-tokens text) ; Fallback para implementação atual
    (catch Exception e
      (println &#34;Erro ao usar tiktoken:&#34; (.getMessage e))
      (count-tokens text))))

;; Exemplo de implementação com API do Ollama (mais preciso)
(defn count-tokens-ollama-precise [text]
  (let [url &#34;http://localhost:11434/api/generate&#34;
        request-body {:model &#34;deepseek-r1&#34; 
                     :prompt text 
                     :stream false
                     :options {:num_predict 0}}]
    (try
      (let [response @(http/post url {:body (json/write-str request-body)})]
        (if (= (:status response) 200)
          (-&gt; response :body (json/read-str :key-fn keyword) :eval_count)
          (count-tokens text))) ; Fallback para heurística
      (catch Exception _
        (count-tokens text))))) ; Fallback para heurística</code></pre>
 <h4 id="2-via-huggingface-transformers">2. <strong>Via HuggingFace Transformers</strong></h4>


  <pre><code class="language-clojure">;; Exemplo usando interop com Python/HuggingFace
(defn create-hf-embeddings [texts]
  (let [model-name &#34;sentence-transformers/all-MiniLM-L6-v2&#34;]
    ;; Usar interop com Python para carregar modelo
    ;; e gerar embeddings densos
    ))

;; Token counting preciso com tiktoken
(defn count-tokens-tiktoken [text]
  ;; Requer interop com Python tiktoken
  ;; pip install tiktoken
  ;; python -c &#34;import tiktoken; print(len(tiktoken.get_encoding(&#39;cl100k_base&#39;).encode(&#39;texto aqui&#39;)))&#34;
  )</code></pre>
 <h4 id="3-comparação-de-performance">3. <strong>Comparação de Performance</strong></h4>
<table>
  <thead>
      <tr>
          <th>Método</th>
          <th>Semântica</th>
          <th>Contexto</th>
          <th>Performance</th>
          <th>Complexidade</th>
          <th>Hardware Mínimo</th>
          <th>Precisão Tokens</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>TF-IDF</td>
          <td>❌</td>
          <td>❌</td>
          <td>⚡⚡⚡</td>
          <td>⚡</td>
          <td>CPU 4 cores, 8GB RAM</td>
          <td>⚠️ Heurística</td>
      </tr>
      <tr>
          <td>Ollama Embeddings</td>
          <td>✅</td>
          <td>✅</td>
          <td>⚡⚡</td>
          <td>⚡⚡</td>
          <td>CPU 8 cores, 16GB RAM</td>
          <td>✅ Preciso</td>
      </tr>
      <tr>
          <td>SBERT/E5</td>
          <td>✅✅</td>
          <td>✅✅</td>
          <td>⚡</td>
          <td>⚡⚡⚡</td>
          <td>GPU 8GB VRAM, 32GB RAM</td>
          <td>✅ Preciso</td>
      </tr>
  </tbody>
</table>
<blockquote>
<p><strong>Recomendação</strong>: Para aplicações em produção, comece com Ollama embeddings (simples de implementar) e evolua para modelos especializados como SBERT ou E5 conforme necessário. Considere seus recursos de hardware ao escolher a abordagem.</p>
<p><strong>⚠️ Importante</strong>: A contagem de tokens heurística pode errar até 2x. Para produção, use <code>count-tokens-ollama-precise</code> ou <code>clojure-tiktoken</code> para precisão.</p></blockquote>
<p>Olha, dá pra turbinar esse nosso RAG de várias formas! Primeiro, a gente poderia melhorar a tokenização usando aqueles métodos mais avançados tipo <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding">BPE</a> ou <a href="https://en.wikipedia.org/wiki/WordPiece">WordPiece</a> - idealmente o mesmo que o modelo usa.</p>
<p>E os embeddings? Seria muito mais eficiente pegar direto do Ollama em vez de fazer na mão. A diferença na busca semântica seria absurda! O TF-IDF que implementamos é ótimo para entender os conceitos, mas embeddings densos modernos capturam nuances semânticas que fazem toda a diferença em aplicações reais.</p>
<p>Quando o projeto crescer, vai ser essencial ter um banco de dados vetorial decente. Imagina lidar com milhares de documentos usando nossa implementação atual? Seria um pesadelo! <a href="https://milvus.io/">Milvus</a>, <a href="https://github.com/facebookresearch/faiss">FAISS</a> ou <a href="https://qdrant.tech/">Qdrant</a> resolveriam isso numa boa. E não podemos esquecer do cache - tanto para embeddings quanto para respostas. Economiza um tempão e reduz a carga no sistema.</p>
<p>A parte de tratamento de erros e logging também precisa de carinho. Já pensou o usuário esperando resposta e o Ollama tá offline? Ou um arquivo corrompido? Precisamos de mensagens amigáveis e um sistema de logging decente pra rastrear problemas. E claro, testes! Sem testes unitários e de integração, qualquer mudança vira uma roleta-russa.</p>
<p>O prompt engineering é outro ponto crucial - dá pra refinar bastante o formato atual. Poderíamos experimentar com exemplos no prompt (few-shot), instruções passo a passo (chain-of-thought), e ser mais específico sobre o formato da resposta.</p>
<h2 id="apêndice">Apêndice</h2>
<h3 id="requisitos-de-hardware-detalhados">Requisitos de Hardware Detalhados</h3>
<p>A performance do sistema RAG depende significativamente do hardware disponível. Aqui estão as configurações recomendadas:</p>
<table>
  <thead>
      <tr>
          <th>Componente</th>
          <th>Mínimo</th>
          <th>Recomendado</th>
          <th>Alto Desempenho</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>CPU</strong></td>
          <td>4 cores (Intel i5/AMD Ryzen 5)</td>
          <td>8 cores (Intel i7/AMD Ryzen 7)</td>
          <td>16+ cores (Intel i9/AMD Ryzen 9)</td>
      </tr>
      <tr>
          <td><strong>RAM</strong></td>
          <td>8 GB</td>
          <td>16 GB</td>
          <td>32+ GB</td>
      </tr>
      <tr>
          <td><strong>GPU</strong></td>
          <td>Integrada</td>
          <td>NVIDIA RTX 3060 (8GB VRAM)</td>
          <td>NVIDIA RTX 4090 (24GB VRAM)</td>
      </tr>
      <tr>
          <td><strong>VRAM</strong></td>
          <td>-</td>
          <td>8 GB</td>
          <td>16+ GB</td>
      </tr>
      <tr>
          <td><strong>Storage</strong></td>
          <td>SSD 256 GB</td>
          <td>SSD 512 GB</td>
          <td>NVMe 1 TB+</td>
      </tr>
      <tr>
          <td><strong>Rede</strong></td>
          <td>100 Mbps</td>
          <td>1 Gbps</td>
          <td>10 Gbps</td>
      </tr>
  </tbody>
</table>
<h4 id="configurações-por-caso-de-uso"><strong>Configurações por Caso de Uso</strong></h4>
<p><strong>🟢 Desenvolvimento/Teste</strong></p>
<ul>
<li>CPU: 4 cores, RAM: 8GB</li>
<li>Modelo: <code>deepseek-r1</code> (CPU only)</li>
<li>Documentos: &lt; 1GB</li>
<li>Performance: ~2-5 segundos por consulta</li>
</ul>
<p><strong>🟡 Produção Pequena</strong></p>
<ul>
<li>CPU: 8 cores, RAM: 16GB, GPU: RTX 3060</li>
<li>Modelo: <code>deepseek-r1</code> (GPU)</li>
<li>Documentos: 1-10GB</li>
<li>Performance: ~1-3 segundos por consulta</li>
</ul>
<p><strong>🔴 Produção Grande</strong></p>
<ul>
<li>CPU: 16+ cores, RAM: 32GB+, GPU: RTX 4090</li>
<li>Modelo: <code>deepseek-r1</code> + embeddings densos</li>
<li>Documentos: 10GB+</li>
<li>Performance: &lt; 1 segundo por consulta</li>
</ul>
<h4 id="otimizações-por-hardware"><strong>Otimizações por Hardware</strong></h4>
<p><strong>CPU Only:</strong></p>


  <pre><code class="language-bash"># Usar modelo otimizado para CPU
ollama pull deepseek-r1:3b  # Versão menor</code></pre>
 <p><strong>GPU Disponível:</strong></p>


  <pre><code class="language-bash"># Usar versão completa com aceleração GPU
ollama pull deepseek-r1</code></pre>
 <p><strong>Múltiplas GPUs:</strong></p>


  <pre><code class="language-bash"># Distribuir carga entre GPUs
CUDA_VISIBLE_DEVICES=0,1 ollama serve</code></pre>
 <hr>
<h2 id="referências">Referências</h2>
<ul>
<li><a href="https://www.pinecone.io/learn/rag/">RAG</a> - Documentação do Pinecone</li>
<li><a href="https://www.pinecone.io/learn/embeddings/">Embedding</a> - Documentação do Pinecone</li>
<li><a href="https://www.pinecone.io/learn/llms/">LLM</a> - Documentação do Pinecone</li>
<li><a href="https://ollama.com/">Ollama</a> - Ferramenta para rodar LLMs localmente</li>
<li><a href="https://clojure.org/">Clojure</a> - Documentação do Clojure</li>
<li><a href="https://github.com/http-kit/http-kit">http-kit</a> - Cliente HTTP para Clojure</li>
<li><a href="https://github.com/clojure/data.json">data.json</a> - Biblioteca JSON para Clojure</li>
<li><a href="https://clojure.github.io/clojure/clojure.test-api.html">clojure.test</a> - Documentação da biblioteca de testes do Clojure</li>
<li><a href="https://github.com/clj-kondo/clj-kondo">clj-kondo</a> - Linter para Clojure</li>
</ul>
]]></content:encoded>
      
      
      <category>RAG,LLM,AI,Langchain</category>
      
      
      
      <dc:creator>Vitor Lobo Ramos</dc:creator>
      
      
      
      
      
      <description>&lt;![CDATA[Um protótipo funcional do zero]]></description>
      
    </item>
    
    <item>
      <title>Busca Semântica com Ollama e PostgreSQL</title>
      <link>https://scovl.github.io/2025/03/25/semantic-postgresql/</link>
      <guid>https://scovl.github.io/2025/03/25/semantic-postgresql/</guid>
      <pubDate>Tue, 25 Mar 2025 12:00:00 &#43;0000</pubDate>
      <description>&lt;![CDATA[<p>Olá, pessoal! 👋</p>
<p>No <a href="/2025/03/23/rag/">artigo anterior</a>, exploramos como construir um sistema RAG (Retrieval-Augmented Generation) usando <a href="https://clojure.org/">Clojure</a> e <a href="https://ollama.com/">Ollama</a> com uma implementação simples de <a href="/post/tf-idf/">TF-IDF</a>. Embora essa abordagem seja excelente para aprender os fundamentos, quando pensamos em soluções de produção, precisamos de algo mais robusto e escalável.</p>
<p>Neste artigo, vamos descobrir como construir um sistema de busca semântica poderoso usando <a href="https://ollama.com/">Ollama</a>, <a href="https://www.postgresql.org/">PostgreSQL</a> e suas extensões para manipulação de vetores. Esta solução é perfeitamente adequada para aplicações de produção e pode servir como base para sistemas RAG, agentes de IA, assistentes em geral. Diferentemente do artigo anterior, vamos usar o <a href="https://ollama.com/">Ollama</a> via Docker assim como o <a href="https://www.postgresql.org/">PostgreSQL</a> e as extensões <a href="https://github.com/pgvector/pgvector">pgvector</a> e <a href="https://github.com/timescale/pgai">pgai</a>.</p>]]></description>
      <content:encoded>&lt;![CDATA[<p>Olá, pessoal! 👋</p>
<p>No <a href="/2025/03/23/rag/">artigo anterior</a>, exploramos como construir um sistema RAG (Retrieval-Augmented Generation) usando <a href="https://clojure.org/">Clojure</a> e <a href="https://ollama.com/">Ollama</a> com uma implementação simples de <a href="/post/tf-idf/">TF-IDF</a>. Embora essa abordagem seja excelente para aprender os fundamentos, quando pensamos em soluções de produção, precisamos de algo mais robusto e escalável.</p>
<p>Neste artigo, vamos descobrir como construir um sistema de busca semântica poderoso usando <a href="https://ollama.com/">Ollama</a>, <a href="https://www.postgresql.org/">PostgreSQL</a> e suas extensões para manipulação de vetores. Esta solução é perfeitamente adequada para aplicações de produção e pode servir como base para sistemas RAG, agentes de IA, assistentes em geral. Diferentemente do artigo anterior, vamos usar o <a href="https://ollama.com/">Ollama</a> via Docker assim como o <a href="https://www.postgresql.org/">PostgreSQL</a> e as extensões <a href="https://github.com/pgvector/pgvector">pgvector</a> e <a href="https://github.com/timescale/pgai">pgai</a>.</p>
<p>A combinação do <a href="https://www.postgresql.org/">PostgreSQL</a> com extensões como <a href="https://github.com/pgvector/pgvector">pgvector</a> e <a href="https://github.com/timescale/pgai">pgai</a>, junto com o <a href="https://ollama.com/">Ollama</a> (que permite executar modelos de linguagem localmente), cria uma solução completa e de alto desempenho para <a href="https://en.wikipedia.org/wiki/Semantic_search">processamento semântico de dados</a>.</p>
<h2 id="entendendo-a-arquitetura">Entendendo a Arquitetura</h2>
<p>A busca semântica vai além da simples correspondência de palavras-chave, capturando o significado e o contexto da sua consulta. Em vez de depender apenas de correspondências exatas, ela utiliza <a href="https://en.wikipedia.org/wiki/Embedding_%28machine_learning%29">embeddings vetoriais</a> para representar o conteúdo semântico do texto (ou qualquer dado não estruturado). Essa abordagem permite que seu sistema recupere resultados contextualmente relevantes, mesmo quando as palavras-chave exatas não estão presentes.</p>
<p>Por exemplo, se você pesquisar por &ldquo;melhores lugares para comer&rdquo;, um <a href="https://en.wikipedia.org/wiki/Semantic_search">sistema de busca semântica</a> pode recuperar documentos sobre &ldquo;restaurantes bem avaliados nas proximidades&rdquo; ou &ldquo;experiências gastronômicas altamente recomendadas&rdquo;, efetivamente capturando sua intenção em vez da formulação exata. A arquitetura para busca semântica com PostgreSQL envolve quatro componentes principais:</p>


  
    
  
  <div class="mermaid">flowchart LR
    A[Ollama] --&gt; B[pgai]
    B --&gt; C[pgvector]
    C --&gt; D[PostgreSQL]
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#9ff,stroke:#333,stroke-width:2px</div>
 <ul>
<li><a href="https://ollama.com/"><strong>Ollama</strong></a>: Ferramenta open-source que permite executar e gerenciar modelos de linguagem de grande escala (LLMs) e modelos de visão (VLMs) localmente no seu computador ou em um servidor cloud, proporcionando maior privacidade e controle sobre os dados.</li>
<li><a href="https://github.com/timescale/pgai"><strong>pgai</strong></a>: Extensão do PostgreSQL que simplifica o armazenamento e recuperação de dados para RAG e outras aplicações de IA, automatizando a criação e gestão de embeddings, facilitando a busca semântica e permitindo a execução de funções de LLM diretamente dentro de consultas SQL.</li>
<li><a href="https://github.com/pgvector/pgvector"><strong>pgvector</strong></a>: Extensão do PostgreSQL que adiciona suporte para armazenar, indexar e consultar embeddings vetoriais de alta dimensionalidade.</li>
<li><a href="https://www.postgresql.org/"><strong>PostgreSQL</strong></a>: O sistema de banco de dados relacional que serve como fundação robusta e escalável para todo o sistema.</li>
</ul>
<hr>
<h2 id="pré-requisitos">Pré-requisitos</h2>
<p>Antes de começar, precisamos garantir que você tenha:</p>
<ol>
<li><strong>Docker e Docker Compose</strong>: Para configurar o ambiente facilmente</li>
<li><strong>PostgreSQL com pgvector e pgai</strong>: Para armazenar e consultar embeddings</li>
</ol>
<blockquote>
<p><strong>NOTA</strong>: No artigo anterior sobre <a href="/2025/03/23/rag/">RAG em Clojure</a>, usamos o <a href="https://ollama.com/">Ollama</a> com <a href="https://ollama.com/models/deepseek-r1">DeepSeek R1</a> baixando o projeto ollama diretamente na máquina. Nesta abordagem, vamos usar o Ollama via Docker. Portanto, recomendo que você feche o Ollama para usar-mos ele inteiramente via Docker aqui nesta abordagem (é necessário fechar para não conflitar com o endpoint do Ollama que vamos usar no Docker Compose).</p></blockquote>
<p>Vamos configurar tudo isso rapidamente usando Docker Compose:</p>


  <pre><code class="language-bash">name: pgai
services:
  db:
    image: timescale/timescaledb-ha:pg17
    environment:
      POSTGRES_PASSWORD: postgres
      # Definir variáveis de ambiente para o host do Ollama
      OLLAMA_HOST: http://ollama:11434
    ports:
      - &#34;5432:5432&#34;
    volumes:
      - data:/home/postgres/pgdata/data
    # Não use a extensão ai até garantir que está instalada corretamente
    command: &#34;-c search_path=public&#34;
    depends_on:
      - ollama
    # Adicionar links explícitos para o serviço Ollama
    links:
      - ollama

  vectorizer-worker:
    image: timescale/pgai-vectorizer-worker:latest
    environment:
      PGAI_VECTORIZER_WORKER_DB_URL: postgres://postgres:postgres@db:5432/postgres
      OLLAMA_HOST: http://ollama:11434
    command: [ &#34;--poll-interval&#34;, &#34;5s&#34;, &#34;--log-level&#34;, &#34;DEBUG&#34; ]
    depends_on:
      - db
      - ollama
    links:
      - ollama

  ollama:
    image: ollama/ollama
    ports:
      - &#34;11434:11434&#34;
    volumes:
      - ollama_data:/root/.ollama
    # Comando direto para iniciar o Ollama
    command: serve

volumes:
  data:
  ollama_data: </code></pre>
 <p>O arquivo <code>docker-compose.yml</code> acima configura uma infraestrutura para busca semântica com três serviços interconectados. O serviço <code>db</code> utiliza o <a href="https://www.timescale.com/">TimescaleDB</a> (que nada mais é que uma versão do <a href="https://www.postgresql.org/">PostgreSQL</a> especializada para otimização de desempenho para dados de séries temporais) com a versão 17, configurando credenciais, mapeamento de portas e um volume persistente para armazenar os dados. Este serviço é configurado para se comunicar com o Ollama através de variáveis de ambiente e links explícitos, garantindo que a comunicação entre os contêineres funcione corretamente.</p>


  
  <div class="mermaid">flowchart TD
    subgraph db [&#34;TimescaleDB (pg17)&#34;]
        db_info[&#34;Ports: 5432:5432&lt;br&gt;Volumes: data:/home/postgres/pgdata/data&lt;br&gt;Environment:&lt;br&gt;POSTGRES_PASSWORD=postgres&lt;br&gt;OLLAMA_HOST=http://ollama:11434&#34;]
    end

    subgraph vectorizer_worker [&#34;pgai-vectorizer-worker&#34;]
        vw_info[&#34;Environment:&lt;br&gt;PGAI_VECTORIZER_WORKER_DB_URL=postgres://postgres:postgres@db:5432/postgres&lt;br&gt;OLLAMA_HOST=http://ollama:11434&lt;br&gt;Command: --poll-interval 5s --log-level DEBUG&#34;]
    end

    subgraph ollama [&#34;Ollama&#34;]
        o_info[&#34;Ports: 11434:11434&lt;br&gt;Volumes: ollama_data:/root/.ollama&lt;br&gt;Command: serve&#34;]
    end

    data[&#34;Data Volume&#34;]
    ollama_data[&#34;Ollama Data Volume&#34;]

    db --- data
    ollama --- ollama_data
    vectorizer_worker --- db
    vectorizer_worker --- ollama
    db --- ollama

    style db fill:#f9f,stroke:#333,stroke-width:2px
    style vectorizer_worker fill:#ccf,stroke:#333,stroke-width:2px
    style ollama fill:#ffc,stroke:#333,stroke-width:2px
    style data fill:#eee,stroke:#333,stroke-width:2px
    style ollama_data fill:#eee,stroke:#333,stroke-width:2px</div>
 <p>O diagrama acima ilustra a arquitetura do sistema de busca semântica com PostgreSQL. No centro, temos três componentes principais: o TimescaleDB (uma versão especializada do PostgreSQL), o pgai-vectorizer-worker (responsável por processar e vetorizar os textos) e o Ollama (que fornece os modelos de IA). As conexões entre os serviços mostram como eles se comunicam: o vectorizer-worker se conecta tanto ao banco de dados quanto ao Ollama para realizar seu trabalho de transformação de textos em vetores.</p>
<p>Os volumes persistentes (representados em cinza) garantem que tanto os dados do PostgreSQL quanto os modelos do Ollama sejam preservados entre reinicializações. Esta arquitetura modular permite escalar cada componente independentemente conforme necessário, enquanto mantém um fluxo de dados eficiente para operações de busca semântica.</p>
<p>O serviço <code>vectorizer-worker</code> é um componente especializado do <a href="https://github.com/timescale/pgai">pgai</a> que monitora o banco de dados a cada 5 segundos, processando automaticamente textos para transformá-los em embeddings vetoriais. Ele se conecta ao banco <a href="https://www.postgresql.org/">PostgreSQL</a> e ao serviço <a href="https://ollama.com/">Ollama</a> para realizar a vetorização dos textos, funcionando como uma ponte entre o armazenamento de dados e o modelo de IA, com logs detalhados para facilitar a depuração durante o desenvolvimento.</p>
<p>Por fim, o serviço <code>ollama</code> fornece a infraestrutura para executar modelos de IA localmente, expondo uma API REST na porta 11434 e armazenando os modelos baixados em um volume persistente. Este design de três camadas (banco de dados, processador de vetores e motor de IA) cria um sistema completo para busca semântica que pode ser iniciado com um simples <code>docker compose up -d</code>, seguido pelo download do modelo de <a href="https://en.wikipedia.org/wiki/Embedding_%28machine_learning%29">embeddings</a> que transformará os textos em vetores.</p>


  <pre><code class="language-bash">docker compose exec ollama ollama pull nomic-embed-text</code></pre>
 <p>Este setup configura um banco de dados PostgreSQL com as extensões <a href="https://github.com/timescale/pgai">pgai</a>, <a href="https://github.com/pgvector/pgvector">pgvector</a> e <a href="https://github.com/timescale/pgvectorscale">pgvectorscale</a>. Também configura o Ollama, que você pode usar para implantar LLMs e modelos de embedding.</p>
<hr>
<h2 id="passos-para-construir-a-busca-semântica">Passos para Construir a Busca Semântica</h2>
<p>Os passos para implementar a busca semântica no PostgreSQL são relativamente simples. Primeiro, vamos habilitar as extensões necessárias, criar uma tabela para armazenar nossos documentos, configurar o <a href="https://github.com/timescale/pgai/tree/main/vectorizer">vectorizer</a> para gerar <a href="https://en.wikipedia.org/wiki/Embedding_%28machine_learning%29">embeddings</a> automaticamente e, finalmente, realizar consultas semânticas.</p>
<h3 id="1-habilitando-as-extensões">1. Habilitando as Extensões</h3>
<p>Primeiro, precisamos habilitar as extensões necessárias no PostgreSQL:</p>


  <pre><code class="language-sql">CREATE EXTENSION IF NOT EXISTS vector CASCADE; 
CREATE EXTENSION IF NOT EXISTS ai CASCADE;</code></pre>
 <h3 id="2-criando-a-tabela-de-documentos">2. Criando a Tabela de Documentos</h3>
<p>Agora, vamos criar uma tabela para armazenar os documentos que queremos pesquisar:</p>


  <pre><code class="language-sql">CREATE TABLE IF NOT EXISTS documentos (
   id SERIAL PRIMARY KEY,
   titulo TEXT NOT NULL,
   conteudo TEXT,
   categoria TEXT,
   data_criacao TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);</code></pre>
 <p>Neste exemplo, criamos uma tabela chamada <code>documentos</code> com quatro colunas: <code>id</code>, <code>titulo</code>, <code>conteudo</code> e <code>categoria</code>. É importante notar que a coluna <code>id</code> é a chave primária da tabela. Outro ponto importante é que a coluna <code>data_criacao</code> é uma coluna de metadados que é gerada automaticamente pelo PostgreSQL.</p>
<h3 id="3-inserindo-documentos">3. Inserindo Documentos</h3>
<p>Podemos inserir documentos manualmente ou usar a função <code>ai.load_dataset</code> do <a href="https://github.com/timescale/pgai">pgai</a> para carregar dados diretamente do <a href="https://huggingface.co/">Hugging Face</a>:</p>


  <pre><code class="language-sql">SELECT ai.load_dataset(
   name =&gt; &#39;Cohere/movies&#39;,
   table_name =&gt; &#39;documentos&#39;,
   if_table_exists =&gt; &#39;append&#39;,
   field_types =&gt; &#39;{&#34;title&#34;: &#34;titulo&#34;, &#34;overview&#34;: &#34;conteudo&#34;, &#34;genres&#34;: &#34;categoria&#34;}&#39;::jsonb
);</code></pre>
 <p>Alternativamente, podemos inserir registros manualmente:</p>


  <pre><code class="language-sql">INSERT INTO documentos (titulo, conteudo, categoria) VALUES 
(&#39;Guia Clojure&#39;, &#39;Clojure é uma linguagem funcional moderna...&#39;, &#39;Programação&#39;),
(&#39;Tutorial RAG&#39;, &#39;Sistemas RAG combinam busca e geração...&#39;, &#39;IA&#39;),
(&#39;PostgreSQL Avançado&#39;, &#39;Técnicas de otimização para PostgreSQL...&#39;, &#39;Banco de Dados&#39;);</code></pre>
 <blockquote>
<p><strong>NOTA</strong>: O <a href="https://huggingface.co/">Hugging Face</a> é uma plataforma de dados e modelos de IA.</p></blockquote>
<p>Agora vamos configurar o vectorizer para gerar embeddings automaticamente.</p>
<h3 id="4-configurando-o-vectorizer">4. Configurando o Vectorizer</h3>
<p>O <a href="https://github.com/timescale/pgai">pgai</a> inclui uma ferramenta chamada <a href="https://github.com/timescale/pgai/tree/main/vectorizer">vectorizer</a> que automatiza a criação e sincronização de embeddings. Esta é uma das funcionalidades mais poderosas desta solução, pois elimina a necessidade de ferramentas externas para criar <a href="https://en.wikipedia.org/wiki/Embedding_%28machine_learning%29">embeddings</a>. Vamos configurá-la:</p>


  <pre><code class="language-sql">SELECT ai.create_vectorizer(
   &#39;public.documentos&#39;::regclass,
   destination =&gt; &#39;documentos_embeddings&#39;,
   embedding =&gt; ai.embedding_ollama(&#39;nomic-embed-text&#39;, 768),
   chunking =&gt; ai.chunking_recursive_character_text_splitter(&#39;conteudo&#39;)
);</code></pre>
 <p>Basicamente, o comando acima faz o seguinte:</p>
<ol>
<li>Cria uma tabela <code>documentos_embeddings</code> para armazenar os vetores</li>
<li>Configura o modelo <code>nomic-embed-text</code> via Ollama para gerar embeddings</li>
<li>Define uma estratégia de chunking para dividir textos longos</li>
<li>Cria automaticamente uma view <code>documentos_embeddings_vectorized</code> que junta os documentos com seus embeddings</li>
</ol>
<p>O <a href="https://github.com/timescale/pgai/tree/main/vectorizer">vectorizer</a> também cuida da sincronização automática dos embeddings quando documentos são inseridos, atualizados ou removidos - sem necessidade de código adicional! Isto simplifica enormemente a manutenção do sistema.</p>
<h3 id="5-realizando-busca-semântica">5. Realizando Busca Semântica</h3>
<p>Agora estamos prontos para realizar buscas semânticas. Usaremos a função <code>ai.ollama_embed</code> para gerar embeddings para nossa consulta e o operador de distância de cosseno (<code>&lt;=&gt;</code>) para encontrar documentos similares:</p>


  <pre><code class="language-sql">WITH query_embedding AS (
    -- Gerar embedding para a consulta
    SELECT ai.ollama_embed(&#39;nomic-embed-text&#39;, &#39;Como implementar RAG em sistemas modernos&#39;, 
                          host =&gt; &#39;http://ollama:11434&#39;) AS embedding
)
SELECT
    d.titulo,
    d.conteudo,
    d.categoria,
    t.embedding &lt;=&gt; (SELECT embedding FROM query_embedding) AS distancia
FROM documentos_embeddings t
LEFT JOIN documentos d ON t.id = d.id
ORDER BY distancia
LIMIT 5;</code></pre>
 <p>Este código SQL realiza uma <a href="https://en.wikipedia.org/wiki/Semantic_search">busca semântica</a> em nossa base de documentos utilizando <a href="https://en.wikipedia.org/wiki/Embedding_%28machine_learning%29">embeddings</a> gerados pelo modelo <code>nomic-embed-text</code> através do <a href="https://ollama.com/">Ollama</a>. Primeiro, criamos uma CTE (Common Table Expression) chamada <code>query_embedding</code> que gera o embedding para nossa consulta &ldquo;Como implementar RAG em sistemas modernos&rdquo;. Em seguida, selecionamos os documentos mais relevantes comparando este embedding de consulta com os embeddings armazenados na tabela <code>documentos_embeddings</code> usando o operador de distância de cosseno (<code>&lt;=&gt;</code>).</p>
<p>O resultado é uma lista ordenada dos documentos mais semanticamente similares à nossa consulta, independentemente de compartilharem as mesmas palavras exatas. Esta é a essência da busca semântica - encontrar conteúdo conceitualmente relacionado, não apenas correspondências de palavras-chave. A coluna <code>distancia</code> nos mostra quão próximo cada documento está da nossa consulta, com valores menores indicando maior similaridade. Limitamos os resultados aos 5 documentos mais relevantes, mas este número pode ser ajustado conforme necessário. O PostgreSQL oferece três operadores para cálculo de similaridade:</p>
<ul>
<li><code>&lt;-&gt;</code>: <a href="https://en.wikipedia.org/wiki/Euclidean_distance">Distância L2 (Euclidiana)</a></li>
<li><code>&lt;#&gt;</code>: <a href="https://en.wikipedia.org/wiki/Dot_product">Produto interno</a></li>
<li><code>&lt;=&gt;</code>: <a href="https://en.wikipedia.org/wiki/Cosine_distance">Distância de cosseno</a> (geralmente a melhor opção)</li>
</ul>
<p>E pronto! Com apenas esses poucos passos, temos um sistema de busca semântica totalmente funcional, diretamente no PostgreSQL. <strong><a href="/2025/03/23/rag/">Para quem acompanhou o artigo anterior sobre a implementação de RAG em Clojure</a></strong>, vale a pena comparar as duas abordagens:</p>
<p>A diferença entre as duas abordagens é bem clara quando olhamos lado a lado. <a href="/2025/03/23/rag/">No artigo anterior sobre RAG em Clojure</a>, usamos uma técnica mais simples <a href="/post/tf-idf/">(TF-IDF)</a> que funciona bem para projetos pequenos e didáticos. É como usar uma bicicleta para se locomover para distâncias curtas. O código em Clojure mantém tudo em memória, o que é ótimo para aprender os conceitos, mas começa a dar problema quando a quantidade de documentos cresce.</p>
<p>Já a abordagem com PostgreSQL + pgai é como trocar a bicicleta por um carro esportivo! Estamos usando embeddings densos gerados por LLMs, que capturam muito melhor o significado semântico dos textos. O PostgreSQL cuida de toda a parte chata de persistência e indexação, permitindo que você escale para milhões de documentos sem suar. Os índices especializados para vetores (como HNSW) fazem buscas em bilhões de embeddings parecerem instantâneas, algo que nossa implementação anterior jamais conseguiria.</p>
<p>O mais legal é que a manutenção fica muito mais simples. Com o <a href="https://github.com/timescale/pgai/tree/main/vectorizer">vectorizer do pgai</a>, você só precisa inserir documentos no banco normalmente, e ele cuida automaticamente de gerar e atualizar os embeddings.</p>
<hr>
<h2 id="integração-com-clojure">Integração com Clojure</h2>
<p>O objetivo deste artigo é mostrar como é fácil construir um sistema de busca semântica usando PostgreSQL e pgai. No entanto, é mostrar também como podemos evoluir à proposta anterior e construir um sistema de busca semântica mais robusto e escalável usando PostgreSQL e pgai e Clojure.</p>


  <pre><code class="language-clojure">;; src/docai/pg.clj
(ns docai.pg
  (:require [next.jdbc :as jdbc]
            [clojure.data.json :as json]))

(def db-spec
  {:dbtype &#34;postgresql&#34;
   :dbname &#34;postgres&#34;
   :host &#34;localhost&#34;
   :user &#34;postgres&#34;
   :password &#34;password&#34;})

(defn query-semantic-search
  &#34;Realiza busca semântica via PostgreSQL&#34;
  [query limit]
  (let [conn (jdbc/get-connection db-spec)
        sql (str &#34;WITH query_embedding AS (&#34;
                 &#34;  SELECT ai.ollama_embed(&#39;nomic-embed-text&#39;, ?, host =&gt; &#39;http://ollama:11434&#39;) AS embedding&#34;
                 &#34;)&#34;
                 &#34;SELECT&#34;
                 &#34;  d.titulo,&#34;
                 &#34;  d.conteudo,&#34;
                 &#34;  d.categoria,&#34;
                 &#34;  t.embedding &lt;=&gt; (SELECT embedding FROM query_embedding) AS distancia&#34;
                 &#34; FROM documentos_embeddings t&#34;
                 &#34; LEFT JOIN documentos d ON t.id = d.id&#34;
                 &#34; ORDER BY distancia&#34;
                 &#34; LIMIT ?&#34;)
        results (jdbc/execute! conn [sql query limit])]
    results))</code></pre>
 <blockquote>
<p><strong>NOTA</strong>: O código acima é um exemplo de como integrar a busca semântica no PostgreSQL com uma aplicação Clojure. O código completo está disponível no <a href="https://github.com/scovl/docai">https://github.com/scovl/docai</a>.</p></blockquote>
<h2 id="configuração-de-contêineres-e-resolução-de-problemas">Configuração de Contêineres e Resolução de Problemas</h2>
<p>Ao trabalhar com contêineres Docker ou Podman, você pode encontrar alguns desafios específicos relacionados à comunicação entre serviços. Vamos explorar algumas dicas para garantir que sua configuração funcione sem problemas:</p>
<h3 id="nomeação-de-contêineres-e-comunicação-entre-serviços">Nomeação de Contêineres e Comunicação entre Serviços</h3>
<p>Quando os serviços estão em contêineres separados, a comunicação entre eles pode ser complicada. Existem várias maneiras de referenciar um contêiner a partir de outro:</p>


  <pre><code class="language-clojure">;; Exemplo de diferentes URLs para alcançar o serviço Ollama
(def alternative-hosts 
  [&#34;http://pgai-ollama-1:11434&#34;    ;; Nome do contêiner específico (mais confiável)
   &#34;http://ollama:11434&#34;           ;; Nome do serviço (conforme definido no arquivo docker/podman-compose)
   &#34;http://172.18.0.2:11434&#34;       ;; IP do contêiner (pode mudar entre reinicializações)
   &#34;http://host.docker.internal:11434&#34; ;; Especial para acessar o host a partir do contêiner
   &#34;http://localhost:11434&#34;])      ;; Funciona apenas se mapeado para a porta do host</code></pre>
 <p>O método mais confiável é usar o nome exato do contêiner (algo como <code>pgai-ollama-1</code>), que pode ser descoberto com o comando <code>docker ps</code> ou <code>podman ps</code>.</p>
<h3 id="solução-de-problemas-de-conexão">Solução de Problemas de Conexão</h3>
<p>Se você estiver enfrentando problemas de conexão, uma abordagem robusta é implementar um sistema de fallback que tente diferentes URLs:</p>


  <pre><code class="language-clojure">(defn call-ollama-api
  &#34;Chama a API do Ollama com múltiplas tentativas de conexão&#34;
  [prompt]
  (let [primary-url &#34;http://ollama:11434/api/generate&#34;
        options {:headers {&#34;Content-Type&#34; &#34;application/json&#34;}
                 :body (json/write-str {:model &#34;deepseek-r1&#34;
                                       :prompt prompt})}
        
        ;; Tentar primeiro com a URL primária
        primary-result (try-single-url primary-url options)]
    
    (if (:success primary-result)
      (:result primary-result)
      (do
        (println &#34;⚠️ Erro na chamada primária, tentando URLs alternativas...&#34;)
        
        ;; Tentar URLs alternativas
        (let [alternative-hosts [&#34;http://pgai-ollama-1:11434&#34; 
                                &#34;http://172.18.0.2:11434&#34; 
                                &#34;http://host.docker.internal:11434&#34; 
                                &#34;http://localhost:11434&#34;]
              successful-result (some (fn [host]
                                       (let [alt-url (str host &#34;/api/generate&#34;)
                                             result (try-single-url alt-url options)]
                                         (when (:success result)
                                           (println &#34;✅ Conexão bem-sucedida com&#34; alt-url)
                                           (:result result))))
                                     alternative-hosts)]
          (or successful-result
              (str &#34;Não foi possível conectar ao Ollama usando nenhum dos endpoints disponíveis.&#34;)))))))</code></pre>
 <p>Esta abordagem tenta vários endpoints diferentes e usa o primeiro que funcionar. A função <code>call-ollama-api</code> primeiro tenta se conectar a uma URL primária e, caso falhe, percorre uma lista de URLs alternativas até encontrar uma conexão bem-sucedida. Para cada tentativa, ela utiliza a função auxiliar <code>try-single-url</code> que encapsula a lógica de tratamento de erros.</p>
<p>A implementação segue um padrão de fallback, onde a função retorna o resultado da primeira conexão bem-sucedida ou uma mensagem de erro caso todas as tentativas falhem. Este método é particularmente útil em ambientes containerizados, onde os endereços de rede podem variar dependendo da configuração do <a href="https://www.docker.com/">Docker</a> ou <a href="https://podman.io/">Podman</a> e da rede interna, garantindo maior resiliência à aplicação.</p>
<p>Acessando <a href="https://github.com/scovl/docai">https://github.com/scovl/docai</a>, você pode ver o código completo e testar a aplicação. Ao executar por exemplo <code>./run.bat postgres</code> temos o seguinte output:</p>


  <pre><code class="language-bash">Inicializando DocAI...
Modo PostgreSQL ativado!
ℹ️ Para usar o Ollama, certifique-se de que ele está em execução com o comando: ollama serve
ℹ️ Usando o modelo deepseek-r1. Se você ainda não o baixou, execute: ollama pull deepseek-r1
Configurando ambiente PostgreSQL para RAG...
✅ Configurado para usar Ollama dentro do contêiner Docker/Podman
🚀 Configurando PostgreSQL para RAG...
✅ Extensões vector e ai habilitadas com sucesso
✅ Tabela de documentos criada com sucesso
✅ Configurado para usar Ollama dentro do contêiner Docker/Podman
✅ Vectorizer já configurado (tabela documentos_embeddings já existe)
Importando documentos para o PostgreSQL...
✅ Documento inserido com ID: 5
✅ Arquivo importado com sucesso: resources\docs\example.md
PostgreSQL RAG pronto! Faça sua pergunta:
Como implementar JWT em Clojure?
Processando...
DEBUG - Processando query no PostgreSQL: Como implementar JWT em Clojure?
DEBUG - Detectada consulta relacionada a JWT, usando busca especial
DEBUG - Encontrados 5 documentos relacionados a JWT
DEBUG - Enviando prompt para o Ollama usando o modelo deepseek-r1
DEBUG - Tamanho do prompt após truncamento: 4442 caracteres
DEBUG - Usando URL do Ollama: http://ollama:11434/api/generate
⚠️ Erro na chamada primária: Erro ao chamar a API do Ollama:  - 
🔄 Tentando URLs alternativas...
🔄 Tentando conectar ao Ollama em http://pgai-ollama-1:11434/api/generate
⚠️ Erro ao chamar a API do Ollama:  Erro ao chamar a API do Ollama:  - 
🔄 Tentando conectar ao Ollama em http://172.18.0.2:11434/api/generate
⚠️ Erro ao chamar a API do Ollama:  Erro ao chamar a API do Ollama:  - 
🔄 Tentando conectar ao Ollama em http://host.docker.internal:11434/api/generate
⚠️ Erro ao chamar a API do Ollama:  Erro ao chamar a API do Ollama:  - 
🔄 Tentando conectar ao Ollama em http://localhost:11434/api/generate
✅ Conexão bem-sucedida com http://localhost:11434/api/generate
&lt;think&gt;
Primeiro, preciso entender como a implementação de JWT em Clojure está relacionada com a integração do Ollama. Sabemos que o documento aborda a criação de tokens JWT usando a biblioteca `buddy.sign.jwt` e a manipulação de chaves privadas com `clojure.java.security`. Além disso, é usada a biblioteca `http-kit` para interação HTTP com o Ollama.

Vou começar analisando os passos necessários para criar um token JWT. Primeiro, é preciso definir os claims que compreendem informações como ID do usuário, nome de usuário e roles. Em seguida, associar um secret key ao token. No documento, há exemplos de como usar uma string secreta ou chaves assimétricas. 

A seguir, entendo que é necessário configurar as dependências no arquivo `project.clj` para incluir as bibliotecas necessárias: `buddy/sign` e `http-kit`. Também é importante garantir que o Ollama esteja rodando com a comando adequado para pulling os modelos e executar as inferências.

Para testar, seria útil executar uma requisição POST para /login usando curl, passando os dados de login como JSON. Depois, usar o token obtido na requisição POST para /rag/query, Including o campo Authorization com o Bearer do token.

Além disso, devo considerar como lidar com as funções de Wrapping em Clojure para garantir que as requisições HTTP sejam encadeadas corretamente. Talvez seja útil estabelecer uma rotina de login que gera o token e a envia, seguida de usar esse token nas consultas RAG.

Finalmente, tenho que lidar com possíveis erros, como se o Ollama não está executando ou houver problemas de autenticação. É importante inspecionar os logs e verificar as respostas das requisições HTTP para entender quais erros estiverem ocorrendo.

No final, vou needear a documentação officially para confirmar se há mais funcionalidades disponíveis que posso explorar após a implementação básica de JWT.
&lt;/think&gt;

Para implementar a autenticação com JWT em Clojure juntamente com a integração do Ollama, siga os passos abaixo. Isso permitirá que você utilize tokens JWT para proteger suas requisições RAG.

### Passo 1: Configurar as dependências

Adicione as seguintes dependências ao seu `project.clj`:

[buddy/sign &#34;3.4.0&#34;]    ; Para geração de signatures e verificação de validade
[buddy/auth &#34;2.6.1&#34;]     ; Para funções de autenticação
[http-kit &#34;2.6.0&#34;]      ; Para manipulação de requisições HTTP
[buddy.core.keys :as keys]  ; Para geração de chaves privadas
[buddy.data.json :as json]  ; Para processamento JSON</code></pre>
 <p>Sucesso total!
Temos um sistema de busca semântica com PostgreSQL, pgvector, pgai e Ollama em Clojure funcionando! 🎉</p>
<p>Este projeto de busca semântica com PostgreSQL pode ser expandido de várias maneiras interessantes. Uma possibilidade é implementar um sistema de feedback do usuário que capture as interações e avaliações das respostas geradas, permitindo o refinamento contínuo dos resultados. Isso poderia ser feito adicionando uma tabela <code>feedback_usuarios</code> que registre a consulta original, a resposta fornecida e a avaliação do usuário (positiva ou negativa). Esses dados poderiam então ser utilizados para ajustar os parâmetros de similaridade ou até mesmo para treinar um modelo de reranking que melhore a relevância dos resultados ao longo do tempo.</p>
<p>Outra expansão valiosa seria a integração com fontes de dados externas em tempo real. Por exemplo, poderíamos criar um sistema de ingestão automática que monitore feeds RSS, APIs ou repositórios Git específicos, extraindo novos conteúdos periodicamente e atualizando nossa base de conhecimento. Isso manteria o sistema sempre atualizado com as informações mais recentes, especialmente útil em domínios que evoluem rapidamente como tecnologia e ciência. A implementação poderia utilizar workers assíncronos em Clojure que processam novas entradas em background, vetorizam o conteúdo e o inserem automaticamente no PostgreSQL sem interrupção do serviço principal. Muito legal não é?</p>
<hr>
<h3 id="persistência-de-modelos-entre-reinicializações">Persistência de Modelos entre Reinicializações</h3>
<p>Um problema comum ao trabalhar com Ollama em contêineres é que os modelos são baixados repetidamente quando os contêineres são recriados. Para evitar isso:</p>
<ol>
<li>
<p>Use volumes para armazenar os dados do Ollama:</p>


  <pre><code class="language-yaml">volumes:
  ollama_data:/root/.ollama</code></pre>
 </li>
<li>
<p>Ao parar os contêineres, evite remover os volumes:</p>


  <pre><code class="language-bash"># Incorreto (remove volumes)
docker compose down --volumes

# Correto (preserva volumes)
docker compose down</code></pre>
 </li>
<li>
<p>Implemente verificações antes de baixar modelos:</p>


  <pre><code class="language-bash"># Verificar se o modelo já existe antes de baixá-lo
docker exec pgai-ollama-1 ollama list | grep &#34;nomic-embed-text&#34; &gt; /dev/null
if [ $? -ne 0 ]; then
  echo &#34;Baixando modelo nomic-embed-text...&#34;
  docker exec pgai-ollama-1 ollama pull nomic-embed-text
else
  echo &#34;Modelo nomic-embed-text já está disponível&#34;
fi</code></pre>
 </li>
</ol>
<p>Seguindo essas práticas, você economizará largura de banda e tempo, além de melhorar significativamente a experiência do usuário.</p>
<h3 id="buscas-especializadas-para-tópicos-específicos">Buscas Especializadas para Tópicos Específicos</h3>
<p>Ao implementar seu sistema RAG, considere adicionar rotas especializadas de busca para certos tópicos. Por exemplo, se seu sistema precisa responder bem a consultas sobre JWT (JSON Web Tokens):</p>


  <pre><code class="language-clojure">(defn query-pg-rag
  &#34;Processa uma consulta com tratamento especial para certos tópicos&#34;
  [query]
  ;; Verificar primeiro se é uma consulta relacionada a JWT
  (let [lower-query (str/lower-case query)
        jwt-keywords [&#34;jwt&#34; &#34;token&#34; &#34;autenticação&#34;]]
    
    (if (some #(str/includes? lower-query %) jwt-keywords)
      ;; Busca especializada para JWT usando SQL direto
      (let [conn (jdbc/get-connection db-spec)
            docs (jdbc/execute! 
                   conn 
                   [&#34;SELECT id, titulo, conteudo FROM documentos 
                     WHERE LOWER(conteudo) LIKE ? LIMIT 5&#34;
                    &#34;%jwt%&#34;])]
        ;; Processar resultados específicos de JWT...
        )
      
      ;; Busca semântica padrão para outros tópicos
      (semantic-search query 5))))</code></pre>
 <p>Esta abordagem híbrida combina busca por palavras-chave para tópicos específicos com busca semântica para consultas gerais, melhorando a precisão global do sistema.</p>
<hr>
<h2 id="conclusão">Conclusão</h2>
<p>Neste artigo, exploramos como construir um sistema de busca semântica robusto usando PostgreSQL, pgvector, pgai e Ollama. Esta abordagem não só oferece melhor precisão em comparação com métodos tradicionais baseados em palavras-chave, mas também é altamente escalável e adequada para ambientes de produção.</p>
<p>Vimos como configurar o ambiente usando Docker/Podman, como lidar com desafios comuns de comunicação entre contêineres, e implementamos estratégias para manter a persistência de modelos e melhorar a experiência do usuário. A combinação de busca semântica com técnicas específicas para tópicos especiais, como JWT, demonstra a flexibilidade desta abordagem.</p>
<p>Para quem já trabalhou com RAG usando abordagens mais simples, como TF-IDF, esta implementação representa um salto significativo em termos de capacidades, mantendo a simplicidade operacional graças às ferramentas modernas que utilizamos.</p>
<p>Quer saber mais sobre como implementar sistemas RAG avançados em seus projetos? Confira nossos outros artigos sobre o assunto e experimente o código completo disponível no <a href="https://github.com/scovl/docai">repositório do DocAI</a>. Estamos ansiosos para ver o que você vai construir!</p>
<hr>
<h2 id="referências">Referências</h2>
<ul>
<li><a href="https://github.com/pgvector/pgvector">Documentação do pgvector</a> - Extensão do PostgreSQL para armazenar, indexar e consultar embeddings vetoriais de alta dimensionalidade.</li>
<li><a href="https://github.com/timescale/pgai">Documentação do pgai</a> - Extensão do PostgreSQL que simplifica o armazenamento e recuperação de dados para RAG e outras aplicações de IA.</li>
<li><a href="https://supabase.com/blog/openai-embeddings-postgres-vector">Embeddings Eficientes com PostgreSQL</a> - Artigo sobre como usar embeddings com PostgreSQL.</li>
<li><a href="https://www.pinecone.io/learn/hnsw-ivfflat/">HNSW vs. IVFFlat para Busca de Similaridade</a> - Artigo sobre as diferenças entre HNSW e IVFFlat para busca de similaridade.</li>
<li><a href="https://ollama.com/">Ollama - Rodando LLMs localmente</a> - Documentação do Ollama, uma ferramenta open-source para executar modelos de linguagem de grande escala localmente.</li>
<li><a href="/2025/03/23/rag/">Artigo anterior sobre RAG com Clojure</a> - Artigo sobre como implementar RAG com Clojure.</li>
</ul>
]]></content:encoded>
      
      
      <category>RAG,PostgreSQL,pgvector,pgai,Ollama,Semantic Search</category>
      
      
      
      <dc:creator>Vitor Lobo Ramos</dc:creator>
      
      
      
      
      
      <description>&lt;![CDATA[Implementando busca semântica com PostgreSQL e Ollama]]></description>
      
    </item>
    
    <item>
      <title>Prometheus e PromQL</title>
      <link>https://scovl.github.io/2025/07/27/prometheus/</link>
      <guid>https://scovl.github.io/2025/07/27/prometheus/</guid>
      <pubDate>Sun, 27 Jul 2025 23:10:18 -0300</pubDate>
      <description>&lt;![CDATA[<p>O <strong><a href="https://prometheus.io/">Prometheus</a></strong> é uma ferramenta open-source de monitoramento de sistemas e aplicações que revolucionou a forma de pensar observabilidade em ambientes distribuídos. Ele coleta e armazena métricas como séries temporais, ou seja, valores numéricos associados a um carimbo de tempo e a pares chave-valor chamados <strong><a href="https://prometheus.io/docs/concepts/data_model/#metric-names-and-labels">labels</a></strong>.</p>
<blockquote>
<p>A potência do Prometheus vem, em parte, da sua linguagem de consulta própria, <strong><a href="https://prometheus.io/docs/prometheus/latest/querying/basics/">PromQL</a></strong>, que permite criar consultas complexas para analisar os dados coletados em tempo real. A interface web integrada (Expression browser) facilita visualizar e explorar métricas, possibilitando análises rápidas para identificar tendências e anomalias.</p>]]></description>
      <content:encoded>&lt;![CDATA[<p>O <strong><a href="https://prometheus.io/">Prometheus</a></strong> é uma ferramenta open-source de monitoramento de sistemas e aplicações que revolucionou a forma de pensar observabilidade em ambientes distribuídos. Ele coleta e armazena métricas como séries temporais, ou seja, valores numéricos associados a um carimbo de tempo e a pares chave-valor chamados <strong><a href="https://prometheus.io/docs/concepts/data_model/#metric-names-and-labels">labels</a></strong>.</p>
<blockquote>
<p>A potência do Prometheus vem, em parte, da sua linguagem de consulta própria, <strong><a href="https://prometheus.io/docs/prometheus/latest/querying/basics/">PromQL</a></strong>, que permite criar consultas complexas para analisar os dados coletados em tempo real. A interface web integrada (Expression browser) facilita visualizar e explorar métricas, possibilitando análises rápidas para identificar tendências e anomalias.</p></blockquote>
<p>Desenvolvido inicialmente na SoundCloud em 2012 por <a href="https://github.com/juliusv">Julius Volz</a> e equipe, o Prometheus foi projetado para ser simples, eficiente e altamente dimensionável. Em 2016, o projeto foi adotado pela <strong><a href="https://www.cncf.io/">Cloud Native Computing Foundation (CNCF)</a></strong> como o segundo projeto hospedado (logo após o <a href="https://kubernetes.io/">Kubernetes</a>), reforçando sua maturidade e ampla adoção pela comunidade.</p>
<blockquote>
<p>Hoje, o Prometheus é um pilar no ecossistema de observabilidade cloud-native, frequentemente usado em conjunto com o Grafana para visualizações avançadas, formando uma poderosa stack de monitoramento.</p></blockquote>
<h2 id="tipos-de-métricas">Tipos de métricas</h2>
<p>O Prometheus suporta quatro tipos principais de métricas:</p>
<ul>
<li>
<p><strong><a href="https://prometheus.io/docs/concepts/metric_types/#counter">Counter (Contador)</a></strong>: Métrica cumulativa que apenas aumenta (ou zera). Indicada para quantificar eventos, como número de requisições ou erros. Por exemplo, um contador <code>http_requests_total</code> incrementa a cada requisição recebida. Contadores nunca diminuem, exceto quando reiniciados. Consultas comuns envolvem a taxa de aumento usando funções como <code>rate()</code> ou <code>increase()</code>, calculando, por exemplo, quantas requisições por segundo ocorreram em determinado intervalo.</p>
</li>
<li>
<p><strong><a href="https://prometheus.io/docs/concepts/metric_types/#gauge">Gauge (Indicador)</a></strong>: Métrica que representa um valor em um instante, podendo tanto aumentar quanto diminuir. Indicado para valores como utilização de CPU, memória ou tamanho de fila – que sobem e descem livremente. Não possui limite mínimo ou máximo fixo. Funções como <code>avg_over_time()</code>, <code>min()</code>, <code>max()</code> e <code>sum()</code> são frequentemente aplicadas sobre gauges para obter médias, mínimos, máximos ou somas ao longo do tempo.</p>
</li>
<li>
<p><strong><a href="https://prometheus.io/docs/concepts/metric_types/#histogram">Histogram (Histograma)</a></strong>: Métrica que contabiliza a distribuição de valores observados em <em>buckets</em> (faixas) predefinidos. É muito utilizada para medir latências (e.g., duração de requisiões) ou outros valores cuja distribuição importa. O Prometheus implementa histogramas através de vários contadores – um por bucket – além de contadores especiais para total de observações (<code>_count</code>) e soma dos valores (<code>_sum</code>). Consultas tipicamente usam <code>histogram_quantile()</code> para extrair percentis a partir dos buckets e funções como <code>rate()</code> ou <code>increase()</code> nos contadores para ver taxas.</p>
</li>
<li>
<p><strong><a href="https://prometheus.io/docs/concepts/metric_types/#summary">Summary (Sumário)</a></strong>: Métrica similar ao histograma, mas os cálculos de percentis e médias são feitos pelo próprio alvo instrumentado. O summary fornece diretamente percentis (por exemplo, latência p95) e contagens/agregados para um conjunto de observações. Entretanto, summaries têm a limitação de não poderem ser agregados facilmente entre múltiplas instâncias (diferente dos histogramas). Em geral, histogramas são preferidos para métricas de latência quando se quer combinar valores de várias fontes, enquanto summaries podem ser úteis para percentis muito específicos em instâncias isoladas.</p>
</li>
</ul>
<blockquote>
<p>Use Histogramas quando precisar agregar latências de múltiplas instâncias e calcular percentis globais. Use Sumários quando os percentis calculados no cliente são suficientes e a agregação não é necessária.</p></blockquote>
<p>Além desses tipos principais, o Prometheus expõe métricas especiais de estado – por exemplo, a métrica interna <code>up</code> indica se um determinado alvo foi coletado com sucesso (valor 1) ou não (0). Essa métrica é muito útil para monitorar disponibilidade de serviços: se um <strong>endpoint</strong> monitorado ficar indisponível, <code>up{instance=&quot;endpoint:porta&quot;} == 0</code> sinaliza falha. Vale notar que não existe um &ldquo;tipo&rdquo; separado para essas métricas de saúde; elas normalmente são gauges (0 ou 1) usadas para esse propósito.</p>
<h2 id="monitoramento-pull-vs-push">Monitoramento pull vs push</h2>
<p>Para entender <strong>pull</strong> vs <strong>push</strong>, imagine cuidar de plantas: no modelo <strong>pull</strong> você vai todo dia verificar se precisam de água; no modelo <strong>push</strong> as próprias plantas enviam um sinal quando precisam ser regadas. Tecnicamente, no monitoramento <strong>pull</strong> um sistema central (como o Prometheus) consulta periodicamente os alvos para coletar métricas – ele &ldquo;puxa&rdquo; as informações. Já no monitoramento <strong>push</strong>, os próprios alvos enviam (<em>empurram</em>) as métricas para um coletor central sem serem solicitados.</p>
<p><img src="https://raw.githubusercontent.com/scovl/scovl.github.io/main/post/images/tsdb/prom-pullvspush.png" alt=""></p>
<p>No Prometheus, prevalece o modelo pull. O servidor Prometheus periodicamente faz <strong>scrape</strong> (raspagem) dos dados de cada alvo exportador via HTTP, no endpoint padrão <code>/metrics</code>. Cada scrape coleta o valor atual de todas as séries expostas naquele alvo.</p>
<p>Os alvos podem ser aplicações instrumentadas que expõem suas métricas diretamente, ou <strong>exporters</strong> (exportadores) que traduzem métricas de sistemas externos para o formato do Prometheus.</p>
<p>Assim, o Prometheus obtém em intervalos regulares (por padrão a cada 15s) as métricas atuais de cada serviço, armazenando-as localmente.</p>
<p>Na imagem acima, a comparação dos modelos de coleta: à esquerda, no modo push os clientes enviam suas métricas proativamente a um gateway; à direita, no modo pull o Prometheus consulta cada cliente periodicamente. O modelo pull tem vantagens em simplicidade e confiabilidade – se um serviço cair, o Prometheus sabe (a métrica <code>up</code> fica 0) e não depende de buffers intermediários.</p>
<p>Já o modelo push pode ser útil para casos específicos, como <em>jobs</em> de curta duração ou ambientes onde não é possível expor um endpoint (nesses casos usa-se o <strong>Pushgateway</strong>, discutido adiante). Em suma, o Prometheus, por padrão, <strong>não</strong> recebe métricas ativamente; ele mesmo vai coletá-las, evitando sobrecarga nos aplicativos monitorados e detectando automaticamente indisponibilidades.</p>
<h2 id="arquitetura-do-prometheus">Arquitetura do Prometheus</h2>
<p>A arquitetura do Prometheus foi concebida para facilitar a coleta de dados de múltiplas fontes de forma confiável e distribuída. O coração do sistema é o <strong><a href="https://prometheus.io/docs/prometheus/latest/components/prometheus/">Prometheus Server</a></strong> principal, responsável por agendar e realizar as coletas (<em>scrapes</em>) de cada alvo monitorado e armazenar as séries temporais resultantes localmente.</p>
<p>A configuração dessas coletas é definida em um arquivo YAML (geralmente <code>prometheus.yml</code>), especificando <strong><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#job_name">jobs</a></strong> e <strong><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#static_configs">targets</a></strong> – por exemplo, &ldquo;coletar métricas do serviço X na URL Y a cada 15 segundos&rdquo;. A figura abaixo (extraída da documentação oficial) ilustra a arquitetura e os componentes do ecossistema Prometheus:</p>
<p><img src="https://raw.githubusercontent.com/scovl/scovl.github.io/refs/heads/main/blog/content/post/images/tsdb/arch.png" alt=""></p>
<p>Em resumo, o fluxo é: o Prometheus <strong>coleta (pull)</strong> métricas dos jobs instrumentados, diretamente dos serviços ou via um componente intermediário de push para jobs efêmeros. Todos os samples coletados são armazenados localmente no banco de dados de séries temporais embutido (<a href="https://prometheus.io/docs/prometheus/latest/storage/tsdb/">TSDB</a>).</p>
<p>Regras definidas podem ser executadas continuamente sobre esses dados – seja para gravar novas séries agregadas (<a href="https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/">recording rules</a>) ou para acionar <strong><a href="https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/">alertas</a></strong>. Os alertas gerados pelo Prometheus são então enviados para o <strong><a href="https://prometheus.io/docs/alerting/latest/alertmanager/">Alertmanager</a></strong> processar. Por fim, ferramentas de visualização como o <strong><a href="https://grafana.com/">Grafana</a></strong> podem consultar o Prometheus para exibir dashboards das métricas coletadas.</p>
<p>O ecossistema Prometheus possui diversos componentes (muitos opcionais) que interagem nessa arquitetura:</p>
<ul>
<li><strong><a href="https://prometheus.io/docs/prometheus/latest/components/prometheus/">Servidor Prometheus</a></strong> – o servidor principal que coleta e armazena as métricas e processa consultas PromQL.</li>
<li><strong><a href="https://prometheus.io/docs/instrumenting/clientlibs/">Bibliotecas cliente</a></strong> – usadas para instrumentar código de aplicações (expondo métricas via /metrics). Há libs oficiais em Go, Java, Ruby, Python, etc.</li>
<li><strong><a href="https://prometheus.io/docs/instrumenting/exporters/">Exporters</a></strong> – programas externos que coletam métricas de serviços ou sistemas terceiros (bancos de dados, servidores web, sistemas operacionais) e as expõem no formato Prometheus. Exemplos: Node Exporter (métricas de sistema Linux), Blackbox Exporter (monitoramento de endpoints externos), etc.</li>
<li><strong><a href="https://prometheus.io/docs/instrumenting/pushing/">Pushgateway</a></strong> – gateway para receber métricas <em>pushed</em> por aplicativos de curta duração ou ambientes onde não dá para o Prometheus puxar diretamente.</li>
<li><strong><a href="https://prometheus.io/docs/alerting/latest/alertmanager/">Alertmanager</a></strong> – componente responsável por receber alertas enviados pelo Prometheus e gerenciar o envio de notificações (email, Slack, PagerDuty etc.), realizando agrupamento, deduplicação e silenciamento conforme configurado.</li>
<li><strong><a href="https://prometheus.io/docs/prometheus/latest/tools/">Ferramentas de suporte</a></strong> – englobam utilitários de linha de comando (como o promtool), exportadores de terceiros, dashboards pré-configurados, entre outros, que facilitam operar e integrar o Prometheus.</li>
</ul>
<p>Essa arquitetura descentralizada (com coleta <strong><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#pull_interval">pull</a></strong> e componentes distintos) torna o Prometheus especialmente adequado a ambientes modernos com microsserviços e orquestração de contêineres (<a href="https://www.docker.com/">Docker</a>, <a href="https://kubernetes.io/">Kubernetes</a>).</p>
<p>Ele foi projetado para funcionar de forma autônoma em cada nó (cada servidor Prometheus é independente, sem dependência de armazenamento distribuído), privilegiando confiabilidade mesmo durante falhas de rede ou de outros serviços. Em caso de problemas graves na infraestrutura, você ainda consegue acessar métricas recentes localmente no Prometheus, que atua como fonte de verdade para diagnosticar incidentes.</p>
<h2 id="labels-e-samples">Labels e Samples</h2>
<p>No Prometheus, <strong><a href="https://prometheus.io/docs/concepts/data_model/#metric-names-and-labels">labels</a></strong> (rótulos) e <strong><a href="https://prometheus.io/docs/concepts/data_model/#samples-and-series">samples</a></strong> (amostras) são conceitos-chave para organizar os dados monitorados.</p>
<p>Uma analogia simples: imagine um guarda-roupa onde cada roupa tem etiquetas indicando cor, tamanho e tipo. Essas etiquetas ajudam a encontrar rapidamente, por exemplo, &ldquo;camisetas verdes tamanho M&rdquo;.</p>
<p>Da mesma forma, no Prometheus cada métrica pode ter vários <strong><a href="https://prometheus.io/docs/concepts/data_model/#metric-names-and-labels">labels</a></strong> (chave=valor) que a qualificam.</p>
<p>Por exemplo, uma métrica <code>app_memory_usage_bytes</code> poderia ter labels como <code>host=&quot;servidor1&quot;</code> e <code>region=&quot;us-east&quot;</code>. Assim podemos filtrar/consultar &ldquo;uso de memória no servidor1&rdquo; apenas buscando por <code>host=&quot;servidor1&quot;</code>.</p>
<p>Os <strong><a href="https://prometheus.io/docs/concepts/data_model/#metric-names-and-labels">labels</a></strong> permitem um modelo de dados multidimensional – ou seja, uma mesma métrica (ex: <code>http_requests_total</code>) é armazenada separadamente para cada combinação de labels (rota=&quot;/login&quot;, método=&ldquo;GET&rdquo;, código=&ldquo;200&rdquo;, etc.). Isso enriquece as análises, pois podemos agregar ou dividir métricas por essas dimensões conforme necessário.</p>
<p><img src="https://raw.githubusercontent.com/scovl/scovl.github.io/master/post/images/tsdb/samples01.png" alt=""></p>
<p>Já os <strong><a href="https://prometheus.io/docs/concepts/data_model/#samples-and-series">samples</a></strong> são as unidades de dado coletadas ao longo do tempo – cada medição individual de uma métrica em um determinado instante.</p>
<p>Voltando à analogia, se pedíssemos a cada criança numa pesquisa que escolhesse 3 balas, as balas escolhidas por cada criança seriam uma <strong>amostra</strong> da preferência de balas.</p>
<p>No contexto do Prometheus, a cada scrape o valor de cada métrica coletada é um sample (com timestamp e valor). Esses samples ficam armazenados como uma série temporal etiquetada, permitindo ver a evolução daquele valor no tempo.</p>
<p>Por exemplo, considere a métrica gauge <code>node_cpu_usage</code> com label <code>host</code>. Para cada host monitorado, teremos uma série separada, e a cada intervalo de coleta obtemos um sample novo do uso de CPU daquele host. Assim, podemos consultar a série para ver como a CPU variou ao longo de um dia inteiro para cada máquina.</p>
<blockquote>
<p><strong>Exemplo de séries temporais no Prometheus</strong>: cada ponto representa um sample (valor observado) etiquetado por instância ou outra dimensão, armazenado em sequência temporal.</p></blockquote>
<p>Em resumo, <strong><a href="https://prometheus.io/docs/concepts/data_model/#metric-names-and-labels">labels</a></strong> fornecem contexto (quem, onde, o quê) e <strong><a href="https://prometheus.io/docs/concepts/data_model/#samples-and-series">samples</a></strong> fornecem o valor numérico no tempo. Essa combinação é o que torna o Prometheus poderoso para agregar métricas semelhantes e, ao mesmo tempo, permitir recortes por dimensão. Vale ressaltar a importância de escolher labels com cardinalidade controlada – ou seja, evitar labels que possam assumir valores extremamente variados (como IDs únicos, URLs completas ou timestamps).</p>
<blockquote>
<p><strong>Nota:</strong> Labels com variação descontrolada podem causar uma explosão de séries e sobrecarregar o Prometheus, conforme discutiremos em melhores práticas.</p></blockquote>
<h2 id="instalação">Instalação</h2>
<p>Existem diversas maneiras de instalar e executar o Prometheus. Aqui vou demonstrar uma configuração simples usando <strong><a href="https://www.docker.com/">Docker</a></strong> e <strong><a href="https://docs.docker.com/compose/">Docker Compose</a></strong>, incluindo o Grafana e uma ferramenta de simulação de métricas chamada <strong><a href="https://github.com/dmitsh/promsim">PromSim</a></strong> (útil para testes). Essa stack de exemplo traz:</p>
<ul>
<li><strong><a href="https://prometheus.io/">Prometheus</a></strong> – servidor de métricas.</li>
<li><strong><a href="https://grafana.com/">Grafana</a></strong> – para dashboards e visualização.</li>
<li><strong><a href="https://github.com/dmitsh/promsim">PromSim</a></strong> – um simulador que expõe métricas aleatórias para exercitar o Prometheus.</li>
</ul>
<p>Comece criando um arquivo <code>docker-compose.yml</code> com o seguinte conteúdo:</p>


  <pre><code class="language-yaml">version: &#34;3&#34;
services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - &#34;9090:9090&#34;
    volumes:
      - &#34;./prometheus.yml:/etc/prometheus/prometheus.yml&#34;
    depends_on:
      - promsim

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - &#34;3000:3000&#34;

  promsim:
    image: sysdigtraining/promsim:latest
    container_name: promsim
    ports:
      - &#34;8080:8080&#34;</code></pre>
 <p>No mesmo diretório, crie o arquivo de configuração <code>prometheus.yml</code> para o Prometheus:</p>


  <pre><code class="language-yaml">global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: &#34;promsim&#34;
    static_configs:
      - targets: [&#34;promsim:8080&#34;]</code></pre>
 <p>Esse arquivo define que o Prometheus fará scrape a cada 15s (<code>scrape_interval</code>) e avalia regras na mesma frequência (<code>evaluation_interval</code>). Em <code>scrape_configs</code>, temos um job chamado &ldquo;promsim&rdquo; que coleta métricas do endereço <code>promsim:8080</code> (nosso container PromSim simulando um alvo de métricas). Agora suba os serviços:</p>


  <pre><code class="language-bash">docker-compose up -d</code></pre>
 <p>Isso iniciará os containers Prometheus, Grafana e PromSim em segundo plano. Após o start, acesse o Grafana em <strong><a href="http://localhost:3000">http://localhost:3000</a></strong> (usuário <strong>admin</strong>, senha <strong>admin</strong> padrão). No Grafana, adicione o Prometheus como fonte de dados: vá em <em>Configuration (engrenagem) &gt; Data Sources</em>, adicione nova fonte do tipo Prometheus com URL <strong><a href="http://prometheus:9090">http://prometheus:9090</a></strong> (que, devido ao Docker Compose, resolve para o container do Prometheus).</p>
<p>Feito isso, você já pode importar ou criar painéis Grafana usando as métricas do Prometheus (inclusive as geradas pelo PromSim). O PromSim estará expondo várias métricas aleatórias – por exemplo, simulando CPU, memória, requisições – permitindo testar consultas e alertas sem precisar de uma aplicação real por trás. Para mais detalhes do PromSim, veja <strong><a href="https://github.com/dmitsh/promsim">a documentação oficial</a></strong>.</p>
<p>Caso queira rodar apenas o Prometheus isoladamente, basta executar o container oficial: <code>docker run -p 9090:9090 prom/prometheus</code>. Depois acesse <strong><a href="http://localhost:9090">http://localhost:9090</a></strong> para abrir a UI nativa do Prometheus:</p>
<p><img src="https://raw.githubusercontent.com/scovl/scovl.github.io/master/post/images/tsdb/ui01.png" alt=""></p>
<p>A interface web padrão do Prometheus inclui os seguintes menus no topo:</p>
<ul>
<li><strong><a href="/alerts">Alerts</a></strong>: lista os alertas ativos e suas informações. Mostra também alertas pendentes e silenciados.</li>
<li><strong><a href="/graph">Graph</a></strong>: permite rodar consultas PromQL e visualizar o resultado em formato gráfico (ou tabela). É útil para explorar interativamente as métricas.</li>
<li><strong><a href="/status">Status</a></strong>: informações sobre o status do servidor Prometheus – memória usada, número de séries ativas, status das coletas, etc.
<ul>
<li><strong><a href="/targets">Targets</a></strong> (na seção Status): mostra todos os alvos configurados e se a coleta está OK (up) ou falhou.</li>
<li><strong><a href="/service-discovery">Service Discovery</a></strong> (também em Status): lista os serviços descobertos via mecanismos dinâmicos (Kubernetes, DNS, etc.).</li>
</ul>
</li>
<li><strong><a href="/classic/targets">Help</a></strong>: link para documentação e ajuda do Prometheus.</li>
</ul>
<p>Além disso, logo abaixo dos menus, a UI oferece algumas opções e campos importantes:</p>
<ul>
<li><strong><a href="https://prometheus.io/docs/prometheus/latest/querying/basics/#time-range-and-resolution-selection">Time range e refresh</a></strong>: controles para selecionar o intervalo de tempo da consulta e atualizar automaticamente.</li>
<li><strong><a href="https://prometheus.io/docs/prometheus/latest/querying/basics/#time-range-and-resolution-selection">Use local time</a></strong>: alterna entre exibir os timestamps no seu fuso horário local ou em UTC.</li>
<li><strong><a href="https://prometheus.io/docs/prometheus/latest/querying/basics/#query-history">Query history</a></strong>: opção para habilitar histórico das consultas feitas (facilita repetir queries recentes).</li>
<li><strong><a href="https://prometheus.io/docs/prometheus/latest/querying/basics/#autocomplete">Autocomplete</a></strong>: opção para habilitar auto-completar de métricas e funções no campo de consulta.</li>
<li><strong><a href="https://prometheus.io/docs/prometheus/latest/querying/basics/#expression-language-promql">Campo de consulta PromQL</a></strong>: onde você escreve a expressão a ser consultada. O Prometheus traz sugestões enquanto você digita (se autocomplete ligado).</li>
<li><strong><a href="https://prometheus.io/docs/prometheus/latest/querying/basics/#execute-and-reset">Botões Execute / Reset</a></strong>: para executar a consulta ou limpar o campo.</li>
<li><strong><a href="https://prometheus.io/docs/prometheus/latest/querying/basics/#graph-and-table">Aba Graph / Table</a></strong>: seleciona se o resultado será plotado em um gráfico ou mostrado como tabela bruta de valores.</li>
<li><strong><a href="https://prometheus.io/docs/prometheus/latest/querying/basics/#evaluation-time">Evaluation time</a></strong>: permite fixar um timestamp específico para avaliar a query (por padrão é &ldquo;now&rdquo;, mas você pode ver valores históricos escolhendo um horário passado).</li>
</ul>
<blockquote>
<p><strong>Dica:</strong> a UI do Prometheus é ótima para explorar e depurar métricas rapidamente, mas para dashboards permanentes e mais bonitos geralmente usamos o Grafana. O Grafana se conecta ao Prometheus via API e permite combinar múltiplas consultas em gráficos customizados.</p></blockquote>
<h3 id="configuração">Configuração</h3>
<p>Após instalar, o principal arquivo a ajustar é o de <strong><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/">configuração do Prometheus</a></strong> (<code>prometheus.yml</code>). Nele definimos os parâmetros globais, jobs de scrape, regras de alerta, etc. Vamos examinar a estrutura básica e algumas customizações comuns. Um exemplo mínimo de <code>prometheus.yml</code> poderia ser:</p>


  <pre><code class="language-yaml">global:
  scrape_interval: 15s

scrape_configs:
  - job_name: &#39;prometheus&#39;
    static_configs:
      - targets: [&#39;localhost:9090&#39;]</code></pre>
 <p>Nesse caso, definimos um intervalo global de scrape de 15s e um job para monitorar o próprio Prometheus (expondo métricas em <a href="http://localhost:9090">localhost:9090</a>). Para monitorar outras aplicações, adicionamos novos blocos em <code>scrape_configs</code>. Por exemplo, para monitorar uma aplicação web rodando na porta 8080 de um host chamado <code>my-app</code>:</p>


  <pre><code class="language-yaml">scrape_configs:
  - job_name: &#39;my-app&#39;
    static_configs:
      - targets: [&#39;my-app:8080&#39;]</code></pre>
 <p>Isso instruirá o Prometheus a coletar periodicamente métricas em <strong><a href="http://my-app:8080/metrics">http://my-app:8080/metrics</a></strong>. Podemos repetir o processo para cada serviço ou componente que queremos incluir, definindo um <code>job_name</code> descritivo e a lista de endpoints (targets).</p>
<p>Para ambientes com muitos alvos ou infraestrutura dinâmica, é inviável gerenciar esses targets manualmente. Nesses casos, o Prometheus oferece integrações de <strong>Service Discovery</strong> (<a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config">Kubernetes</a>, <a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#ec2_sd_config">AWS EC2</a>, <a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#consul_sd_config">Consul</a>, <a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dns_sd_config">DNS</a>, etc.) e também o <strong>file-based discovery</strong> (descoberta via arquivos).</p>
<blockquote>
<p>Este último permite apontar para um ou mais arquivos JSON externos contendo a lista de targets. Assim, ferramentas externas ou scripts podem atualizar esses arquivos conforme os serviços mudam, e o Prometheus percebe as alterações automaticamente. Por exemplo, poderíamos alterar o job acima para usar arquivo:</p></blockquote>


  <pre><code class="language-yaml">scrape_configs:
  - job_name: &#39;my-app&#39;
    file_sd_configs:
      - files:
          - /etc/prometheus/targets/my-app.json</code></pre>
 <p>E no arquivo <code>/etc/prometheus/targets/my-app.json</code> colocar algo como:</p>


  <pre><code class="language-json">[
  {
    &#34;labels&#34;: {
      &#34;job&#34;: &#34;my-app&#34;,
      &#34;env&#34;: &#34;production&#34;
    },
    &#34;targets&#34;: [
      &#34;my-app1:8080&#34;,
      &#34;my-app2:8080&#34;
    ]
  }
]</code></pre>
 <p>Nesse JSON, especificamos dois targets (dois instâncias da aplicação <code>my-app</code>) e também atribuímos labels adicionais a essas instâncias (<code>env: production</code>, por exemplo). Assim, se futuramente adicionarmos <code>my-app3:8080</code>, basta atualizar o JSON – o Prometheus recarrega periodicamente ou quando o arquivo muda. Esse método facilita escalabilidade e automação da configuração de alvos.</p>
<p>Outro ponto de configuração importante é a <strong>retenção de dados</strong>. Por padrão, o Prometheus guarda as séries temporais localmente por 15 dias. Em ambientes de produção, pode ser necessário ajustar esse período.</p>
<p>Você pode definir a flag de inicialização <code>--storage.tsdb.retention.time</code> (ou configurar no serviço) para algo maior, por exemplo <code>30d</code> para reter ~1 mês de métricas. Tenha em mente que aumentar a retenção aumenta proporcionalmente o consumo de disco e memória.</p>
<p>Também é possível limitar por tamanho de disco (<code>--storage.tsdb.retention.size</code>), se preferir. Caso precise de retenção muito longa (meses/anos), é recomendável integrar com soluções de armazenamento remoto em vez de manter tudo no Prometheus (falaremos disso em <em>Melhores Práticas</em>).</p>
<p>Exemplo de definição de retenção no <strong><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration-file">systemd</a></strong> (ExecStart):</p>


  <pre><code class="language-bash">/opt/prometheus/prometheus \
  --config.file=/opt/prometheus/prometheus.yml \
  --storage.tsdb.retention.time=30d</code></pre>
 <blockquote>
<p><strong>Nota:</strong> O formato aceita unidades como <code>h</code>, <code>d</code>, <code>w</code>, <code>y</code>. Você também pode usar a opção <code>--storage.tsdb.retention.size</code> para definir um tamanho máximo (por ex: <code>50GB</code>), o que ocorrer primeiro (tempo ou tamanho) aciona a limpeza de dados antigos.</p></blockquote>
<p>Em instalações via pacote ou container, normalmente a estrutura de diretórios do Prometheus é assim:</p>


  <pre><code class="language-">/opt/prometheus/
├── prometheus (binário)
├── promtool   (binário utilitário)
├── prometheus.yml (configuração)
├── consoles/  (arquivos HTML da UI &#34;classic&#34;)
├── console_libraries/ (bibliotecas JS para consoles)
└── data/      (armazenamento local das séries temporais)</code></pre>
 <p>A pasta <code>data/</code> merece destaque – ali ficam todos os dados das métricas coletadas. Abordaremos sua estrutura interna na seção &ldquo;Under the Hood&rdquo;.</p>
<blockquote>
<p>Em resumo, após instalar, você deve editar o <code>prometheus.yml</code> para incluir todos os targets que deseja monitorar (seja listando estaticamente ou via mecanismos dinâmicos) e ajustar parâmetros globais (intervalos, regras, retenção).</p></blockquote>
<p>Depois reinicie o serviço/container do Prometheus para aplicar as alterações. Para validar se a sintaxe do arquivo está correta antes de reiniciar, podemos usar o <strong><a href="https://prometheus.io/docs/prometheus/latest/tools/promtool/">promtool</a></strong> conforme abaixo.</p>
<h2 id="-instrumentação">🔍 Instrumentação</h2>
<p>A <strong>instrumentação</strong> é o processo de inserir coleta de métricas em sistemas e aplicações. No contexto Prometheus, podemos dividir em dois tipos:</p>
<h3 id="-instrumentação-direta-na-aplicação">📊 Instrumentação direta (na aplicação)</h3>
<p>Significa instrumentar o próprio código da aplicação ou serviço para expor métricas de negócio ou de desempenho relevantes. Você adiciona pontos de métrica no código (<a href="https://prometheus.io/docs/concepts/metric_types/#counter">counters</a>, <a href="https://prometheus.io/docs/concepts/metric_types/#gauge">gauges</a>, etc.) usando uma biblioteca cliente do Prometheus.</p>
<p>Assim, a própria aplicação passa a expor um endpoint <code>/metrics</code> com dados em tempo real sobre si mesma (latência de requisições, uso de memória interno, tamanho de fila, etc.).</p>
<p>Essa abordagem dá controle granular – os desenvolvedores escolhem o que medir – e tende a fornecer métricas altamente específicas e úteis para diagnosticar o comportamento da aplicação.</p>
<h3 id="-instrumentação-indireta-via-exporters">🔄 Instrumentação indireta (via exporters)</h3>
<p>Refere-se a coletar métricas de sistemas externos ou legados através de componentes intermediários chamados <strong><a href="https://prometheus.io/docs/instrumenting/exporters/">exporters</a></strong>. Em vez de modificar o sistema alvo, você roda um exporter que coleta informações daquele sistema (geralmente via APIs existentes, comandos ou leitura de arquivos) e as expõe no formato Prometheus.</p>
<p>O Prometheus então faz scrape nesse exporter. Essa abordagem é comum para: sistemas operacionais, bancos de dados, servidores web, ou qualquer software que não tenha suporte nativo ao Prometheus.</p>
<p>Por exemplo, há exporters para <strong><a href="https://github.com/prometheus/mysqld_exporter">MySQL</a></strong>, <strong><a href="https://github.com/prometheus/postgres_exporter">PostgreSQL</a></strong>, <strong><a href="https://github.com/nginxinc/nginx-prometheus-exporter">Apache/Nginx</a></strong>, <strong><a href="https://github.com/oliver006/redis_exporter">Redis</a></strong>, entre muitos outros, que traduzem métricas desses sistemas para o formato esperado.</p>
<p>Ambos os tipos são importantes. A instrumentação direta fornece métricas sob medida da aplicação (por exemplo, quantas transações processou, quantos usuários ativos, etc.), enquanto a indireta garante visibilidade de componentes de infraestrutura e softwares de terceiros sem precisar alterar eles.</p>
<p>A seguir, veremos exemplos de instrumentação indireta (principais exporters) e de instrumentação direta em algumas linguagens.</p>
<h3 id="instrumentação-indireta-exporters">Instrumentação indireta: Exporters</h3>
<p><strong>Ecossistema nativo:</strong> O Prometheus já oferece diversos exporters oficiais ou mantidos pela comunidade para sistemas populares. Alguns exemplos:</p>
<ul>
<li>
<p><strong><a href="https://github.com/prometheus/node_exporter">Node Exporter</a></strong> (Linux): Coleta métricas de sistema operacional Linux – CPU, memória, disco, rede, entropia, stats de kernel, etc. É imprescindível para monitorar VMs ou servidores bare metal. Basta executar o binário do node_exporter no host; ele abre :9100/metrics com dezenas de métricas padronizadas (cpu_seconds_total, node_filesystem_usage_bytes, etc.). Essas métricas dão uma visibilidade completa do estado do host, permitindo identificar gargalos de recurso.</p>
</li>
<li>
<p><strong><a href="https://github.com/prometheus/wmic_exporter">Windows Exporter</a></strong> (Windows): Equivalente para plataformas Windows (antigo WMI exporter). Coleta CPU, memória, disco, contadores do Windows, etc., expondo em :9182/metrics (porta padrão). Assim, ambiente heterogêneos também podem ser monitorados.</p>
</li>
<li>
<p><strong><a href="https://github.com/prometheus/blackbox_exporter">Blackbox Exporter</a></strong>: Útil para monitorar <em>externamente</em> a disponibilidade de serviços. Ele executa <em>probes</em> do tipo ICMP (ping), HTTP(S), DNS, TCP, etc., simulando a experiência do usuário externo. Você configura módulos de probe (ex: checar HTTP 200 em determinada URL dentro de 2s) e o Prometheus chama o Blackbox passando o alvo a testar. Se a resposta falha ou excede tempo, métricas como <code>probe_success</code>=0 ou <code>probe_duration_seconds</code> indicam problema. É excelente para monitorar uptime de sites e endpoints de fora para dentro.</p>
</li>
<li>
<p><strong><a href="https://prometheus.io/docs/instrumenting/exporters/">Exporters de aplicações</a></strong>: Há muitos: PostgreSQL exporter, Redis exporter, JMX exporter (Java), SNMP exporter (equipamentos de rede), etc. Em geral, se você usar alguma tecnologia popular, provavelmente já existe um exporter pronto (a documentação oficial lista dezenas: <strong><a href="https://prometheus.io/docs/instrumenting/exporters/">Exporters e integrações</a></strong>).</p>
</li>
</ul>
<blockquote>
<p><strong>Como usar exporters?</strong> Normalmente é executar o binário do exporter próximo do serviço alvo, e então adicionar um job no <code>prometheus.yml</code> apontando para o endpoint do exporter. Por exemplo, para Node Exporter em várias máquinas, você rodaria node_exporter em cada host (porta 9100) e adicionaria algo como:</p></blockquote>


  <pre><code class="language-yaml">scrape_configs:
  - job_name: &#39;node&#39;
    static_configs:
      - targets: [&#39;host1:9100&#39;, &#39;host2:9100&#39;, ...]</code></pre>
 <p>Assim o Prometheus coletará as métricas de cada máquina. Cada métrica virá automaticamente com labels como <code>instance=&quot;host1:9100&quot;</code> e outras específicas (o Node Exporter adiciona label <code>job=&quot;node&quot;</code> e por vezes labels como <code>cpu=&quot;0&quot;</code> para métricas por CPU, etc.).</p>
<blockquote>
<p>Em resumo, a instrumentação indireta via exporters é fundamental para trazer para o Prometheus dados de componentes que não expõem nativamente as métricas. É um jeito de <em>bridge</em> (ponte) entre sistemas legados e o moderno mundo do Prometheus.</p></blockquote>
<h2 id="configuração-avançada">Configuração Avançada</h2>
<h3 id="discovery-dinâmico-e-relabeling">Discovery Dinâmico e Relabeling</h3>
<p>Em ambientes modernos com infraestrutura dinâmica (Kubernetes, cloud, microsserviços), configurar targets manualmente no <code>prometheus.yml</code> torna-se inviável. O Prometheus oferece mecanismos de <strong>Service Discovery</strong> que permitem descobrir automaticamente alvos para monitoramento, e o <strong>Relabeling</strong> permite transformar dinamicamente essas descobertas durante o processo de configuração.</p>
<h4 id="service-discovery">Service Discovery</h4>
<p>O Prometheus suporta diversos mecanismos de descoberta automática:</p>
<ul>
<li><strong><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config">Kubernetes</a></strong>: Descobre pods, serviços, endpoints automaticamente baseado em labels e anotações.</li>
<li><strong><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#ec2_sd_config">AWS EC2</a></strong>: Encontra instâncias EC2 baseado em tags.</li>
<li><strong><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#consul_sd_config">Consul</a></strong>: Usa o Consul como fonte de verdade para serviços.</li>
<li><strong><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#dns_sd_config">DNS</a></strong>: Resolve nomes DNS para descobrir alvos.</li>
<li><strong><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#file_sd_config">File-based</a></strong>: Lê targets de arquivos JSON/YAML que podem ser atualizados externamente.</li>
</ul>
<p><strong>Exemplo de discovery Kubernetes:</strong></p>


  <pre><code class="language-yaml">scrape_configs:
  - job_name: &#39;kubernetes-pods&#39;
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.&#43;)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]&#43;)(?::\d&#43;)?;(\d&#43;)
        replacement: $1:$2
        target_label: __address__</code></pre>
 <h4 id="relabeling">Relabeling</h4>
<p>O <strong>relabeling</strong> é uma funcionalidade poderosa que permite transformar labels, nomes de targets, endereços e outros metadados durante o processo de discovery. É fundamental para:</p>
<ul>
<li><strong>Filtrar targets indesejados</strong> (ex: excluir pods de teste)</li>
<li><strong>Adicionar/remover labels</strong> dinamicamente</li>
<li><strong>Transformar endereços</strong> (ex: mascarar IPs internos)</li>
<li><strong>Agrupar targets</strong> logicamente</li>
</ul>
<p><strong>Exemplo prático de relabeling:</strong></p>


  <pre><code class="language-yaml">relabel_configs:
  # Manter apenas pods com annotation prometheus.io/scrape=true
  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
    action: keep
    regex: true
  
  # Extrair namespace como label
  - source_labels: [__meta_kubernetes_namespace]
    action: replace
    target_label: namespace
  
  # Adicionar label de ambiente baseado no namespace
  - source_labels: [namespace]
    regex: &#39;prod-.*&#39;
    replacement: &#39;production&#39;
    target_label: environment
  
  # Remover porta padrão se não especificada
  - source_labels: [__address__]
    regex: &#39;(.&#43;):8080&#39;
    target_label: instance
    replacement: &#39;$1&#39;
  
  # Filtrar targets que começam com &#39;test&#39;
  - action: drop
    source_labels: [__meta_kubernetes_pod_name]
    regex: &#39;test.*&#39;</code></pre>
 <p><strong>Casos de uso comuns:</strong></p>
<ul>
<li><strong>Filtros de ambiente</strong>: Manter apenas pods de produção, excluindo dev/test</li>
<li><strong>Mascaramento de dados sensíveis</strong>: Remover IPs internos ou informações de debug</li>
<li><strong>Agregação por labels</strong>: Agrupar targets por região, datacenter, time</li>
<li><strong>Normalização de nomes</strong>: Padronizar nomes de instâncias ou serviços</li>
</ul>
<blockquote>
<p><strong>Importante</strong>: O relabeling é aplicado <strong>antes</strong> do scrape, então você pode usar <code>__meta_*</code> labels (metadados do discovery) para tomar decisões sobre quais targets monitorar e como rotulá-los.</p></blockquote>
<h2 id="promql-os-fundamentos">PromQL: Os Fundamentos</h2>
<p>PromQL é a linguagem de consulta poderosa usada pelo Prometheus para extrair dados de métricas e configurar alertas. Seu principal objetivo é possibilitar a análise e monitoramento de métricas (como requisições HTTP por segundo ou a média de utilização de CPU por servidor) por meio de expressões que definem cálculos específicos.</p>
<p>O PromQL suporta funções matemáticas, operações booleanas e de comparação, além de agrupamento de dados e agregações. Ela também conta com recursos avançados, como subconsultas e funções de análise temporal.</p>
<p>As consultas PromQL podem ser executadas através da interface web do Prometheus, de APIs ou de bibliotecas de clientes. Em resumo, a PromQL é essencial para monitorar e analisar o desempenho de sistemas com eficiência e precisão.</p>
<p>A linguagem também possibilita a criação de gráficos e painéis de visualização para métricas, utilizando ferramentas como o Grafana. Desta forma, a PromQL se mostra fundamental para obter insights rápidos sobre o comportamento de aplicações e infraestruturas.</p>
<p>Nesta seção, vamos explorar os fundamentos da PromQL — incluindo seletores, tipos de vetores e operadores básicos — e demonstrar como criar consultas simples para analisar dados de métricas.</p>
<h3 id="time-series-database-tsdb">Time Series Database (TSDB)</h3>
<p>O Prometheus armazena os dados em um formato binário chamado TSDB (Time Series Database). O TSDB é um banco de dados de séries temporais otimizado para armazenar métricas de forma eficiente.</p>
<p>Para simplificar o entendimento, imagine que você tem um diário onde registra, todos os dias e nos mesmos horários, informações como a temperatura do ar, velocidade do vento e pressão atmosférica.</p>
<blockquote>
<p>Essas informações são armazenadas em ordem cronológica (por tempo) e podem ser consultadas para ver como variam ao longo do tempo. Essa é a essência de um banco de dados de série temporal: armazenar e consultar dados que possuem uma dimensão temporal.</p></blockquote>
<p>Monitorar métricas a partir de um banco de dados de séries temporais traz várias vantagens:</p>
<ul>
<li><strong>Análise histórica:</strong> Por armazenar dados em ordem cronológica, é possível analisar tendências e padrões ao longo do tempo. Isso ajuda a entender como o desempenho do sistema evolui e identificar tendências que possam indicar problemas futuros.</li>
<li><strong>Identificação de problemas:</strong> Com dados históricos, podemos investigar incidentes passados para identificar causas raiz de problemas de desempenho ou disponibilidade.</li>
<li><strong>Alertas baseados no tempo:</strong> Dados históricos permitem criar alertas que consideram tendências temporais, como alertar quando um recurso tem desempenho abaixo do normal em horários específicos ou quando há tendências de crescimento preocupantes.</li>
<li><strong>Armazenamento escalável:</strong> Bancos de dados de séries temporais são projetados para lidar com grandes volumes de dados e escalar horizontalmente, permitindo armazenar métricas sem perda de desempenho.</li>
<li><strong>Integração com outras ferramentas:</strong> A maioria das ferramentas de monitoramento suporta a coleta de dados de TSDBs, facilitando a integração com diversos sistemas de análise e observabilidade.</li>
</ul>
<p>Em resumo, usar um banco de dados de série temporal permite coletar, armazenar e analisar dados de métricas de desempenho ao longo do tempo, possibilitando identificar problemas, tendências e padrões com facilidade.</p>
<p>O PromQL (Prometheus Query Language) é a linguagem usada para consultar essas métricas armazenadas no Prometheus. Com o PromQL, os usuários criam consultas complexas para extrair informações acionáveis das métricas. Algumas capacidades importantes do PromQL incluem:</p>
<ul>
<li><strong>Funções de agregação:</strong> Permitem resumir dados ao longo do tempo ou por categorias, como média, soma, máximo e mínimo. Por exemplo, podemos usar <code>avg()</code> para calcular a média de uma métrica ao longo de um período.</li>
<li><strong>Funções de filtragem:</strong> Permitem selecionar subconjuntos das métricas com base em critérios. Por exemplo, podemos usar seletores para filtrar por rótulos (labels) específicos, como pegar apenas métricas de um serviço ou data center específico.</li>
<li><strong>Funções de transformação:</strong> Permitem transformar os dados brutos em valores mais úteis. Por exemplo, a função <code>rate()</code> calcula a taxa de mudança de um contador (como número de requisições por segundo) a partir da diferença entre dois pontos no tempo.</li>
</ul>
<p>PromQL também suporta operações matemáticas básicas (adição, subtração, multiplicação e divisão) para combinar métricas ou ajustar seus valores. Além disso, permite o uso de operadores lógicos (como <code>and</code> e <code>or</code>) para combinar expressões e criar consultas ainda mais complexas.</p>
<p>Recursos avançados, como uso de rótulos (labels) para selecionar séries específicas e subconsultas aninhadas, tornam a PromQL uma linguagem poderosa e flexível. A seguir, exploraremos em detalhes esses conceitos e como utilizá-los na prática.</p>
<h3 id="seletores-de-métricas">Seletores de métricas</h3>
<p>Os seletores em PromQL funcionam como filtros que permitem escolher uma ou mais séries de métricas específicas para consulta. Existem dois tipos principais de seletores:</p>
<ul>
<li><strong>Seletor por nome de métrica:</strong> Seleciona séries pelo nome da métrica. Por exemplo, <code>http_requests_total</code> retorna todas as séries temporais cuja métrica tenha esse nome.</li>
<li><strong>Seletor por label:</strong> Seleciona séries com base em um ou mais labels (rótulos) e seus valores. Por exemplo, se uma métrica <code>http_requests_total</code> possui os labels <code>method</code> e <code>handler</code>, podemos filtrar pelas séries onde <code>method=&quot;GET&quot;</code> e <code>handler=&quot;/api/v1/users&quot;</code> escrevendo:</li>
</ul>


  <pre><code class="language-promql">http_requests_total{method=&#34;GET&#34;, handler=&#34;/api/v1/users&#34;}</code></pre>
 <p>Para combinar seletores de label, usamos operadores de correspondência (matchers) como <code>=</code>, <code>!=</code>, <code>=~</code> (regex correspondente) e <code>!~</code> (regex negativa). Esses operadores servem para comparar valores de labels (ou aplicar expressões regulares) ao selecionar as séries desejadas. Veja alguns exemplos:</p>
<ul>
<li><strong>Selecionar todas as métricas cujo nome começa com &ldquo;http&rdquo;:</strong></li>
</ul>


  <pre><code class="language-promql">{__name__=~&#34;http.*&#34;}</code></pre>
 <p>Aqui, usamos o label especial <code>__name__</code> (que representa o nome da métrica) com uma expressão regular para corresponder qualquer métrica cujo nome comece com &ldquo;http&rdquo;.</p>
<ul>
<li><strong>Selecionar séries que possuem o label <code>status</code> com valor exatamente &ldquo;error&rdquo;:</strong></li>
</ul>


  <pre><code class="language-promql">{status=&#34;error&#34;}</code></pre>
 <ul>
<li><strong>Selecionar séries que possuem o label <code>app</code> com valor &ldquo;frontend&rdquo; ou &ldquo;backend&rdquo;:</strong></li>
</ul>


  <pre><code class="language-promql">{app=~&#34;frontend|backend&#34;}</code></pre>
 <p>Nesse caso, o operador regex <code>=~</code> com o padrão <code>frontend|backend</code> faz o seletor pegar séries cujo label <code>app</code> seja &ldquo;frontend&rdquo; <strong>ou</strong> &ldquo;backend&rdquo;.</p>
<p>Ao usar expressões regulares em seletores, é importante ter cuidado para não selecionar séries indesejadas. Por exemplo, um seletor como <code>{job=~&quot;prom.*&quot;}</code> traria <strong>todas</strong> as séries cujos labels <code>job</code> começam com &ldquo;prom&rdquo; — isso poderia incluir séries que não eram o alvo pretendido (como um job auxiliar relacionado).</p>
<p>Portanto, sempre procure ser o mais específico possível nos seletores para evitar correspondências acidentais.</p>
<h3 id="tipos-de-expressões-em-promql">Tipos de expressões em PromQL</h3>
<p>PromQL oferece vários tipos de expressões para manipular as séries temporais coletadas pelo Prometheus. As principais incluem:</p>
<ul>
<li><strong>Expressões aritméticas:</strong> Realizam cálculos matemáticos entre séries de métricas ou entre séries e constantes. Por exemplo, podemos somar duas métricas (<code>metric_a + metric_b</code>), subtrair (<code>metric_a - metric_b</code>), multiplicar (<code>metric_a * 100</code> para converter em porcentagem), etc. Exemplo:</li>
</ul>


  <pre><code class="language-promql">node_cpu_seconds_total{mode=&#34;system&#34;} / node_cpu_seconds_total{mode=&#34;idle&#34;} * 100</code></pre>
 <p>Aqui calculamos a porcentagem de tempo que a CPU está no modo <code>&quot;system&quot;</code> em relação ao tempo no modo <code>&quot;idle&quot;</code>.</p>
<ul>
<li><strong>Funções de agregação:</strong> Agrupam e resumem séries temporais. As funções incluem <code>sum</code> (soma), <code>avg</code> (média), <code>max</code> (máximo), <code>min</code> (mínimo), <code>count</code> (contagem), entre outras. Por exemplo:</li>
</ul>


  <pre><code class="language-promql">sum(rate(http_requests_total[5m])) by (job)</code></pre>
 <p>Nesta consulta, calculamos a taxa de requisições HTTP nos últimos 5 minutos (<code>rate(http_requests_total[5m])</code>) e em seguida somamos por <code>job</code>, ou seja, obtemos a taxa total por job.</p>
<ul>
<li><strong>Funções de filtro:</strong> Filtram séries temporais com base em valores ou labels. Por exemplo, a função <code>topk(5, metric)</code> retorna as 5 séries com os maiores valores para a métrica especificada. Exemplo:</li>
</ul>


  <pre><code class="language-promql">topk(5, http_requests_total)</code></pre>
 <p>Isso retornará as 5 séries de <code>http_requests_total</code> com os maiores valores.</p>
<ul>
<li>
<p><strong>Funções de transformação:</strong> Transformam séries temporais de maneiras específicas. Exemplos incluem:</p>
<ul>
<li><code>rate()</code>: calcula a taxa de aumento por segundo de um contador (derivada primeira) em uma janela de tempo.</li>
<li><code>irate()</code>: similar ao <code>rate()</code>, mas calcula a taxa instantânea entre os dois pontos de dados mais recentes.</li>
<li><code>increase()</code>: calcula o total acumulado que o contador aumentou durante o período.</li>
<li><code>delta()</code>: calcula a diferença absoluta entre o primeiro e o último valor em uma janela de tempo.</li>
<li><code>histogram_quantile()</code>: calcula um quantil (por exemplo, 0.95 para 95º percentil) a partir de um histograma.</li>
</ul>
<p>Exemplo de transformação com <code>histogram_quantile</code>:</p>


  <pre><code class="language-promql">histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))</code></pre>
 <p>Acima, estamos calculando o 95º percentil da distribuição de duração de requisições HTTP nos últimos 5 minutos, usando as séries <code>_bucket</code> do histograma de duração.</p>
</li>
<li>
<p><strong>Expressões booleanas (comparações):</strong> Avaliam condições verdadeiras ou falsas sobre os valores de séries temporais. Os operadores de comparação incluem <code>==</code> (igual), <code>!=</code> (diferente), <code>&gt;</code> (maior que), <code>&lt;</code> (menor que), <code>&gt;=</code> (maior ou igual) e <code>&lt;=</code> (menor ou igual). Por padrão, ao comparar duas séries, o resultado é uma série booleana (1 para true, 0 para false) <strong>apenas para as combinações de séries que correspondem exatamente nos labels</strong> (veremos mais sobre correspondência de vetores adiante). Também é possível usar o modificador <code>bool</code> para forçar o resultado booleano a ser retornado.</p>
<p>Um exemplo de expressão booleana combinada com cálculo:</p>
</li>
</ul>


  <pre><code class="language-promql">rate(http_requests_total{status_code=~&#34;5..&#34;}[1m]) 
  &gt; rate(http_requests_total{status_code=~&#34;2..&#34;}[1m]) * 0.1</code></pre>
 <p>Esta consulta verifica se a taxa de requisições HTTP com códigos de status 5xx no último minuto é maior que 10% da taxa de requisições 2xx no mesmo período. O resultado será uma série temporal booleana indicando, para cada combinação de labels, se a condição é verdadeira (1) ou falsa (0). Essa abordagem é útil em alertas.</p>
<h3 id="vector-vs-range-vector">Vector vs. Range Vector</h3>
<p>Em PromQL, existem dois tipos principais de vetor que podem ser retornados em consultas: <strong>Instant Vector</strong> (vetor instantâneo) e <strong>Range Vector</strong> (vetor de intervalo).</p>
<ul>
<li>
<p><strong>Instant Vector (Vetor Instantâneo):</strong> Representa um conjunto de amostras (valor + timestamp) de múltiplas séries temporais, todas no mesmo instante no tempo. Cada série temporal no resultado possui os mesmos labels originais e um único valor correspondente ao momento da avaliação. Por exemplo, a expressão <code>cpu_usage{instance=&quot;webserver-1&quot;}</code> retornaria, no momento atual, o valor mais recente da métrica <code>cpu_usage</code> para a instância <code>webserver-1</code>.</p>
</li>
<li>
<p><strong>Range Vector (Vetor de Intervalo):</strong> Representa um conjunto de séries temporais, onde cada série contém um conjunto de amostras dentro de um intervalo de tempo especificado. Em vez de um único valor, cada série traz todos os pontos (timestamp, valor) coletados naquele intervalo. Range vectors são obtidos usando a sintaxe <code>[&lt;duração&gt;]</code> após um seletor de métrica. Por exemplo, <code>cpu_usage{instance=&quot;webserver-1&quot;}[5m]</code> retorna os últimos 5 minutos de dados da métrica <code>cpu_usage</code> para a instância <code>webserver-1</code>. As funções como <code>rate()</code>, <code>increase()</code> e <code>avg_over_time()</code> tipicamente esperam um range vector como entrada.</p>
</li>
</ul>
<p><strong>Exemplos de uso de Instant e Range vectors:</strong></p>
<ul>
<li>Selecionando o valor <strong>atual</strong> (instantâneo) da métrica <code>cpu_usage</code> para a instância <code>&quot;webserver-1&quot;</code>:</li>
</ul>


  <pre><code class="language-promql">cpu_usage{instance=&#34;webserver-1&#34;}</code></pre>
 <ul>
<li>Calculando a diferença instantânea entre duas métricas (Instant Vector resultante):</li>
</ul>


  <pre><code class="language-promql">http_requests_total - http_requests_failed</code></pre>
 <p>Acima, subtraímos, para cada combinação de labels correspondente, o valor atual de <code>http_requests_failed</code> do valor atual de <code>http_requests_total</code>.</p>
<ul>
<li>Selecionando uma janela de <strong>5 minutos</strong> de dados da métrica <code>cpu_usage</code> para cada instância (Range Vector):</li>
</ul>


  <pre><code class="language-promql">cpu_usage[5m]</code></pre>
 <ul>
<li>Calculando a taxa (por segundo) de <code>cpu_usage</code> nos últimos 5 minutos para cada instância (note que <code>rate()</code> retorna um Instant Vector, com a taxa calculada para cada série):</li>
</ul>


  <pre><code class="language-promql">rate(cpu_usage[5m])</code></pre>
 <ul>
<li>Obtendo o valor <strong>máximo</strong> da métrica <code>network_traffic</code> em um intervalo de 30 minutos, separado por instância:</li>
</ul>


  <pre><code class="language-promql">max_over_time(network_traffic[30m]) by (instance)</code></pre>
 <blockquote>
<p>Resumindo: um <strong>Instant Vector</strong> é adequado para consultas que requerem o valor atual (ou de um instante específico) de uma métrica, enquanto um <strong>Range Vector</strong> é necessário para consultas que envolvem cálculo ao longo do tempo (taxas, médias móveis, etc.). Muitas funções do PromQL, como <code>rate</code> e <code>avg_over_time</code>, só funcionam com range vectors porque precisam de vários pontos de dados para produzir um resultado.</p></blockquote>
<h3 id="segurança-do-seletor-seletores-seguros-vs-inseguros">Segurança do seletor (Seletores seguros vs inseguros)</h3>
<p>Ao escrever consultas PromQL, é importante construir seletores de métricas que capturem exatamente as séries desejadas, evitando resultados imprecisos ou indesejados. Alguns seletores podem ser considerados &ldquo;inseguros&rdquo; porque podem abranger séries não pretendidas.</p>
<p>Por exemplo, usar uma correspondência de prefixo muito genérica em um label pode ser problemático. Considere o seletor de label <code>job=~&quot;prom.*&quot;</code>. Ele selecionará todas as séries de métricas cujo label <code>job</code> começa com &ldquo;prom&rdquo;.</p>
<p>Isso pode incluir não apenas o job principal &ldquo;prometheus&rdquo;, mas também qualquer outro job cujo nome comece com essas letras (por exemplo, um serviço &ldquo;promtail&rdquo; ou &ldquo;prometheus-exporter&rdquo;). O resultado pode ser uma consulta retornando séries inesperadas.</p>
<p>Para garantir seletores &ldquo;seguros&rdquo;, siga algumas práticas:</p>
<ul>
<li><strong>Seja explícito nos valores de label:</strong> Prefira usar correspondência exata (<code>=</code> ou <code>!=</code>) ou regex precisas. Por exemplo, se você quer métricas do job Prometheus, use <code>job=&quot;prometheus&quot;</code> em vez de um regex genérico.</li>
<li><strong>Evite padrões muito abrangentes:</strong> Como regra, só use regex se realmente precisar capturar múltiplos valores similares. Mesmo assim, tente restringir o padrão. Regex tendem a ser menos eficientes, pois precisam testar o padrão contra todos os valores conhecidos de um label, e podem indicar que talvez a configuração dos labels deva ser melhorada.</li>
<li><strong>Conheça seus labels:</strong> Entenda quais labels cada métrica possui e quais valores são possíveis. Isso ajuda a criar seletores que não tragam surpresas.</li>
</ul>
<p>Exemplos comparando seletores seguros vs inseguros:</p>
<ul>
<li><strong>Seguro:</strong> <code>http_requests_total{job=&quot;webserver&quot;, status=&quot;error&quot;}</code> – seleciona exatamente as séries de requisições HTTP do serviço <code>webserver</code> que possuem o status &ldquo;error&rdquo;.</li>
<li><strong>Inseguro:</strong> <code>http_requests_total{status=~&quot;err.*&quot;}</code> – poderia acidentalmente pegar algo como &ldquo;erroneous&rdquo; ou &ldquo;errata&rdquo; se esses fossem valores de status, além de &ldquo;error&rdquo;. Prefira <code>status=&quot;error&quot;</code> se é esse o valor exato desejado.</li>
<li><strong>Seguro:</strong> <code>{__name__=~&quot;^http_.*_total$&quot;}</code> – seleciona métricas cujo nome começa com &ldquo;http_&rdquo; e termina com &ldquo;_total&rdquo;.</li>
<li><strong>Inseguro:</strong> <code>{__name__=~&quot;http&quot;}</code> (sem âncoras ou wildcards definidos) – esse seletor está incompleto e potencialmente inválido. Sempre especifique padrões completos, por exemplo <code>http.*</code> se a intenção é &ldquo;começa com http&rdquo;.</li>
</ul>
<p>Em suma, construa seletores de forma cuidadosa para evitar incluir séries indesejadas. Isso garante que suas consultas retornem dados precisos e também evita sobrecarregar o Prometheus com resultados excessivos.</p>
<h3 id="obsolescência-do-vetor-instantâneo-staleness">Obsolescência do vetor instantâneo (Staleness)</h3>
<p>Um detalhe importante ao usar vetores instantâneos: o Prometheus possui um mecanismo de <em>staleness</em> (obsolescência) para lidar com séries temporais que não receberam novos dados em um intervalo de tempo.</p>
<p>Por padrão, se uma métrica não tiver amostras recentes (normalmente nos últimos 5 minutos), o PromQL considerará essa série como <strong>ausente</strong> ou retornará um valor <code>NaN</code> (not a number) em vez de continuar mostrando um valor antigo. Isso evita apresentar dados &ldquo;velhos&rdquo; como se fossem atuais.</p>
<p>Porém, em algumas consultas, especialmente ao criar alertas, queremos detectar explicitamente quando uma métrica parou de ser enviada. Existem maneiras de lidar com isso:</p>
<ul>
<li><strong>Aumentar a janela de consulta</strong>: Em vez de consultar apenas o valor instantâneo, podemos consultar em uma janela de tempo para ver se há dados recentes. Por exemplo, usar uma subconsulta com intervalo:</li>
</ul>


  <pre><code class="language-promql">http_requests_total[5m]</code></pre>
 <p>garante que estamos inspecionando 5 minutos de dados. Ou então, usar funções como <code>max_over_time(metric[5m])</code> para pegar o último valor nos últimos 5 minutos.</p>
<ul>
<li><strong>Usar funções de ausência</strong>: O PromQL oferece a função <code>absent()</code> que retorna 1 se a expressão dentro dela não retornar nenhum dado. Por exemplo:</li>
</ul>


  <pre><code class="language-promql">absent(rate(http_requests_total[5m]))</code></pre>
 <p>retornará 1 (com um label indicando a série buscada) se <strong>nenhuma</strong> série <code>http_requests_total</code> tiver dados nos últimos 5 minutos – ou seja, indicando que possivelmente a coleta parou. Caso exista qualquer dado, <code>absent()</code> retorna uma série vazia.</p>
<p>Também há a variante <code>absent_over_time(metric[duração])</code>, que verifica se <em>no intervalo dado</em> a métrica esteve ausente o tempo todo.</p>
<ul>
<li><strong>Combinar com condições booleanas</strong>: Podemos filtrar séries pelo timestamp de sua última amostra. A função <code>timestamp(metric)</code> retorna o timestamp da última amostra daquela métrica. Assim, expressões como:</li>
</ul>


  <pre><code class="language-promql">timestamp(cpu_usage) &lt; time() - 30</code></pre>
 <p>identificam séries cujo último timestamp é inferior a 30 segundos atrás, ou seja, possivelmente desatualizadas.</p>
<p>Exemplos práticos:</p>
<ul>
<li><strong>Verificar métricas ausentes</strong>:</li>
</ul>


  <pre><code class="language-promql">http_requests_total unless absent(rate(http_requests_total[5m]))</code></pre>
 <p>Aqui, usamos <code>unless</code> (que retorna a série da esquerda exceto quando a da direita existe) para só manter <code>http_requests_total</code> se ela não estiver ausente nos últimos 5m. Isso efetivamente filtra fora séries que não receberam dados recentes.</p>
<ul>
<li><strong>Filtrar instâncias inativas (não reportando)</strong>:</li>
</ul>


  <pre><code class="language-promql">cpu_usage unless absent_over_time(cpu_usage[2m])</code></pre>
 <p>Essa consulta retornaria <code>cpu_usage</code> atual apenas para instâncias que tiveram dados nos últimos 2 minutos. Se alguma instância parou de reportar (logo, ausente nos últimos 2m), ela será excluída do resultado.</p>
<ul>
<li><strong>Combinar timestamp e booleano</strong>:</li>
</ul>


  <pre><code class="language-promql">cpu_usage * on(instance) group_left() ((time() - timestamp(cpu_usage)) &lt; 30)</code></pre>
 <p>Esta expressão resulta no valor de <code>cpu_usage</code> apenas para instâncias cujo último timestamp tem menos de 30 segundos de idade. Estamos multiplicando o valor atual de <code>cpu_usage</code> por uma condição booleana que vale 1 apenas para instâncias atualizadas recentemente (e 0 para instâncias atrasadas).</p>
<p>O uso de <code>* on(instance) group_left()</code> garante que combinamos corretamente cada instância com sua condição booleana.</p>
<p>Em resumo, devido ao comportamento de <em>staleness</em>, um vetor instantâneo pode não mostrar valores de métricas atrasadas. Para contornar isso, podemos usar janelas de tempo maiores ou funções especiais como <code>absent()</code> para tratar casos de ausência de dados.</p>
<h3 id="funções-matemáticas-e-clamping">Funções Matemáticas e Clamping</h3>
<p>As funções em PromQL permitem manipular e processar métricas de diversas formas. Dentre as mais comuns estão as <strong>funções matemáticas</strong>, que realizam operações aritméticas sobre as séries de métricas. Temos desde as operações básicas até funções matemáticas de biblioteca. Alguns exemplos:</p>
<ul>
<li><code>sqrt(vector)</code>: retorna a raiz quadrada de cada valor no vetor.</li>
<li><code>exp(vector)</code>: retorna o exponencial (e^x) de cada valor.</li>
<li><code>ln(vector)</code>: logaritmo natural.</li>
<li><code>log10(vector)</code>, <code>log2(vector)</code>: logaritmos base 10 e base 2, respectivamente.</li>
<li><code>ceil(vector)</code>, <code>floor(vector)</code>: arredondamento para cima ou baixo.</li>
</ul>
<p>Além disso, PromQL fornece funções para limitar valores extremos (<em>clamping</em>). As funções <code>clamp_min(vector, scalar)</code> e <code>clamp_max(vector, scalar)</code> limitam os valores de um vetor a um mínimo ou máximo especificado. Por exemplo:</p>
<ul>
<li><code>clamp_min(metric, 0)</code>: garante que nenhum valor da série <code>metric</code> seja menor que 0 (valores negativos seriam substituídos por 0).</li>
<li><code>clamp_max(usage_ratio, 1)</code>: garante que valores acima de 1 em <code>usage_ratio</code> (por exemplo, 100% de uso) sejam reduzidos para 1.</li>
</ul>
<p>Essas funções de clamping são úteis para evitar que ruídos ou anomalias atrapalhem visualizações. Por exemplo, se um cálculo produz temporariamente um valor negativo ou um valor absurdamente alto por conta de algum atraso ou jitter, podemos usar clamping para limitar a escala dos gráficos.</p>
<p><strong>Exemplos de uso de funções matemáticas e clamping:</strong></p>
<ul>
<li>Calcular a <strong>média</strong> dos valores de uma métrica nos últimos 5 minutos:</li>
</ul>


  <pre><code class="language-promql">avg_over_time(metric_name[5m])</code></pre>
 <ul>
<li>Calcular a <strong>soma</strong> dos valores de uma métrica nos últimos 10 minutos:</li>
</ul>


  <pre><code class="language-promql">sum_over_time(metric_name[10m])</code></pre>
 <ul>
<li>Calcular o <strong>máximo</strong> valor de uma métrica nos últimos 1 hora, filtrando por um label:</li>
</ul>


  <pre><code class="language-promql">max_over_time(metric_name{label=&#34;value&#34;}[1h])</code></pre>
 <ul>
<li>Limitar o valor de uma métrica entre 0 e 100:</li>
</ul>


  <pre><code class="language-promql">clamp_min(clamp_max(metric_name, 100), 0)</code></pre>
 <p><em>(Aplica <code>clamp_max</code> para limitar a 100 e depois <code>clamp_min</code> para garantir mínimo 0.)</em></p>
<ul>
<li>Converter uma fração em porcentagem e garantir que não passe de 100%:</li>
</ul>


  <pre><code class="language-promql">clamp_max(success_ratio * 100, 100)</code></pre>
 <p>Supondo que <code>success_ratio</code> seja uma métrica ou expressão que resulta em um valor entre 0 e 1 (por exemplo, proporção de sucesso), multiplicamos por 100 para obter porcentagem e usamos <code>clamp_max</code> para nunca exibir acima de 100%.</p>
<p>Conhecer e utilizar essas funções permite realizar consultas mais avançadas e obter insights mais precisos a partir dos dados coletados.</p>
<h3 id="timestamps-e-funções-de-tempo-e-data">Timestamps e Funções de Tempo e Data</h3>
<p>No PromQL, <em>timestamps</em> (carimbos de tempo) são representados internamente como números de ponto flutuante indicando segundos desde a época Unix (1º de janeiro de 1970, 00:00:00 UTC).</p>
<p>Embora normalmente não precisemos lidar diretamente com o valor numérico do timestamp, há funções úteis relacionadas ao tempo:</p>
<ul>
<li>
<p><code>time()</code>: retorna o timestamp Unix do momento atual (momento da avaliação da consulta). Pode ser utilizado, por exemplo, para calcular diferenças de tempo.
<em>Exemplo:</em> <code>time() - 3600</code> produziria um valor de timestamp correspondente a uma hora atrás.</p>
</li>
<li>
<p><code>timestamp(vetor)</code>: retorna, para cada série no vetor dado, o timestamp da última amostra daquela série. Útil para comparações e detecção de desatualização (como visto anteriormente).</p>
</li>
</ul>
<p>Além disso, existem funções para extrair componentes de data/hora do timestamp de cada amostra de uma série:</p>
<ul>
<li><code>day_of_week(vetor)</code>: retorna o dia da semana (0–6, onde 0 = domingo, 1 = segunda, etc.) de cada amostra no vetor dado.</li>
<li><code>hour(vetor)</code>: retorna a hora (0–23) do timestamp de cada amostra.</li>
<li><code>day(vetor)</code>, <code>month(vetor)</code>, <code>year(vetor)</code>: retornam respectivamente o dia do mês, o mês (1–12) e o ano do timestamp de cada amostra.</li>
</ul>
<p>Essas funções permitem criar consultas que dependem da hora ou dia. Por exemplo, você pode querer detectar padrões diurnos ou disparar alertas apenas em dias úteis.</p>
<p><strong>Exemplos de uso de funções de tempo e data:</strong></p>
<ul>
<li>Obter o timestamp atual (como escalar):</li>
</ul>


  <pre><code class="language-promql">time()</code></pre>
 <ul>
<li>Extrair a hora atual do dia como um valor (0–23):</li>
</ul>


  <pre><code class="language-promql">hour(vector( time() ))</code></pre>
 <p>Aqui, <code>vector(time())</code> converte o escalar retornado por <code>time()</code> em um vetor (necessário porque <code>hour()</code> espera um vetor). O resultado é um vetor com um único valor: a hora do dia.</p>
<ul>
<li>Calcular a média de uma métrica por hora do dia, nos últimos 24h (usando subconsulta para separar por hora):</li>
</ul>


  <pre><code class="language-promql">avg_over_time(my_metric[1h])[24h:1h]</code></pre>
 <p>Essa expressão é uma subconsulta que calcula <code>avg_over_time(my_metric[1h])</code> (média de <code>my_metric</code> em cada janela de 1h) para cada hora nas últimas 24 horas. Isso produz uma série de 24 pontos, um para cada hora, que pode ser útil para observar a variação horária.</p>
<ul>
<li>Selecionar o valor médio da métrica <code>my_metric</code> no último dia:</li>
</ul>


  <pre><code class="language-promql">avg_over_time(my_metric[1d])</code></pre>
 <p>(Assumindo que há dados suficientes para cobrir o último dia inteiro.)</p>
<ul>
<li><strong>Nota:</strong> Para consultar um período específico (entre timestamps específicos), não há uma sintaxe direta dentro do PromQL. Em vez disso, usa-se a API de consulta de intervalos do Prometheus (fornecendo <code>start</code> e <code>end</code> no request) ou ferramentas como Grafana para delimitar visualmente o período. Dentro do PromQL, operações de tempo são relativas (como &ldquo;últimos 5 minutos&rdquo;, &ldquo;últimas 24h&rdquo;, etc.) em relação ao momento de avaliação.</li>
</ul>
<h3 id="counter-range-vectors-agregação-temporal-e-subconsultas">Counter Range Vectors, Agregação Temporal e Subconsultas</h3>
<p><strong>Counter Range Vectors</strong>: Contadores são métricas que apenas aumentam (ou resetam para zero e voltam a aumentar). Exemplos: número total de requisições atendidas, bytes transferidos, etc. Quando consultamos diretamente um <em>counter</em> como range vector, obteremos uma série de pontos que só crescem (com eventuais resets). Para extrair informações úteis (como taxa de eventos por segundo ou aumentos em determinado período) usamos funções especiais:</p>
<ul>
<li><code>rate(counter[5m])</code>: Calcula a <strong>taxa média por segundo</strong> de incremento do contador nos últimos 5 minutos. Essa função já lida corretamente com resets do contador (ignorando as quedas abruptas devido a resets e calculando a taxa considerando isso).</li>
<li><code>irate(counter[5m])</code>: Calcula a <strong>taxa instantânea</strong> (baseada apenas nos dois pontos mais recentes dentro dos 5 minutos). É mais ruidosa, mas pode reagir mais rapidamente a mudanças repentinas.</li>
<li><code>increase(counter[1h])</code>: Calcula <strong>quanto o contador aumentou</strong> no último 1 hora. Essencialmente integra a taxa ao longo do período.</li>
</ul>
<p><strong>Agregação através do tempo (Aggregating Across Time)</strong>: Às vezes, queremos primeiro agregar os dados e depois analisar a evolução temporal dessa agregação. As <strong>subconsultas</strong> nos permitem isso. Uma <em>subquery</em> (subconsulta) é quando temos uma expressão do PromQL seguida de um intervalo entre colchetes e possivelmente uma resolução, por exemplo: <code>expr[duração:passo]</code>. Isso faz o Prometheus avaliar <code>expr</code> repetidamente ao longo do intervalo dado, produzindo um range vector como resultado.</p>
<p>Por exemplo, <code>avg_over_time(rate(http_requests_total[1m])[24h:1h])</code> funciona assim:</p>
<ul>
<li>Internamente, <code>rate(http_requests_total[1m])</code> é avaliado para cada passo de 1h dentro das últimas 24h, gerando a taxa média por minuto calculada a cada hora.</li>
<li>Em seguida, <code>avg_over_time(...[24h:1h])</code> calcula a média desses 24 valores (um por hora) <strong>no tempo atual</strong>. Na prática, isso nos daria a média da taxa horária de requisições no dia.</li>
</ul>
<p>Subconsultas são muito poderosas e foram aprimoradas a partir do Prometheus 2.7. Com elas é possível, por exemplo, calcular tendências, baselines e sazonalidade de forma compacta.</p>
<p><strong>Exemplos avançados de subconsultas e análise de tendências:</strong></p>
<ul>
<li><strong>Tendência de taxa de erro (janela móvel):</strong> Calcular a média da taxa de erros em janelas de 1 hora, ao longo das últimas 24 horas:</li>
</ul>


  <pre><code class="language-promql">avg_over_time(
  rate(http_requests_total{status=~&#34;5..&#34;}[1m])[24h:1h]
)</code></pre>
 <p>Essa consulta gera 24 pontos (taxa de erro média de cada hora nas últimas 24h) e depois calcula a média disso tudo (ou seja, a média diária da taxa de erro). Poderíamos também omitir a função externa para simplesmente visualizar a série das últimas 24 horas e identificar padrões de aumento ou redução de erros ao longo do dia.</p>
<ul>
<li><strong>Baseline de performance (comparação com média histórica):</strong> Comparar a performance atual com a média da última semana:</li>
</ul>


  <pre><code class="language-promql">rate(http_requests_total[5m]) 
  / avg_over_time(rate(http_requests_total[5m])[7d])</code></pre>
 <p>Essa consulta produz uma razão: valores acima de 1 indicam que a taxa atual de requisições está <strong>acima</strong> da média semanal; valores abaixo de 1, abaixo da média. Isso pode ser útil para identificar desvios significativos de tráfego.</p>
<ul>
<li><strong>Detecção de anomalia sazonal (padrão horário):</strong> Comparar o tráfego atual com o padrão do último dia:</li>
</ul>


  <pre><code class="language-promql">rate(http_requests_total[5m]) 
  / avg_over_time(rate(http_requests_total[5m])[24h:1h])</code></pre>
 <p>Aqui, o denominador <code>avg_over_time(...[24h:1h])</code> produz a média da taxa em cada hora do dia anterior. Dividindo a taxa atual por esse valor da mesma hora ontem, podemos identificar se o tráfego está anormalmente alto ou baixo para este horário do dia.</p>
<ul>
<li><strong>Diferença diária (subconsulta com offset):</strong> Para calcular a diferença em uma métrica entre hoje e ontem, podemos usar <code>offset</code>. Exemplo:</li>
</ul>


  <pre><code class="language-promql">my_metric - my_metric offset 1d</code></pre>
 <p>Isso resulta em quanto <code>my_metric</code> variou em comparação com exatamente 24 horas atrás.</p>
<ul>
<li><strong>Soma acumulada (exemplo de subconsulta):</strong></li>
</ul>


  <pre><code class="language-promql">sum(my_counter) - sum(my_counter) offset 1d</code></pre>
 <p>Este exemplo soma o contador <code>my_counter</code> (provavelmente de várias instâncias) e subtrai o valor de 1 dia atrás, mostrando o incremento total em um dia. Essa é outra forma de calcular algo similar a <code>increase(my_counter[1d])</code>.</p>
<p>Em todos esses casos, as subconsultas <code>[ ... ]</code> estão permitindo observar ou reutilizar resultados ao longo do tempo dentro de uma única expressão.</p>
<h3 id="histogramas-mudança-de-tipo-alteração-de-labels-e-ordenação">Histogramas, Mudança de Tipo, Alteração de Labels e Ordenação</h3>
<p><strong>Histogramas:</strong> Em Prometheus, histogramas são uma forma de metricar distribuições de valores (duração de requisições, tamanho de payloads, etc.). Um histograma clássico consiste em múltiplas séries: por convenção, se a métrica base é <code>request_duration_seconds</code>, as séries serão:</p>
<ul>
<li><code>request_duration_seconds_bucket{le=&quot;0.1&quot;, ...}</code> (um bucket contando quantas observações &lt;= 0.1s)</li>
<li>vários outros buckets com diferentes limites <code>le</code> (le = limite inferior ou igual)</li>
<li><code>request_duration_seconds_count</code> (contagem total de observações)</li>
<li><code>request_duration_seconds_sum</code> (soma total dos valores observados)</li>
</ul>
<p>Para analisar histogramas, geralmente somamos as séries <code>_bucket</code> <em>por limite</em> para agregar todas as instâncias ou rótulos de interesse. <strong>É crucial incluir o label <code>le</code> ao agregar buckets.</strong> Por exemplo, a forma correta de agregar um histograma de duração por job seria:</p>


  <pre><code class="language-promql">sum by (job, le) (rate(http_request_duration_seconds_bucket[5m]))</code></pre>
 <p>Depois de agregado adequadamente, podemos aplicar <code>histogram_quantile()</code> para extrair quantis (p50, p90, p99, etc.).</p>
<p><strong>Trabalhando corretamente com histogramas:</strong></p>
<ul>
<li><em>Exemplo canônico (p99 de latência HTTP)</em>:</li>
</ul>


  <pre><code class="language-promql">histogram_quantile(
  0.99, 
  sum(rate(http_request_duration_seconds_bucket[5m])) by (le)
)</code></pre>
 <p>Esse retorna o 99º percentil da duração das requisições HTTP considerando todos os buckets. Note o uso de <code>by (le)</code> dentro do sum.</p>
<ul>
<li><em>Agregando por labels extras:</em> Se quisermos o percentil por <code>job</code> e <code>instance</code>, por exemplo, devemos manter esses labels na agregação, além do <code>le</code>:</li>
</ul>


  <pre><code class="language-promql">histogram_quantile(
  0.95, 
  sum(rate(http_request_duration_seconds_bucket[5m])) by (job, instance, le)
)</code></pre>
 <ul>
<li>
<p><em>Evitando erro comum:</em> <strong>Nunca</strong> esqueça o <code>by (le)</code> ao somar buckets de um histograma clássico. Por exemplo, isto está <strong>errado</strong>:</p>


  <pre><code class="language-promql"># Exemplo INCORRETO - ausência de by(le)
histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])))</code></pre>
 </li>
</ul>
<p>Sem agrupar por <code>le</code>, os valores de buckets se somam indevidamente, tornando o resultado do quantil incorreto.</p>
<p>No Prometheus 3.0, foram introduzidos os <strong>histogramas nativos</strong> (ainda experimentais). Eles visam simplificar e tornar mais eficiente o uso de histogramas (evitando lidar com dezenas de séries <code>_bucket</code>).</p>
<p>Com histogramas nativos, existem inclusive novas funções como <code>histogram_count()</code>, <code>histogram_sum()</code> e <code>histogram_avg()</code> para extrair diretamente contagem, soma e média dos histogramas.</p>
<p>Além disso, há a função <code>histogram_fraction()</code> para calcular frações entre limites. Embora seja um recurso promissor, a maioria dos usuários ainda trabalha com histogramas clássicos <code>_bucket</code> até que os nativos se estabilizem.</p>
<p><strong>Mudança de tipo (conversão Escalar &lt;-&gt; Vetor):</strong> Em algumas situações avançadas, você pode precisar converter escalares em vetores ou vice-versa:</p>
<ul>
<li><code>scalar(vetor)</code> – Converte um vetor de uma única série temporal (com um único valor) em um escalar simples. Isso é útil, por exemplo, quando você calculou um valor mínimo ou máximo e quer usá-lo em uma comparação global.
<em>Exemplo:</em> <code>scalar(min(up{job=&quot;webserver&quot;}))</code> – isso resultará em um escalar 0 ou 1 indicando se <strong>alguma</strong> instância do job &ldquo;webserver&rdquo; está caída (0 se o mínimo for 0, ou seja, pelo menos uma instância está down; 1 se todas estão up).</li>
<li><code>vector(escalar)</code> – O oposto, pega um escalar e o transforma em um vetor (sem labels). Útil se você precisa combinar um número puro com séries.
<em>Exemplo:</em> <code>vector(1)</code> – produziria um vetor contendo apenas o valor 1.</li>
</ul>
<p><strong>Alteração de Labels:</strong> Às vezes é necessário renomear ou copiar labels nas séries. Funções úteis:</p>
<ul>
<li><code>label_replace(vetor, &quot;label_destino&quot;, &quot;valor_novo&quot;, &quot;label_origem&quot;, &quot;regex&quot;)</code> – Retorna um vetor a partir de outro, adicionando ou modificando um label. Ele pega o valor do <code>label_origem</code> que case com a regex fornecida e o coloca em <code>label_destino</code> usando <code>valor_novo</code> (onde <code>'$1'</code> pode referenciar grupos da regex).
<em>Exemplo:</em></li>
</ul>


  <pre><code class="language-promql">label_replace(my_metric, &#34;new_label&#34;, &#34;$1&#34;, &#34;old_label&#34;, &#34;(.*)&#34;)</code></pre>
 <p>Isso criaria (ou sobrescreveria) <code>new_label</code> em cada série de <code>my_metric</code>, copiando exatamente o valor de <code>old_label</code> (já que <code>(.*)</code> captura todo o valor e <code>$1</code> insere ele).</p>
<ul>
<li><code>label_join(vetor, &quot;label_destino&quot;, &quot;sep&quot;, &quot;label1&quot;, &quot;label2&quot;, ...)</code> – Concatena múltiplos labels em um só. Ex: <code>label_join(my_metric, &quot;instance_job&quot;, &quot;-&quot;, &quot;instance&quot;, &quot;job&quot;)</code> criaria um novo label <code>instance_job</code> juntando os valores de <code>instance</code> e <code>job</code> separados por um <code>-</code>.</li>
</ul>
<p>Essas funções não são usadas com frequência em consultas ad-hoc, mas podem ser muito úteis ao preparar métricas para certas comparações ou ao lidar com diferenças de rotulagem entre métricas.</p>
<p><strong>Ordenação (Sorting):</strong> Para ordenar resultados, podemos usar as funções <code>sort(vector)</code> (ordem crescente) e <code>sort_desc(vector)</code> (ordem decrescente). Isso pode ser útil quando estamos interessados no topo ou no final de uma lista de resultados (embora muitas vezes <code>topk</code> e <code>bottomk</code> já cubram esses casos).</p>
<p>Exemplos rápidos:</p>
<ul>
<li>
<p>Ordenar todas as instâncias pelo uso de CPU decrescente:</p>


  <pre><code class="language-promql">sort_desc(rate(node_cpu_seconds_total{mode!=&#34;idle&#34;}[5m])) by (instance))</code></pre>
 <p><em>(Aqui somamos as CPUs por instância implicitamente ao usar o <code>by (instance)</code> na expressão, e depois ordenamos.)</em></p>
</li>
<li>
<p>Ordenar alfabéticamente por valor de um label (pouco comum, mas possível):</p>


  <pre><code class="language-promql">sort(my_metric)</code></pre>
 <p><em>(Se <code>my_metric</code> é um escalar ou tem apenas um valor por série, <code>sort()</code> essencialmente ordenará pelos labels já que os valores podem ser iguais.)</em></p>
</li>
</ul>
<h3 id="valores-ausentes-absent--missing-values">Valores ausentes (Absent / Missing Values)</h3>
<p>Valores ausentes podem ocorrer quando uma métrica não é reportada (por exemplo, um serviço caiu ou foi desligado). Em consultas PromQL, um valor ausente simplesmente não aparece no resultado. Entretanto, podemos detectar explicitamente a ausência de séries usando a função <code>absent()</code> mencionada anteriormente.</p>
<p>Recapitulando o uso de <code>absent()</code>:</p>
<ul>
<li><code>absent(metric)</code> – Retorna uma série sem labels (ou com labels especificados na consulta) com valor 1 se <strong>nenhuma série</strong> correspondente a <code>metric</code> está presente, ou retorna nada (vazio) caso contrário. Isso é muito útil em regras de alerta: um alerta de &ldquo;TargetDown&rdquo; pode ser escrito como <code>absent(up{job=&quot;myjob&quot;} == 1)</code> para disparar quando nenhum alvo daquele job estiver up.</li>
</ul>
<p>Exemplo:</p>


  <pre><code class="language-promql">absent(up{job=&#34;node&#34;} == 1)</code></pre>
 <p>Acima, a expressão <code>up{job=&quot;node&quot;} == 1</code> resultaria em 1 para cada instância de <code>node</code> que esteja up, então <code>absent(...)</code> retornaria 1 (sem label) se nenhuma instância de <code>node</code> estiver up (ou seja, o resultado dentro foi vazio). Se pelo menos uma instância estiver up, <code>absent</code> não retorna nada.</p>
<p>Da mesma forma, <code>absent_over_time(metric[5m])</code> verifica se <em>nenhum</em> ponto de <code>metric</code> apareceu nos últimos 5 minutos.</p>
<p><strong>Importante:</strong> Ao visualizar dados no gráfico do Prometheus ou Grafana, séries ausentes simplesmente não aparecem. Por isso, ao compor dashboards ou alertas, pode ser útil usar consultas que retornem 0 explicitamente quando algo está ausente para facilitar a visualização. Uma técnica é usar a operação <code>OR</code> com <code>absent()</code>. Exemplo:</p>


  <pre><code class="language-promql">rate(http_requests_total[5m]) or absent(http_requests_total)</code></pre>
 <p>Isso retornará a taxa de requisições normalmente; se nenhuma série existir, em vez de nada, retornará 1 (ou outro valor constante) indicando ausência.</p>
<h3 id="funções-avançadas-e-menos-conhecidas">Funções avançadas e menos conhecidas</h3>
<p>Algumas funções do PromQL são menos conhecidas, mas podem ser extremamente poderosas em cenários específicos:</p>
<ul>
<li>
<p><strong><code>resets(counter[interval])</code>:</strong> Conta quantas vezes um contador &ldquo;resetou&rdquo; (voltou a zero) no período. Útil para detectar reinicializações de aplicativos ou problemas de coleta.
<em>Exemplos:</em></p>


  <pre><code class="language-promql">resets(http_requests_total[5m])</code></pre>
 <p>Contaria quantos resets ocorreram no <code>http_requests_total</code> nos últimos 5 minutos. Se esse número for &gt; 0 constantemente, pode indicar que o serviço está reiniciando frequentemente (se o contador for interno ao serviço) ou que há overflow de contadores.</p>
</li>
<li>
<p><strong><code>changes(series[interval])</code>:</strong> Conta quantas vezes o valor de uma série mudou durante o intervalo. Isso vale para qualquer métrica (não apenas counters). Pode indicar instabilidade ou flapping.
<em>Exemplo:</em></p>


  <pre><code class="language-promql">changes(process_start_time_seconds[5m]) &gt; 0</code></pre>
 <p>O exemplo acima retornaria 1 para instâncias cujo <code>process_start_time_seconds</code> (normalmente um timestamp de início do processo) tenha mudado nos últimos 5 minutos — ou seja, o processo reiniciou nesse período.</p>
</li>
<li>
<p><strong><code>predict_linear(series[interval], passos_no_futuro)</code>:</strong> Realiza uma extrapolação linear do valor da série com base na tendência nos últimos intervalos e prevê o valor daqui a X segundos (informado em <code>passos_no_futuro</code>). Útil para prever quando algo alcançará um certo limite.
<em>Exemplo:</em></p>


  <pre><code class="language-promql">predict_linear(node_filesystem_free_bytes[1h], 3600) &lt; 0</code></pre>
 <p>Poderia ser usado para alertar se a tendência de queda do espaço livre prevê que em 1 hora (<code>3600</code> segundos) o espaço chegaria a zero.</p>
</li>
<li>
<p><strong><code>holt_winters(series[interval], sf, tf)</code>:</strong> Embora mais comum no Graphite, o PromQL também tem uma função de previsão chamada <code>holt_winters</code> (Holt-Winters, série temporal com tendência e sazonalidade). Aceita uma série (geralmente resultado de subconsulta) e realiza suavização exponencial dupla. No entanto, essa função é raramente usada diretamente em alertas, servindo mais para visualização de tendências suavizadas.</p>
</li>
<li>
<p><strong>Funções para histogramas nativos (Prometheus 3.x):</strong></p>
<ul>
<li><code>histogram_count()</code> e <code>histogram_sum()</code> – Retornam, respectivamente, a contagem total e a soma total das observações de histogramas (clássicos ou nativos). Para histogramas clássicos, esses usam as séries <code>_count</code> e <code>_sum</code> internas; para nativos, usam os valores codificados.</li>
<li><code>histogram_avg()</code> – Computa a média dos valores observados em cada histograma, equivalente a <code>histogram_sum/histogram_count</code>.</li>
<li><code>histogram_fraction(lower, upper, hist)</code> – Estima a fração de observações dentro do intervalo <code>[lower, upper]</code> para cada histograma. Útil, por exemplo, para calcular <em>Apdex</em> (fração de requisições abaixo de um certo limiar de latência).</li>
</ul>
</li>
</ul>
<p>Lembrando que algumas dessas funções mais novas podem requerer flags experimentais, dependendo da versão do Prometheus.</p>
<h3 id="operadores-aritméticos-e-correspondência-de-vetores-simples">Operadores Aritméticos e Correspondência de Vetores Simples</h3>
<p>PromQL permite usar operadores binários entre séries temporais para calcular novas séries. Os operadores aritméticos são: <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>%</code> (módulo) e <code>^</code> (exponenciação). Eles podem operar entre:</p>
<ul>
<li>Escalar e escalar (ex.: <code>2 * 3</code>)</li>
<li>Vetor e escalar (o escalar aplica-se a todos os valores do vetor; ex.: <code>metric * 100</code>)</li>
<li>Vetor e vetor (aqui entra o conceito de correspondência de vetores)</li>
</ul>
<p>Quando aplicamos um operador entre dois vetores (Instant Vectors), o PromQL realiza a operação <strong>par a par</strong> entre séries que &ldquo;correspondem&rdquo; umas às outras. Essa correspondência por padrão requer que as séries tenham exatamente os mesmos labels (nome da métrica pode ser diferente, mas os rótulos-chave e seus valores devem coincidir).</p>
<p>Exemplo simples: se temos as séries <code>metric_a{host=&quot;A&quot;, env=&quot;prod&quot;}</code> com valor X e <code>metric_b{host=&quot;A&quot;, env=&quot;prod&quot;}</code> com valor Y, então <code>metric_a + metric_b</code> retornará <code>{host=&quot;A&quot;, env=&quot;prod&quot;}</code> com valor X+Y. Se não houver correspondência exata de labels entre alguma série de <code>metric_a</code> e alguma de <code>metric_b</code>, aquela combinação não aparece no resultado.</p>
<p><strong>Correspondência simples</strong>: Por padrão, todos os labels (exceto o nome da métrica) devem casar entre as duas séries para a operação acontecer. É possível ajustar isso com modificadores que veremos adiante (<code>on</code> e <code>ignoring</code>).</p>
<p>Se quisermos forçar a operação em todas as combinações (o que raramente faz sentido), há o modificador <code>cross_join</code> (PromQL &gt;2.9), mas geralmente ele não é utilizado porque o comportamento padrão já é suficiente.</p>
<p>Os operadores também podem ser usados com o modificador <code>bool</code>, mas isso só se aplica a operadores de comparação, não aos aritméticos.</p>
<p>Exemplos práticos de operadores aritméticos:</p>
<ul>
<li>
<p><strong>Soma de métricas</strong>:</p>


  <pre><code class="language-promql">http_requests_total{status=&#34;200&#34;} &#43; http_requests_total{status=&#34;500&#34;}</code></pre>
 <p>Aqui, somamos as séries de requisições com status 200 e as com status 500, casando por quaisquer outros labels (por exemplo, instância). O resultado é o total combinado de requisições de sucesso e erro.</p>
</li>
<li>
<p><strong>Diferença de métricas</strong>:</p>


  <pre><code class="language-promql">node_memory_MemTotal_bytes - node_memory_MemFree_bytes</code></pre>
 <p>Calcula a memória em uso (diferença entre total e livre) para cada instância, assumindo que ambas as métricas compartilham os labels de instância.</p>
</li>
<li>
<p><strong>Multiplicação por escalar</strong>:</p>


  <pre><code class="language-promql">cpu_usage * 100</code></pre>
 <p>Converte a métrica <code>cpu_usage</code> (talvez como fração 0–1) em porcentagem.</p>
</li>
<li>
<p><strong>Combinação de dois vetores diferentes</strong>:</p>


  <pre><code class="language-promql">errors_total / requests_total</code></pre>
 <p>Pode calcular a taxa de erro (assumindo que <code>errors_total</code> e <code>requests_total</code> compartilham labels como serviço/endpoint). Isso exige correspondência exata de labels.</p>
</li>
</ul>
<p>No caso acima, se <code>errors_total</code> existir para um determinado label e <code>requests_total</code> não, essa combinação não retorna resultado. Podemos usar <em>vector matching</em> avançado (próxima seção) para ajustar essas situações.</p>
<h3 id="correspondência-de-séries-temporais-on-ignoring-group_left-group_right">Correspondência de Séries Temporais: <code>on()</code>, <code>ignoring()</code>, <code>group_left</code>, <code>group_right</code></h3>
<p>Quando combinamos métricas diferentes (vetor-vetor), muitas vezes precisamos controlar quais labels são usados para fazer o <em>join</em> (união) entre as séries de cada lado da operação. É aqui que entram os modificadores <code>on</code> e <code>ignoring</code>, e os operadores de junção externa <code>group_left</code> e <code>group_right</code>:</p>
<ul>
<li>
<p><strong><code>on(lista_de_labels)</code></strong>: Especifica explicitamente quais labels devem ser usados para casar as séries ao aplicar o operador. Todos os demais labels são ignorados no matching (exceto os do <code>on</code> listados).
<em>Exemplo:</em></p>


  <pre><code class="language-promql">errors_total / on(instance) requests_total</code></pre>
 <p>Aqui dizemos: combine séries de <code>errors_total</code> e <code>requests_total</code> que tenham o mesmo valor de <code>instance</code>. Labels diferentes de <code>instance</code> serão ignorados na comparação. Isso é útil se, por exemplo, <code>errors_total</code> tem um label <code>status=&quot;5xx&quot;</code> enquanto <code>requests_total</code> não tem o label <code>status</code>. Sem o <code>on(instance)</code>, essas séries não casariam por terem conjuntos de labels distintos.</p>
</li>
<li>
<p><strong><code>ignoring(lista_de_labels)</code></strong>: O inverso do <code>on</code>. Use todos os labels <em>exceto</em> os listados para fazer o matching. Ou seja, finge que os labels mencionados não existem nos vetores ao procurar pares correspondentes.
<em>Exemplo:</em></p>


  <pre><code class="language-promql">cpu_usage{cpu=&#34;total&#34;} / ignoring(cpu) cpu_quota</code></pre>
 <p>Suponha que <code>cpu_usage</code> tenha um label <code>cpu</code> (núcleo) e valor <code>&quot;total&quot;</code> para indicar uso total da CPU, enquanto <code>cpu_quota</code> não tem esse label (aplica a todo CPU). O <code>ignoring(cpu)</code> permite desconsiderar essa diferença, casando as séries somente pelos outros labels (por exemplo, pod ou contêiner, se for o caso).</p>
</li>
<li>
<p><strong>Junções um-para-muitos (many-to-one)</strong>: Por padrão, se houver múltiplas séries de um lado que poderiam casar com uma série do outro, a operação não ocorre e o resultado é vazio para evitar ambiguidades. No entanto, às vezes desejamos permitir isso — por exemplo, dividir uma métrica total por número de CPUs, onde a métrica total não tem o label <code>cpu</code> mas a de contagem de CPU tem (múltiplas séries, uma por core).
Para isso, usamos <code>group_left</code> ou <code>group_right</code> em conjunto com <code>on</code>/<code>ignoring</code>:</p>
<ul>
<li><strong><code>group_left(label1, label2, ...)</code></strong>: Indica que as séries do lado esquerdo do operador devem permanecer separadas (muitas) enquanto as do lado direito serão &ldquo;espalhadas&rdquo; para casar. Em outras palavras, permite que uma única série do lado direito seja usada para múltiplas do lado esquerdo. Opcionalmente, podemos listar labels que serão <strong>copiados</strong> do lado direito para o resultado final.</li>
<li><strong><code>group_right(label1, label2, ...)</code></strong>: O contrário, mantém o lado direito com muitas séries e espalha o lado esquerdo.</li>
</ul>
<p><em>Exemplo (adicionando labels com group_left):</em></p>


  <pre><code class="language-promql">rate(http_requests_total[5m]) 
  * on(instance) 
  group_left(job, environment) 
  up</code></pre>
 <p>Nesse exemplo, <code>rate(http_requests_total[5m])</code> produz séries talvez com labels <code>instance</code> e outros, mas digamos que não tenha <code>job</code> nem <code>environment</code> explicitamente (ou queremos copiar do <code>up</code>). A série <code>up</code> (métrica de saúde do alvo) tem <code>job</code>, <code>instance</code>, e <code>environment</code>. Estamos multiplicando as duas métricas apenas casando por <code>instance</code> (<code>on(instance)</code>). Como do lado direito (<code>up</code>) há possivelmente apenas uma série por instance (valor 0 ou 1), e do lado esquerdo pode haver múltiplas (por caminho de requisição, status, etc.), usamos <code>group_left(job, environment)</code> para dizer: permite que a mesma série de <code>up</code> case com múltiplas séries de requests do lado esquerdo, e traga os labels <code>job</code> e <code>environment</code> dessa série de <code>up</code> para o resultado final. Assim, o resultado terá a taxa de requests por <code>instance</code> mas agora enriquecido com os labels de job e environment.</p>
<p><em>Exemplo (many-to-one sem copiar labels):</em></p>


  <pre><code class="language-promql">cpu_usage 
  / on(instance) 
  group_right 
  cpu_count</code></pre>
 <p>Suponha que <code>cpu_usage{instance=&quot;A&quot;}</code> representa o uso total de CPU (consolidado) em determinada máquina, e <code>cpu_count{instance=&quot;A&quot;, cpu=&quot;0&quot;}</code> e <code>cpu_count{instance=&quot;A&quot;, cpu=&quot;1&quot;}</code> etc. representam 1 para cada CPU física (cada core). Se somarmos <code>cpu_count by (instance)</code> obteríamos o número de CPUs por instância, mas podemos diretamente dividir usando o truque do <code>group_right</code>. Aqui, cada série de <code>cpu_usage</code> (uma por instancia) será comparada com múltiplas séries de <code>cpu_count</code> (uma por CPU). Sem <code>group_right</code>, não casaria por haver múltiplas séries do lado direito para o mesmo instance. Com <code>group_right</code>, permitimos isso e, por não especificar labels a copiar, o resultado herda os labels do lado esquerdo (<code>cpu_usage</code>), e a operação divisão é feita para cada combinação (na prática repetindo o mesmo valor de <code>cpu_usage</code> para cada CPU e dividindo por o respectivo <code>cpu_count</code> – o que acaba resultando no mesmo valor para cada CPU). Talvez nesse caso específico fosse melhor já agrupar <code>cpu_count</code> antes de dividir, mas esse exemplo ilustra a sintaxe.</p>
</li>
<li>
<p><strong>Operador de conjunto <code>union</code>:</strong> PromQL não possui um operador explícito &ldquo;UNION&rdquo; nomeado, mas podemos realizar união de resultados simplesmente listando expressões separadas por vírgula em uma consulta. Por exemplo:</p>


  <pre><code class="language-promql">metric_a, metric_b</code></pre>
 <p>Isso retorna todas as séries de <code>metric_a</code> e todas as de <code>metric_b</code>. Não é muito comum em consultas manuais, mas pode ser útil para junção visual.</p>
</li>
</ul>
<p>Resumindo, os modificadores <code>on</code> e <code>ignoring</code> controlam <strong>quais</strong> labels considerar ao casar séries de métricas diferentes, e <code>group_left</code>/<code>group_right</code> controlam <strong>como lidar</strong> quando há cardinalidades diferentes (um-para-muitos). Combiná-los corretamente é fundamental para escrever consultas que envolvam múltiplas métricas.</p>
<h3 id="operadores-lógicos-and-or-unless">Operadores Lógicos: <code>and</code>, <code>or</code>, <code>unless</code></h3>
<p>Além dos operadores aritméticos e de comparação, PromQL também suporta operadores lógicos para vetores. Esses operadores não criam valores numéricos novos, mas sim filtram ou combinam séries com base em condições booleanas.</p>
<ul>
<li>
<p><strong><code>and</code>:</strong> Retém apenas as séries que aparecem em <strong>ambos</strong> os operandos. Em outras palavras, é uma interseção: uma série do lado esquerdo só passa se existe uma série exatamente igual do lado direito (considerando labels) e vice-versa. O valor resultante de cada série será o valor do lado esquerdo (padrão) ou, se usado como comparador, segue regras de comparador bool.
Uso típico: aplicar uma condição a um resultado. Por exemplo:</p>


  <pre><code class="language-promql">(vector1 comparacao const) and vector1</code></pre>
 <p>Isso retornaria apenas as séries de <code>vector1</code> que atendem à comparação (pois o comparador produzirá 1 para as séries que satisfazem, e então <code>and</code> manterá apenas essas).</p>
</li>
<li>
<p><strong><code>or</code>:</strong> União de séries. Retorna séries que estão em <strong>pelo menos um</strong> dos lados. Se a mesma série (mesmos labels) aparece em ambos, o valor resultante será o do lado esquerdo (padrão) ou pode ser modificado com bool se estivermos combinando booleanos. É útil para combinar resultados diferentes.
Por exemplo:</p>


  <pre><code class="language-promql">vector_a or vector_b</code></pre>
 <p>Isso dá todas as séries de <code>vector_a</code> e <code>vector_b</code>. Se alguma série estiver presente nos dois, aparece uma vez só (com valor de <code>vector_a</code>).</p>
</li>
<li>
<p><strong><code>unless</code>:</strong> Retém as séries do lado esquerdo <strong>a menos que</strong> elas também apareçam no lado direito. Equivale a diferença de conjuntos: resultado = esquerda \ direita. (Obs: O lado direito só importa pelos labels, seus valores são ignorados).
Por exemplo:</p>


  <pre><code class="language-promql">up{job=&#34;api&#34;} unless up{job=&#34;api&#34;, region=&#34;us-east&#34;}</code></pre>
 <p>Isso retornaria as séries <code>up</code> do job &ldquo;api&rdquo; <strong>que não têm</strong> region=&ldquo;us-east&rdquo;, ou seja, efetivamente filtra fora todas as instâncias da região us-east.</p>
</li>
</ul>
<p>Os operadores lógicos são avaliados após todos os cálculos numéricos serem feitos. Isso significa que podemos usá-los tanto em métricas brutas quanto em resultados de expressões.</p>
<p><strong>Exemplos práticos combinando comparações e operadores lógicos:</strong></p>
<ul>
<li>
<p><strong>Contar instâncias ativas em dois grupos diferentes:</strong></p>


  <pre><code class="language-promql">sum(up{job=&#34;node&#34;} == 1) or sum(up{job=&#34;db&#34;} == 1)</code></pre>
 <p>Esse exemplo usa <code>== 1</code> para converter as séries <code>up</code> em booleanas (1 para up, 0 para down) e soma para contar quantas estão up em cada job. O <code>or</code> aqui faz a união, retornando duas séries (uma para node e outra para db) com o valor de quantas instâncias estão up em cada.</p>
</li>
<li>
<p><strong>Filtrar top 5 de um conjunto e combinar com outro critério:</strong></p>


  <pre><code class="language-promql">topk(5, rate(http_requests_total[5m])) and ignoring(instance) (rate(errors_total[5m]) &gt; 0)</code></pre>
 <p>Esse exemplo hipotético pegaria as 5 maiores taxas de requisição (independente de instância) e então, através do <code>and</code> com <code>ignoring(instance)</code> e a condição de erros, manteria somente aquelas cujo serviço (ignorando instâncias) está apresentando erros. Bastante específico, mas demonstra o uso combinado: <code>topk</code> produz séries; a outra parte produz 1/0 para serviços com erro; o <code>and ignoring(instance)</code> casa por serviço e filtra.</p>
</li>
</ul>
<p>Lembrando que, se quisermos comparar valores de uma série com um número e obter diretamente 1 ou 0, podemos usar o modificador <code>bool</code>. Exemplo: <code>vector1 &gt; bool 10</code> retornaria um vetor com valor 1 para séries onde <code>vector1</code> &gt; 10 e 0 caso contrário (mantendo os labels). Sem <code>bool</code>, ele retornaria as próprias séries (com seus valores originais) porém filtradas pelas que atendem à condição.</p>
<h3 id="resumo-de-operadores-de-conjunto-conjuntos-de-séries">Resumo de operadores de conjunto (conjuntos de séries)</h3>
<p>Já falamos sobre <code>on</code>, <code>ignoring</code>, <code>group_left</code>, <code>group_right</code> e os operadores lógicos. Vale reforçar:</p>
<ul>
<li><code>on</code> / <code>ignoring</code>: controlam quais labels fazem parte da comparação entre séries ao aplicar um operador binário.</li>
<li><code>group_left</code> / <code>group_right</code>: permitem matching many-to-one e definem de que lado as séries duplicadas ficam.</li>
<li><code>and</code>, <code>or</code>, <code>unless</code>: operam em nível de conjunto de séries (interseção, união, diferença).</li>
</ul>
<p>Além disso, quando usamos agregadores (como <code>sum</code>, <code>avg</code> etc.), usamos <code>by</code> ou <code>without</code> para controlar quais labels serão preservados ou removidos. Isso às vezes é referido como agrupar por labels, mas é diferente de <code>on/ignoring</code> (que é para matching de operadores).</p>
<p><strong>Recapitulando agregação com <code>by</code> e <code>without</code>:</strong></p>
<ul>
<li><code>sum by(label1, label2) (expr)</code> – Soma os valores de <code>expr</code> agrupando séries que compartilham os mesmos valores de <code>label1</code> e <code>label2</code>. Os labels <code>label1</code> e <code>label2</code> serão mantidos no resultado, e todos os outros serão descartados (exceto aqueles usados no by).</li>
<li><code>avg without(labelX) (expr)</code> – Calcula a média removendo <code>labelX</code> da distinção. Isso significa agrupar por todas as outras labels, ou seja, fundir séries que diferem apenas em <code>labelX</code>.</li>
</ul>
<p>Exemplo: <code>sum by(job) (up == 0)</code> – contaria quantas instâncias estão down por job. Aqui <code>up == 0</code> produz 1 para instâncias down. Agrupando por job e somando, obtemos a contagem de instâncias não ativas de cada job.</p>
<h2 id="funções-essenciais-do-promql">Funções Essenciais do PromQL</h2>
<p>As funções essenciais do PromQL são aquelas mais utilizadas no dia a dia para monitoramento e análise de métricas. Elas permitem transformar dados brutos em informações acionáveis, calculando taxas, agregações e estatísticas importantes.</p>
<h3 id="funções-de-taxa-e-incremento">Funções de Taxa e Incremento</h3>
<p>As funções mais fundamentais para trabalhar com contadores são <code>rate()</code> e <code>increase()</code>:</p>
<p><strong><code>rate(counter[interval])</code></strong>: Calcula a taxa média por segundo de incremento do contador no intervalo especificado. Esta função lida automaticamente com resets do contador.</p>


  <pre><code class="language-promql">rate(http_requests_total[5m])</code></pre>
 <p><strong><code>increase(counter[interval])</code></strong>: Calcula quanto o contador aumentou no intervalo especificado.</p>


  <pre><code class="language-promql">increase(http_requests_total[1h])</code></pre>
 <p><strong><code>irate(counter[interval])</code></strong>: Calcula a taxa instantânea baseada apenas nos dois pontos mais recentes. É mais ruidosa, mas reage mais rapidamente a mudanças.</p>


  <pre><code class="language-promql">irate(http_requests_total[5m])</code></pre>
 <h3 id="funções-de-agregação">Funções de Agregação</h3>
<p>As funções de agregação permitem resumir dados de múltiplas séries:</p>
<p><strong><code>sum(expr) by (label1, label2)</code></strong>: Soma os valores agrupando por labels específicos.</p>


  <pre><code class="language-promql">sum(rate(http_requests_total[5m])) by (job)</code></pre>
 <p><strong><code>avg(expr) by (label1, label2)</code></strong>: Calcula a média agrupando por labels específicos.</p>


  <pre><code class="language-promql">avg(rate(node_cpu_seconds_total{mode=&#34;user&#34;}[5m])) by (instance)</code></pre>
 <p><strong><code>count(expr) by (label1, label2)</code></strong>: Conta o número de séries agrupando por labels.</p>


  <pre><code class="language-promql">count(up) by (job)</code></pre>
 <p><strong><code>max(expr) by (label1, label2)</code></strong>: Retorna o valor máximo agrupando por labels.</p>


  <pre><code class="language-promql">max(rate(http_requests_total[5m])) by (endpoint)</code></pre>
 <p><strong><code>min(expr) by (label1, label2)</code></strong>: Retorna o valor mínimo agrupando por labels.</p>


  <pre><code class="language-promql">min(rate(http_requests_total[5m])) by (endpoint)</code></pre>
 <h3 id="funções-de-percentil-e-histograma">Funções de Percentil e Histograma</h3>
<p><strong><code>histogram_quantile(quantile, histogram)</code></strong>: Calcula um quantil específico a partir de um histograma.</p>


  <pre><code class="language-promql">histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))</code></pre>
 <p><strong><code>quantile(quantile, expr)</code></strong>: Calcula um quantil específico de uma expressão.</p>


  <pre><code class="language-promql">quantile(0.95, rate(http_requests_total[5m]))</code></pre>
 <h3 id="funções-de-filtro-e-seleção">Funções de Filtro e Seleção</h3>
<p><strong><code>topk(k, expr)</code></strong>: Retorna as k séries com os maiores valores.</p>


  <pre><code class="language-promql">topk(5, rate(http_requests_total[5m]))</code></pre>
 <p><strong><code>bottomk(k, expr)</code></strong>: Retorna as k séries com os menores valores.</p>


  <pre><code class="language-promql">bottomk(5, rate(http_requests_total[5m]))</code></pre>
 <h3 id="funções-de-tempo">Funções de Tempo</h3>
<p><strong><code>avg_over_time(expr[interval])</code></strong>: Calcula a média dos valores no intervalo especificado.</p>


  <pre><code class="language-promql">avg_over_time(http_requests_total[5m])</code></pre>
 <p><strong><code>sum_over_time(expr[interval])</code></strong>: Calcula a soma dos valores no intervalo especificado.</p>


  <pre><code class="language-promql">sum_over_time(http_requests_total[5m])</code></pre>
 <p><strong><code>max_over_time(expr[interval])</code></strong>: Retorna o valor máximo no intervalo especificado.</p>


  <pre><code class="language-promql">max_over_time(cpu_usage[1h])</code></pre>
 <p><strong><code>min_over_time(expr[interval])</code></strong>: Retorna o valor mínimo no intervalo especificado.</p>


  <pre><code class="language-promql">min_over_time(memory_usage[1h])</code></pre>
 <h3 id="funções-de-detecção-de-ausência">Funções de Detecção de Ausência</h3>
<p><strong><code>absent(expr)</code></strong>: Retorna 1 se a expressão não retornar nenhum dado, caso contrário retorna nada.</p>


  <pre><code class="language-promql">absent(up{job=&#34;webserver&#34;})</code></pre>
 <p><strong><code>absent_over_time(expr[interval])</code></strong>: Verifica se a métrica esteve ausente durante todo o intervalo.</p>


  <pre><code class="language-promql">absent_over_time(up{job=&#34;webserver&#34;}[5m])</code></pre>
 <h2 id="promql-avançado">PromQL Avançado</h2>
<p>O PromQL oferece recursos avançados para consultas complexas e análises sofisticadas. Esta seção aborda tópicos mais avançados como correspondência de vetores, subconsultas, operadores lógicos e funções especializadas.</p>
<h3 id="correspondência-de-séries-temporais-on-ignoring-group_left-group_right-1">Correspondência de Séries Temporais: <code>on()</code>, <code>ignoring()</code>, <code>group_left</code>, <code>group_right</code></h3>
<p>Quando combinamos métricas diferentes (vetor-vetor), muitas vezes precisamos controlar quais labels são usados para fazer o <em>join</em> (união) entre as séries de cada lado da operação. É aqui que entram os modificadores <code>on</code> e <code>ignoring</code>, e os operadores de junção externa <code>group_left</code> e <code>group_right</code>:</p>
<ul>
<li>
<p><strong><code>on(lista_de_labels)</code></strong>: Especifica explicitamente quais labels devem ser usados para casar as séries ao aplicar o operador. Todos os demais labels são ignorados no matching (exceto os do <code>on</code> listados).
<em>Exemplo:</em></p>


  <pre><code class="language-promql">errors_total / on(instance) requests_total</code></pre>
 <p>Aqui dizemos: combine séries de <code>errors_total</code> e <code>requests_total</code> que tenham o mesmo valor de <code>instance</code>. Labels diferentes de <code>instance</code> serão ignorados na comparação. Isso é útil se, por exemplo, <code>errors_total</code> tem um label <code>status=&quot;5xx&quot;</code> enquanto <code>requests_total</code> não tem o label <code>status</code>. Sem o <code>on(instance)</code>, essas séries não casariam por terem conjuntos de labels distintos.</p>
</li>
<li>
<p><strong><code>ignoring(lista_de_labels)</code></strong>: O inverso do <code>on</code>. Use todos os labels <em>exceto</em> os listados para fazer o matching. Ou seja, finge que os labels mencionados não existem nos vetores ao procurar pares correspondentes.
<em>Exemplo:</em></p>


  <pre><code class="language-promql">cpu_usage{cpu=&#34;total&#34;} / ignoring(cpu) cpu_quota</code></pre>
 <p>Suponha que <code>cpu_usage</code> tenha um label <code>cpu</code> (núcleo) e valor <code>&quot;total&quot;</code> para indicar uso total da CPU, enquanto <code>cpu_quota</code> não tem esse label (aplica a todo CPU). O <code>ignoring(cpu)</code> permite desconsiderar essa diferença, casando as séries somente pelos outros labels (por exemplo, pod ou contêiner, se for o caso).</p>
</li>
<li>
<p><strong>Junções um-para-muitos (many-to-one)</strong>: Por padrão, se houver múltiplas séries de um lado que poderiam casar com uma série do outro, a operação não ocorre e o resultado é vazio para evitar ambiguidades. No entanto, às vezes desejamos permitir isso — por exemplo, dividir uma métrica total por número de CPUs, onde a métrica total não tem o label <code>cpu</code> mas a de contagem de CPU tem (múltiplas séries, uma por core).
Para isso, usamos <code>group_left</code> ou <code>group_right</code> em conjunto com <code>on</code>/<code>ignoring</code>:</p>
<ul>
<li><strong><code>group_left(label1, label2, ...)</code></strong>: Indica que as séries do lado esquerdo do operador devem permanecer separadas (muitas) enquanto as do lado direito serão &ldquo;espalhadas&rdquo; para casar. Em outras palavras, permite que uma única série do lado direito seja usada para múltiplas do lado esquerdo. Opcionalmente, podemos listar labels que serão <strong>copiados</strong> do lado direito para o resultado final.</li>
<li><strong><code>group_right(label1, label2, ...)</code></strong>: O contrário, mantém o lado direito com muitas séries e espalha o lado esquerdo.</li>
</ul>
<p><em>Exemplo (adicionando labels com group_left):</em></p>


  <pre><code class="language-promql">rate(http_requests_total[5m]) 
  * on(instance) 
  group_left(job, environment) 
  up</code></pre>
 <p>Nesse exemplo, <code>rate(http_requests_total[5m])</code> produz séries talvez com labels <code>instance</code> e outros, mas digamos que não tenha <code>job</code> nem <code>environment</code> explicitamente (ou queremos copiar do <code>up</code>). A série <code>up</code> (métrica de saúde do alvo) tem <code>job</code>, <code>instance</code>, e <code>environment</code>. Estamos multiplicando as duas métricas apenas casando por <code>instance</code> (<code>on(instance)</code>). Como do lado direito (<code>up</code>) há possivelmente apenas uma série por instance (valor 0 ou 1), e do lado esquerdo pode haver múltiplas (por caminho de requisição, status, etc.), usamos <code>group_left(job, environment)</code> para dizer: permite que a mesma série de <code>up</code> case com múltiplas séries de requests do lado esquerdo, e traga os labels <code>job</code> e <code>environment</code> dessa série de <code>up</code> para o resultado final. Assim, o resultado terá a taxa de requests por <code>instance</code> mas agora enriquecido com os labels de job e environment.</p>
<p><em>Exemplo (many-to-one sem copiar labels):</em></p>


  <pre><code class="language-promql">cpu_usage 
  / on(instance) 
  group_right 
  cpu_count</code></pre>
 <p>Suponha que <code>cpu_usage{instance=&quot;A&quot;}</code> representa o uso total de CPU (consolidado) em determinada máquina, e <code>cpu_count{instance=&quot;A&quot;, cpu=&quot;0&quot;}</code> e <code>cpu_count{instance=&quot;A&quot;, cpu=&quot;1&quot;}</code> etc. representam 1 para cada CPU física (cada core). Se somarmos <code>cpu_count by (instance)</code> obteríamos o número de CPUs por instância, mas podemos diretamente dividir usando o truque do <code>group_right</code>. Aqui, cada série de <code>cpu_usage</code> (uma por instancia) será comparada com múltiplas séries de <code>cpu_count</code> (uma por CPU). Sem <code>group_right</code>, não casaria por haver múltiplas séries do lado direito para o mesmo instance. Com <code>group_right</code>, permitimos isso e, por não especificar labels a copiar, o resultado herda os labels do lado esquerdo (<code>cpu_usage</code>), e a operação divisão é feita para cada combinação (na prática repetindo o mesmo valor de <code>cpu_usage</code> para cada CPU e dividindo por o respectivo <code>cpu_count</code> – o que acaba resultando no mesmo valor para cada CPU). Talvez nesse caso específico fosse melhor já agrupar <code>cpu_count</code> antes de dividir, mas esse exemplo ilustra a sintaxe.</p>
</li>
<li>
<p><strong>Operador de conjunto <code>union</code>:</strong> PromQL não possui um operador explícito &ldquo;UNION&rdquo; nomeado, mas podemos realizar união de resultados simplesmente listando expressões separadas por vírgula em uma consulta. Por exemplo:</p>


  <pre><code class="language-promql">metric_a, metric_b</code></pre>
 <p>Isso retorna todas as séries de <code>metric_a</code> e todas as de <code>metric_b</code>. Não é muito comum em consultas manuais, mas pode ser útil para junção visual.</p>
</li>
</ul>
<p>Resumindo, os modificadores <code>on</code> e <code>ignoring</code> controlam <strong>quais</strong> labels considerar ao casar séries de métricas diferentes, e <code>group_left</code>/<code>group_right</code> controlam <strong>como lidar</strong> quando há cardinalidades diferentes (um-para-muitos). Combiná-los corretamente é fundamental para escrever consultas que envolvam múltiplas métricas.</p>
<h3 id="operadores-lógicos-and-or-unless-1">Operadores Lógicos: <code>and</code>, <code>or</code>, <code>unless</code></h3>
<p>Além dos operadores aritméticos e de comparação, PromQL também suporta operadores lógicos para vetores. Esses operadores não criam valores numéricos novos, mas sim filtram ou combinam séries com base em condições booleanas.</p>
<ul>
<li>
<p><strong><code>and</code>:</strong> Retém apenas as séries que aparecem em <strong>ambos</strong> os operandos. Em outras palavras, é uma interseção: uma série do lado esquerdo só passa se existe uma série exatamente igual do lado direito (considerando labels) e vice-versa. O valor resultante de cada série será o valor do lado esquerdo (padrão) ou, se usado como comparador, segue regras de comparador bool.
Uso típico: aplicar uma condição a um resultado. Por exemplo:</p>


  <pre><code class="language-promql">(vector1 comparacao const) and vector1</code></pre>
 <p>Isso retornaria apenas as séries de <code>vector1</code> que atendem à comparação (pois o comparador produzirá 1 para as séries que satisfazem, e então <code>and</code> manterá apenas essas).</p>
</li>
<li>
<p><strong><code>or</code>:</strong> União de séries. Retorna séries que estão em <strong>pelo menos um</strong> dos lados. Se a mesma série (mesmos labels) aparece em ambos, o valor resultante será o do lado esquerdo (padrão) ou pode ser modificado com bool se estivermos combinando booleanos. É útil para combinar resultados diferentes.
Por exemplo:</p>


  <pre><code class="language-promql">vector_a or vector_b</code></pre>
 <p>Isso dá todas as séries de <code>vector_a</code> e <code>vector_b</code>. Se alguma série estiver presente nos dois, aparece uma vez só (com valor de <code>vector_a</code>).</p>
</li>
<li>
<p><strong><code>unless</code>:</strong> Retém as séries do lado esquerdo <strong>a menos que</strong> elas também apareçam no lado direito. Equivale a diferença de conjuntos: resultado = esquerda \ direita. (Obs: O lado direito só importa pelos labels, seus valores são ignorados).
Por exemplo:</p>


  <pre><code class="language-promql">up{job=&#34;api&#34;} unless up{job=&#34;api&#34;, region=&#34;us-east&#34;}</code></pre>
 <p>Isso retornaria as séries <code>up</code> do job &ldquo;api&rdquo; <strong>que não têm</strong> region=&ldquo;us-east&rdquo;, ou seja, efetivamente filtra fora todas as instâncias da região us-east.</p>
</li>
</ul>
<p>Os operadores lógicos são avaliados após todos os cálculos numéricos serem feitos. Isso significa que podemos usá-los tanto em métricas brutas quanto em resultados de expressões.</p>
<p><strong>Exemplos práticos combinando comparações e operadores lógicos:</strong></p>
<ul>
<li>
<p><strong>Contar instâncias ativas em dois grupos diferentes:</strong></p>


  <pre><code class="language-promql">sum(up{job=&#34;node&#34;} == 1) or sum(up{job=&#34;db&#34;} == 1)</code></pre>
 <p>Esse exemplo usa <code>== 1</code> para converter as séries <code>up</code> em booleanas (1 para up, 0 para down) e soma para contar quantas estão up em cada job. O <code>or</code> aqui faz a união, retornando duas séries (uma para node e outra para db) com o valor de quantas instâncias estão up em cada.</p>
</li>
<li>
<p><strong>Filtrar top 5 de um conjunto e combinar com outro critério:</strong></p>


  <pre><code class="language-promql">topk(5, rate(http_requests_total[5m])) and ignoring(instance) (rate(errors_total[5m]) &gt; 0)</code></pre>
 <p>Esse exemplo hipotético pegaria as 5 maiores taxas de requisição (independente de instância) e então, através do <code>and</code> com <code>ignoring(instance)</code> e a condição de erros, manteria somente aquelas cujo serviço (ignorando instâncias) está apresentando erros. Bastante específico, mas demonstra o uso combinado: <code>topk</code> produz séries; a outra parte produz 1/0 para serviços com erro; o <code>and ignoring(instance)</code> casa por serviço e filtra.</p>
</li>
</ul>
<p>Lembrando que, se quisermos comparar valores de uma série com um número e obter diretamente 1 ou 0, podemos usar o modificador <code>bool</code>. Exemplo: <code>vector1 &gt; bool 10</code> retornaria um vetor com valor 1 para séries onde <code>vector1</code> &gt; 10 e 0 caso contrário (mantendo os labels). Sem <code>bool</code>, ele retornaria as próprias séries (com seus valores originais) porém filtradas pelas que atendem à condição.</p>
<h3 id="subconsultas-e-análise-temporal-avançada">Subconsultas e Análise Temporal Avançada</h3>
<p><strong>Counter Range Vectors</strong>: Contadores são métricas que apenas aumentam (ou resetam para zero e voltam a aumentar). Exemplos: número total de requisições atendidas, bytes transferidos, etc. Quando consultamos diretamente um <em>counter</em> como range vector, obteremos uma série de pontos que só crescem (com eventuais resets). Para extrair informações úteis (como taxa de eventos por segundo ou aumentos em determinado período) usamos funções especiais:</p>
<ul>
<li><code>rate(counter[5m])</code>: Calcula a <strong>taxa média por segundo</strong> de incremento do contador nos últimos 5 minutos. Essa função já lida corretamente com resets do contador (ignorando as quedas abruptas devido a resets e calculando a taxa considerando isso).</li>
<li><code>irate(counter[5m])</code>: Calcula a <strong>taxa instantânea</strong> (baseada apenas nos dois pontos mais recentes dentro dos 5 minutos). É mais ruidosa, mas pode reagir mais rapidamente a mudanças repentinas.</li>
<li><code>increase(counter[1h])</code>: Calcula <strong>quanto o contador aumentou</strong> no último 1 hora. Essencialmente integra a taxa ao longo do período.</li>
</ul>
<p><strong>Agregação através do tempo (Aggregating Across Time)</strong>: Às vezes, queremos primeiro agregar os dados e depois analisar a evolução temporal dessa agregação. As <strong>subconsultas</strong> nos permitem isso. Uma <em>subquery</em> (subconsulta) é quando temos uma expressão do PromQL seguida de um intervalo entre colchetes e possivelmente uma resolução, por exemplo: <code>expr[duração:passo]</code>. Isso faz o Prometheus avaliar <code>expr</code> repetidamente ao longo do intervalo dado, produzindo um range vector como resultado.</p>
<p>Por exemplo, <code>avg_over_time(rate(http_requests_total[1m])[24h:1h])</code> funciona assim:</p>
<ul>
<li>Internamente, <code>rate(http_requests_total[1m])</code> é avaliado para cada passo de 1h dentro das últimas 24h, gerando a taxa média por minuto calculada a cada hora.</li>
<li>Em seguida, <code>avg_over_time(...[24h:1h])</code> calcula a média desses 24 valores (um por hora) <strong>no tempo atual</strong>. Na prática, isso nos daria a média da taxa horária de requisições no dia.</li>
</ul>
<p>Subconsultas são muito poderosas e foram aprimoradas a partir do Prometheus 2.7. Com elas é possível, por exemplo, calcular tendências, baselines e sazonalidade de forma compacta.</p>
<p><strong>Exemplos avançados de subconsultas e análise de tendências:</strong></p>
<ul>
<li>
<p><strong>Tendência de taxa de erro (janela móvel):</strong> Calcular a média da taxa de erros em janelas de 1 hora, ao longo das últimas 24 horas:</p>


  <pre><code class="language-promql">avg_over_time(
  rate(http_requests_total{status=~&#34;5..&#34;}[1m])[24h:1h]
)</code></pre>
 <p>Essa consulta gera 24 pontos (taxa de erro média de cada hora nas últimas 24h) e depois calcula a média disso tudo (ou seja, a média diária da taxa de erro). Poderíamos também omitir a função externa para simplesmente visualizar a série das últimas 24 horas e identificar padrões de aumento ou redução de erros ao longo do dia.</p>
</li>
<li>
<p><strong>Baseline de performance (comparação com média histórica):</strong> Comparar a performance atual com a média da última semana:</p>


  <pre><code class="language-promql">rate(http_requests_total[5m]) 
  / avg_over_time(rate(http_requests_total[5m])[7d])</code></pre>
 <p>Essa consulta produz uma razão: valores acima de 1 indicam que a taxa atual de requisições está <strong>acima</strong> da média semanal; valores abaixo de 1, abaixo da média. Isso pode ser útil para identificar desvios significativos de tráfego.</p>
</li>
<li>
<p><strong>Detecção de anomalia sazonal (padrão horário):</strong> Comparar o tráfego atual com o padrão do último dia:</p>


  <pre><code class="language-promql">rate(http_requests_total[5m]) 
  / avg_over_time(rate(http_requests_total[5m])[24h:1h])</code></pre>
 <p>Aqui, o denominador <code>avg_over_time(...[24h:1h])</code> produz a média da taxa em cada hora do dia anterior. Dividindo a taxa atual por esse valor da mesma hora ontem, podemos identificar se o tráfego está anormalmente alto ou baixo para este horário do dia.</p>
</li>
<li>
<p><strong>Diferença diária (subconsulta com offset):</strong> Para calcular a diferença em uma métrica entre hoje e ontem, podemos usar <code>offset</code>. Exemplo:</p>


  <pre><code class="language-promql">my_metric - my_metric offset 1d</code></pre>
 <p>Isso resulta em quanto <code>my_metric</code> variou em comparação com exatamente 24 horas atrás.</p>
</li>
<li>
<p><strong>Soma acumulada (exemplo de subconsulta):</strong></p>


  <pre><code class="language-promql">sum(my_counter) - sum(my_counter) offset 1d</code></pre>
 <p>Este exemplo soma o contador <code>my_counter</code> (provavelmente de várias instâncias) e subtrai o valor de 1 dia atrás, mostrando o incremento total em um dia. Essa é outra forma de calcular algo similar a <code>increase(my_counter[1d])</code>.</p>
</li>
</ul>
<p>Em todos esses casos, as subconsultas <code>[ ... ]</code> estão permitindo observar ou reutilizar resultados ao longo do tempo dentro de uma única expressão.</p>
<h3 id="funções-avançadas-e-especializadas">Funções Avançadas e Especializadas</h3>
<p>Algumas funções do PromQL são menos conhecidas, mas podem ser extremamente poderosas em cenários específicos:</p>
<ul>
<li>
<p><strong><code>resets(counter[interval])</code>:</strong> Conta quantas vezes um contador &ldquo;resetou&rdquo; (voltou a zero) no período. Útil para detectar reinicializações de aplicativos ou problemas de coleta.
<em>Exemplos:</em></p>


  <pre><code class="language-promql">resets(http_requests_total[5m])</code></pre>
 <p>Contaria quantos resets ocorreram no <code>http_requests_total</code> nos últimos 5 minutos. Se esse número for &gt; 0 constantemente, pode indicar que o serviço está reiniciando frequentemente (se o contador for interno ao serviço) ou que há overflow de contadores.</p>
</li>
<li>
<p><strong><code>changes(series[interval])</code>:</strong> Conta quantas vezes o valor de uma série mudou durante o intervalo. Isso vale para qualquer métrica (não apenas counters). Pode indicar instabilidade ou flapping.
<em>Exemplo:</em></p>


  <pre><code class="language-promql">changes(process_start_time_seconds[5m]) &gt; 0</code></pre>
 <p>O exemplo acima retornaria 1 para instâncias cujo <code>process_start_time_seconds</code> (normalmente um timestamp de início do processo) tenha mudado nos últimos 5 minutos — ou seja, o processo reiniciou nesse período.</p>
</li>
<li>
<p><strong><code>predict_linear(series[interval], passos_no_futuro)</code>:</strong> Realiza uma extrapolação linear do valor da série com base na tendência nos últimos intervalos e prevê o valor daqui a X segundos (informado em <code>passos_no_futuro</code>). Útil para prever quando algo alcançará um certo limite.
<em>Exemplo:</em></p>


  <pre><code class="language-promql">predict_linear(node_filesystem_free_bytes[1h], 3600) &lt; 0</code></pre>
 <p>Poderia ser usado para alertar se a tendência de queda do espaço livre prevê que em 1 hora (<code>3600</code> segundos) o espaço chegaria a zero.</p>
</li>
<li>
<p><strong><code>holt_winters(series[interval], sf, tf)</code>:</strong> Embora mais comum no Graphite, o PromQL também tem uma função de previsão chamada <code>holt_winters</code> (Holt-Winters, série temporal com tendência e sazonalidade). Aceita uma série (geralmente resultado de subconsulta) e realiza suavização exponencial dupla. No entanto, essa função é raramente usada diretamente em alertas, servindo mais para visualização de tendências suavizadas.</p>
</li>
<li>
<p><strong>Funções para histogramas nativos (Prometheus 3.x):</strong></p>
<ul>
<li><code>histogram_count()</code> e <code>histogram_sum()</code> – Retornam, respectivamente, a contagem total e a soma total das observações de histogramas (clássicos ou nativos). Para histogramas clássicos, esses usam as séries <code>_count</code> e <code>_sum</code> internas; para nativos, usam os valores codificados.</li>
<li><code>histogram_avg()</code> – Computa a média dos valores observados em cada histograma, equivalente a <code>histogram_sum/histogram_count</code>.</li>
<li><code>histogram_fraction(lower, upper, hist)</code> – Estima a fração de observações dentro do intervalo <code>[lower, upper]</code> para cada histograma. Útil, por exemplo, para calcular <em>Apdex</em> (fração de requisições abaixo de um certo limiar de latência).</li>
</ul>
</li>
</ul>
<p>Lembrando que algumas dessas funções mais novas podem requerer flags experimentais, dependendo da versão do Prometheus.</p>
<h2 id="promql-na-prática">PromQL na Prática</h2>
<p>O PromQL é a linguagem de consulta do Prometheus que permite extrair insights valiosos das métricas coletadas. Vamos explorar exemplos práticos de consultas comuns para uso diário em monitoramento.</p>
<h3 id="consultas-básicas-de-disponibilidade">Consultas Básicas de Disponibilidade</h3>
<p><strong>Verificar se todos os targets estão up:</strong></p>


  <pre><code class="language-promql">up == 1</code></pre>
 <p><strong>Contar quantos targets estão down:</strong></p>


  <pre><code class="language-promql">count(up == 0)</code></pre>
 <p><strong>Taxa de disponibilidade por job:</strong></p>


  <pre><code class="language-promql">avg(up) by (job)</code></pre>
 <h3 id="métricas-de-sistema-node-exporter">Métricas de Sistema (Node Exporter)</h3>
<p><strong>CPU médio por instância:</strong></p>


  <pre><code class="language-promql">avg(rate(node_cpu_seconds_total{mode=&#34;user&#34;}[5m])) by (instance)</code></pre>
 <p><strong>Uso de memória em porcentagem:</strong></p>


  <pre><code class="language-promql">(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100</code></pre>
 <p><strong>Uso de disco por filesystem:</strong></p>


  <pre><code class="language-promql">(node_filesystem_size_bytes - node_filesystem_free_bytes) / node_filesystem_size_bytes * 100</code></pre>
 <p><strong>Taxa de I/O de disco:</strong></p>


  <pre><code class="language-promql">rate(node_disk_io_time_seconds_total[5m])</code></pre>
 <h3 id="métricas-de-aplicação-web">Métricas de Aplicação Web</h3>
<p><strong>Taxa de requisições por segundo (QPS):</strong></p>


  <pre><code class="language-promql">rate(http_requests_total[5m])</code></pre>
 <p><strong>Taxa de erro por endpoint:</strong></p>


  <pre><code class="language-promql">rate(http_requests_total{status=~&#34;5..&#34;}[5m])</code></pre>
 <p><strong>Latência p95 (percentil 95):</strong></p>


  <pre><code class="language-promql">histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))</code></pre>
 <p><strong>Latência p99 (percentil 99):</strong></p>


  <pre><code class="language-promql">histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))</code></pre>
 <p><strong>Tempo de resposta médio:</strong></p>


  <pre><code class="language-promql">rate(http_request_duration_seconds_sum[5m]) / rate(http_request_duration_seconds_count[5m])</code></pre>
 <h3 id="métricas-de-banco-de-dados">Métricas de Banco de Dados</h3>
<p><strong>Conexões ativas do PostgreSQL:</strong></p>


  <pre><code class="language-promql">pg_stat_database_numbackends</code></pre>
 <p><strong>Taxa de transações por segundo:</strong></p>


  <pre><code class="language-promql">rate(pg_stat_database_xact_commit[5m]) &#43; rate(pg_stat_database_xact_rollback[5m])</code></pre>
 <p><strong>Tamanho de tabelas (PostgreSQL):</strong></p>


  <pre><code class="language-promql">pg_stat_user_tables_size_bytes</code></pre>
 <h3 id="métricas-de-containerkubernetes">Métricas de Container/Kubernetes</h3>
<p><strong>CPU por pod:</strong></p>


  <pre><code class="language-promql">sum(rate(container_cpu_usage_seconds_total{container!=&#34;&#34;}[5m])) by (pod)</code></pre>
 <p><strong>Memória por pod:</strong></p>


  <pre><code class="language-promql">sum(container_memory_usage_bytes{container!=&#34;&#34;}) by (pod)</code></pre>
 <p><strong>Pods por namespace:</strong></p>


  <pre><code class="language-promql">count(kube_pod_info) by (namespace)</code></pre>
 <h3 id="alertas-comuns">Alertas Comuns</h3>
<p><strong>Alerta de CPU alta:</strong></p>


  <pre><code class="language-promql">100 - (avg(irate(node_cpu_seconds_total{mode=&#34;idle&#34;}[5m])) by (instance) * 100) &gt; 80</code></pre>
 <p><strong>Alerta de memória alta:</strong></p>


  <pre><code class="language-promql">(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 &gt; 85</code></pre>
 <p><strong>Alerta de disco cheio:</strong></p>


  <pre><code class="language-promql">(node_filesystem_size_bytes - node_filesystem_free_bytes) / node_filesystem_size_bytes * 100 &gt; 90</code></pre>
 <p><strong>Alerta de latência alta:</strong></p>


  <pre><code class="language-promql">histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le)) &gt; 1</code></pre>
 <p><strong>Alerta de taxa de erro alta:</strong></p>


  <pre><code class="language-promql">rate(http_requests_total{status=~&#34;5..&#34;}[5m]) / rate(http_requests_total[5m]) &gt; 0.05</code></pre>
 <h3 id="consultas-avançadas">Consultas Avançadas</h3>
<p><strong>Top 5 instâncias com maior CPU:</strong></p>


  <pre><code class="language-promql">topk(5, 100 - (avg(irate(node_cpu_seconds_total{mode=&#34;idle&#34;}[5m])) by (instance) * 100))</code></pre>
 <p><strong>Soma de métricas por região:</strong></p>


  <pre><code class="language-promql">sum(rate(http_requests_total[5m])) by (region)</code></pre>
 <p><strong>Diferença de métricas entre períodos:</strong></p>


  <pre><code class="language-promql">increase(http_requests_total[1h]) - increase(http_requests_total[1h] offset 1h)</code></pre>
 <p><strong>Métrica com label dinâmico:</strong></p>


  <pre><code class="language-promql">rate(http_requests_total{endpoint=~&#34;/api/.*&#34;}[5m])</code></pre>
 <h3 id="dicas-de-performance">Dicas de Performance</h3>
<p><strong>Use intervalos apropriados:</strong></p>
<ul>
<li>Para alertas: <code>[5m]</code> ou <code>[1m]</code></li>
<li>Para dashboards: <code>[1h]</code> para tendências</li>
<li>Evite <code>[24h]</code> em consultas frequentes</li>
</ul>
<p><strong>Prefira <code>rate()</code> sobre <code>irate()</code> para alertas:</strong></p>


  <pre><code class="language-promql"># Bom para alertas (mais estável)
rate(http_requests_total[5m])

# Melhor para dashboards (mais responsivo)
irate(http_requests_total[5m])</code></pre>
 <p><strong>Agregue quando possível:</strong></p>


  <pre><code class="language-promql"># Em vez de somar muitas séries
sum(rate(http_requests_total[5m])) by (job)

# Evite isso em métricas com alta cardinalidade
sum(rate(http_requests_total[5m]))</code></pre>
 <h3 id="exemplos-de-recording-rules">Exemplos de Recording Rules</h3>
<p><strong>Regra para QPS agregado:</strong></p>


  <pre><code class="language-yaml">groups:
- name: recording_rules
  rules:
    - record: job:http_requests:rate5m
      expr: sum(rate(http_requests_total[5m])) by (job)</code></pre>
 <p><strong>Regra para latência p95:</strong></p>


  <pre><code class="language-yaml">    - record: job:http_request_duration_seconds:p95
      expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, job))</code></pre>
 <blockquote>
<p><strong>Dica</strong>: Use recording rules para pré-calcular consultas complexas e frequentes. Isso melhora a performance e reduz a carga no Prometheus.</p></blockquote>
<h3 id="instrumentação-direta-exemplos-por-linguagem">Instrumentação direta: exemplos por linguagem</h3>
<p>Agora vejamos como instrumentar aplicações escritas em algumas linguagens populares. A ideia geral em qualquer linguagem é: instalar a biblioteca cliente do Prometheus, criar métricas (<a href="https://prometheus.io/docs/concepts/metric_types/#counter">counters</a>, <a href="https://prometheus.io/docs/concepts/metric_types/#gauge">gauges</a>, etc.) em pontos estratégicos do código, e expor um endpoint HTTP <code>/metrics</code> onde essas métricas são servidas (em formato de texto). O Prometheus então coleta nesse endpoint.</p>
<h4 id="java-micrometer--cliente-java-do-prometheus">Java (Micrometer / Cliente Java do Prometheus)</h4>
<p>Em Java, uma abordagem comum é usar o <strong><a href="https://micrometer.io/">Micrometer</a></strong> – uma biblioteca de instrumentação que suporta múltiplos backends (Prometheus, Graphite, etc.). O Micrometer foi adotado pelo Spring Boot, por exemplo, facilitando a exposição de métricas. Passos básicos:</p>
<ol>
<li>
<p><strong>Dependências:</strong> Adicione ao seu projeto (pom.xml ou build.gradle) a dependência do Micrometer e do registry Prometheus. Exemplo (Maven):</p>


  <pre><code class="language-xml">&lt;dependency&gt;
    &lt;groupId&gt;io.micrometer&lt;/groupId&gt;
    &lt;artifactId&gt;micrometer-core&lt;/artifactId&gt;
    &lt;version&gt;1.10.4&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;io.micrometer&lt;/groupId&gt;
    &lt;artifactId&gt;micrometer-registry-prometheus&lt;/artifactId&gt;
    &lt;version&gt;1.10.4&lt;/version&gt;
&lt;/dependency&gt;</code></pre>
 </li>
<li>
<p><strong>Registrar métricas:</strong> Em sua aplicação, configure um <code>MeterRegistry</code> do Prometheus e registre métricas. Por exemplo, em uma classe de configuração Spring:</p>


  <pre><code class="language-java">@Bean
PrometheusMeterRegistry prometheusRegistry() {
    return new PrometheusMeterRegistry(PrometheusConfig.DEFAULT);
}</code></pre>
 <p>Você pode então criar contadores, gauges, etc. usando esse registry:</p>


  <pre><code class="language-java">Counter requestCount = Counter.builder(&#34;myapp_requests_total&#34;)
                              .description(&#34;Total de requisições&#34;)
                              .register(prometheusRegistry());
// Usar requestCount.inc(); em pontos apropriados do código</code></pre>
 <p>Ou usar anotações/filtros prontos do Spring Boot Actuator que medem tempos de resposta automaticamente.</p>
</li>
<li>
<p><strong>Expor endpoint /metrics:</strong> Se estiver usando Spring Boot Actuator, habilite a endpoint Prometheus. No application.properties:</p>


  <pre><code class="language-">management.endpoints.web.exposure.include=prometheus
management.endpoint.prometheus.enabled=true</code></pre>
 <p>Isso fará o Actuator expor <code>/actuator/prometheus</code> com as métricas no formato Prometheus. O Prometheus pode então fazer scrape nessa URL. (Alternativamente, sem Spring, você poderia iniciar um HTTP server manualmente que responda com <code>prometheusRegistry.scrape()</code> output).</p>
</li>
<li>
<p><strong>Verificar métricas:</strong> Ao rodar a aplicação, acesse <a href="http://localhost:8080/actuator/prometheus">http://localhost:8080/actuator/prometheus</a> (por exemplo) e você verá todas as métricas registradas, inclusive padrões do JVM (o Micrometer já fornece métricas de memória, CPU, GC, etc. por padrão) e as personalizadas que você adicionou.</p>
</li>
</ol>
<blockquote>
<p>Em resumo, no Java/Spring o processo pode ser muito simples aproveitando frameworks existentes. Para outras aplicações Java sem Spring, existe também o cliente Java do Prometheus (simpleclient) onde você manualmente gerencia as métricas e HTTP endpoint.</p></blockquote>
<h4 id="javascriptnodejs">JavaScript/Node.js</h4>
<p>No Node.js podemos usar o pacote <strong>prom-client</strong> para instrumentação:</p>
<ol>
<li>
<p><strong>Instalar pacote:</strong> <code>npm install prom-client</code>.</p>
</li>
<li>
<p><strong>Criar métricas no código:</strong> Por exemplo, vamos medir o tempo de resposta de uma rota Express:</p>


  <pre><code class="language-js">const express = require(&#39;express&#39;);
const promClient = require(&#39;prom-client&#39;);
const app = express();

// Cria um histogram para tempos de resposta em segundos
const httpResponseHist = new promClient.Histogram({
  name: &#39;myapp_http_response_duration_seconds&#39;,
  help: &#39;Tempo de resposta das requisições HTTP (segundos)&#39;,
  labelNames: [&#39;route&#39;, &#39;method&#39;]
});</code></pre>
 <p>Aqui usamos um Histogram (poderia ser Summary também). Antes de enviar a resposta na rota, registramos a observação:</p>


  <pre><code class="language-js">app.get(&#39;/example&#39;, (req, res) =&gt; {
  const end = httpResponseHist.startTimer({ route: &#39;/example&#39;, method: &#39;GET&#39; });
  // ... lógica da rota ...
  res.send(&#34;Hello World&#34;);
  end(); // marca o fim do timer e observa a duração no histogram
});</code></pre>
 <p>O <em>prom-client</em> possui métodos convenientes para medir duração com <code>Histogram.startTimer()</code> que retorna uma função para encerrar e registrar.</p>
</li>
<li>
<p><strong>Expor as métricas:</strong> Precisamos servir as métricas via HTTP para o Prometheus. Podemos criar um endpoint <code>/metrics</code>:</p>


  <pre><code class="language-js">app.get(&#39;/metrics&#39;, async (req, res) =&gt; {
  res.set(&#39;Content-Type&#39;, promClient.register.contentType);
  res.end(await promClient.register.metrics());
});</code></pre>
 <p>Isso coleta todas as métricas registradas e retorna no formato de texto padrão.</p>
</li>
<li>
<p><strong>Iniciar server:</strong> Por fim, inicie seu servidor Node (por ex, <code>app.listen(3000)</code>). Então a URL <a href="http://localhost:3000/metrics">http://localhost:3000/metrics</a> mostrará as métricas.</p>
</li>
<li>
<p><strong>Configurar Prometheus:</strong> Adicione no <code>prometheus.yml</code> um job apontando para o serviço Node, porta 3000 (ou a porta usada) e path <code>/metrics</code>. Exemplo:</p>


  <pre><code class="language-yaml">scrape_configs:
  - job_name: &#39;my-nodeapp&#39;
    static_configs:
      - targets: [&#39;my-node-host:3000&#39;]</code></pre>
 <p>(Se o Node está no mesmo Docker Compose do Prometheus, pode usar o nome de serviço do container e porta.)</p>
</li>
</ol>
<p>A partir daí, o Prometheus coletará as métricas do seu app Node. Você poderá consultar coisas como <code>rate(myapp_http_response_duration_seconds_count[5m])</code> ou <code>histogram_quantile(0.9, rate(myapp_http_response_duration_seconds_bucket[5m]))</code> para ver percentis de latência.</p>
<h4 id="python-flask-etc">Python (Flask, etc.)</h4>
<p>Em Python, há o pacote <strong>prometheus_client</strong>. Exemplo integrando com Flask:</p>
<ol>
<li>
<p><strong>Instalação:</strong> <code>pip install prometheus_client</code>.</p>
</li>
<li>
<p><strong>Criação de métricas:</strong> Digamos que queremos contar requisições e medir duração. Podemos usar um Histogram ou Summary. Aqui um Summary:</p>


  <pre><code class="language-python">from flask import Flask, request
from prometheus_client import Summary, Counter, start_http_server

app = Flask(__name__)
REQUEST_TIME = Summary(&#39;myapp_request_processing_seconds&#39;, &#39;Tempo de processamento por rota&#39;, [&#39;endpoint&#39;])
REQUEST_COUNT = Counter(&#39;myapp_requests_total&#39;, &#39;Total de requisições&#39;, [&#39;endpoint&#39;, &#39;http_status&#39;])</code></pre>
 <p>Decoramos a rota para coletar métricas:</p>


  <pre><code class="language-python">@app.route(&#34;/example&#34;)
def example():
    with REQUEST_TIME.labels(endpoint=&#34;/example&#34;).time():  # inicia timer automático
        # ... lógica do endpoint ...
        response = &#34;Hello World&#34;
    REQUEST_COUNT.labels(endpoint=&#34;/example&#34;, http_status=200).inc()
    return response</code></pre>
 <p>O <code>Summary.time()</code> funciona como context manager medindo o tempo dentro do bloco. Também incrementamos um counter de requests totais por endpoint e status.</p>
</li>
<li>
<p><strong>Expor métricas:</strong> Podemos fazer de duas formas – ou usamos o servidor HTTP interno do prometheus_client ou integramos com Flask. Uma maneira simples: iniciar um <em>thread</em> do servidor metrics separado:</p>


  <pre><code class="language-python">if __name__ == &#34;__main__&#34;:
    start_http_server(8000)  # inicia servidor em porta 8000
    app.run(host=&#34;0.0.0.0&#34;, port=5000)</code></pre>
 </li>
</ol>
<p>O <code>start_http_server(8000)</code> fará com que em <a href="http://localhost:8000/metrics">http://localhost:8000/metrics</a> tenhamos as métricas (note: ele por default expõe em /metrics automaticamente). Nesse caso, o Prometheus deve apontar para porta 8000 do app.</p>
<p>Alternativamente, há integração para Flask (via middleware) que poderia expor /metrics no próprio Flask app.</p>
<ol start="4">
<li><strong>Prometheus config:</strong> Similar aos anteriores, adicionar job apontando para o endpoint do metrics (host e porta usados).</li>
</ol>
<p>Após esses passos, seu app Python estará fornecendo métricas. Você pode conferir acessando <a href="http://localhost:8000/metrics">http://localhost:8000/metrics</a> e vendo as séries nomeadas <code>myapp_request_processing_seconds_*</code> e <code>myapp_requests_total</code> entre outras (o client lib Python também expõe métricas padrão do processo Python como uso de memória do processo, CPU, etc.).</p>
<h3 id="ferramentas-legadas-e-fechadas">Ferramentas legadas e fechadas</h3>
<p>Uma dificuldade comum é monitorar sistemas legados ou softwares proprietários que não oferecem métricas no formato Prometheus. Nesses casos, há alguns padrões de solução:</p>
<ul>
<li>
<p><strong><a href="https://prometheus.io/docs/instrumenting/exporters/">Exporters externos</a></strong>: Como já mencionado, se existir um exporter compatível (oficial ou da comunidade) para aquela ferramenta, ele é o caminho mais fácil – rodar o exporter e configurá-lo como alvo. Por exemplo, para monitorar um servidor Oracle proprietário, pode haver um exporter que conecta no Oracle e extrai estatísticas via queries.</p>
</li>
<li>
<p><strong><a href="https://prometheus.io/docs/instrumenting/writing_exporters/#writing-a-bridge-exporter">Bridges personalizadas</a>:</strong> Caso não exista um exporter pronto, podemos criar um processo intermediário (<em>bridge</em>) que consulta a ferramenta legada de alguma forma (API REST, CLI, leitura de arquivos de log) e expõe resultados em /metrics. Essencialmente, isso é escrever um pequeno exporter sob medida. Ferramentas de script como Python facilitam isso – você coleta os dados e usa <code>prometheus_client</code> para expor.</p>
</li>
<li>
<p><strong><a href="https://prometheus.io/docs/instrumenting/writing_exporters/#writing-a-bridge-exporter">Integrações via gateway ou plugins</a>:</strong> Alguns ambientes possuem hooks para métricas. Por exemplo, aplicações .NET legadas podem exportar contadores no Windows Performance Counters – aí usar o Windows Exporter para pegá-los. Em casos extremos, você pode usar o Pushgateway como ponte: o sistema legado faz push de alguma métrica básica para o gateway (não ideal, mas possível).</p>
</li>
</ul>
<blockquote>
<p>Em resumo, <strong>sempre</strong> é possível integrar algo ao Prometheus, ainda que indiretamente. A comunidade já produziu exporters para muitos sistemas fechados (WebLogic, SAP, etc.). E como último recurso, extrair dados e expor manualmente não é tão complexo graças às bibliotecas cliente disponíveis.</p></blockquote>
<h2 id="alertmanager">Alertmanager</h2>
<p>O <strong><a href="https://prometheus.io/docs/alerting/latest/alertmanager/">Alertmanager</a></strong> complementa o Prometheus no tratamento de alertas. Enquanto o Prometheus detecta condições de alerta (com base nas métricas e regras definidas), ele delega ao Alertmanager a função de envio de notificações e gerenciamento desses alertas. Isso inclui agregar alertas similares, evitar duplicações, silenciar alertas durante manutenção, e encaminhá-los para canais apropriados (e-mail, sistemas de chat, PagerDuty, etc.).</p>
<p><strong>Alta Disponibilidade:</strong> O Alertmanager suporta configuração em cluster para alta disponibilidade. Quando múltiplas instâncias do Alertmanager estão ativas, elas se comunicam entre si para deduplicar alertas vindos de dois Prometheus idênticos, garantindo que apenas uma notificação seja enviada mesmo quando múltiplas fontes detectam o mesmo problema.</p>
<blockquote>
<p>Como funciona: você define no Prometheus regras de alerta (no arquivo de configuração ou separado) com expressões PromQL que identificam situações problemáticas. Por exemplo: &ldquo;se a métrica <code>up</code> de um servidor for 0 por 5 minutos, dispare alerta&rdquo;.</p></blockquote>
<p>Quando a condição é verdadeira, o Prometheus gera um evento de alerta e o envia para o Alertmanager (que está configurado na seção <code>alerting</code> do prometheus.yml).</p>
<blockquote>
<p>O Alertmanager então aplica suas próprias regras de roteamento: por exemplo, enviar alertas de severidade crítica para um webhook do Slack e para email da equipe X, alertas menos graves só para email, etc&hellip;</p></blockquote>
<p><strong>Exemplo prático:</strong> Vamos configurar um alerta de servidor fora do ar com notificação no Slack.</p>
<ol>
<li><strong>Definir regra de alerta (Prometheus):</strong> Crie um arquivo <code>alert.rules.yml</code>:</li>
</ol>


  <pre><code class="language-yaml">groups:
- name: instance_down
  rules:
    - alert: InstanceDown
      expr: up == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: &#34;Instância {{ $labels.instance }} fora do ar&#34;
        description: &#34;O alvo {{ $labels.instance }} não respondeu às coletas por mais de 1 minuto.&#34;</code></pre>
 <p>Essa regra verifica a métrica <code>up</code> de todos os alvos; se qualquer um estiver com valor 0 (significa alvo inacessível) por 1 minuto contínuo, aciona o alerta <strong>InstanceDown</strong> com severidade <strong>critical</strong>. As anotações fornecem um resumo e descrição usando templating (inserindo o label instance do alvo problemático).</p>
<ol start="2">
<li><strong>Incluir regra e Alertmanager na config do Prometheus:</strong> No <code>prometheus.yml</code>, adicionar:</li>
</ol>


  <pre><code class="language-yaml">rule_files:
  - &#34;alert.rules.yml&#34;

alerting:
  alertmanagers:
    - static_configs:
        - targets: [&#39;alertmanager:9093&#39;]</code></pre>
 <p>Aqui presumimos que o Alertmanager está rodando e acessível no endereço <code>alertmanager:9093</code> (no Docker Compose, por ex.). O Prometheus agora carrega as regras de alerta e sabe para onde enviar notificações.</p>
<ol start="3">
<li><strong>Configurar o Alertmanager (alertmanager.yml):</strong> Exemplo mínimo para Slack:</li>
</ol>


  <pre><code class="language-yaml">route:
  group_by: [&#39;alertname&#39;]
  receiver: &#39;time-slack&#39;
receivers:
  - name: &#39;time-slack&#39;
    slack_configs:
      - api_url: &#39;https://hooks.slack.com/services/T000/B000/XXXXX&#39;  # Webhook do Slack
        channel: &#39;#alerts&#39;
        send_resolved: true
        title: &#34;{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}&#34;
        text: &#34;{{ range .Alerts }}{{ .Annotations.description }}{{ end }}&#34;</code></pre>
 <p>Esse config muito básico diz: todos alertas (não importa o grupo_by, etc.) irão para o receptor nomeado &rsquo;time-slack&rsquo;, que tem um slack_config apontando para um webhook do Slack no canal <strong>#alerts</strong>. O <code>title</code> e <code>text</code> da mensagem aproveitam as anotações definidas na regra (summary e description).</p>
<p>O valor <code>send_resolved: true</code> indica para notificar também quando o alerta for resolvido.</p>
<p>Em produção, o Alertmanager pode ter rotas mais elaboradas – por exemplo, roteando com base em labels de alerta (team=A vai para equipe A, severidade critical pode mandar SMS, etc.), escalonamento, agrupamento por determinados campos (como agrupar todos alertas do mesmo datacenter numa só notificação), etc.</p>
<ol start="4">
<li><strong>Executar e testar:</strong> Rode o Alertmanager com esse config (no Docker ou binário). Quando um alerta InstanceDown ocorrer, o Prometheus vai enviar para o Alertmanager, que em seguida usará a integração <a href="https://prometheus.io/docs/alerting/latest/configuration/#slack-receiver">Slack</a> para postar no canal configurado uma mensagem com título &ldquo;Instância X fora do ar&rdquo; e descrição com detalhes.</li>
</ol>
<p>Esse foi um exemplo focado em Slack, mas o Alertmanager suporta muitos outros <strong>receivers</strong>: e-mail (SMTP), PagerDuty, OpsGenie, VictorOps, Webhooks genéricos, entre outros. Com ele, você ganha flexibilidade para gerenciar o &ldquo;barulho&rdquo; de alertas: por exemplo, suprimir alertas filhos quando um pai já ocorreu (<a href="https://prometheus.io/docs/alerting/latest/configuration/#inhibition">inhibition</a>), ou silenciar certos alertas durante janelas de manutenção planejada.</p>
<blockquote>
<p><strong>Observação:</strong> O Alertmanager não é obrigatório – você pode rodar o Prometheus sem ele se não precisar de notificações externas. Porém, para qualquer ambiente de produção, é altamente recomendado configurá-lo para não depender de ficar olhando a página /alerts manualmente. Em outro artigo abordaremos em detalhes boas práticas de configuração do Alertmanager.</p></blockquote>
<h3 id="alertmanager-avançado-silencing-e-inhibition">Alertmanager Avançado: Silencing e Inhibition</h3>
<p>Em ambientes de produção com muitos alertas, o <strong>&ldquo;alert fatigue&rdquo;</strong> (fadiga de alertas) pode ser um problema sério. O Alertmanager oferece funcionalidades avançadas para gerenciar esse cenário: <strong>silencing</strong> (silenciamento) e <strong>inhibition</strong> (inibição).</p>
<h4 id="silencing">Silencing</h4>
<p>O <strong>silencing</strong> permite suprimir temporariamente alertas específicos, geralmente durante janelas de manutenção planejada. Isso evita spam desnecessário quando você já sabe que um serviço estará indisponível.</p>
<p><strong>Exemplo de configuração de silence:</strong></p>


  <pre><code class="language-yaml"># Via API do Alertmanager (POST /api/v1/silences)
{
  &#34;matchers&#34;: [
    {
      &#34;name&#34;: &#34;alertname&#34;,
      &#34;value&#34;: &#34;InstanceDown&#34;,
      &#34;isRegex&#34;: false
    },
    {
      &#34;name&#34;: &#34;instance&#34;,
      &#34;value&#34;: &#34;web-server-01:9100&#34;,
      &#34;isRegex&#34;: false
    }
  ],
  &#34;startsAt&#34;: &#34;2023-12-01T10:00:00Z&#34;,
  &#34;endsAt&#34;: &#34;2023-12-01T12:00:00Z&#34;,
  &#34;createdBy&#34;: &#34;admin&#34;,
  &#34;comment&#34;: &#34;Manutenção programada do servidor web-01&#34;
}</code></pre>
 <p><strong>Silencing via interface web:</strong>
O Alertmanager oferece uma interface web em <code>/silences</code> onde você pode criar silences interativamente, especificando:</p>
<ul>
<li><strong>Matchers</strong>: Labels que identificam os alertas a silenciar</li>
<li><strong>Duração</strong>: Período de silenciamento (início e fim)</li>
<li><strong>Comentário</strong>: Justificativa para o silence</li>
</ul>
<h4 id="inhibition">Inhibition</h4>
<p>A <strong>inhibition</strong> permite suprimir alertas secundários quando um alerta primário já está ativo. Por exemplo, se um servidor caiu (alerta crítico), não faz sentido alertar sobre &ldquo;disco quase cheio&rdquo; ou &ldquo;alta latência&rdquo; na mesma instância.</p>
<p><strong>Exemplo de configuração de inhibition:</strong></p>


  <pre><code class="language-yaml">inhibit_rules:
  # Se um alerta critical estiver ativo, suprimir warnings da mesma instância
  - source_match:
      severity: &#39;critical&#39;
    target_match:
      severity: &#39;warning&#39;
    equal: [&#39;instance&#39;, &#39;job&#39;]
  
  # Se um datacenter estiver down, suprimir alertas de serviços internos
  - source_match:
      alertname: &#39;DatacenterDown&#39;
    target_match:
      severity: &#39;warning&#39;
    equal: [&#39;datacenter&#39;]
  
  # Se CPU estiver 100%, suprimir alertas de alta latência
  - source_match:
      alertname: &#39;HighCPUUsage&#39;
      severity: &#39;critical&#39;
    target_match:
      alertname: &#39;HighLatency&#39;
    equal: [&#39;instance&#39;]</code></pre>
 <p><strong>Casos de uso comuns:</strong></p>
<ul>
<li><strong>Alertas de infraestrutura</strong>: Se um rack/datacenter caiu, suprimir alertas de serviços que dependem dele</li>
<li><strong>Alertas de aplicação</strong>: Se um serviço crítico está down, não alertar sobre métricas secundárias</li>
<li><strong>Alertas de dependência</strong>: Se um banco de dados está inacessível, suprimir alertas de aplicações que dependem dele</li>
</ul>
<h4 id="routing-avançado">Routing Avançado</h4>
<p>O Alertmanager permite roteamento sofisticado baseado em labels de alerta:</p>


  <pre><code class="language-yaml">route:
  group_by: [&#39;alertname&#39;, &#39;cluster&#39;, &#39;service&#39;]
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h
  
  routes:
    # Alertas críticos vão para PagerDuty &#43; Slack
    - match:
        severity: critical
      receiver: &#39;pager-duty-critical&#39;
      continue: true  # Continua para o próximo receiver
    
    # Alertas críticos também vão para Slack
    - match:
        severity: critical
      receiver: &#39;slack-critical&#39;
    
    # Alertas de infraestrutura vão para equipe de infra
    - match:
        job: node
      receiver: &#39;infra-team&#39;
    
    # Alertas de aplicação vão para equipe de dev
    - match:
        job: app
      receiver: &#39;dev-team&#39;
    
    # Default: todos os outros alertas vão para Slack geral
    - receiver: &#39;slack-general&#39;

receivers:
  - name: &#39;pager-duty-critical&#39;
    pagerduty_configs:
      - service_key: &#39;your-pagerduty-key&#39;
  
  - name: &#39;slack-critical&#39;
    slack_configs:
      - api_url: &#39;https://hooks.slack.com/services/...&#39;
        channel: &#39;#alerts-critical&#39;
  
  - name: &#39;infra-team&#39;
    slack_configs:
      - api_url: &#39;https://hooks.slack.com/services/...&#39;
        channel: &#39;#infra-alerts&#39;
  
  - name: &#39;dev-team&#39;
    slack_configs:
      - api_url: &#39;https://hooks.slack.com/services/...&#39;
        channel: &#39;#dev-alerts&#39;
  
  - name: &#39;slack-general&#39;
    slack_configs:
      - api_url: &#39;https://hooks.slack.com/services/...&#39;
        channel: &#39;#monitoring&#39;</code></pre>
 <p><strong>Recursos avançados:</strong></p>
<ul>
<li><strong>Agrupamento inteligente</strong>: <code>group_by</code> agrupa alertas similares em uma notificação</li>
<li><strong>Tempo de espera</strong>: <code>group_wait</code> aguarda antes de enviar o primeiro alerta do grupo</li>
<li><strong>Intervalo de repetição</strong>: <code>repeat_interval</code> define quando reenviar alertas não resolvidos</li>
<li><strong>Roteamento condicional</strong>: <code>continue: true</code> permite múltiplos receivers para o mesmo alerta</li>
</ul>
<h2 id="pushgateway">PushGateway</h2>
<p>O <strong><a href="https://prometheus.io/docs/instrumenting/pushing/">Pushgateway</a></strong> é um componente auxiliar do ecossistema Prometheus que permite coletar métricas via modelo <em>push</em> em situações específicas. A ideia é que certos jobs ou aplicativos efêmeros, que não têm como serem raspados diretamente (por exemplo, um script cron que executa e termina rapidamente), possam empurrar suas métricas para um gateway intermediário. O Prometheus então coleta essas métricas do Pushgateway posteriormente.</p>
<p>Funciona assim: o job de curta duração (ou qualquer processo que não viva tempo suficiente para ser raspado) envia um HTTP POST para o Pushgateway com suas métricas no formato Prometheus. O Pushgateway armazena essas métricas em memória e as expõe em seu próprio <code>/metrics</code>. O Prometheus configura um scrape no Pushgateway, coletando tudo que estiver lá.</p>
<p><strong>Porém,</strong> é importante entender que o Pushgateway deve ser usado com moderação e propósito claro. Ele não é um agente genérico para substituir o modelo pull. Alguns pontos de atenção destacados pela documentação oficial:</p>
<ul>
<li>Se múltiplas instâncias usam um mesmo Pushgateway, ele vira um ponto central de falha e potencial gargalo.</li>
<li>Você perde a detecção automática de <em>down</em> (já que as métricas são push, o Prometheus não sabe se um job não está rodando ou só não teve métricas recentes).</li>
<li>O Pushgateway <strong>não expira</strong> automaticamente séries que foram enviadas. Uma vez que uma métrica é empurrada, ela ficará lá até ser sobrescrita ou manualmente apagada via API do Pushgateway. Isso significa que métricas de jobs antigos podem ficar persistindo como &ldquo;fantasmas&rdquo;, exigindo que você gerencie remoção ou inclusão de algum label de <em>instance</em> para distingui-las.</li>
</ul>
<p>Devido a esses aspectos, o uso recomendado do Pushgateway é <strong>capturar resultados de jobs batch de nível de serviço</strong> – isto é, trabalhos que não pertencem a uma única máquina ou instância específica, mas sim algo como &ldquo;um script de limpeza de banco que roda uma vez por dia&rdquo;.</p>
<p>Nesse caso, o job emite (push) uma métrica do tipo &ldquo;usuarios_deletados_total{job=&ldquo;cleanup&rdquo;} 123&rdquo; e termina. O Pushgateway guarda esse valor.</p>
<p>O Prometheus, ao raspar, terá essa informação agregada do job. Como esse tipo de job não tem um &ldquo;endpoint&rdquo; próprio para scrap, o Pushgateway serve como cache.</p>
<p>Para outros cenários, onde o push é considerado porque há firewall/NAT impedindo scrapes, a documentação sugere alternativas melhores – como rodar Prometheus perto dos alvos (dentro da rede) ou usar algo como o <strong><a href="https://github.com/prometheus/pushprox">PushProx</a></strong> para atravessar firewalls mantendo o modelo pull. E para jobs cron por máquina, que têm contexto de host, recomenda-se usar o <strong><a href="https://github.com/prometheus/node_exporter#textfile-collector">Node Exporter Textfile Collector</a></strong> (escrever métricas em um arquivo que o Node Exporter lê), ao invés do Pushgateway.</p>
<blockquote>
<p>Resumindo: o Pushgateway é útil, mas <strong>somente</strong> em casos específicos. Evite usá-lo para coletar métricas de serviços normais (isso seria “usar push por preguiça”, e acarretaria problemas de dados stale e falta de detecção de falha). Use-o para jobs batch pontuais, e mesmo assim, sem abusar – lembre-se de limpar métricas antigas se necessário, ou projetar os labels de modo que cada job substitua seu próprio valor sem acumular lixo.</p></blockquote>
<h2 id="federação">Federação</h2>
<p>A <strong>federação</strong> no Prometheus permite que uma instância do Prometheus (geralmente chamada de <strong>federadora</strong> ou <strong>global</strong>) faça scrape em endpoints de outras instâncias do Prometheus (<strong>federadas</strong>) para obter um subconjunto de suas métricas.</p>
<p>Em outras palavras, é uma forma de <strong>hierarquizar</strong> o monitoramento: por exemplo, você pode ter um Prometheus por data center coletando tudo localmente, e um Prometheus global que apenas busca métricas já agregadas de cada data center para ter uma visão geral corporativa.</p>
<p>Existem dois casos de uso principais para federação:</p>
<ol>
<li>
<p><strong><a href="https://prometheus.io/docs/prometheus/latest/federation/">Agregação hierárquica</a></strong>: como no exemplo acima, onde cada Prometheus local faz o trabalho pesado e calcula agregados (soma de CPU por datacenter, latência média de serviço X por datacenter, etc.), e o Prometheus global só extrai essas séries agregadas prontas. Isso dá uma visão do todo sem sobrecarregar a instância global com todas as séries detalhadas.</p>
</li>
<li>
<p><strong><a href="https://prometheus.io/docs/prometheus/latest/federation/">Checagens cruzadas ou seletivas</a></strong>: Puxar algumas poucas métricas de outra instância para comparações. Exemplo: você tem um Prometheus dedicado a HAProxy e outro para um app front-end, pode federar a métrica de QPS do HAProxy no Prometheus do front-end para checar se ambos observam o mesmo tráfego. Normalmente, isso é usado até mesmo apenas para alertas (você pode configurar alertas usando essas poucas métricas federadas).</p>
</li>
</ol>
<p><strong><a href="https://prometheus.io/docs/prometheus/latest/federation/#when-not-to-use-federation">Quando NÃO usar federação</a>:</strong> a tentação de federar tudo de todos os Prometheus em um “super Prometheus” central deve ser evitada. Pegar todas as séries de instâncias filhas e centralizar em uma só instância global traz vários problemas:</p>
<ul>
<li><strong>Escalabilidade limitada:</strong> O desempenho do Prometheus é limitado pelos recursos de um único nó (não escala horizontalmente). Se você puxa tudo para um só servidor global, no fim do dia você está limitado ao throughput e memória de uma máquina. Isso anula a distribuição de carga que múltiplas instâncias proporcionam.</li>
<li><strong>Performance e carga duplicada:</strong> Além de sobrecarregar a instância global ao ter que armazenar e consultar tudo, a própria operação de federação (expor /federate e responder a scraping) gera carga nas instâncias filhas. Se a consulta federada não for focada (usar expressões match[] genéricas demais), pode consumir muitos recursos para as instâncias fonte servirem esses dados.</li>
<li><strong>Confiabilidade reduzida:</strong> Você adiciona um ponto extra de falha. Se o link entre uma instância local e a global cair, a instância global “fica cega” àquele segmento. E pior, se você centralizou a avaliação de certos alertas só no global, pode ficar sem alertas (falso negativo) caso o global perca conexão com os locais. A recomendação de especialistas é sempre que possível avaliar alertas o mais localmente possível – por exemplo, um alerta “serviço X caiu” deve ser definido no Prometheus que coleta serviço X, não em um global distante, exatamente para não depender de rede.</li>
<li><strong>Delay e possíveis inconsistências:</strong> A federação não é instantânea; há latência entre um dado ser coletado no Prometheus filho e ser federado pelo pai. Além disso, condições de corrida podem fazer o global perder algumas amostras ou ver valores ligeiramente diferentes (por exemplo, contadores que resetaram podem parecer estranhos). Para uns poucos agregados isso é tolerável, mas se você federar tudo e depender disso para alertar, pode ter sutilezas indesejadas.</li>
<li><strong>Complexidade de configuração e segurança:</strong> É mais complexo gerenciar dois níveis de Prometheus, com configurações de match[], externas labels únicas por instância, etc. Também é necessário expor o endpoint /federate das instâncias filhas – o que pode ampliar a superfície de ataque ou requerer configurações TLS, autenticação, caso atravesse redes não confiáveis.</li>
</ul>
<p>Em razão desses fatores, a federação deve ser usada <strong>apenas</strong> para casos de uso bem planejados (tipicamente agregações de baixo volume ou métricas específicas). Não é a solução adequada para retenção de longo prazo nem para alta disponibilidade.</p>
<blockquote>
<p><strong>NOTA:</strong> Para necessidades de <strong>escalabilidade horizontal</strong> e <strong>armazenamento de longo prazo</strong>, surgiram outros projetos que complementam o Prometheus, como <strong>Thanos</strong>, <strong>Cortex</strong> e <strong>Mimir</strong> (Grafana Labs). Essas soluções armazenam as séries em storage distribuído (objeto, bigtable, etc.) e permitem “juntar” múltiplas instâncias como se fossem uma só, suportando consultas globais e retenção virtualmente infinita. Exploraremos essas alternativas em outro artigo, mas adianta-se que elas resolvem muitos dos problemas de tentar usar federação pura para esses fins.</p></blockquote>
<h2 id="remote-write-e-remote-read">Remote Write e Remote Read</h2>
<p>O Prometheus pode ser configurado para enviar suas métricas em tempo real para bancos externos (<strong>remote write</strong>) e buscar dados históricos de outros sistemas (<strong>remote read</strong>). Essa funcionalidade é fundamental para integração com soluções de armazenamento de longo prazo, compliance e análise de dados.</p>
<h3 id="remote-write">Remote Write</h3>
<p>O <strong>remote write</strong> permite que o Prometheus envie amostras coletadas para sistemas externos em tempo real, mantendo uma cópia local. Isso é útil para:</p>
<ul>
<li><strong>Retenção de longo prazo</strong>: Enviar dados para sistemas como InfluxDB, TimescaleDB, ou soluções cloud</li>
<li><strong>Compliance e auditoria</strong>: Manter métricas por meses/anos para requisitos regulatórios</li>
<li><strong>Machine Learning</strong>: Integrar com plataformas de ML para análise preditiva</li>
<li><strong>Correlação de dados</strong>: Combinar métricas com logs e traces em sistemas unificados</li>
</ul>
<p><strong>Exemplo de configuração:</strong></p>


  <pre><code class="language-yaml">remote_write:
  - url: &#34;https://longterm.example.com/api/v1/write&#34;
    basic_auth:
      username: &#34;prometheus&#34;
      password: &#34;password&#34;
    write_relabel_configs:
      - source_labels: [__name__]
        regex: &#39;node_.*&#39;
        action: keep
    queue_config:
      max_samples_per_send: 1000
      max_shards: 30
      capacity: 2500</code></pre>
 <p><strong>Configurações importantes:</strong></p>
<ul>
<li><strong><code>url</code></strong>: Endpoint do sistema de destino</li>
<li><strong><code>basic_auth</code></strong>: Autenticação básica (também suporta TLS)</li>
<li><strong><code>write_relabel_configs</code></strong>: Filtros para enviar apenas métricas específicas</li>
<li><strong><code>queue_config</code></strong>: Configurações de buffer e performance</li>
</ul>
<h3 id="remote-read">Remote Read</h3>
<p>O <strong>remote read</strong> permite que o Prometheus busque dados históricos de sistemas externos, como se fossem parte do seu TSDB local. Isso é útil para:</p>
<ul>
<li><strong>Consultas históricas</strong>: Acessar dados antigos sem manter tudo localmente</li>
<li><strong>Migração de dados</strong>: Transição gradual entre sistemas de armazenamento</li>
<li><strong>Análise retrospectiva</strong>: Investigar incidentes passados com dados completos</li>
</ul>
<p><strong>Exemplo de configuração:</strong></p>


  <pre><code class="language-yaml">remote_read:
  - url: &#34;https://longterm.example.com/api/v1/read&#34;
    basic_auth:
      username: &#34;prometheus&#34;
      password: &#34;password&#34;
    read_recent: true
    required_matchers:
      - label: &#34;job&#34;
        value: &#34;node&#34;</code></pre>
 <h3 id="casos-de-uso-típicos">Casos de Uso Típicos</h3>
<p><strong>1. Integração com Grafana Cloud:</strong></p>


  <pre><code class="language-yaml">remote_write:
  - url: &#34;https://prometheus-prod-XX-XXX.grafana.net/api/prom/push&#34;
    basic_auth:
      username: &#34;12345&#34;
      password: &#34;glc_eyJvIjoiOTk5OTkiLCJuIjoiYWRtaW4iLCJpIjoiMTIzNDU2Nzg5MCJ9&#34;</code></pre>
 <p><strong>2. Envio para InfluxDB:</strong></p>


  <pre><code class="language-yaml">remote_write:
  - url: &#34;http://influxdb:8086/api/v2/prom/write?org=myorg&amp;bucket=prometheus&#34;
    basic_auth:
      username: &#34;admin&#34;
      password: &#34;password&#34;</code></pre>
 <p><strong>3. Múltiplos destinos:</strong></p>


  <pre><code class="language-yaml">remote_write:
  - url: &#34;https://backup-storage.example.com/write&#34;
    write_relabel_configs:
      - source_labels: [__name__]
        regex: &#39;.*&#39;
        action: keep
  - url: &#34;https://ml-platform.example.com/metrics&#34;
    write_relabel_configs:
      - source_labels: [__name__]
        regex: &#39;app_.*&#39;
        action: keep</code></pre>
 <blockquote>
<p><strong>Importante</strong>: Remote write/read não substitui o armazenamento local do Prometheus. O TSDB local continua sendo usado para consultas recentes e alertas. O remote write é <strong>aditivo</strong> - você mantém os dados locais e envia uma cópia para sistemas externos.</p></blockquote>
<h2 id="under-the-hood">Under the Hood</h2>
<p>Nesta seção, vamos dissecar o funcionamento interno do armazenamento de dados do Prometheus – o <strong><a href="https://prometheus.io/docs/introduction/architecture/#time-series-database">Time Series Database</a></strong> (TSDB) local – e entender por que ele consome recursos como consome.</p>
<p>Quando instalamos o Prometheus, uma pasta de dados (por padrão chamada <code>data/</code>) é usada para persistir as séries temporais coletadas. Dentro dela, os dados são organizados em blocos de tempo fixo. Por padrão, cada <strong>bloco</strong> cobre 2 horas de métricas. Após duas horas de coleta, o Prometheus fecha aquele bloco e inicia outro.</p>
<p>Periodicamente, vários blocos menores podem ser compactados em blocos maiores (por exemplo, 5 blocos de 2h podem ser mesclados num bloco de 10h de dados, e assim por diante). A estrutura de arquivos típica em <code>data/</code> é assim (exemplo simplificado):</p>


  <pre><code class="language-">data/
├── 01GZY5ABCD.../       # pasta de um bloco de dados
│   ├── meta.json        # metadados do bloco
│   ├── index            # índice para busca das séries no bloco
│   ├── chunks/          # pedaços contendo os samples comprimidos
│   └── tombstones       # (pode estar vazio) marcações de deleção
├── 01GZY1WXYZ.../       # outro bloco (mais antigo, por ex)
│   └── ...
├── chunks_head/         # chunks do bloco &#34;head&#34; atual (em uso)
└── wal/                 # Write-Ahead Log (log de escrita recente)
    ├── 00000000
    ├── 00000001
    └── checkpoint.000001/ ...</code></pre>
 <p>Cada bloco de 2h é identificado por um <strong><a href="https://github.com/prometheus/prometheus/blob/main/tsdb/encoding/ulid.go">ULID</a></strong> (ID único lexicograficamente ordenável) que compõe o nome da pasta. Dentro de um bloco, temos:</p>
<ul>
<li><strong>meta.json:</strong> arquivo JSON com metadados do bloco (faixa de tempo coberta, stats de quantas séries/amostras contém, histórico de compactação, etc.).</li>
<li><strong>index:</strong> arquivo de índice invertido para permitir procurar séries rapidamente pelo nome e labels, e localizar em quais chunks estão seus dados.</li>
<li><strong>chunks/</strong>: diretório contendo os arquivos binários de chunks de dados. Os <em>chunks</em> são os blocos comprimidos de amostras das séries. Cada arquivo (nomeado como 000001, 000002, &hellip;) contém muitos chunks. O tamanho máximo de cada arquivo é ~512MB para facilitar gerenciamento.</li>
<li><strong>tombstones:</strong> arquivo que registra intervalos de dados deletados manualmente (via API de delete), se houver.</li>
</ul>
<p>Além dos blocos fechados, existe o <strong><a href="https://prometheus.io/docs/introduction/architecture/#head-block">Head block</a></strong> (bloco atual em memória) que armazena as métricas em curso. Os dados mais recentes (últimas ~2h) residem em memória para escrita rápida e consultas de curtíssimo prazo.</p>
<p>A cada 2h, o Prometheus “dissolve” parte do Head em um bloco persistente e libera daquela memória. Vamos inspecionar um exemplo de <strong>meta.json</strong> para entender seus campos:</p>


  <pre><code class="language-json">{
    &#34;ulid&#34;: &#34;01BKGTZQ1SYQJTR4PB43C8PD98&#34;,
    &#34;minTime&#34;: 1602237600000,
    &#34;maxTime&#34;: 1602244800000,
    &#34;stats&#34;: {
        &#34;numSamples&#34;: 553673232,
        &#34;numSeries&#34;: 1346066,
        &#34;numChunks&#34;: 4440437
    },
    &#34;compaction&#34;: {
        &#34;level&#34;: 1,
        &#34;sources&#34;: [
            &#34;01EM65SHSX4VARXBBHBF0M0FDS&#34;,
            &#34;01EM6GAJSYWSQQRDY782EA5ZPN&#34;
        ]
    },
    &#34;version&#34;: 1
}</code></pre>
 <p>Explicando os campos principais:</p>
<ul>
<li><strong>ulid:</strong> Identificador único do bloco (um código 128-bit parecido com <a href="https://en.wikipedia.org/wiki/Universally_unique_identifier">UUID</a>). Ele é também o nome da pasta do bloco.</li>
<li><strong>minTime e maxTime:</strong> Timestamp inicial e final (epoch em milissegundos) cobertos pelos samples deste bloco. No exemplo, corresponde a um intervalo de 2h.</li>
<li><strong>stats:</strong> Estatísticas do bloco – quantas amostras (<a href="https://prometheus.io/docs/introduction/architecture/#head-block">numSamples</a>), séries (<a href="https://prometheus.io/docs/introduction/architecture/#head-block">numSeries</a>) e chunks (<a href="https://prometheus.io/docs/introduction/architecture/#head-block">numChunks</a>) estão armazenados nele. No exemplo real acima, temos ~1,34 milhão de séries distintas, totalizando 553 milhões de amostras em ~4,44 milhões de chunks dentro desse bloco de 2h. Esses números dão uma noção do volume de dados.</li>
<li><strong>compaction:</strong> Informa o histórico de compactação. <strong><a href="https://prometheus.io/docs/introduction/architecture/#head-block">level</a></strong> indica quantas vezes já foi compactado (1 significa um bloco resultante da junção de outros menores). <strong><a href="https://prometheus.io/docs/introduction/architecture/#head-block">sources</a></strong> lista os IDs dos blocos que foram combinados para formar este (no caso, dois blocos anteriores). Se o bloco foi gerado direto do Head (dados “originais”), às vezes sources contém ele próprio.</li>
<li><strong>version:</strong> Versão do formato do bloco/arquivo (para compatibilidade futura).</li>
</ul>
<p>Com isso, entendemos que cada bloco é imutável depois de escrito. Se novos dados chegam daquele intervalo, seria criado um bloco novo via compaction. Isso facilita a confiabilidade – dados históricos não mudam.</p>
<p>O <strong>arquivo de índice (index)</strong> serve para mapear as séries e labels aos chunks dentro do bloco. Ele funciona como um índice invertido: dado um nome de métrica e um conjunto de labels, encontra os IDs das séries correspondentes e, então, aponta para os chunks onde estão os dados daquela série.</p>
<p>Assim, ao fazer uma consulta, o Prometheus carrega o índice do bloco relevante e consegue buscar rapidamente somente os chunks necessários (por exemplo, pula chunks inteiros que estão fora do range de tempo consultado, usando informações de minTime/maxtime dos chunks).</p>
<p>O índice é altamente otimizado e comprimido – usa conceitos de <a href="https://prometheus.io/docs/introduction/architecture/#posting-lists">posting lists</a> (listas de IDs de séries para cada label-valor) e <a href="https://prometheus.io/docs/introduction/architecture/#symbol-table">tabelas de símbolos</a> para strings únicas. Esses detalhes avançados fogem do escopo aqui, mas o importante é: o índice permite que mesmo com milhões de séries por bloco, o Prometheus consiga localizar dados sem varrer tudo linearmente.</p>
<p>Finalmente, o <strong><a href="https://prometheus.io/docs/introduction/architecture/#write-ahead-log">WAL (Write-Ahead Log)</a></strong> é um log de transações recente onde cada amostra coletada é gravada imediatamente no disco antes de ser inserida na memória do Head. Isso garante que, se o Prometheus cair inesperadamente, ao voltar ele pode reprocessar o WAL e recuperar as amostras que ainda não tinham sido compactadas em blocos.</p>
<p>O WAL consiste em arquivos sequenciais (<code>00000000</code>, <code>00000001</code>, etc.) que vão acumulando as escritas. Periodicamente, o Prometheus faz um checkpoint (snapshot do head) e limpa parte do WAL já aplicado.</p>
<p>Em caso de crash, ele lê desde o último checkpoint para restaurar o estado do Head.</p>
<h3 id="gerenciamento-de-memória-pelo-prometheus">Gerenciamento de memória pelo Prometheus</h3>
<p>O Prometheus armazena as séries temporais em memória para rápido acesso às métricas recentes, enquanto grava continuamente os novos dados no disco (WAL) para durabilidade. Isso pode levar a alto uso de RAM e espaço em disco.</p>
<p><img src="https://raw.githubusercontent.com/scovl/scovl.github.io/main/post/images/tsdb/prom-mem02.png" alt=""></p>
<p>Como mencionado, o Prometheus mantém em RAM todas as séries ativas do bloco atual (tipicamente últimas 2 horas de dados por série). Essa decisão arquitetural visa desempenho: consultas sobre dados recentes (que são as mais comuns, e.g. alertas e dashboards de curto prazo) não precisam esperar leitura de disco – os valores já estão na memória.</p>
<p>Além disso, novas amostras sendo inseridas a cada segundo/minuto são agregadas a estruturas em memória (evitando I/O de disco a cada operação, que seria inviável em alta escala). O resultado é que o <strong>consumo de RAM</strong> do Prometheus cresce com o número de séries ativas e com a frequência de coleta.</p>
<p>Estima-se, por experiências reportadas, que cada série ativa consome em torno de <strong>~3 KB de RAM</strong> (depende de labels, comprimento do nome, etc.). Portanto, 1 milhão de séries pode usar na ordem de 3–4 GB de RAM apenas para manter o head da TSDB.</p>
<p>Em paralelo, o Prometheus escreve todas as amostras no WAL (em disco) para não perdê-las em caso de crash. A cada 2 horas, ele então compacta esses dados quentes em um bloco de 2h comprimido e libera a memória correspondente. Ou seja, há um ciclo onde a memória vai sendo ocupada pelas amostras recentes, e de hora em hora (na verdade 2h) há um flush para disco que esvazia um pouco a memória (mas novas séries podem surgir e ocupar de novo).</p>
<p>O <em>design</em> de manter dados recentes em memória traz a consequência de que <strong>o uso de RAM aumenta com a carga de métricas e não é liberado até que os blocos sejam fechados ou as séries cessem</strong>. Em períodos de pico (muitas séries novas aparecendo rapidamente), o Prometheus pode chegar a consumir muita memória para acompanhar.</p>
<p>Se faltar RAM, o processo corre risco de OOM (matar por falta de memória) ou, no melhor caso, o sistema operacional vai começar a usar swap – o que degrada muito a performance. Na imagem acima, vemos que tanto a RAM quanto o armazenamento em disco podem crescer substancialmente à medida que aumentamos o volume de dados monitorados.</p>
<blockquote>
<p><strong>Quanto mais dias de retenção mantidos no Prometheus, mais recursos são usados e maior o esforço para consultas longas. Manter dados históricos demais pode sobrecarregar a memória e o disco, além de dificultar encontrar informações recentes relevantes.</strong></p></blockquote>
<p>Embora possamos configurar retenções longas (30, 60 dias), isso não significa que o Prometheus foi otimizado para operar eficientemente com esse histórico todo localmente. Lembre-se: ele não indexa por data de forma distribuída – consultas que abrangem muitos dias terão que ler vários blocos do disco e processar um grande volume de amostras.</p>
<p>Na prática, reter além de algumas semanas começa a tornar as consultas bem lentas e o uso de disco muito alto (sem falar nos backups dessa quantidade de data). Consultas extensas acabam exigindo leitura de múltiplos blocos e processamento de grandes volumes de dados, o que impacta diretamente a performance do sistema.</p>
<p>A imagem acima ilustra que, à medida que guardamos mais dias, o custo de recursos cresce e pode inclusive ofuscar tendências atuais no meio de tanto dado antigo.</p>
<p><img src="https://raw.githubusercontent.com/scovl/scovl.github.io/main/post/images/tsdb/prom-mem03.png" alt=""></p>
<p>A filosofia do Prometheus é ser a ferramenta de <strong>monitoramento em tempo real</strong> e de curto/médio prazo.</p>
<p>Para análises históricas longas ou compliance (guardar métricas por 1 ano, por exemplo), a solução comum é integrar um back-end de longo prazo (Thanos, Cortex, databases remotas) que arquivem esses dados, enquanto o Prometheus local mantém só o necessário para operação/alertas recentes.</p>
<p>Assim você tem o melhor dos dois mundos: rapidez no real-time e histórico completo disponível quando precisar, sem sobrecarregar o Prometheus diariamente.</p>
<blockquote>
<p>Todas as amostras recentes residem na memória principal (Head), com flush periódico para disco a cada 2 horas. O WAL no disco captura as escritas para garantir durabilidade. Em situação de carga extrema, o OS pode usar swap, mas isso deve ser evitado pois degrada o desempenho.</p></blockquote>
<p>Vamos recapitular o ciclo de vida dos dados no Prometheus e seu impacto em memória/disco:</p>
<ul>
<li>
<p><strong>Head Block (memória):</strong> Novas séries e amostras entram aqui. As séries ativas ocupam estruturas na heap da aplicação Go do Prometheus. A cada amostra recebida, ela também é anexada no <strong><a href="https://prometheus.io/docs/introduction/architecture/#write-ahead-log">WAL</a></strong> (no SSD/disco) para registro permanente. Durante até ~2h, os dados ficam disponíveis no Head para consultas instantâneas. Por isso, consultas e alertas em dados &ldquo;frescos&rdquo; são muito rápidas.</p>
</li>
<li>
<p><strong>Flush para bloco persistente:</strong> Quando o intervalo de 2h se completa, o Prometheus corta o bloco (na verdade ele espera 2h ou 1h30 dependendo de certas condições) e escreve um <strong><a href="https://prometheus.io/docs/introduction/architecture/#head-block">novo bloco</a></strong> no diretório data (contendo aqueles 2h de amostras agora imutáveis, já comprimidas). Em seguida, libera da memória boa parte das estruturas referentes àquele intervalo. O head então mantém somente as séries ainda ativas que extrapolem o próximo bloco.</p>
</li>
<li>
<p><strong>Compaction:</strong> Após algumas rotações de bloco, o Prometheus agrupa blocos menores em blocos maiores (por exemplo, une 5 blocos de 2h em 1 bloco de 10h, e assim por diante). Isso ocorre em segundo plano e ajuda a reduzir o número de arquivos e melhorar compressão geral. Compaction consome CPU/disk I/O, mas é intercalado para não interferir muito.</p>
</li>
<li>
<p><strong>Retenção e cleanup:</strong> Quando um bloco excede a retenção configurada (ex: ficou mais velho que 15 dias), ele é marcado para deleção. A limpeza ocorre periodicamente e remove blocos expirados. Importante: a remoção não é imediata ao passar do prazo – o processo de cleanup roda em intervalos (até 2h de delay). Durante a limpeza, o Prometheus deleta os diretórios daqueles blocos antigos, liberando espaço em disco.</p>
</li>
<li>
<p><strong>Reinício e recuperação:</strong> Se o Prometheus reiniciar ou cair, na inicialização ele precisa recarregar o estado. Ele vai abrir todos os blocos persistentes (apenas meta e índice, sem carregar todos os dados) e principalmente processar o WAL para recriar o Head com as amostras que ainda não estavam em bloco. Esse processo de recuperação do WAL pode demorar dependendo do tamanho (por isso há checkpoint para otimizar). Ao final, o sistema retorna ao estado como se nunca tivesse parado (exceto pelos minutos offline onde dados podem ter se perdido se os alvos não suportam retroativa).</p>
</li>
</ul>
<p>Tudo isso explica por que o Prometheus consome <strong>bastante memória</strong>: ele aposta em manter as séries recentes acessíveis e indexadas para respostas rápidas.</p>
<p>Num Prometheus com muitos alvos ou alta cardinalidade (muitas combinações de labels), o consumo de RAM pode facilmente ser o principal limitador. Conforme mencionado anteriormente, 1 milhão de séries ativas pode exigir vários GB de RAM, portanto planeje a capacidade de acordo com o volume de métricas esperado.</p>
<p>Infelizmente, não há muito <strong>tunings</strong> manuais a fazer na memória além de reduzir a quantidade de dados: <strong>menos séries ou menor frequência de coleta</strong> = menos uso de RAM. O Prometheus não tem um mecanismo interno de shard automático ou flush mais frequente (o flush é fixo ~2h por design).</p>
<p>Então, as soluções se resumem a <strong>escalar verticalmente</strong> (máquinas com mais memória, CPU, disco rápido) ou <strong>escalar horizontalmente</strong> (dividir a carga entre vários Prometheus, cada um monitorando uma parte das targets). Nas melhores práticas a seguir, daremos dicas para mitigar esses desafios de desempenho e dimensionamento.</p>
<h3 id="native-histograms-recurso-experimental">Native Histograms (Recurso Experimental)</h3>
<p>O Prometheus introduziu <strong>Native Histograms</strong> como um recurso experimental nas versões mais recentes (2.40+). Essa funcionalidade representa uma evolução significativa na forma como histogramas são armazenados e consultados.</p>
<h4 id="diferenças-dos-histogramas-tradicionais">Diferenças dos Histogramas Tradicionais</h4>
<p><strong>Histogramas tradicionais:</strong></p>
<ul>
<li>Usam buckets predefinidos (ex: 0.1, 0.5, 1.0, 2.5, 5.0, 10.0)</li>
<li>Cada bucket gera uma série separada (<code>_bucket</code>)</li>
<li>Requerem múltiplas séries para representar uma distribuição</li>
<li>Limitados pela granularidade dos buckets</li>
</ul>
<p><strong>Native Histograms:</strong></p>
<ul>
<li>Usam buckets dinâmicos e adaptativos</li>
<li>Armazenam a distribuição completa em uma única série</li>
<li>Permitem maior precisão nos percentis</li>
<li>Reduzem significativamente o número de séries</li>
</ul>
<h4 id="configuração-1">Configuração</h4>
<p>Para habilitar native histograms, adicione a flag experimental:</p>


  <pre><code class="language-bash">prometheus --enable-feature=native-histograms</code></pre>
 <p>Ou no Docker:</p>


  <pre><code class="language-yaml">command:
  - &#39;--enable-feature=native-histograms&#39;</code></pre>
 <h4 id="exemplo-de-uso">Exemplo de Uso</h4>
<p><strong>Instrumentação com native histograms (Go):</strong></p>


  <pre><code class="language-go">import (
    &#34;github.com/prometheus/client_golang/prometheus&#34;
    &#34;github.com/prometheus/client_golang/prometheus/promauto&#34;
)

var (
    requestDuration = promauto.NewHistogram(prometheus.HistogramOpts{
        Name: &#34;http_request_duration_seconds&#34;,
        Help: &#34;Duration of HTTP requests&#34;,
        NativeHistogramBucketFactor: 1.1,  // Fator de crescimento dos buckets
        NativeHistogramMaxBucketNumber: 100, // Máximo de buckets
    })
)</code></pre>
 <p><strong>Consulta de percentis:</strong></p>


  <pre><code class="language-promql"># Percentil 95 usando native histogram
histogram_quantile(0.95, rate(http_request_duration_seconds[5m]))

# Percentil 99
histogram_quantile(0.99, rate(http_request_duration_seconds[5m]))</code></pre>
 <h4 id="vantagens">Vantagens</h4>
<ul>
<li><strong>Menos séries</strong>: Uma métrica de latência que antes gerava 10+ séries agora gera apenas 1</li>
<li><strong>Maior precisão</strong>: Buckets adaptativos capturam melhor a distribuição real</li>
<li><strong>Melhor performance</strong>: Menos overhead de armazenamento e consulta</li>
<li><strong>Compatibilidade</strong>: Funciona com todas as funções PromQL existentes</li>
</ul>
<h4 id="considerações">Considerações</h4>
<ul>
<li><strong>Experimental</strong>: Ainda em desenvolvimento, pode ter mudanças na API</li>
<li><strong>Migração</strong>: Requer atualização das bibliotecas cliente</li>
<li><strong>Compatibilidade</strong>: Funciona apenas com versões recentes do Prometheus</li>
</ul>
<blockquote>
<p><strong>Nota</strong>: Native histograms são especialmente úteis para métricas de latência em aplicações de alta performance, onde a precisão dos percentis é crítica.</p></blockquote>
<h2 id="melhores-práticas">Melhores Práticas</h2>
<p>Depois de entender a mecânica interna do Prometheus, é válido reunir algumas recomendações para tirar o melhor proveito da ferramenta de forma escalável e confiável.</p>
<h3 id="planejamento-de-capacidade">Planejamento de Capacidade</h3>
<ul>
<li>
<p><strong>Estime volume de métricas e retenção:</strong> Antes de implantar, faça uma estimativa do número de séries que você vai coletar e defina uma retenção condizente. Lembre que por padrão são 15 dias. Se não precisar de tudo isso para monitoramento diário, retenções menores aliviam recursos. Ao contrário, se precisar de mais tempo histórico, esteja ciente do aumento de disco e possivelmente avalie armazenamento remoto.</p>
</li>
<li>
<p><strong>Monitore o Prometheus em si:</strong> &ldquo;Quis custodiet ipsos custodes?&rdquo; – o Prometheus expõe suas próprias métricas (no endpoint /metrics dele). Use um outro Prometheus ou a mesma instância para monitorar métricas como <code>prometheus_tsdb_head_series</code> (número de séries no head), <code>prometheus_tsdb_head_samples_appended_total</code> (samples inseridos por segundo), <code>prometheus_engine_query_duration_seconds</code> (latência das consultas), etc. Isso alerta para crescimento de cardinalidade inesperado ou consultas muito pesadas rodando.</p>
</li>
<li>
<p><strong>Dimensione hardware adequadamente:</strong> Regra empírica: 1 CPU core pode processar aproximadamente até 200k amostras por segundo (varia, mas é uma ideia). Memória, calcule ~3kB por série ativa. Disco: ~1-2 bytes por amostra armazenada comprimida (15 dias, 200 milhões de amostras ~ 200-300MB). Use SSDs rápidos – operações de WAL e blocos beneficiam de I/O rápido.</p>
</li>
</ul>
<h3 id="organização-de-métricas-e-labels">Organização de Métricas e Labels</h3>
<ul>
<li>
<p><strong>Consistência na nomeação:</strong> Siga convenções de nomenclatura para facilitar a vida. Use nomes descritivos e padronizados (letras minúsculas, separadas por underscores, unidade no sufixo se aplicável: <code>_seconds</code>, <code>_bytes</code>, <code>_total</code> para contadores acumulativos). Por exemplo, prefira <code>app_memory_usage_bytes</code> a algo como <code>MemUsed</code> ou outras variações inconsistentes. Isso ajuda todo mundo a entender do que se trata sem ambiguidade.</p>
</li>
<li>
<p><strong>Labels estratégicos:</strong> Anexe labels que façam sentido de consulta, mas evite rotular com informações que tenham alta cardinalidade ou unicidade. Um bom label é algo como <code>region</code>, <code>datacenter</code>, <code>instance</code> (desde que este não seja único por métrica – use instance só onde faz sentido). Maus labels incluem: ID de requisição, nome de usuário, URL completa (em vez de caminho genérico), timestamp, IP dinâmico de cliente. Esses valores criam um número enorme de séries distintas. Lembre-se: cada combinação diferente de labels vira <strong>uma série separada</strong> no TSDB. Se você tiver 1000 usuários e rotular métricas por usuário, virou 1000 séries onde antes podia ser 1 ou algumas. Leve isso em conta.</p>
</li>
<li>
<p><strong>Explosão de cardinalidade:</strong> É um dos problemas mais comuns. Por exemplo, adicionar um label <code>product_id</code> a uma métrica de pedidos, onde product_id pode assumir dezenas de milhares de valores, multiplicará as séries. Isso pode levar o Prometheus a consumir toda memória e travar. Portanto, só use labels cujo conjunto de valores possível seja <strong>limitado e relativamente pequeno</strong>. (Regra de bolso: algumas dezenas ou poucas centenas de valores diferentes por label no máximo. Mais que isso, pense duas vezes se é necessário.) Caso precise monitorar algo muito cardinal (ex: métricas por usuário único), talvez o Prometheus não seja a ferramenta adequada ou você precisa agregá-las antes de expor.</p>
</li>
</ul>
<h4 id="o-inimigo-nº-1-explosão-de-cardinalidade">O Inimigo nº 1: Explosão de Cardinalidade</h4>
<p><strong>A cardinalidade é o maior desafio do Prometheus.</strong> Cada combinação única de labels cria uma série temporal separada no TSDB. Quando você adiciona labels com valores altamente variáveis (como IDs de usuário, timestamps, URLs completas, ou IPs dinâmicos), você está multiplicando exponencialmente o número de séries armazenadas.</p>
<p><strong>Por que é tão perigoso:</strong></p>
<ul>
<li><strong>Consumo de memória:</strong> Cada série ativa consome ~3kB de RAM. Milhares de séries = gigabytes de memória</li>
<li><strong>Performance de consultas:</strong> Mais séries = consultas mais lentas e maior uso de CPU</li>
<li><strong>Instabilidade:</strong> Cardinalidade excessiva pode fazer o Prometheus travar ou reiniciar constantemente</li>
<li><strong>Custos de armazenamento:</strong> Mais séries = mais dados para armazenar e processar</li>
</ul>
<p><strong>Exemplos de labels perigosos:</strong></p>
<ul>
<li><code>user_id</code> (pode ter milhões de valores únicos)</li>
<li><code>request_id</code> (único por requisição)</li>
<li><code>timestamp</code> (muda a cada scrape)</li>
<li><code>ip_address</code> (muito variável)</li>
<li><code>full_url</code> (em vez de usar <code>endpoint</code> ou <code>path</code>)</li>
</ul>
<p><strong>Soluções práticas:</strong></p>
<ul>
<li><strong>Agregação prévia:</strong> Agregue métricas antes de expô-las ao Prometheus</li>
<li><strong>Labels limitados:</strong> Use apenas labels com valores limitados e previsíveis</li>
<li><strong>Métricas de resumo:</strong> Em vez de métricas por item individual, use métricas de contagem/total</li>
<li><strong>Filtros inteligentes:</strong> Use relabeling para remover labels problemáticos</li>
<li><strong>Monitoramento ativo:</strong> Monitore <code>prometheus_tsdb_head_series</code> para detectar crescimento anormal</li>
</ul>
<p><strong>Regra de ouro:</strong> Se você não consegue prever quantos valores diferentes um label pode ter, provavelmente não deveria usá-lo no Prometheus.</p>
<ul>
<li><strong>Métricas altas vs baixas cardinalidades:</strong> Prefira métricas mais agregadas. Por exemplo, em vez de registrar uma métrica separada para cada item em fila (que não faz sentido), registre o tamanho da fila como um gauge. Em vez de métricas por sessão de usuário, exponha total global ou por categoria de usuário. Enfim, modele os dados de forma a minimizar detalhes desnecessários.</li>
</ul>
<h3 id="consultas-promql-eficientes">Consultas (PromQL) Eficientes</h3>
<ul>
<li>
<p><strong>Cuidado com funções custosas:</strong> Algumas funções PromQL podem ser muito úteis, porém custosas. <code>topk()</code> e <code>bottomk()</code>, por exemplo, obrigam o engine a ordenar muitas séries para achar o top N – pode ser caro se aplicado numa métrica com milhares de séries. Use-as com moderação (talvez em queries de background para dashboard, mas evite em alertas críticos se possível). Similar para agregações sem restrição: <code>sum by (label)</code> onde label tem muitos valores, o Prometheus terá que materializar todas combinações.</p>
</li>
<li>
<p><strong>Use intervalos de tempo adequados:</strong> Querys do tipo <em>[5m]</em>, <em>[1h]</em> etc. definem quanto tempo de dados vão considerar. Evite pedir mais do que precisa. Por exemplo, se um alerta precisa saber a taxa nos últimos 5 minutos, não use 1h. Intervalos maiores = mais dados lidos e processados. Num gráfico, também não exagere no zoom out se não for necessário – muitos dados tornam a renderização e transmissão pesadas.</p>
</li>
<li>
<p><strong>Prefira <code>rate()</code> ou <code>increase()</code> para contadores ao invés de <code>irate()</code> para alertas contínuos:</strong> A função <code>irate()</code> calcula instantaneamente a derivada entre os dois últimos pontos – isso é útil às vezes, mas tende a ser muito &ldquo;barulhento&rdquo; (variação instante a instante). Em dashboards e alertas gerais, <code>rate()</code> numa janela de pelo menos 1m ou 5m é mais estável e representativo da taxa média. Use <code>irate</code> somente quando quer realmente capturar spikes momentâneos e tem alta frequência de scrape.</p>
</li>
<li>
<p><strong>Agregue no scraping quando possível:</strong> Se você já sabe que nunca vai olhar cada instância individual de certa métrica, poderia agregá-la antes mesmo de enviar. Exemplo: se você tem 10 threads fazendo trabalho idêntico e só quer saber o total combinado, exponha uma única métrica total e não 10 separadas. Claro que isso depende do caso de uso – muitas vezes queremos o detalhe – mas é algo a pensar.</p>
</li>
<li>
<p><strong>Limite consultas no UI:</strong> O Prometheus permite rodar qualquer PromQL ad-hoc no UI ou via API. Em ambientes compartilhados, controle o acesso ou conscientize os usuários para não rodarem consultas insanas (tipo um sum sem nenhum label em milhões de séries por 365d) que possam afetar a performance. Você pode habilitar autenticação/TLS e até colocar um proxy com quotas se for necessário proteger a API de uso indevido.</p>
</li>
</ul>
<h3 id="arquitetura-e-escalabilidade">Arquitetura e Escalabilidade</h3>
<ul>
<li>
<p><strong>Sharding (divisão de carga):</strong> Se chegar ao ponto de um único Prometheus não dar conta (seja por limite de CPU/RAM ou por questões organizacionais), considere dividir os alvos entre múltiplas instâncias. Por exemplo, rodar um Prometheus por cluster Kubernetes, ou por ambiente (dev/prod), ou por região geográfica. Cada um monitora só seu âmbito. Você pode replicar as regras de alertas em todos (assim cada local alerta independentemente). Para métricas globais, use federação ou uma camada agregadora (como Thanos) para unificar se necessário.</p>
</li>
<li>
<p><strong>Alta disponibilidade:</strong> O Prometheus em si não é HA – ele é stand-alone. Se cair, fica um buraco de coleta enquanto estiver fora. Uma prática comum em produção é rodar <strong>dois Prometheus em paralelo coletando os mesmos alvos</strong> (nas mesmas configurações) – assim, se um falhar, o outro continua e nenhuma métrica se perde. O Alertmanager pode receber alertas duplicados de ambos, mas ele deduplica automaticamente (precisa configurar ambos Prometheus com o mesmo external_label cluster). Essa abordagem gasta mais recursos (coleta em dobro), mas é simples e efetiva para HA de alertas.</p>
</li>
<li>
<p><strong>Longo prazo e agregação global:</strong> Conforme citado, se precisar <em>escalar horizontalmente</em> de verdade ou guardar métricas por longos períodos, vale integrar soluções como <strong>Thanos, Cortex ou Grafana Mimir</strong>. Essas ferramentas armazenam dados em base de dados distribuída (por exemplo, S3 ou BigTable no caso do Thanos/Cortex) e permitem rodar consultas PromQL que abrangem múltiplos Prometheus &ldquo;como se fosse um só&rdquo;.</p>
</li>
</ul>
<blockquote>
<p>O Thanos, por exemplo, atua como um <em>sidecar</em> pegando os dados de cada Prometheus e enviando para o objeto storage, depois uma camada de <em>querier</em> unifica as consultas. O Grafana Mimir segue arquitetura semelhante, nascida da experiência do Cortex, permitindo <strong>escala praticamente ilimitada (bilhões de séries) e alta disponibilidade</strong>, com compatibilidade total com PromQL e remote write. Claro, adicionam complexidade – mas são soluções maduras mantidas pela CNCF/Grafana Labs.</p></blockquote>
<ul>
<li><strong>Federação bem aplicada:</strong> Caso use federação, siga a orientação de federar apenas métricas já agregadas e necessárias globalmente. Por exemplo, federar só métricas começando com <code>job:</code> (indicando que são resultados de recording rules já agregadas). Não federar todas as métricas crus. E realize alertas localmente, deixando o global só para visualização.</li>
</ul>
<h3 id="segurança">Segurança</h3>
<ul>
<li>
<p><strong>Não exponha sem proteção em redes inseguras:</strong> O Prometheus, por padrão, não tem autenticação nem TLS habilitados. Se você for disponibilizar a interface ou API em rede pública ou multi-tenant, coloque-o atrás de um proxy reverso que implemente TLS e autenticação (básica, OAuth, o que for). Alternativamente, rode em rede interna/VPN somente. Há flags experimentais para TLS direto e auth no Prometheus, mas a abordagem recomendada ainda é usar um proxy (por exemplo, Nginx, Traefik, etc).</p>
</li>
<li>
<p><strong>Controle acesso à API:</strong> Considere habilitar autorização se for um ambiente com vários usuários ou multi-time. Infelizmente, o Prometheus não suporta múltiplos níveis de usuário nativamente. A solução costuma ser segregar instâncias ou novamente um proxy que filtre rotas. Por exemplo, impedir acesso direto ao <code>/api/v1/admin</code> (que possui comandos de deleção de dados).</p>
</li>
<li>
<p><strong>Atualizações e patches:</strong> Mantenha o Prometheus atualizado – a cada versão há otimizações e correções, inclusive de segurança. E.g., compressão de WAL veio ativada por padrão na 2.20, reduzindo disco pela metade. Versões mais novas introduziram <em>native histograms</em> (experimental) e melhorias de desempenho. Então acompanhe o changelog oficial e planeje upgrade regularmente (Prometheus é bem compatível retroativamente em dados e configs, upgrades diretos costumam ser tranquilos).</p>
</li>
<li>
<p><strong>Isolamento de rede para exporters:</strong> Exporters muitas vezes expõem métricas sensíveis (por exemplo, o Node Exporter expõe informações de hardware, usuários logados etc.). É boa prática deixar esses endpoints acessíveis só pelo Prometheus, não abertos ao mundo. Use firewalls/regras de segurança nos hosts ou config de container network para limitar.</p>
</li>
<li>
<p><strong>Naming anti-collision:</strong> Se você usa rótulos <em>externos</em> (external_labels) para identificar instâncias em um contexto federado ou HA, garanta que cada Prometheus tenha um label único (e.g., <code>cluster=&quot;eu-west-1&quot;</code>). Isso evita confusão de métricas vindas de origens diferentes no caso de junção (Thanos, federação) e ajuda a filtrar.</p>
</li>
</ul>
<h3 id="backup-recovery-e-upgrade">Backup, Recovery e Upgrade</h3>
<p>Em ambientes de produção, é fundamental ter estratégias robustas para backup, recuperação de falhas e upgrades do Prometheus. Esses aspectos são frequentemente negligenciados, mas são críticos para manter a continuidade do monitoramento.</p>
<h4 id="backup-de-dados">Backup de Dados</h4>
<p>O Prometheus armazena dados no diretório <code>data/</code> que contém os blocos de séries temporais. Para fazer backup consistente:</p>
<p><strong>Backup a quente (recomendado):</strong></p>


  <pre><code class="language-bash"># Parar o Prometheus para garantir consistência
sudo systemctl stop prometheus

# Fazer backup do diretório data
tar -czf prometheus-backup-$(date &#43;%Y%m%d).tar.gz /opt/prometheus/data/

# Reiniciar o Prometheus
sudo systemctl start prometheus</code></pre>
 <p><strong>Backup a frio (alternativa):</strong></p>


  <pre><code class="language-bash"># Usar promtool para verificar integridade antes do backup
promtool tsdb check /opt/prometheus/data/

# Fazer backup apenas dos blocos fechados (mais seguro)
find /opt/prometheus/data/ -name &#34;*.json&#34; -exec tar -czf prometheus-blocks-$(date &#43;%Y%m%d).tar.gz {} \;</code></pre>
 <p><strong>Backup de configuração:</strong></p>


  <pre><code class="language-bash"># Backup dos arquivos de configuração
cp /etc/prometheus/prometheus.yml /backup/prometheus.yml.$(date &#43;%Y%m%d)
cp /etc/prometheus/alert.rules.yml /backup/alert.rules.yml.$(date &#43;%Y%m%d)</code></pre>
 <h4 id="recuperação-de-falhas">Recuperação de Falhas</h4>
<p><strong>Restauração de dados:</strong></p>


  <pre><code class="language-bash"># Parar o Prometheus
sudo systemctl stop prometheus

# Restaurar backup
tar -xzf prometheus-backup-20231201.tar.gz -C /

# Verificar integridade dos dados
promtool tsdb check /opt/prometheus/data/

# Reiniciar
sudo systemctl start prometheus</code></pre>
 <p><strong>Recuperação de WAL corrompido:</strong></p>


  <pre><code class="language-bash"># Se o WAL estiver corrompido, pode ser necessário recriar
rm -rf /opt/prometheus/data/wal/
rm -rf /opt/prometheus/data/chunks_head/

# Reiniciar - o Prometheus recriará o WAL
sudo systemctl start prometheus</code></pre>
 <h4 id="estratégias-de-upgrade">Estratégias de Upgrade</h4>
<p><strong>Upgrade direto (mais comum):</strong></p>


  <pre><code class="language-bash"># Fazer backup antes do upgrade
sudo systemctl stop prometheus
tar -czf prometheus-backup-pre-upgrade.tar.gz /opt/prometheus/data/

# Baixar nova versão
wget https://github.com/prometheus/prometheus/releases/download/v2.45.0/prometheus-2.45.0.linux-amd64.tar.gz
tar -xzf prometheus-2.45.0.linux-amd64.tar.gz

# Substituir binário
cp prometheus-2.45.0.linux-amd64/prometheus /opt/prometheus/
cp prometheus-2.45.0.linux-amd64/promtool /opt/prometheus/

# Verificar configuração
/opt/prometheus/promtool check config /etc/prometheus/prometheus.yml

# Reiniciar
sudo systemctl start prometheus</code></pre>
 <p><strong>Upgrade com rollback:</strong></p>


  <pre><code class="language-bash"># Manter versão anterior
cp /opt/prometheus/prometheus /opt/prometheus/prometheus.backup

# Fazer upgrade
# ... (mesmo processo acima)

# Se houver problemas, rollback
sudo systemctl stop prometheus
cp /opt/prometheus/prometheus.backup /opt/prometheus/prometheus
sudo systemctl start prometheus</code></pre>
 <h4 id="considerações-importantes">Considerações Importantes</h4>
<p><strong>Compatibilidade de dados:</strong></p>
<ul>
<li>O Prometheus mantém compatibilidade retroativa de dados entre versões menores</li>
<li>Upgrades major (ex: 2.x para 3.x) podem requerer migração de dados</li>
<li>Sempre verifique o changelog oficial antes de upgrades</li>
</ul>
<p><strong>Tempo de recuperação:</strong></p>
<ul>
<li>O Prometheus pode demorar para processar o WAL após reinicialização</li>
<li>Em ambientes com muitas séries, a recuperação pode levar minutos</li>
<li>Monitore <code>prometheus_tsdb_wal_replay_duration_seconds</code> durante recuperação</li>
</ul>
<p><strong>Backup automatizado:</strong></p>


  <pre><code class="language-bash">#!/bin/bash
# Script de backup automatizado
DATE=$(date &#43;%Y%m%d_%H%M%S)
BACKUP_DIR=&#34;/backup/prometheus&#34;

# Criar backup
sudo systemctl stop prometheus
tar -czf $BACKUP_DIR/prometheus-$DATE.tar.gz /opt/prometheus/data/
sudo systemctl start prometheus

# Manter apenas últimos 7 backups
find $BACKUP_DIR -name &#34;prometheus-*.tar.gz&#34; -mtime &#43;7 -delete</code></pre>
 <p><strong>Monitoramento de integridade:</strong></p>


  <pre><code class="language-yaml"># Alertas para problemas de backup/recuperação
groups:
- name: prometheus_backup
  rules:
    - alert: PrometheusBackupFailed
      expr: time() - prometheus_build_info &gt; 86400  # Mais de 1 dia sem restart
      for: 1h
      labels:
        severity: warning
      annotations:
        summary: &#34;Prometheus não foi reiniciado recentemente (possível problema de backup)&#34;</code></pre>
 <blockquote>
<p><strong>Importante</strong>: Sempre teste backups e procedimentos de recuperação em ambiente de desenvolvimento antes de aplicar em produção. A integridade dos dados de monitoramento é tão crítica quanto os dados da aplicação.</p></blockquote>
<p>Seguindo essas práticas, você deverá manter seu ambiente Prometheus funcionando de forma mais suave, evitando as armadilhas comuns de desempenho e garantindo que as métricas coletadas realmente agreguem valor (e alertas disparem quando devem, sem falso positivos ou negativos).</p>
<h2 id="operação-e-manutenção">Operação e Manutenção</h2>
<h3 id="promtool">Promtool</h3>
<p>O <strong>promtool</strong> é uma ferramenta de linha de comando que acompanha o Prometheus, fornecendo utilitários para verificar configurações e depurar dados. Algumas utilizações comuns do promtool:</p>
<ul>
<li><strong>Checar sintaxe de configuração:</strong> Antes de subir uma alteração no <code>prometheus.yml</code>, rode <code>promtool check config prometheus.yml</code>. Ele apontará erros de sintaxe ou campos desconhecidos, ajudando a evitar falhas no start do servidor.</li>
<li><strong>Validar regras de alerta ou gravação:</strong> Se você definiu arquivos externos de regras (YAML de alertas ou recording rules), use <code>promtool check rules minhas_regras.yml</code>. Ele analisará as expressões PromQL e a formatação.</li>
<li><strong>Testar expressão de alerta:</strong> O promtool permite avaliar manualmente expressões em um dado instantâneo ou série de tempo para ver se disparariam alerta. Útil em CI ou para garantir que a lógica está correta.</li>
<li><strong>Checar integridade do TSDB:</strong> Com o comando <code>promtool tsdb check /path/para/dados</code> é possível inspecionar o banco local de séries temporais em busca de inconsistências ou corrupção.</li>
<li><strong>Converter formatos de dados de métrica:</strong> Há como transformar arquivos de métricas entre formatos (por exemplo, de texto para JSON e vice-versa) usando <code>promtool convert metrics --from=txt --to=json arquivo.txt</code>.</li>
</ul>
<p>Essas são apenas algumas funções. Em suma, o promtool é seu amigo para garantir que o ambiente Prometheus está consistente e saudável – use-o sempre que fizer mudanças significativas na configuração.</p>
<h2 id="conclusão">Conclusão</h2>
<p>Neste artigo, exploramos em detalhes o Prometheus – desde conceitos fundamentais até seu funcionamento interno e implicações práticas de operação. Vimos como ele implementa um banco de dados de séries temporais altamente eficiente, mantendo dados recentes em memória para rapidez e usando compressão e segmentação em blocos para histórico em disco.</p>
<p>Também analisamos aspectos como modelo de coleta pull, linguagem de consulta poderosa, uso intensivo de recursos proporcionais ao volume de métricas, e formas de contornar limitações (sejam arquiteturais ou de escala) com boas práticas e ferramentas auxiliares.</p>
<p>Esses pontos mostram como o Prometheus alia eficiência técnica a flexibilidade operacional, permitindo que equipes monitorem ambientes complexos e em constante evolução, ao mesmo tempo em que enfrentam desafios de escala e desempenho com soluções práticas e acessíveis.</p>
<p>O Prometheus se destaca no ecossistema de monitoramento por sua simplicidade de implantação e por ter sido projetado desde o início para ambientes de microsserviços e infraestrutura dinâmica. Seu modelo multidimensional de métricas com labels e o PromQL possibilitam análises ricas e alertas robustos com relativamente pouco esforço de configuração.</p>
<p>É notável como em poucos anos ele se tornou um dos pilares da observabilidade moderna, ao lado de ferramentas complementares para logs (ELK stack) e <em>tracing</em> (Jaeger, etc.).</p>
<p>Por outro lado, entendemos que o Prometheus não resolve tudo sozinho: retenção de longo prazo, alta disponibilidade nativa e escalabilidade horizontal são pontos fora do escopo do core do Prometheus.</p>
<p>Em vez de tentar ser distribuído, o projeto optou por interfaces (remote write/read) e pela filosofia de componibilidade – cabendo a outras peças (como Thanos ou Mimir) suprir essas demandas quando necessárias.</p>
<p>Essa decisão de design mantém o Prometheus &ldquo;enxuto&rdquo; e confiável, mas significa que para crescer além de certo limite, precisamos arquitetar bem a solução de monitoramento abrangendo outros componentes.</p>
<p>Recapitulando alguns aprendizados chave:</p>
<ul>
<li>Organize bem suas métricas e labels para evitar sobrecarga de cardinalidade.</li>
<li>Monitore o próprio Prometheus e ajuste a capacidade conforme crescimento.</li>
<li>Use Alertmanager e outras integrações para ter um uso completo (coleta, armazenamento, alerta, visualização).</li>
<li>Em caso de grandes escalas, parta para sharding ou ferramentas de escala distribuída – não force um Prometheus único a fazer trabalho demais.</li>
<li>Leve em conta segurança e isolamento, pois monitoramento também lida com informações sensíveis do ambiente.</li>
</ul>
<p>Esperamos que este guia tenha fornecido insights valiosos, tanto para iniciantes entenderem os conceitos do Prometheus quanto para usuários experientes refinarem sua utilização. Compreender o &ldquo;under the hood&rdquo; do Prometheus ajuda a antecipar comportamentos, otimizar configurações e evitar armadilhas comuns na operação diária.</p>
<p>O Prometheus continua em rápida evolução (com melhorias na TSDB, novos recursos como Exemplos Exemplares e Native Histograms em teste, etc.), e o ecossistema ao seu redor também. Fique atento a atualizações e boas práticas emergentes – a comunidade CNCF e blogs como o <em>Robust Perception</em> regularmente publicam conteúdos de alto nível a respeito.</p>
<p>No mais, boas métricas e bons alertas!</p>
<hr>
<h2 id="referências">Referências</h2>
<ul>
<li><strong>Documentação Oficial do Prometheus</strong> – especialmente a <a href="https://prometheus.io/docs/introduction/overview/">Overview</a> , <a href="https://prometheus.io/docs/concepts/metric_types/">Metric Types</a> , <a href="https://prometheus.io/docs/practices/naming/">Best Practices</a> e seção de <a href="https://prometheus.io/docs/prometheus/latest/storage/">Storage</a> .</li>
<li><strong>Blog Robust Perception (Brian Brazil)</strong> – várias postagens aprofundadas, por exemplo: <a href="https://www.robustperception.io/federation-what-is-it-good-for/">&ldquo;Federation, what is it good for?&rdquo;</a> , <a href="https://www.robustperception.io/how-much-ram-does-prometheus-2-x-need-for-cardinality-and-ingestion/">&ldquo;How much RAM does Prometheus 2.x need&hellip;&rdquo;</a> , <a href="https://www.robustperception.io/using-json-file-service-discovery-with-prometheus">&ldquo;Using JSON file service discovery&rdquo;</a> .</li>
<li><strong>Ganesh Vernekar – Série de artigos &ldquo;Prometheus TSDB&rdquo;</strong> – <em>Parts 1-7</em> no blog do Ganesh (engenheiro Grafana Labs) detalhando a fundo a arquitetura do TSDB. Em especial, <a href="https://ganeshvernekar.com/blog/prometheus-tsdb-persistent-block-and-its-index/">Parte 4: Blocos persistentes e Índice</a> .</li>
<li><strong>Livro &ldquo;Prometheus Up &amp; Running&rdquo; (O&rsquo;Reilly, 2019)</strong> – de Brian Brazil, ótima introdução abrangendo do básico a casos avançados.</li>
<li><strong>Livro &ldquo;The Prometheus Book&rdquo; de James Turnbull</strong> – guia prático cobrindo instalação, instrumentação e alertas (disponível online).</li>
<li><strong>Hands-On Infrastructure Monitoring with Prometheus</strong> (Packt) – livro focado em exemplos práticos de uso do Prometheus em cenários reais.</li>
<li><strong>Monitoring Microservices and Containerized Applications</strong> (Apress) – aborda Prometheus em contexto de microsserviços/Kubernetes.</li>
<li><strong>Comparativos Prometheus vs. outras ferramentas:</strong> Artigos como <em>&ldquo;Prometheus vs. ELK&rdquo;</em>, <em>&ldquo;Prometheus vs. Grafana Mimir (Cortex)&rdquo;</em>, e posts do blog da BetterStack sobre melhores práticas.</li>
<li><strong>Grafana Mimir</strong> – <a href="https://grafana.com/oss/mimir/">Página oficial</a>  e anúncio do lançamento em 2022, mostrando como escalar Prometheus para 1 bilhão de séries.</li>
<li><strong>Datadog e New Relic</strong> – documentações e sites oficiais para entender ofertas de monitoramento proprietárias integradas (APM, Logs, etc.), útil para ver diferenças de escopo.</li>
<li><strong>Nagios/Core e Zabbix</strong> – documentação e comunidade, para contexto histórico de monitoramento (foco em disponibilidade, sem TSDB nativo).</li>
<li><strong>ELK Stack</strong> – docs Elastic e blogs de terceiros comparando com Prometheus (focando que ELK é logs e Prometheus métricas).</li>
<li><strong>CNCF Observability Landscape</strong> – projetos e ferramentas relacionadas, para quem quiser explorar além (OpenTelemetry, Fluentd, etc.).</li>
</ul>
]]></content:encoded>
      
      
      <category>Prometheus,Grafana,Monitoring,TSDB,DevOps,Observability,PromQL</category>
      
      
      
      
      
      
      
      <description>&lt;![CDATA[Guia completo]]></description>
      
    </item>
    
    <item>
      <title>Técnicas Avançadas para RAG em Produção</title>
      <link>https://scovl.github.io/2025/03/28/rag02/</link>
      <guid>https://scovl.github.io/2025/03/28/rag02/</guid>
      <pubDate>Fri, 28 Mar 2025 12:00:00 &#43;0000</pubDate>
      <description>&lt;![CDATA[<h2 id="introdução">Introdução</h2>
<p>Olá pessoal! 👋</p>
<p>Nos artigos anteriores, exploramos como <a href="/2025/03/23/rag/">implementar um RAG básico em Clojure</a> em memória e como <a href="/2025/03/25/semantic-postgresql/">construir um sistema de busca semântica com PostgreSQL e Ollama</a>. Agora, vamos dar o próximo passo: transformar nosso protótipo em um sistema RAG pronto para produção.</p>
<p>Como muitos desenvolvedores já descobriram, criar um protótipo funcional de <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> com alguns documentos é relativamente simples. O verdadeiro desafio começa quando precisamos escalar esse sistema para lidar com milhares de documentos, garantir respostas precisas e manter o desempenho sob carga. Neste artigo, vamos explorar técnicas avançadas para superar esses desafios e levar nosso <a href="https://github.com/scovl/docai">DocAI</a> para um novo patamar de qualidade e confiabilidade.</p>]]></description>
      <content:encoded>&lt;![CDATA[<h2 id="introdução">Introdução</h2>
<p>Olá pessoal! 👋</p>
<p>Nos artigos anteriores, exploramos como <a href="/2025/03/23/rag/">implementar um RAG básico em Clojure</a> em memória e como <a href="/2025/03/25/semantic-postgresql/">construir um sistema de busca semântica com PostgreSQL e Ollama</a>. Agora, vamos dar o próximo passo: transformar nosso protótipo em um sistema RAG pronto para produção.</p>
<p>Como muitos desenvolvedores já descobriram, criar um protótipo funcional de <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> com alguns documentos é relativamente simples. O verdadeiro desafio começa quando precisamos escalar esse sistema para lidar com milhares de documentos, garantir respostas precisas e manter o desempenho sob carga. Neste artigo, vamos explorar técnicas avançadas para superar esses desafios e levar nosso <a href="https://github.com/scovl/docai">DocAI</a> para um novo patamar de qualidade e confiabilidade.</p>
<h2 id="da-teoria-à-produção-os-desafios-reais">Da Teoria à Produção: Os Desafios Reais</h2>
<blockquote>
<p>&ldquo;No papel, implementar um sistema <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> parece simples—conectar um banco de dados vetorial, processar documentos, incorporar os dados, incorporar a consulta, consultar o <a href="https://en.wikipedia.org/wiki/Vector_database">banco de dados vetorial</a> e gerar a resposta com o <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a>. Mas na prática, transformar um protótipo em uma aplicação de alto desempenho é um desafio completamente diferente.&rdquo;</p></blockquote>
<p>Ao migrarmos do <a href="/2025/03/23/rag/">TF-IDF em memória</a> para <a href="/2025/03/25/semantic-postgresql/">PostgreSQL/pgvector/pgai</a>, demos um grande salto de qualidade. Porém, à medida que o volume de dados cresce e os casos de uso se tornam mais complexos, novos desafios surgem:</p>
<ul>
<li><strong>Escalabilidade</strong>: Como lidar com milhões de documentos sem degradar o desempenho?</li>
<li><strong>Precisão</strong>: Como garantir que estamos recuperando o contexto mais relevante para cada consulta?</li>
<li><strong>Eficiência</strong>: Como reduzir latência e custos de processamento?</li>
<li><strong>Confiabilidade</strong>: Como evitar alucinações e respostas incorretas?</li>
<li><strong>Manutenção</strong>: Como monitorar e melhorar continuamente o sistema?</li>
</ul>
<p>Antes de mergulharmos nas técnicas avançadas, precisamos entender que o impacto mais significativo no desempenho de um sistema <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> não vem apenas de usar o modelo de linguagem mais recente. Os verdadeiros ganhos vêm de três fatores fundamentais:</p>
<ul>
<li><strong>Qualidade dos dados</strong>: Dados bem estruturados e relevantes são a base de todo sistema RAG eficaz.</li>
<li><strong>Preparação adequada</strong>: Como os dados são processados, limpos e organizados.</li>
<li><strong>Processamento eficiente</strong>: Como os dados são recuperados e utilizados durante a inferência.</li>
</ul>
<p>Mesmo com o avanço dos <a href="https://en.wikipedia.org/wiki/Large_language_model">LLMs</a>, esperar que modelos maiores corrijam magicamente problemas em dados defeituosos não é uma estratégia viável. O futuro da <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">IA</a> não está em um único modelo que sabe tudo, mas em sistemas que combinam <a href="https://en.wikipedia.org/wiki/Large_language_model">LLMs</a>, modelos multimodais e ferramentas de suporte que trabalham juntos de forma integrada. Dito isto, para construir um sistema <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> robusto, precisamos responder a várias perguntas importantes como:</p>
<ul>
<li>Como construir <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">mecanismos de recuperação robustos</a>?</li>
<li>Qual o papel da <a href="https://en.wikipedia.org/wiki/Embedding_model">qualidade dos embeddings</a> no desempenho da recuperação?</li>
<li>Como adaptar estratégias de <a href="https://en.wikipedia.org/wiki/Chunking_%28data_storage%29">chunking</a> dinamicamente?</li>
<li>Como o <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a> pode interpretar dados de forma eficaz?</li>
<li>Uma cadeia de <a href="https://en.wikipedia.org/wiki/Large_language_model">LLMs</a> ajudaria a refinar as respostas? Vale o custo?</li>
<li>Como prevenir alucinações mantendo a diversidade das respostas?</li>
<li>Como integrar entradas <a href="https://en.wikipedia.org/wiki/Multimodal_learning">multimodais</a> (texto, imagens, tabelas) em um pipeline <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a>?</li>
<li>Quais estratégias de <a href="https://en.wikipedia.org/wiki/Cache_%28computing%29">cache</a> reduzem chamadas de API redundantes e latência?</li>
<li>Como automatizar a <a href="https://en.wikipedia.org/wiki/Evaluation_of_retrieval_systems">avaliação da recuperação</a> para melhoria contínua?</li>
</ul>
<h2 id="armadilhas-comuns-e-como-evitá-las">Armadilhas Comuns e Como Evitá-las</h2>
<p>Baseado na nossa experiência com o <a href="https://github.com/scovl/docai">DocAI</a> e nos desafios relatados pela comunidade, identificamos quatro armadilhas principais que podem comprometer sistemas RAG:</p>
<h3 id="armadilha-1-a-falsa-sensação-de-relevância">Armadilha 1: A Falsa Sensação de Relevância</h3>
<p>Uma busca por vizinhos mais próximos sempre retornará algum resultado, mas como saber se é realmente útil? Alguns documentos podem parecer relevantes com base na similaridade vetorial, mas não fornecem o contexto adequado para responder à pergunta do usuário.</p>
<blockquote>
<p><strong>Solução</strong>: Implementar verificação de relevância pós-recuperação usando <a href="https://huggingface.co/cross-encoder">cross-encoders</a> ou filtros baseados em regras. No <a href="https://www.postgresql.org/">PostgreSQL</a>, podemos fazer isso com:</p></blockquote>


  <pre><code class="language-sql">-- Primeiro recuperamos candidatos usando busca vetorial
WITH candidatos AS (
  SELECT id, titulo, conteudo, embedding &lt;=&gt; query_embedding AS distancia
  FROM documentos_embeddings
  ORDER BY distancia
  LIMIT 20
),
-- Depois aplicamos filtro secundário para verificar relevância real
filtrados AS (
  SELECT id, titulo, conteudo, distancia
  FROM candidatos
  WHERE 
    -- Filtro baseado em regras (exemplo: deve conter palavras-chave)
    conteudo ILIKE &#39;%&#39; || &#39;palavra_chave&#39; || &#39;%&#39;
    -- Ou usar um modelo secundário para avaliar relevância
    -- ai.evaluate_relevance(conteudo, &#39;consulta_original&#39;) &gt; 0.7  -- ⚠️ Nota: Função experimental no pgai
)
SELECT * FROM filtrados ORDER BY distancia LIMIT 5;</code></pre>
 <p>Este código SQL demonstra uma abordagem de duas fases para melhorar a qualidade da recuperação em sistemas RAG. Na primeira fase, utilizamos a <a href="https://en.wikipedia.org/wiki/Vector_database">busca vetorial</a> para recuperar 20 candidatos iniciais ordenados por <a href="https://en.wikipedia.org/wiki/Vector_database">similaridade vetorial</a> (usando o operador <code>&lt;=&gt;</code> do <a href="https://en.wikipedia.org/wiki/Vector_database">pgvector</a> para calcular a distância entre embeddings). Esta etapa prioriza a velocidade e a amplitude da recuperação.</p>
<p>Na segunda fase, aplicamos filtros mais refinados para verificar a relevância real dos documentos recuperados. Isso pode incluir filtros baseados em regras (como busca por palavras-chave usando <code>ILIKE</code>) ou até mesmo modelos secundários de avaliação de relevância (como sugerido no comentário sobre a função experimental do <a href="https://github.com/timescale/pgai">pgai</a>). Esta abordagem em duas etapas equilibra eficiência e precisão, permitindo que o sistema primeiro capture um conjunto amplo de candidatos potenciais e depois refine os resultados para apresentar apenas os documentos verdadeiramente relevantes para a consulta do usuário.</p>
<h3 id="armadilha-2-tamanho-inadequado-de-chunks">Armadilha 2: Tamanho Inadequado de Chunks</h3>
<p>Dividir documentos em chunks menores é uma prática padrão, mas qual é o tamanho ideal?</p>
<ul>
<li>Chunks muito pequenos perdem contexto crucial</li>
<li>Chunks muito grandes diluem a recuperação com detalhes irrelevantes</li>
</ul>
<blockquote>
<p><strong>Solução</strong>: Adaptar a estratégia de chunking ao tipo de conteúdo. No nosso <a href="/2025/03/25/semantic-postgresql/">PostgreSQL RAG</a>, usamos chunking recursivo:</p></blockquote>


  <pre><code class="language-sql">-- Podemos ajustar os parâmetros de chunking para diferentes tipos de documentos
SELECT ai.create_vectorizer(
   &#39;documentos_tecnicos&#39;::regclass,
   destination =&gt; &#39;embeddings_tecnicos&#39;,
   embedding =&gt; ai.embedding_ollama(&#39;nomic-embed-text&#39;, 768),
   -- Chunks maiores para documentos técnicos que precisam de mais contexto
   chunking =&gt; ai.chunking_recursive_character_text_splitter(&#39;conteudo&#39;, 
                                                           chunk_size =&gt; 1500, 
                                                           chunk_overlap =&gt; 200)
);</code></pre>
 <p>Para documentos técnicos, que geralmente contêm informações densas e interconectadas, configuramos chunks maiores (1500 caracteres) com uma sobreposição significativa (200 caracteres).</p>
<p>Isso permite preservar mais contexto dentro de cada chunk, o que é crucial para a compreensão de conceitos técnicos complexos. O uso do <code>chunking_recursive_character_text_splitter</code> implementa uma estratégia de divisão recursiva que respeita a estrutura natural do texto, enquanto o modelo de embedding <code>nomic-embed-text</code> com 768 dimensões captura as nuances semânticas do conteúdo técnico. Esta <a href="https://en.wikipedia.org/wiki/Chunking_%28data_storage%29">abordagem adaptativa de chunking</a> é fundamental para equilibrar a granularidade da recuperação com a preservação do contexto necessário para respostas precisas em sistemas <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a>.</p>
<h3 id="armadilha-3-falta-de-monitoramento-contínuo">Armadilha 3: Falta de Monitoramento Contínuo</h3>
<p>Como garantir que seu sistema permaneça eficaz ao longo do tempo? <a href="https://www.databricks.com/br/glossary/llmops">LLMOps</a> não é apenas sobre implantação, mas sobre o monitoramento contínuo da qualidade.</p>
<blockquote>
<p><strong>Solução</strong>: Implementar métricas de avaliação como:</p>
<ul>
<li>Comparações com respostas conhecidas (ground truth)</li>
<li>Detecção de drift em embeddings</li>
<li>Monitoramento de latência e taxa de falhas</li>
</ul></blockquote>
<h3 id="armadilha-4-consultas-complexas-em-pipelines-simples">Armadilha 4: Consultas Complexas em Pipelines Simples</h3>
<p>Muitas consultas do mundo real são complexas demais para uma única etapa de recuperação. Se uma pergunta requer sintetizar várias informações, um pipeline RAG padrão pode falhar.</p>
<p><strong>Solução</strong>: Implementar fluxos de trabalho mais sofisticados:</p>
<ul>
<li>Workflows com agentes</li>
<li>Recuperação multi-hop</li>
<li>Geração dinâmica de prompts</li>
</ul>
<h2 id="técnicas-avançadas-de-otimização">Técnicas Avançadas de Otimização</h2>
<p>Agora que entendemos os fundamentos e as armadilhas comuns, vamos explorar técnicas específicas para melhorar cada componente do nosso sistema RAG.</p>
<h3 id="re-ranqueamento-de-chunks">Re-ranqueamento de Chunks</h3>


  
    
  
  <div class="mermaid">flowchart LR
    subgraph &#34;Primeira Fase&#34;
        Q[Consulta] --&gt; EMB[Embedding da Consulta]
        EMB --&gt; SIM[Busca por Similaridade Vetorial]
        DB[(Base Vetorial)] --&gt; SIM
        SIM --&gt; IC[Chunks Iniciais]
    end
    
    subgraph &#34;Re-ranqueamento&#34;
        IC --&gt; PAIR[Pares Consulta-Chunk]
        Q2[Consulta Original] --&gt; PAIR
        PAIR --&gt; CENC[Cross-Encoder]
        CENC --&gt; SCORE[Scores de Relevância]
        SCORE --&gt; SORT[Ordenação por Relevância]
        SORT --&gt; RC[Chunks Re-ranqueados]
    end
    
    IC -.-&gt; |Top-K Chunks| PAIR
    RC --&gt; GEN[Geração de Resposta]
    
    style Q fill:#f9f,stroke:#333,stroke-width:2px
    style Q2 fill:#f9f,stroke:#333,stroke-width:2px
    style CENC fill:#ffc,stroke:#333,stroke-width:2px
    style RC fill:#9f9,stroke:#333,stroke-width:2px
    style GEN fill:#99f,stroke:#333,stroke-width:2px</div>
 <p>O diagrama acima ilustra o processo de re-ranqueamento em um sistema RAG, dividido em duas fases principais:</p>
<ol>
<li>
<p>Na &ldquo;Primeira Fase&rdquo;, o fluxo começa com a consulta do usuário que é transformada em um embedding vetorial. Este embedding é então utilizado para realizar uma busca por <a href="https://en.wikipedia.org/wiki/Vector_database">similaridade vetorial</a> na base de dados vetoriais, resultando em um conjunto inicial de chunks relevantes.</p>
</li>
<li>
<p>A segunda fase, &ldquo;Re-ranqueamento&rdquo;, representa o refinamento desses resultados iniciais. Os chunks recuperados são combinados com a consulta original para formar pares consulta-chunk. Estes pares são processados por um <a href="https://en.wikipedia.org/wiki/Cross-encoder">cross-encoder</a>, um modelo especializado que avalia a relevância contextual entre a consulta e cada chunk. O cross-encoder gera scores de relevância que permitem uma ordenação mais precisa, resultando em chunks re-ranqueados que são finalmente utilizados para a geração da resposta final.</p>
</li>
</ol>
<p>Esta abordagem em duas etapas combina a eficiência computacional dos embeddings (que permitem busca rápida em grandes bases de dados) com a precisão dos cross-encoders (que capturam melhor as relações semânticas entre consulta e documento), superando as limitações de cada método quando usado isoladamente. Abordagem conceitual de como implementar re-ranqueamento com cross-encoder em Clojure:</p>


  <pre><code class="language-clojure">;; Exemplo conceitual de como implementar re-ranqueamento com cross-encoder
(defn rerank-results
  &#34;Re-classifica resultados usando cross-encoder para melhorar a precisão&#34;
  [query initial-results n]
  (let [;; Em um cenário real, usaríamos uma biblioteca Clojure para acessar modelos
        ;; Como o clj-huggingface ou wrapper Java para transformers
        cross-encoder (load-cross-encoder &#34;cross-encoder/ms-marco-MiniLM-L-6-v2&#34;)
        
        ;; Preparar pares de consulta-documento para avaliação
        pairs (map (fn [doc] [query (:conteudo doc)]) initial-results)
        
        ;; Obter scores de relevância do cross-encoder
        scores (predict-with-cross-encoder cross-encoder pairs)
        
        ;; Associar scores aos resultados originais
        results-with-scores (map-indexed 
                              (fn [idx doc] 
                                (assoc doc :relevance_score (nth scores idx)))
                              initial-results)
        
        ;; Ordenar por score de relevância (do maior para o menor)
        reranked-results (sort-by :relevance_score &gt; results-with-scores)]
    
    ;; Retornar apenas os top-n resultados
    (take n reranked-results)))

;; Funções auxiliares (implementações dependeriam da biblioteca específica usada)
(defn load-cross-encoder [model-name]
  ;; Carregar modelo cross-encoder usando Java interop ou biblioteca específica
  (println &#34;Carregando modelo&#34; model-name)
  {:model-name model-name})

(defn predict-with-cross-encoder [model pairs]
  ;; Executar predição do cross-encoder nos pares consulta-documento
  ;; Retorna um vetor de scores de relevância
  (println &#34;Avaliando&#34; (count pairs) &#34;pares com&#34; (:model-name model))
  (vec (repeatedly (count pairs) #(rand))))</code></pre>
 <p>No contexto do <a href="/2025/03/25/semantic-postgresql/">DocAI com PostgreSQL</a>, podemos implementar isso como:</p>


  <pre><code class="language-clojure">;; Exemplo de implementação de re-ranqueamento em Clojure para DocAI
(defn rerank-results
  &#34;Re-classifica resultados usando cross-encoder&#34;
  [query initial-results]
  (let [conn (jdbc/get-connection db-spec)
        ;; Construir array de IDs para consulta SQL
        ids (str/join &#34;,&#34; (map :id initial-results))
        ;; Consulta SQL que utiliza função do pgai para re-classificação
        sql (str &#34;SELECT d.id, d.titulo, d.conteudo, 
                 ai.relevance_score(&#39;&#34; query &#34;&#39;, d.conteudo) AS relevance  -- ⚠️ Nota: Função experimental no pgai
                 FROM documentos d 
                 WHERE d.id IN (&#34; ids &#34;) 
                 ORDER BY relevance DESC&#34;)]
    (jdbc/execute! conn [sql])))</code></pre>
 <p>O primeiro código demonstra uma implementação conceitual de re-ranqueamento usando um cross-encoder em Clojure. Ele recebe uma consulta e resultados iniciais, utiliza um modelo cross-encoder para avaliar a relevância de cada documento em relação à consulta, e então reordena os resultados com base nos scores obtidos. As funções auxiliares simulam a integração com modelos de machine learning, embora em um cenário real seria necessário utilizar bibliotecas específicas para acessar modelos de linguagem.</p>
<p>O segundo exemplo mostra uma implementação mais prática no contexto de um sistema <a href="/2025/03/25/semantic-postgresql/">DocAI integrado com PostgreSQL</a>. Neste caso, o re-ranqueamento é delegado a uma função SQL (<code>ai.relevance_score</code>) que avalia a relevância entre a consulta e o conteúdo do documento diretamente no banco de dados. Esta abordagem aproveita as capacidades de IA incorporadas no PostgreSQL através de extensões como pgai, simplificando a arquitetura ao mover o processamento de relevância para o banco de dados.</p>
<p>Ambas as implementações ilustram diferentes estratégias para melhorar a precisão dos resultados em sistemas RAG. A primeira abordagem oferece mais controle e flexibilidade ao processar o re-ranqueamento na aplicação, enquanto a segunda aproveita as capacidades do banco de dados para simplificar a arquitetura e potencialmente melhorar o desempenho ao reduzir a transferência de dados entre a aplicação e o banco de dados. A escolha entre estas abordagens dependerá dos requisitos específicos do sistema, incluindo considerações de desempenho, escalabilidade e facilidade de manutenção.</p>
<hr>
<h3 id="estratégias-de-chunking-dinâmico">Estratégias de Chunking Dinâmico</h3>
<p>Em vez de usar um tamanho fixo para todos os chunks, podemos implementar estratégias dinâmicas que se adaptam ao conteúdo:</p>
<ul>
<li><strong>Chunking Semântico</strong>: Dividir o texto em unidades semanticamente coerentes</li>
<li><strong>Chunking Hierárquico</strong>: Manter múltiplas granularidades do mesmo conteúdo</li>
<li><strong>Chunking Adaptativo</strong>: Ajustar tamanho com base em características do documento</li>
</ul>


  <pre><code class="language-clojure">;; Função conceitual para chunking hierárquico
(defn create-hierarchical-chunks
  &#34;Cria chunks em múltiplos níveis de granularidade&#34;
  [document]
  (let [;; Divisão em parágrafos
        paragraphs (split-paragraphs document)
        ;; Divisão em seções
        sections (split-sections document)
        ;; Documento completo
        full-doc [{:content document :level &#34;document&#34;}]
        ;; Combinar todos os níveis
        all-chunks (concat full-doc
                          (map #(hash-map :content % :level &#34;section&#34;) sections)
                          (map #(hash-map :content % :level &#34;paragraph&#34;) paragraphs))]
    ;; Inserir no PostgreSQL com metadados sobre o nível
    (doseq [chunk all-chunks]
      (jdbc/execute! db-spec
                    [&#34;INSERT INTO documentos_hierarquicos 
                     (conteudo, nivel_granularidade) VALUES (?, ?)&#34;
                     (:content chunk) (:level chunk)]))))</code></pre>
 <p>O código acima implementa uma estratégia de <a href="https://en.wikipedia.org/wiki/Chunking_%28data_storage%29">chunking hierárquico</a> em <a href="https://clojure.org/">Clojure</a>, uma técnica avançada para sistemas <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> que mantém múltiplas representações do mesmo conteúdo em diferentes níveis de granularidade. A função <code>create-hierarchical-chunks</code> divide um documento em três níveis: documento completo, seções e parágrafos, preservando assim tanto o contexto amplo quanto os detalhes específicos.</p>
<p>Esta abordagem permite que o sistema de recuperação escolha a granularidade mais apropriada dependendo da consulta, oferecendo flexibilidade que um chunking de tamanho fixo não consegue proporcionar.</p>
<p>A implementação utiliza funções auxiliares como <code>split-paragraphs</code> e <code>split-sections</code> (não mostradas no código) para segmentar o documento de forma inteligente, respeitando a estrutura semântica do texto. Cada <a href="https://en.wikipedia.org/wiki/Chunk_%28data_storage%29">chunk</a> é armazenado no <a href="https://www.postgresql.org/">PostgreSQL</a> junto com metadados sobre seu nível de granularidade, permitindo consultas que podem priorizar diferentes níveis dependendo do tipo de pergunta.</p>
<p>Esta técnica é particularmente valiosa para documentos longos e estruturados, como artigos técnicos ou documentação, onde tanto o contexto geral quanto detalhes específicos podem ser relevantes dependendo da natureza da consulta do usuário.</p>
<hr>
<h3 id="workflows-com-agentes-para-consultas-complexas">Workflows com Agentes para Consultas Complexas</h3>
<p>Para consultas que exigem raciocínio em várias etapas, podemos implementar agentes que decompõem o problema:</p>


  
  <div class="mermaid">flowchart TB
    Q[Consulta Original] --&gt; AN[Analisador de Consulta]
    AN --&gt; SQ1[Sub-questão 1]
    AN --&gt; SQ2[Sub-questão 2]
    AN --&gt; SQ3[Sub-questão 3]
    
    SQ1 --&gt; R1[RAG Específico 1]
    SQ2 --&gt; R2[RAG Específico 2]
    SQ3 --&gt; R3[RAG Específico 3]
    
    R1 --&gt; A1[Resposta Parcial 1]
    R2 --&gt; A2[Resposta Parcial 2]
    R3 --&gt; A3[Resposta Parcial 3]
    
    A1 --&gt; S[Sintetizador]
    A2 --&gt; S
    A3 --&gt; S
    
    S --&gt; FR[Resposta Final]
    
    style Q fill:#f9f,stroke:#333,stroke-width:2px
    style S fill:#bbf,stroke:#333,stroke-width:2px
    style FR fill:#bfb,stroke:#333,stroke-width:2px</div>
 <p>Este diagrama ilustra uma arquitetura de <a href="https://en.wikipedia.org/wiki/Workflow">workflow</a> baseada em agentes para processamento de consultas complexas em sistemas <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a>. O fluxo começa com uma consulta do usuário que é analisada por um componente <a href="https://en.wikipedia.org/wiki/Query_parser">Analisador</a>, responsável por decompor a pergunta original em sub-questões mais específicas e gerenciáveis. Cada sub-questão é então direcionada para um pipeline <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> especializado, permitindo recuperações contextuais mais precisas.</p>
<p>A abordagem <a href="https://en.wikipedia.org/wiki/Divide_and_conquer_algorithm">divide-e-conquista</a> demonstrada no diagrama permite que o sistema lide com perguntas que exigiriam conhecimento de diferentes domínios ou documentos. Cada <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> especializado pode utilizar diferentes bases de conhecimento, estratégias de recuperação ou até mesmo modelos de linguagem otimizados para domínios específicos, resultando em respostas parciais de alta qualidade para cada aspecto da consulta.</p>
<p>O componente Sintetizador atua como o elemento integrador final, combinando as respostas parciais em uma resposta coerente e abrangente. Esta arquitetura modular não apenas melhora a precisão das respostas para consultas complexas, mas também oferece maior transparência no processo de raciocínio, permitindo identificar quais fontes contribuíram para cada parte da resposta final. O resultado é um sistema RAG mais robusto, capaz de lidar com consultas que exigem raciocínio em múltiplas etapas e integração de informações de diversas fontes.</p>


  <pre><code class="language-clojure">(defn agent-rag-workflow
  &#34;Implementa um workflow de agente para consultas complexas&#34;
  [query]
  (let [;; Passo 1: Analisar a consulta e identificar sub-questões
        sub-questions (analyze-query query)
        ;; Passo 2: Buscar informações para cada sub-questão
        sub-answers (map #(retrieve-and-generate %) sub-questions)
        ;; Passo 3: Sintetizar respostas parciais em uma resposta final
        final-context (str/join &#34;\n\n&#34; sub-answers)
        final-prompt (str &#34;Com base nas seguintes informações:\n\n&#34; 
                         final-context 
                         &#34;\n\nResponda à pergunta original: &#34; query)
        final-answer (generate-response final-prompt)]
    final-answer))

(defn analyze-query
  &#34;Divide uma consulta complexa em sub-questões&#34;
  [query]
  (let [prompt (str &#34;Divida a seguinte pergunta em sub-questões independentes:\n\n&#34; query)
        response (call-ollama-api prompt)
        ;; Parsear a resposta para extrair as sub-questões
        sub-questions (parse-sub-questions response)]
    sub-questions))</code></pre>
 <p>Uma implementação mais robusta de workflows com agentes envolve várias etapas adicionais. Trataremos deste assunto em um próximo artigo.</p>
<hr>
<h4 id="arquitetura-de-agentes-avançada">Arquitetura de Agentes Avançada</h4>
<p>Os sistemas de agentes RAG mais sofisticados aplicam o conceito de <strong>ReAct</strong> (Raciocínio + Ação) para processar consultas complexas:</p>


  
  <div class="mermaid">flowchart TB
    subgraph &#34;Arquitetura ReAct para RAG&#34;
    Q[Consulta do Usuário] --&gt; PL[Planejador]
    PL --&gt; PLAN[Plano de Execução]
    PLAN --&gt; RT[Roteador]
    
    RT --&gt;|Sub-tarefa 1| AS[Agente de Pesquisa]
    RT --&gt;|Sub-tarefa 2| AR[Agente de Raciocínio]
    RT --&gt;|Sub-tarefa 3| AC[Agente de Cálculo]
    
    AS --&gt; OR[Orquestrador]
    AR --&gt; OR
    AC --&gt; OR
    
    OR --&gt; SI[Sintetizador]
    SI --&gt; RES[Resposta Final]
    end
    
    subgraph &#34;Ferramentas e Recursos&#34;
    AS -.-&gt; VDB[(Base Vetorial)]
    AR -.-&gt; LLM[Modelo de Linguagem]
    AC -.-&gt; CALC[Ferramentas de Cálculo]
    end
    
    style Q fill:#f9f,stroke:#333,stroke-width:2px
    style PLAN fill:#ffc,stroke:#333,stroke-width:2px
    style RT fill:#9cf,stroke:#333,stroke-width:2px
    style AS fill:#bbf,stroke:#333,stroke-width:2px
    style AR fill:#bbf,stroke:#333,stroke-width:2px
    style AC fill:#bbf,stroke:#333,stroke-width:2px
    style SI fill:#bfb,stroke:#333,stroke-width:2px
    style RES fill:#f99,stroke:#333,stroke-width:2px</div>
 <p>Este diagrama ilustra uma arquitetura avançada ReAct para sistemas <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a>, mostrando como uma consulta complexa é processada através de múltiplos componentes especializados. O fluxo começa com a consulta do usuário sendo analisada por um <a href="https://en.wikipedia.org/wiki/Workflow">Planejador</a>, que cria um plano estruturado de execução.</p>
<p>Este plano é então gerenciado por um <a href="https://en.wikipedia.org/wiki/Routing">Roteador</a> que distribui sub-tarefas para agentes especializados (Pesquisa, Raciocínio e Cálculo), cada um interagindo com recursos específicos como bases de dados vetoriais, LLMs ou ferramentas de cálculo.</p>
<p>A força desta arquitetura está na sua capacidade de decompor problemas complexos em tarefas gerenciáveis e especializadas, permitindo que cada componente se concentre no que faz melhor. O <a href="https://en.wikipedia.org/wiki/Orchestration">Orquestrador</a> coordena os resultados dos diferentes agentes, enquanto o <a href="https://en.wikipedia.org/wiki/Synthesis">Sintetizador</a> integra todas as informações em uma resposta final coerente. Esta abordagem modular não apenas melhora a precisão das respostas, mas também aumenta a transparência do processo de raciocínio e facilita a depuração e otimização de componentes individuais do sistema RAG.</p>
<ul>
<li><strong>Planejador</strong>: Analisa a consulta e cria um plano de execução</li>
<li><strong>Roteador</strong>: Direciona sub-consultas para ferramentas especializadas</li>
<li><strong>Agentes Especializados</strong>: Executam tarefas específicas
<ul>
<li>Agente de Pesquisa: Recupera informações da base de conhecimento</li>
<li>Agente de Raciocínio: Realiza inferências lógicas sobre os dados recuperados</li>
<li>Agente de Cálculo: Processa cálculos e análises numéricas</li>
</ul>
</li>
<li><strong>Orquestrador</strong>: Gerencia o fluxo de informações entre agentes</li>
<li><strong>Sintetizador</strong>: Combina as respostas em um resultado coerente</li>
</ul>
<p>Vamos analisar o código abaixo para entender como funciona um sistema ReAct para RAG:</p>


  <pre><code class="language-clojure">;; Exemplo conceitual de um sistema ReAct para RAG
(defn react-agent
  &#34;Implementa um agente ReAct para consultas complexas&#34;
  [query]
  (let [;; Determinar se a consulta precisa de um plano
        plan-needed? (complex-query? query)
        ;; Se necessário, criar um plano
        execution-plan (when plan-needed?
                         (create-execution-plan query))
        ;; Executar o plano ou a consulta direta
        result (if plan-needed?
                 (execute-plan execution-plan)
                 (simple-rag-query query))]
    result))

(defn execute-plan
  &#34;Executa um plano com agentes especializados&#34;
  [plan]
  (loop [steps (:steps plan)
         context {}
         responses []]
    (if (empty? steps)
      ;; Sintetizar respostas em um resultado final
      (synthesize-responses responses (:query plan))
      (let [current-step (first steps)
            agent-type (:agent current-step)
            ;; Determinar qual agente especializado usar
            agent-fn (case agent-type
                       :search search-agent
                       :reasoning reasoning-agent
                       :calculation calculation-agent
                       :default default-agent)
            ;; Executar o agente com o contexto atual
            step-result (agent-fn (:input current-step) context)
            ;; Atualizar o contexto com o resultado
            updated-context (assoc context (:id current-step) step-result)]
        (recur (rest steps) 
               updated-context 
               (conj responses step-result))))))</code></pre>
 <p>O código implementa um agente ReAct (Reasoning + Acting) para consultas complexas em um sistema <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a>. A função principal <code>react-agent</code> avalia se a consulta requer um plano de execução complexo ou pode ser processada diretamente. Para consultas complexas, cria-se um plano estruturado que é executado pela função <code>execute-plan</code>, que utiliza um loop para processar cada etapa do plano sequencialmente.</p>
<p>O sistema emprega agentes especializados (busca, raciocínio, cálculo) selecionados dinamicamente com base no tipo de tarefa. Cada agente contribui com resultados parciais que são acumulados em um contexto compartilhado, permitindo que etapas posteriores utilizem informações de etapas anteriores. Finalmente, todas as respostas são sintetizadas em um resultado coerente.</p>
<p>Esta arquitetura modular permite decompor problemas complexos em tarefas gerenciáveis, melhorando a precisão e facilitando a manutenção do sistema.Para implementações detalhadas de sistemas de agentes RAG, consulte:</p>
<ul>
<li><a href="https://docs.llamaindex.ai/en/stable/examples/agent/react_agent.html">LlamaIndex - Implementando ReAct Agents</a></li>
<li><a href="https://python.langchain.com/docs/modules/agents/agent_types/multi_agent">LangChain - Multi-Agent Systems</a></li>
<li><a href="https://huggingface.co/blog/autonomous-agents">HuggingFace - Agentes Autônomos</a></li>
</ul>
<h4 id="casos-de-uso-para-workflows-de-agentes">Casos de Uso para Workflows de Agentes</h4>
<p>Os workflows com agentes são particularmente úteis em cenários como:</p>
<ul>
<li><strong>Pesquisa Científica</strong>: Onde diversas fontes precisam ser consultadas e relacionadas</li>
<li><strong>Diagnóstico de Problemas</strong>: Quando é necessário seguir uma árvore de decisão</li>
<li><strong>Análise de Documentos Complexos</strong>: Como contratos ou documentação técnica</li>
<li><strong>Planejamento Estratégico</strong>: Onde múltiplas dimensões precisam ser consideradas</li>
</ul>
<hr>
<h3 id="pipelines-multimodais">Pipelines Multimodais</h3>
<p>Integrar entradas multimodais (texto, imagens, tabelas) em um pipeline RAG pode enriquecer significativamente o contexto:</p>


  
  <div class="mermaid">flowchart LR
    subgraph &#34;Documento Misto&#34;
    TXT[Texto]
    IMG[Imagens]
    TBL[Tabelas]
    end
    
    subgraph &#34;Processadores Específicos&#34;
    TXT --&gt; TXT_P[Processador de Texto]
    IMG --&gt; IMG_P[Processador de Imagem]
    TBL --&gt; TBL_P[Processador de Tabela]
    end
    
    subgraph &#34;Embeddings&#34;
    TXT_P --&gt; TXT_E[Embedding de Texto]
    IMG_P --&gt; IMG_E[Embedding de Imagem]
    TBL_P --&gt; TBL_E[Embedding de Tabela]
    end
    
    TXT_E --&gt; FUS[Fusão de Representações]
    IMG_E --&gt; FUS
    TBL_E --&gt; FUS
    
    FUS --&gt; DB[(Base de Dados Multimodal)]
    Q[Consulta do Usuário] --&gt; Q_PROC[Processador de Consulta]
    Q_PROC --&gt; RAG[Motor RAG]
    DB --&gt; RAG
    RAG --&gt; RES[Resposta Multimodal]
    
    style TXT fill:#f9f,stroke:#333,stroke-width:2px
    style IMG fill:#9cf,stroke:#333,stroke-width:2px
    style TBL fill:#fcf,stroke:#333,stroke-width:2px
    style FUS fill:#ff9,stroke:#333,stroke-width:2px
    style DB fill:#9f9,stroke:#333,stroke-width:2px
    style RES fill:#f99,stroke:#333,stroke-width:2px</div>
 <p>O diagrama acima ilustra uma arquitetura de pipeline multimodal para sistemas <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a>, demonstrando como diferentes tipos de conteúdo (texto, imagens e tabelas) podem ser processados e integrados em um único sistema de recuperação. O fluxo começa com a extração desses diferentes elementos de um documento misto, cada um seguindo para processadores especializados que compreendem as características únicas de cada modalidade.</p>
<p>Na camada de embeddings, cada tipo de conteúdo é transformado em representações vetoriais específicas para sua modalidade - textos são processados por modelos de linguagem, imagens por modelos de visão computacional, e tabelas por processadores estruturados. O componente de fusão de representações é crucial nesta arquitetura, pois combina estas diferentes representações vetoriais em um formato unificado que pode ser armazenado e consultado eficientemente na base de dados multimodal.</p>
<p>Quando uma consulta do usuário é recebida, ela passa pelo processador de consulta que determina quais modalidades são relevantes para a pergunta, e o motor <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> recupera as informações apropriadas da base de dados multimodal. Esta abordagem permite que o sistema forneça respostas enriquecidas que incorporam conhecimento de múltiplas modalidades, resultando em uma experiência mais completa e contextualmente relevante para o usuário, especialmente para consultas que se beneficiam de informações visuais ou estruturadas além do texto puro.</p>


  <pre><code class="language-clojure">(defn process-multimodal-document
  &#34;Processa um documento que contém texto e imagens&#34;
  [doc-path]
  (let [;; Extrair texto
        text-content (extract-text doc-path)
        ;; Identificar e extrair imagens
        image-paths (extract-images doc-path)
        ;; Gerar descrições para as imagens usando um modelo de visão
        image-descriptions (map #(describe-image %) image-paths)
        ;; Combinar texto e descrições de imagens
        enriched-content (str text-content &#34;\n\n&#34;
                             &#34;O documento contém as seguintes imagens:\n&#34;
                             (str/join &#34;\n&#34; image-descriptions))]
    ;; Inserir no banco de dados
    (jdbc/execute! db-spec
                  [&#34;INSERT INTO documentos (titulo, conteudo) VALUES (?, ?)&#34;
                   (extract-title doc-path) enriched-content])))</code></pre>
 <hr>
<h4 id="arquitetura-multimodal-completa">Arquitetura Multimodal Completa</h4>
<p>Uma implementação mais completa de pipelines multimodais requer vários componentes especializados:</p>


  
  <div class="mermaid">flowchart TD
    DOC[Documento Multimodal] --&gt; DETECT[Detector de Tipo]
    DETECT --&gt; EXTRACT[Extração de Componentes]
    
    EXTRACT --&gt; TX[Componentes de Texto]
    EXTRACT --&gt; IMG[Componentes de Imagem]
    EXTRACT --&gt; TBL[Componentes de Tabela]
    EXTRACT --&gt; AUD[Componentes de Áudio]
    
    TX --&gt; TX_PROC[Processador de Texto]
    IMG --&gt; IMG_PROC[Processador de Imagem]
    TBL --&gt; TBL_PROC[Processador de Tabela]
    AUD --&gt; AUD_PROC[Processador de Áudio]
    
    TX_PROC --&gt; TX_EMB[Embedding de Texto]
    IMG_PROC --&gt; IMG_EMB[Embedding de Imagem]
    TBL_PROC --&gt; TBL_EMB[Embedding de Tabela]
    AUD_PROC --&gt; AUD_EMB[Embedding de Áudio]
    
    TX_EMB --&gt; FUSION[Fusão de Representações]
    IMG_EMB --&gt; FUSION
    TBL_EMB --&gt; FUSION
    AUD_EMB --&gt; FUSION
    
    FUSION --&gt; META[Adição de Metadados]
    META --&gt; STORE[Armazenamento em PostgreSQL]
    
    subgraph &#34;Modelos Específicos&#34;
        TX_PROC -.- TEXT_MODEL[Modelo de Texto]
        IMG_PROC -.- CLIP[CLIP]
        TBL_PROC -.- TABLE_MODEL[Modelo de Tabela]
        AUD_PROC -.- AUDIO_MODEL[Modelo de Áudio]
        FUSION -.- FLAMINGO[Flamingo]
    end
    
    style DOC fill:#f9f,stroke:#333,stroke-width:2px
    style FUSION fill:#ff9,stroke:#333,stroke-width:2px
    style META fill:#9cf,stroke:#333,stroke-width:2px
    style STORE fill:#9f9,stroke:#333,stroke-width:2px</div>
 <p>O diagrama acima ilustra uma arquitetura para <a href="https://en.wikipedia.org/wiki/Multimodal_AI">processamento de documentos multimodais</a> em sistemas RAG avançados. O fluxo começa com um documento multimodal que passa por um <a href="https://en.wikipedia.org/wiki/Type_detection">detector de tipo</a>, seguido pela <a href="https://en.wikipedia.org/wiki/Component_extraction">extração de componentes</a> que separa o conteúdo em diferentes modalidades: texto, imagem, tabela e áudio. Cada tipo de componente é então direcionado para um processador especializado, projetado para extrair informações significativas específicas daquela modalidade.</p>
<p>Após o processamento inicial, cada componente é transformado em uma <a href="https://en.wikipedia.org/wiki/Embedding_model">representação vetorial (embedding)</a> usando modelos especializados para cada modalidade - <a href="https://en.wikipedia.org/wiki/Text_embedding">modelos de texto para componentes textuais</a>, <a href="https://en.wikipedia.org/wiki/CLIP">CLIP para imagens</a>, <a href="https://en.wikipedia.org/wiki/Table_embedding">modelos específicos para tabelas</a> e <a href="https://en.wikipedia.org/wiki/Audio_embedding">áudio</a>. Estes embeddings são então combinados através de um processo de fusão de representações, que cria uma compreensão unificada e coerente do documento multimodal, potencialmente utilizando modelos como o <a href="https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model">Flamingo</a> que são projetados para integração multimodal.</p>
<p>A etapa final do pipeline envolve a adição de <a href="https://en.wikipedia.org/wiki/Metadata">metadados estruturados</a> à <a href="https://en.wikipedia.org/wiki/Unified_representation">representação unificada</a> e seu armazenamento em um banco de dados <a href="https://www.postgresql.org/">PostgreSQL</a> otimizado para <a href="https://en.wikipedia.org/wiki/Vector_database">busca vetorial</a> com <a href="https://github.com/pgvector/pgvector">pgvector</a>.</p>
<p>Esta arquitetura modular permite que o sistema <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> processe eficientemente documentos complexos contendo múltiplos tipos de mídia, mantendo as relações semânticas entre diferentes componentes e possibilitando recuperação mais precisa quando consultado. Os modelos específicos destacados no diagrama (<code>TEXT_MODEL</code>, <code>CLIP</code>, <code>TABLE_MODEL</code>, <code>AUDIO_MODEL</code> e <code>FLAMINGO</code>) representam as tecnologias de ponta que podem ser empregadas em cada etapa do processamento.</p>
<p>O código abaixo implementa um pipeline avançado para processamento de documentos multimodais em <a href="https://clojure.org/">Clojure</a>, demonstrando uma abordagem sofisticada para lidar com conteúdo heterogêneo em sistemas <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a>:</p>


  <pre><code class="language-clojure">;; Exemplo de pipeline multimodal mais elaborado
(defn advanced-multimodal-processor
  &#34;Pipeline completo para processamento multimodal&#34;
  [document-path]
  (let [;; Determinar tipo de documento
        doc-type (detect-document-type document-path)
        
        ;; Extrair componentes por tipo
        components (case doc-type
                     :pdf (extract-pdf-components document-path)
                     :doc (extract-doc-components document-path)
                     :webpage (extract-webpage-components document-path)
                     (extract-text-components document-path))
        
        ;; Processar cada componente com seu processador especializado
        processed-components (map process-component components)
        
        ;; Gerar embeddings multimodais
        embeddings (map #(generate-multimodal-embedding % doc-type) processed-components)
        
        ;; Criar representação unificada
        unified-representation {:components processed-components
                               :embeddings embeddings
                               :metadata {:doc-type doc-type
                                         :path document-path
                                         :extracted-at (java.util.Date.)}}]
    
    ;; Armazenar no PostgreSQL com schema adequado para multimodalidade
    (store-multimodal-document unified-representation)))

(defn process-component
  &#34;Processa um componente baseado em seu tipo&#34;
  [component]
  (case (:type component)
    :text (process-text (:content component))
    :image (process-image (:content component))
    :table (process-table (:content component))
    :chart (process-chart (:content component))
    :audio (process-audio (:content component))
    (:content component))) ;; Fallback para tipos desconhecidos</code></pre>
 <p>A função principal <code>advanced-multimodal-processor</code> orquestra todo o fluxo, começando pela detecção do tipo de documento, seguida pela extração de componentes específicos para cada formato (PDF, DOC, páginas web), processamento especializado de cada componente, geração de embeddings multimodais e finalmente o armazenamento da representação unificada no PostgreSQL. Esta arquitetura modular permite que o sistema processe de forma inteligente diferentes tipos de mídia dentro do mesmo documento.</p>
<p>A função auxiliar <code>process-component</code> exemplifica o tratamento especializado para cada modalidade, direcionando o conteúdo para processadores específicos com base no tipo do componente (texto, imagem, tabela, gráfico ou áudio). Esta abordagem granular garante que cada tipo de conteúdo receba o tratamento mais apropriado, maximizando a qualidade da informação extraída e sua representação vetorial.</p>
<p>O resultado é um sistema <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> verdadeiramente <a href="https://en.wikipedia.org/wiki/Multimodal_AI">multimodal</a>, capaz de compreender e recuperar informações de documentos complexos que combinam texto, elementos visuais e dados estruturados, proporcionando respostas mais completas e contextualmente ricas para as consultas dos usuários.</p>
<hr>
<h4 id="esquema-postgresql-para-dados-multimodais">Esquema PostgreSQL para Dados Multimodais</h4>
<p>Para armazenar e recuperar eficientemente dados multimodais no PostgreSQL:</p>


  <pre><code class="language-sql">-- Tabela principal para documentos multimodais
CREATE TABLE documentos_multimodais (
    id SERIAL PRIMARY KEY,
    titulo TEXT NOT NULL,
    doc_type TEXT,
    metadata JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Tabela para componentes específicos
CREATE TABLE componentes_documento (
    id SERIAL PRIMARY KEY,
    documento_id INTEGER REFERENCES documentos_multimodais(id) ON DELETE CASCADE,
    tipo_componente TEXT NOT NULL,
    conteudo TEXT,
    posicao INTEGER,
    metadados JSONB
);

-- Tabela para embeddings de texto
CREATE TABLE embeddings_texto (
    id SERIAL PRIMARY KEY,
    componente_id INTEGER REFERENCES componentes_documento(id) ON DELETE CASCADE,
    embedding VECTOR(768)
);

-- Tabela para embeddings de imagem
CREATE TABLE embeddings_imagem (
    id SERIAL PRIMARY KEY,
    componente_id INTEGER REFERENCES componentes_documento(id) ON DELETE CASCADE,
    embedding VECTOR(512)
);</code></pre>
 <p>Este esquema (scheme) <a href="https://en.wikipedia.org/wiki/SQL">SQL</a> estabelece uma estrutura robusta para armazenar e gerenciar documentos multimodais no <a href="https://www.postgresql.org/">PostgreSQL</a>. A arquitetura é composta por quatro tabelas interconectadas: uma tabela principal (<code>documentos_multimodais</code>) que armazena metadados gerais dos documentos, uma tabela para componentes específicos (<code>componentes_documento</code>) que fragmenta cada documento em suas partes constituintes (texto, imagens, etc.), e duas tabelas especializadas para armazenar embeddings vetoriais de diferentes modalidades (<code>embeddings_texto</code> e <code>embeddings_imagem</code>). Esta estrutura relacional permite uma organização hierárquica do conteúdo, mantendo a integridade referencial através de chaves estrangeiras.</p>
<p>A separação dos <a href="https://en.wikipedia.org/wiki/Embedding_model">embeddings</a> por tipo de modalidade é particularmente importante, pois diferentes tipos de conteúdo geralmente requerem modelos de embedding distintos com dimensionalidades variadas (768 para texto e 512 para imagens no exemplo). Esta abordagem modular facilita a implementação de consultas multimodais eficientes, permitindo buscas por similaridade em cada modalidade separadamente ou de forma combinada.</p>
<p>Além disso, o uso de campos <a href="https://www.postgresql.org/docs/current/datatype-json.html">JSONB</a> para metadados oferece flexibilidade para armazenar informações adicionais sem necessidade de alterar o esquema, tornando o sistema adaptável a diferentes tipos de documentos e requisitos de aplicação. Para implementações detalhadas de <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> multimodal, consulte:</p>
<ul>
<li><a href="https://docs.llamaindex.ai/en/stable/examples/multi_modal/">MultiModal RAG com LlamaIndex</a></li>
<li><a href="https://towardsdatascience.com/a-comprehensive-guide-to-multimodal-rag-ea72c387c6e8">Comprehensive Guide to MultiModal RAG</a></li>
<li><a href="https://huggingface.co/blog/idefics">Projeto IDEFICS para RAG Multimodal</a></li>
<li><a href="https://supabase.com/blog/image-search-using-ai-embeddings">Supabase - Image Search com pgvector</a></li>
</ul>
<hr>
<h4 id="desafios-de-implementação-multimodal">Desafios de Implementação Multimodal</h4>
<p>A implementação de pipelines multimodais traz desafios específicos:</p>
<ol>
<li><strong>Alinhamento de Representações</strong>: Garantir que diferentes modalidades possam ser comparadas</li>
<li><strong>Gerenciamento de Recursos</strong>: Modelos multimodais são computacionalmente exigentes</li>
<li><strong>Estratégias de Fusão</strong>: Decidir quando fundir informações de diferentes modalidades
<ul>
<li>Fusão Precoce: Combinar antes do embedding</li>
<li>Fusão Tardia: Manter embeddings separados e combinar apenas no ranking final</li>
</ul>
</li>
</ol>
<blockquote>
<p>No próximo artigo, exploraremos em profundidade como expandir o DocAI para oferecer suporte total a conteúdo multimodal, com exemplos práticos de implementação e otimização de desempenho.</p></blockquote>
<hr>
<h3 id="estratégias-de-cache">Estratégias de Cache</h3>
<p>Implementar caching pode reduzir drasticamente a latência e os custos:</p>


  
  <div class="mermaid">flowchart TD
    Q[Consulta] --&gt; CH1{Cache L1?}
    CH1 --&gt;|Sim| RES1[Resposta do Cache L1]
    CH1 --&gt;|Não| CH2{Cache L2?}
    
    CH2 --&gt;|Sim| RES2[Resposta do Cache L2]
    CH2 --&gt;|Não| CH3{Cache L3?}
    
    CH3 --&gt;|Sim| RES3[Resposta do Cache L3]
    CH3 --&gt;|Não| PROC[Processamento RAG Completo]
    
    PROC --&gt; RES4[Nova Resposta]
    RES4 --&gt; STORE[Armazenar em Cache]
    STORE --&gt; RES[Resposta Final]
    
    RES1 --&gt; RES
    RES2 --&gt; RES
    RES3 --&gt; RES
    
    subgraph &#34;Camadas de Cache&#34;
    CH1
    CH2
    CH3
    end
    
    style Q fill:#f9f,stroke:#333,stroke-width:2px
    style PROC fill:#ffc,stroke:#333,stroke-width:2px
    style RES fill:#9f9,stroke:#333,stroke-width:2px
    style STORE fill:#9cf,stroke:#333,stroke-width:2px</div>
 <p>O diagrama acima ilustra uma estratégia de cache em múltiplas camadas para sistemas RAG, uma técnica fundamental para otimizar tanto a latência quanto os custos operacionais. A arquitetura implementa três níveis de cache <code>(L1, L2 e L3)</code>, cada um representando diferentes compromissos entre velocidade e abrangência. O cache <code>L1</code> tipicamente armazena respostas exatas para consultas idênticas, oferecendo resposta instantânea quando há correspondência perfeita. O cache <code>L2</code> pode armazenar respostas para consultas semanticamente similares, enquanto o cache <code>L3</code> pode conter resultados parciais como embeddings pré-calculados ou chunks recuperados anteriormente.</p>
<p>Esta abordagem em cascata permite que o sistema evite o processamento <a href="https://en.wikipedia.org/wiki/Retrieval-Augmented_Generation">RAG</a> completo sempre que possível, reduzindo significativamente o tempo de resposta e a carga computacional. Quando uma consulta não encontra correspondência em nenhum nível de cache, apenas então o sistema executa o fluxo completo de <a href="https://en.wikipedia.org/wiki/Retrieval-Augmented_Generation">RAG</a>, que inclui geração de embeddings, recuperação de contexto e inferência do <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a>.</p>
<blockquote>
<p>A nova resposta gerada é então armazenada no cache apropriado para uso futuro, criando um sistema que se torna progressivamente mais eficiente à medida que processa mais consultas. A implementação de uma estratégia de cache multicamada como esta pode reduzir custos operacionais em até 70% em sistemas de produção com padrões de consulta repetitivos.</p></blockquote>
<p>Além da economia de recursos, a redução na latência melhora significativamente a experiência do usuário, com respostas quase instantâneas para consultas frequentes. Para maximizar a eficácia, é importante implementar políticas de expiração de cache e estratégias de invalidação para garantir que as informações permaneçam atualizadas, especialmente em domínios onde os dados subjacentes mudam com frequência. Abaixo, um exemplo de implementação de cache de dois níveis em Clojure:</p>


  <pre><code class="language-clojure">;; Implementação de cache de dois níveis em Clojure
(def embedding-cache (atom {}))
(def response-cache (atom {}))

(defn cached-embed
  &#34;Gera embedding para texto com cache&#34;
  [text]
  (if-let [cached (@embedding-cache text)]
    cached
    (let [embedding (generate-embedding text)]
      (swap! embedding-cache assoc text embedding)
      embedding)))

(defn cached-rag-query
  &#34;Executa consulta RAG com cache&#34;
  [query]
  (if-let [cached (@response-cache query)]
    (do
      (println &#34;Cache hit for query!&#34;)
      cached)
    (let [;; Processo RAG normal
          response (full-rag-process query)]
      ;; Armazenar no cache apenas para consultas não-pessoais
      (when (not (personal-query? query))
        (swap! response-cache assoc query response))
      response)))</code></pre>
 <p>O código acima implementa uma estratégia de cache de dois níveis em <a href="https://clojure.org/">Clojure</a> para otimizar sistemas <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a>. O primeiro nível (<code>embedding-cache</code>) armazena embeddings já calculados para textos, evitando a regeneração desses vetores que é computacionalmente intensiva. O segundo nível (<code>response-cache</code>) armazena respostas completas para consultas anteriores, permitindo retornar resultados instantaneamente quando uma consulta idêntica é feita novamente.</p>
<p>A função <code>cached-embed</code> verifica primeiro se o embedding já existe no cache antes de gerá-lo, enquanto <code>cached-rag-query</code> implementa lógica similar para respostas completas, incluindo uma verificação inteligente para evitar o cache de consultas pessoais.</p>
<p>Em produção com maior escala, esta abordagem poderia ser estendida para utilizar <a href="https://redis.io/">Redis</a> ou outras soluções de cache distribuído, mantendo os mesmos princípios fundamentais. Para o <a href="https://www.postgresql.org/">PostgreSQL</a>, podemos implementar <a href="https://www.postgresql.org/docs/current/pgvector-embeddings.html">cache de embeddings diretamente no banco</a>:</p>


  <pre><code class="language-sql">-- Criar tabela de cache para embeddings de consultas frequentes
CREATE TABLE IF NOT EXISTS query_embedding_cache (
  query_text TEXT PRIMARY KEY,
  embedding VECTOR(768),
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  hit_count INTEGER DEFAULT 1
);

-- Função para obter embedding com cache
CREATE OR REPLACE FUNCTION get_cached_embedding(query TEXT)
RETURNS VECTOR AS $$
DECLARE
  cached_embedding VECTOR(768);
BEGIN
  -- Verificar se existe no cache
  SELECT embedding INTO cached_embedding
  FROM query_embedding_cache
  WHERE query_text = query;
  
  -- Se existe, atualizar contador e retornar
  IF FOUND THEN
    UPDATE query_embedding_cache 
    SET hit_count = hit_count &#43; 1 
    WHERE query_text = query;
    RETURN cached_embedding;
  ELSE
    -- Gerar novo embedding
    cached_embedding := ai.ollama_embed(&#39;nomic-embed-text&#39;, query);  -- ⚠️ Nota: Verifique a disponibilidade desta função na sua instalação
    
    -- Armazenar no cache
    INSERT INTO query_embedding_cache (query_text, embedding)
    VALUES (query, cached_embedding);
    
    RETURN cached_embedding;
  END IF;
END;
$$ LANGUAGE plpgsql;</code></pre>
 <p>Este código SQL implementa um sistema de cache para embeddings de consultas no PostgreSQL, otimizando significativamente o desempenho de sistemas RAG em produção. A tabela <code>query_embedding_cache</code> armazena o texto da consulta como chave primária, junto com seu <a href="https://www.postgresql.org/docs/current/pgvector-embeddings.html">embedding vetorial</a>, <a href="https://www.postgresql.org/docs/current/functions-datetime.html">timestamp de criação</a> e um <a href="https://www.postgresql.org/docs/current/functions-math.html">contador de acessos</a>. Esta estrutura não apenas evita o recálculo de embeddings para consultas repetidas, mas também fornece dados valiosos sobre padrões de uso através do campo <code>hit_count</code>.</p>
<p>A função <code>get_cached_embedding</code> encapsula a lógica de cache com uma interface limpa: quando uma consulta é recebida, ela primeiro verifica se o embedding já existe no cache. Se encontrado, incrementa o contador de acessos e retorna imediatamente o embedding armazenado, economizando o custo computacional da geração de embeddings. Caso contrário, gera um novo embedding usando o modelo &rsquo;nomic-embed-text&rsquo; via <a href="https://ollama.com/">Ollama</a>, armazena-o no cache para uso futuro e o retorna.</p>
<p>Esta implementação reduz significativamente a latência para consultas repetidas, diminui a carga nos serviços de embedding, e proporciona uma base para análises de desempenho e otimização contínua. A abordagem é particularmente eficaz em cenários onde os usuários tendem a fazer perguntas semelhantes ou quando o sistema processa grandes volumes de consultas, resultando em economia de recursos computacionais e melhoria na experiência do usuário com respostas mais rápidas.</p>
<h4 id="estratégias-avançadas-de-cache-para-rag">Estratégias Avançadas de Cache para RAG</h4>
<p>Para sistemas RAG em produção, podemos implementar estratégias de cache mais sofisticadas:</p>
<ol>
<li>
<p><a href="https://en.wikipedia.org/wiki/Multilevel_cache"><strong>Cache em Múltiplas Camadas</strong></a>:</p>
<ul>
<li>L1: Cache em memória para consultas muito frequentes</li>
<li>L2: Cache em banco de dados para persistência entre reinicializações</li>
<li>L3: Cache distribuído (como <a href="https://redis.io/">Redis</a>) para sistemas escaláveis</li>
</ul>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/Time_to_live"><strong>Políticas de Expiração Inteligentes</strong></a>:</p>
<ul>
<li>TTL (Time-to-Live) baseado na frequência de uso</li>
<li>Invalidação seletiva quando documentos relacionados são atualizados</li>
<li>Cache semântico que agrupa consultas similares</li>
</ul>
</li>
<li>
<p><strong>Pré-Computação e Cache Preditivo</strong>:</p>
<ul>
<li>Analisar padrões de consulta para pré-computar respostas prováveis</li>
<li>Gerar embeddings para variações comuns de consultas</li>
</ul>
</li>
</ol>


  <pre><code class="language-clojure">;; Exemplo de implementação de cache com Redis para alta disponibilidade
(defn distributed-cached-rag-query
  &#34;Executa consulta RAG com cache distribuído&#34;
  [query]
  (let [cache-key (str &#34;rag:query:&#34; (digest/md5 query))
        ;; Verificar no Redis
        cached-response (redis/get cache-key)]
    (if cached-response
      ;; Usar resposta em cache
      (do
        (redis/incr (str cache-key &#34;:hits&#34;))
        (json/read-str cached-response))
      ;; Gerar nova resposta
      (let [response (full-rag-process query)
            ;; Serializar e armazenar no Redis com TTL
            _ (redis/setex cache-key 
                          (* 60 60 24) ;; 24 horas
                          (json/write-str response))
            ;; Registrar metadados para análise
            _ (redis/hmset (str cache-key &#34;:meta&#34;)
                          {&#34;timestamp&#34; (System/currentTimeMillis)
                           &#34;query_length&#34; (count query)
                           &#34;query_type&#34; (determine-query-type query)})]
        response))))</code></pre>
 <p>Para implementações detalhadas de estratégias de cache para RAG, consulte:</p>
<ul>
<li><a href="https://docs.llamaindex.ai/en/stable/module_guides/querying/query_engine/query_engine_caching">LlamaIndex - Query Engine Caching</a></li>
<li><a href="https://python.langchain.com/docs/modules/model_io/llms/llm_caching">LangChain - Caching para LLM Applications</a></li>
<li><a href="https://redis.io/docs/stack/search/reference/vectors/">Redis Vector Database for RAG</a></li>
</ul>
<hr>
<h2 id="monitoramento-e-métricas-llmops-na-prática">Monitoramento e Métricas: LLMOps na Prática</h2>
<p>Para garantir que nosso sistema RAG continue funcionando bem em produção, precisamos monitorar métricas chave:</p>


  
  <div class="mermaid">flowchart TB
    subgraph &#34;Ciclo de Monitoramento RAG&#34;
    direction TB
    LOG[Logs de Interações] --&gt; METR[Cálculo de Métricas]
    METR --&gt; ANOM[Detecção de Anomalias]
    ANOM --&gt; ALER[Alertas e Relatórios]
    ALER --&gt; OPT[Otimização do Sistema]
    OPT --&gt; LOG
    end
    
    subgraph &#34;Métricas RAG&#34;
    direction LR
    METR_OP[Métricas Operacionais]
    METR_Q[Métricas de Qualidade]
    METR_F[Métricas de Feedback]
    end
    
    METR --- METR_OP
    METR --- METR_Q
    METR --- METR_F
    
    style LOG fill:#f9f,stroke:#333,stroke-width:2px
    style METR fill:#ffc,stroke:#333,stroke-width:2px
    style ANOM fill:#f99,stroke:#333,stroke-width:2px
    style OPT fill:#9f9,stroke:#333,stroke-width:2px</div>
 <p>O diagrama acima ilustra o ciclo completo de <a href="https://en.wikipedia.org/wiki/Monitoring">monitoramento</a> para <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">sistemas RAG</a> em produção. No centro do processo estão os &ldquo;<a href="https://en.wikipedia.org/wiki/Log_file">Logs de Interações</a>&rdquo;, que capturam dados detalhados sobre cada <a href="https://en.wikipedia.org/wiki/Query">consulta</a> processada pelo sistema, incluindo a pergunta original, os <a href="https://en.wikipedia.org/wiki/Information_retrieval">documentos recuperados</a>, a <a href="https://en.wikipedia.org/wiki/Natural_language_generation">resposta gerada</a> e <a href="https://en.wikipedia.org/wiki/Performance_metric">métricas de desempenho</a>.</p>
<p>Estes logs alimentam o &ldquo;<a href="https://en.wikipedia.org/wiki/Metric_%28mathematics%29">Cálculo de Métricas</a>&rdquo;, que transforma <a href="https://en.wikipedia.org/wiki/Raw_data">dados brutos</a> em <a href="https://en.wikipedia.org/wiki/Key_performance_indicator">indicadores acionáveis</a> distribuídos em três categorias principais: <a href="https://en.wikipedia.org/wiki/Operational_efficiency">operacionais</a> (<a href="https://en.wikipedia.org/wiki/Latency_%28engineering%29">latência</a>, <a href="https://en.wikipedia.org/wiki/Throughput">throughput</a>), <a href="https://en.wikipedia.org/wiki/Data_quality">qualidade</a> (<a href="https://en.wikipedia.org/wiki/Precision_and_recall">precisão</a>, <a href="https://en.wikipedia.org/wiki/Relevance_%28information_retrieval%29">relevância</a>) e <a href="https://en.wikipedia.org/wiki/Feedback">feedback</a> (avaliações dos usuários). A &ldquo;<a href="https://en.wikipedia.org/wiki/Anomaly_detection">Detecção de Anomalias</a>&rdquo; monitora continuamente estas métricas para identificar desvios significativos dos padrões esperados, gerando &ldquo;<a href="https://en.wikipedia.org/wiki/Alert_management">Alertas e Relatórios</a>&rdquo; que orientam a &ldquo;<a href="https://en.wikipedia.org/wiki/System_optimization">Otimização do Sistema</a>&rdquo;, fechando assim o ciclo de <a href="https://en.wikipedia.org/wiki/Continuous_improvement">melhoria contínua</a>.</p>
<p>Este fluxo de trabalho representa a essência do <a href="https://en.wikipedia.org/wiki/MLOps">LLMOps</a> aplicado a sistemas RAG, onde o monitoramento não é apenas <a href="https://en.wikipedia.org/wiki/Reactive_programming">reativo</a>, mas <a href="https://en.wikipedia.org/wiki/Proactive">proativo</a> na identificação de oportunidades de melhoria. A estrutura tripartite das métricas garante uma <a href="https://en.wikipedia.org/wiki/Holism">visão holística</a> do desempenho: enquanto as métricas operacionais asseguram a <a href="https://en.wikipedia.org/wiki/Technical_efficiency">eficiência técnica</a> do sistema, as métricas de qualidade avaliam a <a href="https://en.wikipedia.org/wiki/Semantic_similarity">precisão semântica</a> das respostas, e as métricas de feedback incorporam a <a href="https://en.wikipedia.org/wiki/Human-centered_design">perspectiva humana</a> na avaliação.</p>
<p>Esta abordagem <a href="https://en.wikipedia.org/wiki/System_integration">integrada</a> permite que <a href="https://en.wikipedia.org/wiki/Engineering_team">equipes de engenharia</a> identifiquem rapidamente <a href="https://en.wikipedia.org/wiki/Bottleneck_%28software%29">gargalos</a>, ajustem <a href="https://en.wikipedia.org/wiki/Information_retrieval">parâmetros de recuperação</a> e melhorem continuamente a <a href="https://en.wikipedia.org/wiki/User_experience">experiência do usuário</a> final, mesmo à medida que o <a href="https://en.wikipedia.org/wiki/Big_data">volume de dados</a> e a <a href="https://en.wikipedia.org/wiki/Query_complexity">complexidade das consultas</a> aumentam. O código abaixo mostra como implementar o log e a avaliação de respostas em <a href="https://en.wikipedia.org/wiki/Clojure">Clojure</a>:</p>


  <pre><code class="language-clojure">;; Estrutura para log e avaliação de respostas
(defn log-rag-interaction
  &#34;Registra uma interação RAG para análise posterior&#34;
  [query retrieved-docs response latency]
  (jdbc/execute! db-spec
                [&#34;INSERT INTO rag_logs 
                 (query, retrieved_docs, response, latency_ms, timestamp)
                 VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP)&#34;
                 query
                 (json/write-str retrieved-docs)
                 response
                 latency]))

;; Função para calcular métricas de desempenho
(defn calculate-rag-metrics
  &#34;Calcula métricas de desempenho para um período&#34;
  [start-date end-date]
  (let [logs (jdbc/execute! db-spec
                           [&#34;SELECT * FROM rag_logs 
                            WHERE timestamp BETWEEN ? AND ?&#34;
                            start-date end-date])
        ;; Métricas de latência
        avg-latency (average-latency logs)
        p95-latency (percentile-latency logs 95)
        ;; Taxa de falhas (quando resposta contém erros específicos)
        failure-rate (failure-rate logs)
        ;; Distribuição de consultas por tópico
        topic-distribution (topic-distribution logs)]
    {:avg_latency avg-latency
     :p95_latency p95-latency
     :failure_rate failure-rate
     :topic_distribution topic-distribution}))</code></pre>
 <p>A função <code>log-rag-interaction</code> captura cada aspecto da interação desde a consulta original até os documentos recuperados, a resposta gerada e o tempo de latência armazenando-os em um banco de dados relacional para análise posterior. Esta abordagem permite rastrear o histórico completo de interações, criando um registro valioso para depuração, otimização e avaliação de desempenho ao longo do tempo.</p>
<p>A função <code>calculate-rag-metrics</code> complementa o sistema de logging ao transformar os dados brutos em métricas acionáveis, calculando indicadores críticos como latência média, percentil 95 de latência (importante para entender outliers), taxa de falhas e distribuição de consultas por tópico.</p>
<p>Esta análise multidimensional permite que as equipes identifiquem não apenas problemas técnicos (como gargalos de desempenho), mas também padrões de uso e áreas temáticas que podem requerer otimização específica. A combinação destas duas funções estabelece um ciclo de feedback contínuo que é essencial para <a href="https://en.wikipedia.org/wiki/Retrieval-Augmented_Generation">sistemas RAG em produção</a>, permitindo melhorias iterativas baseadas em dados reais de uso.</p>
<h3 id="métricas-de-qualidade-específicas-para-rag">Métricas de Qualidade Específicas para RAG</h3>
<p>Além das métricas operacionais comuns (latência, disponibilidade), sistemas RAG requerem métricas específicas para avaliar a qualidade das respostas:</p>
<h4 id="1-métricas-de-relevância-do-contexto">1. Métricas de Relevância do Contexto</h4>
<ul>
<li><strong>Precision@K</strong>: Proporção de chunks recuperados que são realmente relevantes para a consulta</li>
<li><strong>Recall@K</strong>: Proporção de chunks relevantes na base de conhecimento que foram recuperados</li>
<li><strong>NDCG (Normalized Discounted Cumulative Gain)</strong>: Avalia se os chunks mais relevantes estão no topo da lista</li>
</ul>
<p>Abaixo, um exemplo de implementação de métricas de relevância do contexto em Clojure:</p>


  <pre><code class="language-clojure">(defn calculate-precision-at-k
  &#34;Calcula Precision@K para uma consulta e seus chunks recuperados&#34;
  [query chunks k expert-judgments]
  (let [retrieved-top-k (take k chunks)
        relevant-count (count (filter #(is-chunk-relevant? % query expert-judgments) 
                                     retrieved-top-k))]
    (double (/ relevant-count (min k (count retrieved-top-k))))))

(defn calculate-recall-at-k
  &#34;Calcula Recall@K para uma consulta&#34;
  [query chunks k all-relevant-chunks expert-judgments]
  (let [retrieved-top-k (take k chunks)
        retrieved-relevant (filter #(is-chunk-relevant? % query expert-judgments) 
                                  retrieved-top-k)
        total-relevant-count (count all-relevant-chunks)]
    (if (pos? total-relevant-count)
      (double (/ (count retrieved-relevant) total-relevant-count))
      1.0))) ;; Se não há chunks relevantes, recall é 1</code></pre>
 <p>A função <code>calculate-precision-at-k</code> mede a proporção de chunks relevantes entre os <code>k</code> primeiros resultados recuperados, comparando-os com julgamentos de especialistas. Já a função <code>calculate-recall-at-k</code> avalia a proporção de chunks relevantes que foram efetivamente recuperados em relação ao total de chunks relevantes disponíveis.</p>
<p>Ambas as métricas são fundamentais para entender a eficácia do sistema de recuperação: <code>precision</code> indica quão precisa é a recuperação (minimizando falsos positivos), enquanto <code>recall</code> mostra quão completa é a recuperação (minimizando falsos negativos). A implementação inclui tratamento para casos especiais, como quando não há chunks relevantes disponíveis, garantindo resultados matematicamente consistentes.</p>
<h4 id="2-métricas-de-qualidade-da-resposta">2. Métricas de Qualidade da Resposta</h4>
<p>Para avaliar a qualidade das respostas geradas por sistemas RAG, é essencial implementar métricas específicas que capturem diferentes dimensões de eficácia. Estas métricas vão além de simples avaliações binárias (correto/incorreto) e permitem uma análise nuançada da performance do sistema. Implementamos as seguintes métricas qualitativas em nosso framework de avaliação:</p>
<ul>
<li><strong>Faithfulness (Fidelidade)</strong>: O grau em que a resposta é suportada pelo contexto fornecido, sem alucinações</li>
<li><strong>Answer Relevancy (Relevância da Resposta)</strong>: Quão bem a resposta aborda a consulta do usuário</li>
<li><strong>Contextual Precision (Precisão Contextual)</strong>: Proporção do contexto utilizado que foi relevante para a resposta</li>
<li><strong>Helpfulness (Utilidade)</strong>: Avaliação subjetiva de quão útil foi a resposta para o usuário</li>
</ul>
<p>Abaixo, um exemplo de implementação de métricas de qualidade da resposta em Clojure:</p>


  <pre><code class="language-clojure">(defn evaluate-response-quality
  &#34;Avalia métricas qualitativas de uma resposta RAG&#34;
  [query context response]
  (let [;; Usar LLM como avaliador
        prompt-faithfulness (str &#34;Avalie a fidelidade da seguinte resposta ao contexto fornecido.\n\n&#34;
                                &#34;Consulta: &#34; query &#34;\n\n&#34;
                                &#34;Contexto: &#34; context &#34;\n\n&#34;
                                &#34;Resposta: &#34; response &#34;\n\n&#34;
                                &#34;A resposta contém informações que não estão no contexto? &#34;
                                &#34;A resposta contradiz o contexto em algum ponto? &#34;
                                &#34;Atribua uma pontuação de 1 a 10, onde 10 significa perfeita fidelidade ao contexto.&#34;)
        
        prompt-relevancy (str &#34;Avalie quão relevante é a resposta para a consulta.\n\n&#34;
                             &#34;Consulta: &#34; query &#34;\n\n&#34;
                             &#34;Resposta: &#34; response &#34;\n\n&#34;
                             &#34;A resposta aborda diretamente a consulta? &#34;
                             &#34;Alguma parte importante da consulta foi ignorada? &#34;
                             &#34;Atribua uma pontuação de 1 a 10, onde 10 significa perfeitamente relevante.&#34;)
        
        ;; Chamar LLM para avaliação
        faithfulness-result (parse-score (call-evaluation-llm prompt-faithfulness))
        relevancy-result (parse-score (call-evaluation-llm prompt-relevancy))]
    
    ;; Retornar resultados agregados
    {:faithfulness faithfulness-result
     :relevancy relevancy-result
     :composite_score (/ (&#43; faithfulness-result relevancy-result) 2.0)}))</code></pre>
 <p>A função recebe três parâmetros principais: a consulta original do usuário (<code>query</code>), o contexto recuperado pelo sistema (<code>context</code>) e a resposta gerada pelo modelo (<code>response</code>). Utilizando esses inputs, a função constrói dois prompts específicos para avaliar diferentes dimensões da qualidade da resposta.</p>
<p>O primeiro prompt avalia a &ldquo;fidelidade&rdquo; <a href="https://en.wikipedia.org/wiki/Faithfulness_%28literary_theory%29">(faithfulness)</a> da resposta, verificando se ela se mantém fiel ao contexto fornecido sem adicionar informações não presentes ou contradizer o material de referência. O segundo prompt avalia a &ldquo;relevância&rdquo; <a href="https://en.wikipedia.org/wiki/Relevance_%28information_retrieval%29">(relevancy)</a>, analisando se a resposta aborda diretamente a consulta do usuário e se cobre todos os aspectos importantes da pergunta. Ambos os prompts são enviados para um <a href="https://github.com/langchain-ai/langchain/blob/main/libs/langchain-core/langchain_core/prompts/prompt.py">LLM avaliador através da função <code>call-evaluation-llm</code></a>, que retorna uma avaliação textual que é então convertida em uma pontuação numérica pela função <code>parse-score</code>.</p>
<p>Por fim, a função agrega os resultados em um mapa contendo as pontuações individuais de fidelidade e relevância, além de calcular uma pontuação composta que é a média das duas métricas. Esta abordagem de &ldquo;LLM como avaliador&rdquo; representa uma técnica avançada no campo de RAG, permitindo avaliações automatizadas que capturam nuances qualitativas difíceis de medir com métricas puramente estatísticas.</p>
<blockquote>
<p>O código demonstra como implementar um sistema de avaliação que pode ser usado para monitoramento contínuo da qualidade das respostas e identificação de áreas para melhoria.</p></blockquote>
<h4 id="3-métricas-de-consenso-entre-modelos">3. Métricas de Consenso entre Modelos</h4>
<p>Uma técnica eficaz é comparar respostas de múltiplos modelos ou configurações:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Model_agreement"><strong>Model Agreement (Concordância de Modelos)</strong></a>: Grau de concordância entre diferentes LLMs para a mesma consulta/contexto</li>
<li><a href="https://en.wikipedia.org/wiki/Embedding"><strong>Embedding Stability (Estabilidade de Embeddings)</strong></a>: Consistência de embeddings entre atualizações de modelos</li>
<li><a href="https://en.wikipedia.org/wiki/Context_utilization_variance"><strong>Context Utilization Variance (Variância de Utilização de Contexto)</strong></a>: Diferenças na forma como os modelos utilizam o contexto</li>
</ul>
<p>Abaixo, um exemplo de implementação de métricas de consenso entre modelos em Clojure:</p>


  <pre><code class="language-clojure">(defn measure-model-agreement
  &#34;Mede concordância entre diferentes modelos para mesma consulta&#34;
  [query context models]
  (let [;; Gerar respostas de cada modelo
        responses (map #(generate-response-with-model % query context) models)
        
        ;; Calcular similaridade semântica entre cada par de respostas
        similarities (for [i (range (count responses))
                          j (range (inc i) (count responses))]
                      (calculate-semantic-similarity 
                        (nth responses i) 
                        (nth responses j)))
        
        ;; Média das similaridades como medida de concordância
        avg-similarity (if (seq similarities)
                         (/ (reduce &#43; similarities) (count similarities))
                         1.0)]
    avg-similarity))</code></pre>
 <p>Esta função implementa uma métrica de concordância entre modelos, uma técnica valiosa para avaliar a robustez de sistemas RAG. Ao gerar respostas para a mesma consulta usando diferentes modelos, a função calcula a similaridade semântica entre cada par de respostas. Uma alta concordância (similaridade) entre modelos diversos sugere que a resposta é mais confiável, enquanto baixa concordância pode indicar ambiguidade nos dados ou questões com a recuperação de contexto.</p>
<p>A implementação utiliza uma abordagem de comparação par a par, onde cada resposta é comparada com todas as outras. A função <code>calculate-semantic-similarity</code> (não mostrada) provavelmente utiliza embeddings para medir quão semanticamente próximas estão duas respostas. O resultado final é uma pontuação média de similaridade que quantifica o nível geral de consenso entre os modelos. Esta métrica é particularmente útil para identificar consultas problemáticas onde diferentes modelos divergem significativamente, sinalizando potenciais áreas para melhoria no pipeline RAG.</p>
<h3 id="automação-da-avaliação-com-llms-como-juízes">Automação da Avaliação com LLMs como Juízes</h3>


  
  <div class="mermaid">flowchart TD
    Q[Consulta do Usuário] --&gt; RAG[Sistema RAG]
    CTX[Contexto Recuperado] --&gt; RAG
    
    RAG --&gt; RESP[Resposta Gerada]
    
    subgraph &#34;Avaliação Automatizada&#34;
        RESP --&gt; JUDGE[LLM Avaliador]
        Q --&gt; JUDGE
        CTX --&gt; JUDGE
        CRIT[Critérios de Avaliação] --&gt; JUDGE
        
        JUDGE --&gt; EVAL[Avaliação Estruturada]
        EVAL --&gt; DB[(Banco de Dados)]
        
        EVAL --&gt; METRICS[Métricas de Qualidade]
        METRICS --&gt; DASH[Dashboard]
        
        EVAL --&gt; INSIGHT[Insights para Melhoria]
        INSIGHT --&gt; REFINE[Refinamento do Sistema]
        REFINE -.-&gt; RAG
    end
    
    style Q fill:#f9f,stroke:#333,stroke-width:2px
    style RESP fill:#9cf,stroke:#333,stroke-width:2px
    style JUDGE fill:#fc9,stroke:#333,stroke-width:2px
    style EVAL fill:#9f9,stroke:#333,stroke-width:2px
    style REFINE fill:#f99,stroke:#333,stroke-width:2px</div>
 <p>O diagrama acima representando o fluxo desde a consulta do usuário até o refinamento contínuo do sistema. No centro do processo está o &ldquo;LLM Avaliador&rdquo; (JUDGE), que recebe três entradas cruciais: a consulta original do usuário, o contexto recuperado e a resposta gerada pelo sistema RAG. Adicionalmente, o avaliador utiliza critérios de avaliação predefinidos para realizar uma análise estruturada e imparcial.</p>
<p>O aspecto mais valioso deste fluxo é o ciclo de feedback que ele estabelece: a avaliação estruturada não apenas alimenta um banco de dados para registro histórico e gera métricas de qualidade para visualização em dashboards, mas também produz insights acionáveis que direcionam o refinamento do sistema. Esta abordagem cíclica permite que o sistema RAG evolua continuamente, aprendendo com suas próprias limitações e melhorando progressivamente a qualidade das respostas, sem necessidade de intervenção humana constante em cada etapa do processo de avaliação. Uma abordagem emergente é usar LLMs como &ldquo;juízes&rdquo; para avaliar automaticamente a qualidade das respostas:</p>


  <pre><code class="language-clojure">(defn llm-judge-evaluation
  &#34;Utiliza LLM como juiz para avaliar respostas RAG&#34;
  [query context response evaluation-criteria]
  (let [;; Construir prompt para avaliação
        evaluation-prompt (str &#34;Você é um avaliador especializado em sistemas RAG. &#34;
                              &#34;Analise a seguinte interação e avalie de acordo com os critérios especificados.\n\n&#34;
                              &#34;Consulta do usuário: &#34; query &#34;\n\n&#34;
                              &#34;Contexto recuperado: &#34; context &#34;\n\n&#34;
                              &#34;Resposta gerada: &#34; response &#34;\n\n&#34;
                              &#34;Critérios de avaliação:\n&#34;
                              evaluation-criteria &#34;\n\n&#34;
                              &#34;Para cada critério, forneça:\n&#34;
                              &#34;1. Uma pontuação de 1-10\n&#34;
                              &#34;2. Justificativa para a pontuação\n&#34;
                              &#34;3. Sugestões específicas para melhoria\n&#34;
                              &#34;Formate sua resposta como JSON.&#34;)
        
        ;; Chamar LLM avaliador (preferivelmente um modelo diferente do usado para gerar a resposta para evitar viés de auto-avaliação)](https://en.wikipedia.org/wiki/Self-assessment)
        judge-response (call-evaluation-llm evaluation-prompt)
        
        ;; Parsear resposta estruturada
        evaluation-results (json/read-str judge-response)]
    
    ;; Registrar avaliação no banco de dados
    (log-evaluation query context response evaluation-results)
    
    ;; Retornar resultados estruturados
    evaluation-results))</code></pre>
 <p>A implementação segue um padrão elegante e prático: primeiro constrói um prompt detalhado que enquadra a tarefa de avaliação, depois chama um modelo <a href="https://en.wikipedia.org/wiki/Self-assessment">LLM dedicado (preferencialmente diferente do usado na geração da resposta para evitar viés de auto-avaliação)</a>, processa a resposta estruturada e finalmente registra os resultados para análise posterior. Esta abordagem permite avaliação contínua e escalável da qualidade do sistema RAG, fornecendo insights acionáveis para refinamento do pipeline sem necessidade de intervenção humana constante.</p>
<p>A função representa uma evolução importante nas práticas de avaliação de RAG, combinando a capacidade de compreensão contextual dos LLMs com a necessidade de feedback estruturado e quantificável.</p>
<h4 id="configuração-de-um-dashboard-de-qualidade-rag">Configuração de um Dashboard de Qualidade RAG</h4>
<p>Para monitoramento contínuo, é essencial configurar um dashboard que acompanhe a evolução das métricas ao longo do tempo:</p>


  <pre><code class="language-clojure">(defn generate-rag-quality-report
  &#34;Gera relatório diário de qualidade do sistema RAG&#34;
  []
  (let [;; Período de avaliação (último dia)
        end-date (java.util.Date.)
        start-date (-&gt; (java.util.Calendar/getInstance)
                       (doto (.setTime end-date)
                             (.add java.util.Calendar/DAY_OF_MONTH -1))
                       (.getTime))
        
        ;; Recuperar logs do período
        logs (jdbc/execute! db-spec
                           [&#34;SELECT * FROM rag_logs 
                             WHERE timestamp BETWEEN ? AND ?&#34;
                            start-date end-date])
        
        ;; Calcular métricas operacionais
        operational-metrics (calculate-operational-metrics logs)
        
        ;; Selecionar amostra aleatória para avaliação qualitativa
        evaluation-sample (take 50 (shuffle logs))
        
        ;; Avaliar qualidade das respostas na amostra
        quality-metrics (evaluate-sample-quality evaluation-sample)
        
        ;; Identificar tendências e anomalias
        trends (detect-quality-trends quality-metrics)
        anomalies (detect-quality-anomalies quality-metrics)
        
        ;; Compilar relatório
        report {:date (format-date end-date)
                :sample_size (count evaluation-sample)
                :operational_metrics operational-metrics
                :quality_metrics quality-metrics
                :trends trends
                :anomalies anomalies
                :recommendations (generate-recommendations trends anomalies)}]
    
    ;; Salvar relatório e enviar notificações se houver anomalias
    (save-quality-report report)
    (when (not-empty anomalies)
      (send-quality-alert report))
    
    report))</code></pre>
 <p>O código acima implementa uma função Clojure chamada <code>generate-rag-quality-report</code> que automatiza a geração de relatórios diários de qualidade para um sistema RAG. A função começa definindo um período de avaliação (último dia), recupera logs de interações RAG desse período do banco de dados, e calcula métricas operacionais básicas. Em seguida, seleciona uma amostra aleatória de 50 interações para uma avaliação qualitativa mais profunda.</p>
<p>O núcleo da função está na avaliação da qualidade das respostas na amostra selecionada, seguida pela identificação de tendências e anomalias nos dados de qualidade. Isso permite que o sistema não apenas meça o desempenho atual, mas também detecte padrões emergentes ou problemas que possam exigir atenção. O relatório final é estruturado como um <a href="https://clojure.org/reference/data_structures">mapa Clojure</a> contendo a data, tamanho da amostra, métricas operacionais, métricas de qualidade, tendências identificadas, anomalias detectadas e recomendações geradas automaticamente.</p>
<p>Um aspecto importante da função é seu mecanismo de alerta: após salvar o relatório no sistema, ela verifica se foram detectadas anomalias e, em caso positivo, envia alertas para os responsáveis. Esta abordagem proativa para monitoramento de qualidade permite que equipes de engenharia e produto intervenham rapidamente quando o desempenho do sistema RAG começa a degradar, antes que os usuários sejam significativamente afetados. O código exemplifica uma implementação prática de <a href="https://en.wikipedia.org/wiki/LLMOps">LLMOps</a>, focando na avaliação contínua e sistemática da qualidade das respostas em um sistema RAG.</p>
<h3 id="integração-com-sistemas-de-feedback-do-usuário">Integração com Sistemas de Feedback do Usuário</h3>
<p>O feedback direto dos usuários é uma fonte valiosa para avaliar a qualidade das respostas:</p>


  <pre><code class="language-clojure">(defn process-user-feedback
  &#34;Processa feedback explícito do usuário&#34;
  [query-id response-id feedback-type feedback-text]
  (let [;; Registrar feedback no banco de dados
        _ (jdbc/execute! db-spec
                        [&#34;INSERT INTO user_feedback 
                          (query_id, response_id, feedback_type, feedback_text, timestamp) 
                          VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP)&#34;
                         query-id response-id feedback-type feedback-text])
        
        ;; Recuperar detalhes da interação
        interaction (jdbc/execute-one! db-spec
                                     [&#34;SELECT query, retrieved_docs, response 
                                       FROM rag_logs WHERE id = ?&#34;
                                      query-id])
        
        ;; Analisar feedback para extrair insights
        feedback-analysis (analyze-user-feedback feedback-type 
                                               feedback-text 
                                               (:query interaction)
                                               (:response interaction))]
    
    ;; Atualizar métricas agregadas
    (update-feedback-metrics feedback-type)
    
    ;; Para feedback negativo, adicionar à fila de revisão manual
    (when (= feedback-type &#34;negative&#34;)
      (add-to-manual-review-queue query-id feedback-analysis))
    
    feedback-analysis))</code></pre>
 <p>A função registra o feedback no banco de dados, recupera os detalhes da interação original, analisa o feedback para extrair insights valiosos e atualiza métricas agregadas. Um aspecto importante é o tratamento especial para <a href="https://en.wikipedia.org/wiki/Negative_feedback">feedback negativo</a>, que é automaticamente adicionado a uma fila de revisão manual, permitindo que a equipe investigue e corrija problemas específicos.</p>
<p>Esta implementação representa um componente crucial de um sistema LLMOps maduro, pois estabelece um ciclo de feedback contínuo entre usuários e desenvolvedores. Ao capturar sistematicamente as avaliações dos usuários e vinculá-las às consultas e respostas específicas, a função permite análises detalhadas sobre o desempenho do sistema, identificação de padrões de falha e oportunidades de melhoria.</p>
<hr>
<h2 id="implementando-no-docai">Implementando no DocAI</h2>
<p>Agora que exploramos várias técnicas avançadas, vamos ver como elas são implementadas no projeto DocAI. Nosso sistema atual já incorpora muitas dessas técnicas para criar um pipeline RAG avançado.</p>
<blockquote>
<p>Caso não saiba o que é o DocAI, você pode ver os artigos anteriores <a href="https://scovl.github.io/2025/03/23/rag/">RAG Simples com Clojure e Ollama</a> e <a href="https://scovl.github.io/2025/03/25/semantic-postgresql/">Busca Semântica com Ollama e PostgreSQL</a>.</p></blockquote>
<h3 id="arquitetura-atual-do-docai">Arquitetura Atual do DocAI</h3>
<p>A arquitetura do DocAI implementa um sistema RAG completo com suporte a agentes para consultas complexas. Os principais componentes são:</p>
<ol>
<li>
<p><a href="https://github.com/docai-ai/docai/blob/main/src/core.clj"><strong>Core (core.clj)</strong></a>: Coordenação central do sistema, implementando a interface CLI e gerenciando o fluxo de dados entre componentes.</p>
</li>
<li>
<p><a href="https://github.com/docai-ai/docai/blob/main/src/llm.clj"><strong>LLM (llm.clj)</strong></a>: Interface com o Ollama para geração de texto e embeddings, abstraindo detalhes de comunicação com a API.</p>
</li>
<li>
<p><a href="https://github.com/docai-ai/docai/blob/main/src/pg.clj"><strong>PostgreSQL (pg.clj)</strong></a>: Implementação da busca semântica com pgvector, incluindo configuração e consultas otimizadas.</p>
</li>
<li>
<p><a href="https://github.com/docai-ai/docai/blob/main/src/document.clj"><strong>Processamento de Documentos (document.clj)</strong></a>: Responsável pela extração, limpeza e preparação de texto de diferentes formatos.</p>
</li>
<li>
<p><a href="https://github.com/docai-ai/docai/blob/main/src/advanced_rag.clj"><strong>Advanced RAG (advanced_rag.clj)</strong></a>:</p>
<ul>
<li>Cache em múltiplos níveis (embeddings e respostas)</li>
<li>Chunking dinâmico adaptado ao tipo de documento</li>
<li>Re-ranqueamento de resultados para melhorar precisão</li>
</ul>
</li>
<li>
<p><a href="https://github.com/docai-ai/docai/blob/main/src/agents.clj"><strong>Sistema de Agentes (agents.clj)</strong></a>:</p>
<ul>
<li>Análise de complexidade de consultas</li>
<li>Decomposição em sub-tarefas</li>
<li>Agentes especializados (busca, raciocínio, cálculo)</li>
<li>Verificação de qualidade das respostas</li>
<li>Síntese de resultados parciais</li>
</ul>
</li>
<li>
<p><a href="https://github.com/docai-ai/docai/blob/main/src/metrics.clj"><strong>Métricas (metrics.clj)</strong></a>: Monitoramento de desempenho e qualidade das respostas.</p>
</li>
</ol>
<p>O fluxo de processamento de consultas inicia em <code>core.clj</code>, que identifica se a consulta requer um pipeline RAG simples ou avançado com agentes:</p>


  <pre><code class="language-clojure">(defn query-advanced-rag
  &#34;Processa uma consulta usando o pipeline RAG avançado&#34;
  [query]
  (println &#34;DEBUG - Processando query com RAG avançado:&#34; query)
  (let [start-time (System/currentTimeMillis)
        ;; Verificar se a consulta precisa do workflow com agentes
        need-agents (agents/needs-agent-workflow? query)
        _ (when need-agents
            (println &#34;DEBUG - Consulta identificada como complexa, usando workflow com agentes&#34;))
        
        ;; Escolher o processamento adequado
        response (if need-agents
                   (agents/process-with-agents query)
                   (adv-rag/advanced-rag-query query))
        
        end-time (System/currentTimeMillis)
        latency (- end-time start-time)]
    
    ;; Registrar métricas
    (metrics/log-rag-interaction query [] response latency)
    
    response))</code></pre>
 <p>Para consultas simples, o pipeline <code>advanced-rag-query</code> realiza:</p>
<ol>
<li>Verificação de cache</li>
<li>Análise de complexidade da consulta</li>
<li>Busca semântica com chunking dinâmico</li>
<li>Formatação de prompt contextualizado</li>
<li>Geração de resposta com o LLM</li>
</ol>
<p>Para consultas complexas, o sistema de agentes em <code>agents.clj</code> entra em ação:</p>


  <pre><code class="language-clojure">(defn execute-agent-workflow
  &#34;Executa o workflow completo de agentes para uma consulta complexa&#34;
  [query]
  (let [;; Verificar cache primeiro
        cached (@agent-cache query)]
    (if cached
      cached
      (let [start-time (System/currentTimeMillis)
            
            ;; Analisar a consulta para determinar intenção e sub-questões
            analysis (analyze-query query)
            primary-intent (get-agent-type (:intent analysis))
            subtasks (or (:sub_questions analysis) [query])
            
            ;; Resultados parciais
            results (atom [])
            
            ;; Executar cada subtarefa em sequência
            _ (doseq [subtask subtasks]
                (let [agent-result (execute-subtask 
                                     subtask 
                                     primary-intent
                                     @results)]
                  (swap! results conj (:response agent-result))))
            
            ;; Gerar resposta final sintetizada
            synthesis-prompt (str &#34;Com base nas seguintes informações:\n\n&#34;
                                 (str/join &#34;\n\n&#34; @results)
                                 &#34;\n\nResponda à pergunta original de forma completa e coerente: &#34; query)
            
            initial-response (llm/call-ollama-api synthesis-prompt)
            
            ;; Obter contexto combinado para verificação
            combined-context (str/join &#34;\n\n&#34; @results)
            
            ;; Verificar a qualidade da resposta
            final-response (verify-response query combined-context initial-response)
            
            duration (- (System/currentTimeMillis) start-time)]
        
        ;; Registrar métricas e resultados
        final-response))))</code></pre>
 <p>O sistema de agentes implementa um workflow sofisticado para consultas complexas:</p>
<ol>
<li>Análise da consulta para identificar intenção e subtarefas</li>
<li>Execução de cada subtarefa com agentes especializados</li>
<li>Acumulação de resultados parciais</li>
<li>Síntese de uma resposta final coerente</li>
<li>Verificação da qualidade da resposta</li>
<li>Armazenamento em cache para consultas futuras</li>
</ol>
<h3 id="diferenciais-do-docai">Diferenciais do DocAI</h3>
<p>O DocAI se destaca por implementar várias técnicas avançadas de RAG em um sistema integrado e modular:</p>
<ul>
<li><strong>Chunking Adaptativo</strong>: Diferentes estratégias de chunking baseadas no tipo de documento:


  <pre><code class="language-clojure">(defn adaptive-chunking-strategy
  &#34;Determina estratégia de chunking com base no tipo de documento&#34;
  [document-type]
  (case document-type
    &#34;article&#34; {:chunk-size 1000 :chunk-overlap 150}
    &#34;code&#34; {:chunk-size 500 :chunk-overlap 50}
    &#34;legal&#34; {:chunk-size 1500 :chunk-overlap 200}
    &#34;qa&#34; {:chunk-size 800 :chunk-overlap 100}
    ;; Default
    {:chunk-size 1000 :chunk-overlap 100}))</code></pre>
 </li>
</ul>
<p>O sistema implementa estratégias de <a href="https://en.wikipedia.org/wiki/Chunking_%28data_storage%29">chunking adaptativas</a> que otimizam a segmentação de documentos conforme seu tipo específico. Esta abordagem reconhece que diferentes conteúdos possuem características únicas que afetam como devem ser divididos para processamento:</p>
<ul>
<li><strong>Artigos</strong>: Chunks maiores (1000 tokens) com sobreposição significativa (150 tokens), preservando o fluxo narrativo e argumentativo</li>
<li><strong>Código-fonte</strong>: Chunks menores (500 tokens) com sobreposição reduzida (50 tokens), respeitando a estrutura modular do código</li>
<li><strong>Documentos legais</strong>: Chunks extensos (1500 tokens) com alta sobreposição (200 tokens), mantendo intactas cláusulas e referências cruzadas</li>
<li><strong>Conteúdo Q&amp;A</strong>: Chunks de tamanho médio (800 tokens) com sobreposição moderada (100 tokens), preservando pares de perguntas e respostas</li>
</ul>
<p>Esta estratégia contextual melhora significativamente a qualidade da recuperação, garantindo que cada tipo de documento seja processado de forma otimizada para seu formato e densidade informacional específicos. A função <code>adaptive-chunking-strategy</code> demonstra uma implementação elegante deste conceito, utilizando pattern matching para selecionar parâmetros otimizados para cada categoria de documento.</p>
<p>Documentos legais, por exemplo, recebem chunks maiores (1500 tokens) devido à sua natureza densa e interconectada, enquanto documentos de perguntas e respostas utilizam uma configuração intermediária (800 tokens). Esta estratégia de chunking contextual melhora significativamente a qualidade da recuperação, garantindo que o contexto semântico seja preservado de forma apropriada para cada tipo específico de conteúdo.</p>
<ul>
<li><strong>Cache Multinível</strong>: Implementação de cache para embeddings e respostas, reduzindo latência e custos:


  <pre><code class="language-clojure">;; Cache para embeddings
(def embedding-cache (atom {}))
;; Cache para respostas
(def response-cache (atom {}))
;; Cache para resultados de agentes
(def agent-cache (atom {}))</code></pre>
 </li>
</ul>
<p>O sistema implementa uma estratégia de <a href="https://en.wikipedia.org/wiki/Cache_hierarchy">cache multinível</a> para otimizar o desempenho e reduzir custos operacionais. Utilizando estruturas de dados atômicas <a href="https://en.wikipedia.org/wiki/Atom_%28data_structure%29">(<code>atom</code>)</a>, o <a href="https://github.com/scovl/docai">DocAI</a> mantém três camadas distintas de cache: para embeddings, respostas completas e resultados de agentes. Esta abordagem permite reutilizar cálculos computacionalmente intensivos como a geração de embeddings, evitando processamento redundante de textos idênticos.</p>
<p>O cache de respostas armazena resultados finais para consultas frequentes, enquanto o cache de agentes preserva resultados intermediários de subtarefas específicas. Esta implementação reduz significativamente a latência do sistema, especialmente para consultas recorrentes, e diminui custos associados a chamadas de API para modelos externos. A estrutura atômica escolhida garante <a href="https://en.wikipedia.org/wiki/Thread_safety">thread-safety</a> em ambientes concorrentes, permitindo atualizações seguras do cache mesmo com múltiplas consultas simultâneas.</p>
<ul>
<li><strong>Verificação de Respostas</strong>: Sistema que avalia e melhora automaticamente as respostas:


  <pre><code class="language-clojure">(defn verify-response
  &#34;Usa um agente crítico para verificar e melhorar uma resposta&#34;
  [query context response]
  (let [prompt (str &#34;Avalie criticamente a seguinte resposta para a consulta do usuário. 
                    Verifique se a resposta é:\n&#34;
                    &#34;1. Fiel ao contexto fornecido\n&#34;
                    &#34;2. Completa (responde todos os aspectos da pergunta)\n&#34;
                    &#34;3. Precisa (não contém informações incorretas)\n\n&#34;
                    &#34;Consulta: &#34; query &#34;\n\n&#34;
                    &#34;Contexto: &#34; (if (&gt; (count context) 300) 
                                  (str (subs context 0 300) &#34;...&#34;) context) &#34;\n\n&#34;
                    &#34;Resposta: &#34; response &#34;\n\n&#34;
                    &#34;Se a resposta for adequada, apenas responda &#39;A resposta está correta&#39;. &#34;
                    &#34;Caso contrário, forneça uma versão melhorada.&#34;)
        verification (llm/call-ollama-api prompt)]

    (if (str/includes? verification &#34;A resposta está correta&#34;)
      response
      (let [improved-version (str/replace verification 
                                         #&#34;(?i).*?\b(a resposta melhorada seria:|versão melhorada:|resposta corrigida:|sugestão de resposta:|aqui está uma versão melhorada:)\s*&#34; 
                                         &#34;&#34;)]
        improved-version))))</code></pre>
 </li>
</ul>
<p>O código acima implementa um sistema de verificação e melhoria automática de respostas, um componente crítico em sistemas <a href="https://en.wikipedia.org/wiki/Retrieval-Augmented_Generation">RAG</a> avançados. A função <code>verify-response</code> atua como um &ldquo;agente crítico&rdquo; que avalia a qualidade das respostas geradas com base em três critérios fundamentais: fidelidade ao contexto fornecido, completude em relação à pergunta original e precisão factual. Este mecanismo de auto-verificação representa uma camada adicional de controle de qualidade que ajuda a mitigar alucinações e imprecisões comuns em sistemas baseados em LLMs.</p>
<p>A implementação utiliza uma abordagem elegante de <a href="https://en.wikipedia.org/wiki/Prompt_engineering">prompt engineering</a>, onde o sistema solicita explicitamente uma avaliação crítica da resposta original. O prompt estruturado inclui a consulta do usuário, um resumo do contexto (limitado a 300 caracteres para evitar sobrecarga) e a resposta gerada, orientando o modelo a realizar uma análise meticulosa. A função então analisa o resultado da verificação, mantendo a resposta original quando considerada adequada ou extraindo uma versão aprimorada quando necessário, utilizando expressões regulares para limpar metadados desnecessários da resposta melhorada.</p>
<p>Este mecanismo de verificação representa uma implementação prática do conceito de <a href="https://en.wikipedia.org/wiki/Constitutional_AI">Constitutional AI</a> ou &ldquo;AI com princípios orientadores&rdquo;, onde um sistema é projetado para avaliar criticamente suas próprias saídas. Ao incorporar esta camada de verificação no pipeline <a href="https://en.wikipedia.org/wiki/Retrieval-Augmented_Generation">RAG</a>, o <a href="https://github.com/scovl/docai">DocAI</a> consegue oferecer respostas mais confiáveis e precisas, reduzindo significativamente o risco de fornecer informações incorretas ou incompletas. Esta abordagem reflexiva é particularmente valiosa em domínios onde a precisão é crucial, como documentação técnica, informações médicas ou análises legais.</p>
<ul>
<li><strong>Métricas Detalhadas</strong>: Sistema de monitoramento que registra todos os aspectos das interações:


  <pre><code class="language-clojure">(metrics/log-rag-interaction query [] response latency)</code></pre>
 </li>
</ul>
<p>O código acima implementa um sistema de monitoramento que registra todos os aspectos das interações, incluindo a consulta do usuário, o tempo de resposta, e a resposta gerada. Este sistema permite acompanhar o desempenho do sistema ao longo do tempo e identificar possíveis problemas ou pontos de melhoria. Isso é essencial para manter o sistema funcionando de forma eficiente e para continuar evoluindo para novas funcionalidades.</p>
<p>Estas implementações demonstram como as técnicas avançadas de RAG discutidas neste artigo podem ser integradas em um sistema coeso, resultando em um assistente de documentação mais inteligente e eficiente.</p>
<h3 id="próximos-passos-para-o-docai">Próximos Passos para o DocAI</h3>
<p>Conforme detalhado no <code>plan.md</code>, o DocAI evoluirá para um sistema RAG Agêntico mais completo, implementando as seguintes melhorias:</p>
<ol>
<li>
<p><strong>Reescrita de Consultas</strong></p>
<ul>
<li>Módulo de reformulação para melhorar a precisão da busca</li>
<li>Expansão de consultas curtas e foco em consultas abrangentes</li>
</ul>
</li>
<li>
<p><strong>Seleção Dinâmica de Fontes</strong></p>
<ul>
<li>Workflow de agentes aprimorado para decidir quais fontes consultar</li>
<li>Integração com APIs externas e pesquisa web</li>
</ul>
</li>
<li>
<p><strong>Framework de Ferramentas para Agentes</strong></p>
<ul>
<li>Sistema de ferramentas para ações específicas</li>
<li>Executores de código, calculadoras e formatadores</li>
</ul>
</li>
<li>
<p><strong>Interface Multimodal</strong></p>
<ul>
<li>Processamento de imagens e geração de gráficos</li>
<li>Suporte a diversos formatos além de texto</li>
</ul>
</li>
</ol>
<p>Estas evoluções manterão a arquitetura modular e extensível do DocAI, permitindo adaptação a diferentes casos de uso e domínios de conhecimento.</p>
<h2 id="integração-com-o-ecossistema">Integração com o Ecossistema</h2>


  
  <div class="mermaid">flowchart TB
    subgraph &#34;Ecossistema DocAI&#34;
        direction TB
        
        DOCAI[Sistema DocAI] --- OLLAMA[Ollama]
        DOCAI --- POSTGRES[PostgreSQL &#43; pgvector]
        
        DOCAI --- API_GATE[API Gateway]
        API_GATE --- WEB_APP[Aplicação Web]
        API_GATE --- CLI[Interface CLI]
        
        DOCAI --- MONITORING[Sistema de Monitoramento]
        MONITORING --- DASHBOARD[Dashboard de Métricas]
        
        DOCAI -.-&gt; FUTURE_INT[Integrações Futuras]
        FUTURE_INT -.-&gt; EXT_API[APIs Externas]
        FUTURE_INT -.-&gt; SEARCH[Motores de Busca]
        FUTURE_INT -.-&gt; TOOLS[Ferramentas de Produtividade]
        
        style DOCAI fill:#f99,stroke:#333,stroke-width:3px
        style OLLAMA fill:#9f9,stroke:#333,stroke-width:2px
        style POSTGRES fill:#99f,stroke:#333,stroke-width:2px
        style FUTURE_INT fill:#ddd,stroke:#333,stroke-width:2px,stroke-dasharray: 5 5
    end</div>
 <p>O diagrama acima mostra como o DocAI se integra ao ecossistema mais amplo de ferramentas e serviços. No centro está o sistema <a href="https://github.com/docai-ai/docai">DocAI</a>, que se conecta diretamente com <a href="https://github.com/ollama/ollama">Ollama</a> para geração de texto e embeddings, e com <a href="https://www.postgresql.org/">PostgreSQL</a> (com <a href="https://github.com/pgvector/pgvector">pgvector</a>) para armazenamento e recuperação de dados vetoriais.</p>
<p>Para interação com usuários, o DocAI se conecta a um <a href="https://en.wikipedia.org/wiki/API_gateway">API Gateway</a> que fornece acesso tanto para uma aplicação web quanto para uma interface de linha de comando (CLI). Um sistema dedicado de monitoramento coleta métricas e as exibe em um dashboard para análise de desempenho.</p>
<p>As linhas tracejadas indicam integrações futuras planejadas, incluindo <a href="https://en.wikipedia.org/wiki/API">APIs externas</a> para busca de informações adicionais, <a href="https://en.wikipedia.org/wiki/Search_engine">motores de busca</a> para ampliar o alcance de recuperação, e <a href="https://en.wikipedia.org/wiki/Productivity">ferramentas de produtividade</a> para aumentar as capacidades do sistema.</p>
<p>Esta arquitetura modular permite que o DocAI se mantenha flexível e adaptável, podendo ser expandido conforme novos requisitos e oportunidades surgem, sempre mantendo seu núcleo robusto de funcionalidades RAG avançadas.</p>
<hr>
<h2 id="conclusão">Conclusão</h2>
<p>Transformar um sistema <a href="https://en.wikipedia.org/wiki/Retrieval-Augmented_Generation">RAG</a> de protótipo para produção requer mais do que apenas escolher as melhores ferramentas - exige uma compreensão profunda de cada componente e como eles trabalham juntos para produzir resultados confiáveis.</p>
<p>O projeto <a href="https://github.com/scovl/docai">DocAI</a> representa uma implementação robusta das técnicas avançadas de RAG discutidas neste artigo. Sua arquitetura modular, com componentes especializados em diferentes aspectos do processo (como Core, LLM, PostgreSQL, Sistema de Agentes e Métricas), demonstra a importância de um design bem estruturado para sistemas RAG em produção.</p>
<p>As técnicas que exploramos - desde re-ranqueamento e chunking dinâmico até workflows com agentes e monitoramento avançado - representam as práticas que separam implementações amadoras de sistemas robustos e prontos para uso em escala.</p>


  
  <div class="mermaid">flowchart LR
    subgraph &#34;Evolução do DocAI&#34;
    direction LR
    BASIC[RAG Básico com TF-IDF] --&gt; PGSQL[PostgreSQL &#43; Embeddings] --&gt; ADV[Sistema RAG Avançado] --&gt; AGT[Sistema RAG Agêntico]
    end
    
    style BASIC fill:#ddf,stroke:#333,stroke-width:2px
    style PGSQL fill:#fdf,stroke:#333,stroke-width:2px
    style ADV fill:#dfd,stroke:#333,stroke-width:2px
    style AGT fill:#ffd,stroke:#333,stroke-width:2px</div>
 <p>Nossa jornada com o <a href="https://github.com/scovl/docai">DocAI</a> evoluiu significativamente, de uma implementação básica com TF-IDF, passando por um sistema com PostgreSQL e embeddings, e agora para uma arquitetura avançada com agentes que pode lidar com casos de uso complexos do mundo real. O próximo passo, conforme detalhado no plano de evolução, será expandir ainda mais essas capacidades para criar um sistema RAG Agêntico completo.</p>
<p>O futuro dos sistemas de IA não está em modelos cada vez maiores, mas na combinação inteligente de componentes especializados que trabalham juntos para superar limitações individuais. O <a href="https://github.com/scovl/docai">DocAI</a> exemplifica esta abordagem, demonstrando como a integração de técnicas avançadas de RAG pode resultar em um sistema mais inteligente, preciso e útil para seus usuários.</p>
<hr>
<h2 id="referências">Referências</h2>
<ul>
<li><a href="/2025/03/25/semantic-postgresql/">Artigo anterior: Busca Semântica com Ollama e PostgreSQL</a> - Nossa implementação básica com PostgreSQL.</li>
<li><a href="https://openai.com/research/clip">CLIP - OpenAI</a> - Modelo para unificar visão e linguagem.</li>
<li><a href="https://towardsdatascience.com/a-comprehensive-guide-to-multimodal-rag-ea72c387c6e8">Comprehensive Guide to MultiModal RAG</a> - Guia detalhado para implementação de RAG multimodal.</li>
<li><a href="https://huggingface.co/cross-encoder">Cross-Encoders - Hugging Face</a> - Modelos para re-ranking em sistemas de recuperação.</li>
<li><a href="https://dailydoseofds.com">Daily Dose of Data Science: RAG Techniques</a> - Artigo sobre técnicas para otimizar sistemas RAG.</li>
<li><a href="https://github.com/timescale/pgai">Documentação do pgai</a> - Extensão do PostgreSQL para aplicações de IA.</li>
<li><a href="https://github.com/pgvector/pgvector">Documentação do pgvector</a> - Extensão do PostgreSQL para embeddings vetoriais.</li>
<li><a href="https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model">Flamingo - DeepMind</a> - Modelo visual de linguagem para tarefas multimodais.</li>
<li><a href="https://www.postgresql.org/docs/current/datatype-json.html">JSONB no PostgreSQL</a> - Documentação sobre o tipo de dados JSONB.</li>
<li><a href="https://python.langchain.com/docs/modules/agents/agent_types/multi_agent">LangChain - Multi-Agent Systems</a> - Implementação de sistemas multi-agentes.</li>
<li><a href="https://python.langchain.com/">LangChain</a> - Biblioteca para desenvolvimento de aplicações baseadas em LLM.</li>
<li><a href="https://docs.llamaindex.ai/en/stable/examples/agent/react_agent.html">LlamaIndex - Implementando ReAct Agents</a> - Guia para implementação de agentes ReAct.</li>
<li><a href="https://docs.llamaindex.ai/">LlamaIndex</a> - Framework para construir aplicações alimentadas por LLM.</li>
<li><a href="https://docs.llamaindex.ai/en/stable/examples/multi_modal/">MultiModal RAG com LlamaIndex</a> - Exemplos de implementação multimodal.</li>
<li><a href="https://ollama.com/">Ollama - Rodando LLMs localmente</a> - Ferramenta para executar LLMs localmente.</li>
<li><a href="https://www.postgresql.org/">PostgreSQL</a> - Sistema de gerenciamento de banco de dados relacional.</li>
<li><a href="https://github.com/scovl/docai">Projeto DocAI</a> - Repositório do projeto DocAI.</li>
</ul>
]]></content:encoded>
      
      
      <category>RAG,LLM,AI,Optimização,Produção,PostgreSQL,Ollama</category>
      
      
      
      <dc:creator>Vitor Lobo Ramos</dc:creator>
      
      
      
      
      
      <description>&lt;![CDATA[Explorando técnicas para otimizar sistemas RAG para uso em produção]]></description>
      
    </item>
    
    <item>
      <title></title>
      <link>https://scovl.github.io/page/about/</link>
      <guid>https://scovl.github.io/page/about/</guid>
      <pubDate>Mon, 28 Jul 2025 00:00:00 &#43;0000</pubDate>
      <description>&lt;![CDATA[<p>Sejam bem vindos ao meu blog!</p>
<p>Me chamo Vitor Lobo e meu nickname no github é scovl (uma brincadeira com a palavra Scoville scale e o meu nome), visto que gosto bastante de pimentas. Sou um engenheiro de software, escritor, gamer nas horas vagas e pesquisador independente. Trabalho com tecnologia há mais de 10 anos e sou apaixonado por arte e cultura, como literatura, cinema, música, quadrinhos, mangás, animes, viajar e conhecer novos lugares!</p>]]></description>
      <content:encoded>&lt;![CDATA[<p>Sejam bem vindos ao meu blog!</p>
<p>Me chamo Vitor Lobo e meu nickname no github é scovl (uma brincadeira com a palavra Scoville scale e o meu nome), visto que gosto bastante de pimentas. Sou um engenheiro de software, escritor, gamer nas horas vagas e pesquisador independente. Trabalho com tecnologia há mais de 10 anos e sou apaixonado por arte e cultura, como literatura, cinema, música, quadrinhos, mangás, animes, viajar e conhecer novos lugares!</p>
<h2 id="projetos">Projetos</h2>
<p>Minha trajetória na programação começou em 2009, quando passei a colaborar com projetos open source. Na época, eu era um usuário ativo do <a href="https://freenode.net/">IRC</a> na freenode, onde tive a oportunidade de conhecer diversos desenvolvedores que influenciaram profundamente minha carreira.</p>
<p>Contribuir com projetos open source foi fundamental para meu aprendizado em programação e, principalmente, para desenvolver habilidades de trabalho em comunidade. Essa experiência teve um impacto significativo no meu crescimento profissional. Abaixo, listo alguns dos projetos open source com os quais tenho colaborado:</p>
<ul>
<li><a href="https://github.com/rochacbruno/marmite">Marmite</a> - Marmite é gerador de site/blog estáticos escrito em Rust.</li>
<li><a href="https://github.com/scovl/checkrc">checkrc</a> - checkrc é um validador de configurações para o freeBSD.</li>
<li><a href="https://github.com/scovl/java-kubernetes">java-kubernetes</a> - java-kubernetes é um projeto da CNCF que tem como objetivo facilitar a configuração e implantação de aplicações java no kubernetes.</li>
<li><a href="https://github.com/scovl/pomodoro">pomodoro</a> - pomodoro é um aplicativo de gerenciamento de tempo escrito para o GNU Emacs.</li>
<li><a href="https://github.com/scovl/dogai">Dogai</a> - Sistema de Detecção de Objetos em Vídeo.</li>
<li><a href="https://github.com/scovl/docai">Doca</a> - Assistente RAG para Documentação Técnica</li>
<li><a href="https://github.com/scovl/saitama">Saitama</a> - Mata processos com um soco (de uma vez).</li>
</ul>
<hr>
<p><em>Obrigado por visitar meu blog! Espero que encontre conteúdo útil aqui.</em></p>
]]></content:encoded>
      
      
      
      
      
      
      
      
    </item>
    
    <item>
      <title></title>
      <link>https://scovl.github.io/page/contact/</link>
      <guid>https://scovl.github.io/page/contact/</guid>
      <pubDate>Mon, 28 Jul 2025 00:00:00 &#43;0000</pubDate>
      <description>&lt;![CDATA[<p>Entre em contato comigo através dos canais abaixo:</p>
<ul>
<li><strong>Email:</strong> <a href="mailto:lobocode@gmail.com">lobocode@gmail.com</a></li>
<li><strong>LinkedIn:</strong> <a href="https://linkedin.com/in/vitor-lobo">linkedin.com/in/vitor-lobo</a></li>
<li><strong>GitHub:</strong> <a href="https://github.com/scovl">github.com/scovl</a></li>
<li><strong>Mastodon:</strong> <a href="https://hachyderm.io/@lobocode">@lobocode</a></li>
</ul>]]></description>
      <content:encoded>&lt;![CDATA[<p>Entre em contato comigo através dos canais abaixo:</p>
<ul>
<li><strong>Email:</strong> <a href="mailto:lobocode@gmail.com">lobocode@gmail.com</a></li>
<li><strong>LinkedIn:</strong> <a href="https://linkedin.com/in/vitor-lobo">linkedin.com/in/vitor-lobo</a></li>
<li><strong>GitHub:</strong> <a href="https://github.com/scovl">github.com/scovl</a></li>
<li><strong>Mastodon:</strong> <a href="https://hachyderm.io/@lobocode">@lobocode</a></li>
</ul>
]]></content:encoded>
      
      
      
      
      
      
      
      
    </item>
    
    <item>
      <title>About</title>
      <link>https://scovl.github.io/en/page/about/</link>
      <guid>https://scovl.github.io/en/page/about/</guid>
      <pubDate>Mon, 15 Jan 2024 00:00:00 &#43;0000</pubDate>
      <description>&lt;![CDATA[<h1 id="about">About</h1>
<p>Hello! I&rsquo;m <strong>Vitor Lobo</strong>, a developer passionate about technology and programming.</p>
<h2 id="about-me">About Me</h2>
<ul>
<li>🚀 <strong>Full Stack Developer</strong></li>
<li>💻 <strong>JavaScript/TypeScript Specialist</strong></li>
<li>🌐 <strong>Modern Web Technologies Enthusiast</strong></li>
<li>📚 <strong>Always learning and sharing knowledge</strong></li>
</ul>
<h2 id="technologies">Technologies</h2>
<ul>
<li><strong>Frontend</strong>: React, Vue.js, Angular</li>
<li><strong>Backend</strong>: Node.js, Python, Go</li>
<li><strong>Database</strong>: PostgreSQL, MongoDB, Redis</li>
<li><strong>Cloud</strong>: AWS, Google Cloud, Azure</li>
<li><strong>DevOps</strong>: Docker, Kubernetes, CI/CD</li>
</ul>
<h2 id="contact">Contact</h2>
<ul>
<li>📧 <strong>Email</strong>: <a href="mailto:lobocode@gmail.com">lobocode@gmail.com</a></li>
<li>💼 <strong>LinkedIn</strong>: <a href="https://linkedin.com/in/vitor-lobo">vitor-lobo</a></li>
<li>🐙 <strong>GitHub</strong>: <a href="https://github.com/scovl">scovl</a></li>
<li>🐘 <strong>Mastodon</strong>: <a href="https://hachyderm.io/@lobocode">@lobocode</a></li>
</ul>
<h2 id="blog">Blog</h2>
<p>This blog is a space to share knowledge about:</p>]]></description>
      <content:encoded>&lt;![CDATA[<h1 id="about">About</h1>
<p>Hello! I&rsquo;m <strong>Vitor Lobo</strong>, a developer passionate about technology and programming.</p>
<h2 id="about-me">About Me</h2>
<ul>
<li>🚀 <strong>Full Stack Developer</strong></li>
<li>💻 <strong>JavaScript/TypeScript Specialist</strong></li>
<li>🌐 <strong>Modern Web Technologies Enthusiast</strong></li>
<li>📚 <strong>Always learning and sharing knowledge</strong></li>
</ul>
<h2 id="technologies">Technologies</h2>
<ul>
<li><strong>Frontend</strong>: React, Vue.js, Angular</li>
<li><strong>Backend</strong>: Node.js, Python, Go</li>
<li><strong>Database</strong>: PostgreSQL, MongoDB, Redis</li>
<li><strong>Cloud</strong>: AWS, Google Cloud, Azure</li>
<li><strong>DevOps</strong>: Docker, Kubernetes, CI/CD</li>
</ul>
<h2 id="contact">Contact</h2>
<ul>
<li>📧 <strong>Email</strong>: <a href="mailto:lobocode@gmail.com">lobocode@gmail.com</a></li>
<li>💼 <strong>LinkedIn</strong>: <a href="https://linkedin.com/in/vitor-lobo">vitor-lobo</a></li>
<li>🐙 <strong>GitHub</strong>: <a href="https://github.com/scovl">scovl</a></li>
<li>🐘 <strong>Mastodon</strong>: <a href="https://hachyderm.io/@lobocode">@lobocode</a></li>
</ul>
<h2 id="blog">Blog</h2>
<p>This blog is a space to share knowledge about:</p>
<ul>
<li><strong>Web Development</strong></li>
<li><strong>Modern Technologies</strong></li>
<li><strong>Best Practices</strong></li>
<li><strong>Tutorials and Tips</strong></li>
<li><strong>Tool Analysis</strong></li>
</ul>
<hr>
<p><em>Thank you for visiting my blog! I hope you find useful content here.</em></p>
]]></content:encoded>
      
      
      
      
      
      
      
      
    </item>
    
    <item>
      <title>Contact</title>
      <link>https://scovl.github.io/en/page/contact/</link>
      <guid>https://scovl.github.io/en/page/contact/</guid>
      <pubDate>Mon, 15 Jan 2024 00:00:00 &#43;0000</pubDate>
      <description>&lt;![CDATA[<h1 id="contact">Contact</h1>
<p>Get in touch with me through the channels below:</p>
<h2 id="-email">📧 Email</h2>
<p><strong><a href="mailto:lobocode@gmail.com">lobocode@gmail.com</a></strong></p>
<h2 id="-linkedin">💼 LinkedIn</h2>
<p><a href="https://linkedin.com/in/vitor-lobo">Vitor Lobo</a></p>
<h2 id="-github">🐙 GitHub</h2>
<p><a href="https://github.com/scovl">scovl</a></p>
<h2 id="-mastodon">🐘 Mastodon</h2>
<p><a href="https://hachyderm.io/@lobocode">@lobocode</a></p>
<h2 id="-social-media">📱 Social Media</h2>
<ul>
<li><strong>Twitter</strong>: <a href="https://twitter.com/lobocode">@lobocode</a></li>
<li><strong>Instagram</strong>: <a href="https://instagram.com/lobocode">@lobocode</a></li>
</ul>
<h2 id="-direct-message">💬 Direct Message</h2>
<p>Feel free to send a direct message through any of the channels above. I&rsquo;m always open to:</p>
<ul>
<li><strong>Collaborations</strong> on interesting projects</li>
<li><strong>Discussions</strong> about technology</li>
<li><strong>Mentoring</strong> for beginner developers</li>
<li><strong>Job opportunities</strong></li>
</ul>
<h2 id="-availability">⏰ Availability</h2>
<ul>
<li><strong>Response</strong>: Usually respond within 24 hours</li>
<li><strong>Hours</strong>: Monday to Friday, 9am to 6pm (BRT)</li>
<li><strong>Languages</strong>: Portuguese and English</li>
</ul>
<hr>
<p><em>Thank you for your interest in getting in touch!</em></p>]]></description>
      <content:encoded>&lt;![CDATA[<h1 id="contact">Contact</h1>
<p>Get in touch with me through the channels below:</p>
<h2 id="-email">📧 Email</h2>
<p><strong><a href="mailto:lobocode@gmail.com">lobocode@gmail.com</a></strong></p>
<h2 id="-linkedin">💼 LinkedIn</h2>
<p><a href="https://linkedin.com/in/vitor-lobo">Vitor Lobo</a></p>
<h2 id="-github">🐙 GitHub</h2>
<p><a href="https://github.com/scovl">scovl</a></p>
<h2 id="-mastodon">🐘 Mastodon</h2>
<p><a href="https://hachyderm.io/@lobocode">@lobocode</a></p>
<h2 id="-social-media">📱 Social Media</h2>
<ul>
<li><strong>Twitter</strong>: <a href="https://twitter.com/lobocode">@lobocode</a></li>
<li><strong>Instagram</strong>: <a href="https://instagram.com/lobocode">@lobocode</a></li>
</ul>
<h2 id="-direct-message">💬 Direct Message</h2>
<p>Feel free to send a direct message through any of the channels above. I&rsquo;m always open to:</p>
<ul>
<li><strong>Collaborations</strong> on interesting projects</li>
<li><strong>Discussions</strong> about technology</li>
<li><strong>Mentoring</strong> for beginner developers</li>
<li><strong>Job opportunities</strong></li>
</ul>
<h2 id="-availability">⏰ Availability</h2>
<ul>
<li><strong>Response</strong>: Usually respond within 24 hours</li>
<li><strong>Hours</strong>: Monday to Friday, 9am to 6pm (BRT)</li>
<li><strong>Languages</strong>: Portuguese and English</li>
</ul>
<hr>
<p><em>Thank you for your interest in getting in touch!</em></p>
]]></content:encoded>
      
      
      
      
      
      
      
      
    </item>
    
    <item>
      <title></title>
      <link>https://scovl.github.io/1/01/01/jsast/</link>
      <guid>https://scovl.github.io/1/01/01/jsast/</guid>
      <pubDate>Mon, 01 Jan 0001 00:00:00 &#43;0000</pubDate>
      <description>&lt;![CDATA[<h2 id="desvendando-asts-com-javascript-typescript-e-esprima-um-guia-amigável-">Desvendando ASTs com JavaScript, TypeScript e Esprima: Um Guia Amigável 🤖</h2>
<p><em>Tempo de Leitura: uns 15-25 minutinhos (ou um café ☕)</em></p>
<p><strong>O que você vai encontrar aqui:</strong></p>
<ul>
<li>Introdução (Sem formalidades!)</li>
<li>ASTs: O Que Raios é Isso?
<ul>
<li>Traduzindo: O que é uma AST?</li>
<li>Pra que serve essa &ldquo;árvore&rdquo;?</li>
<li>Os 3 Passos Mágicos da AST</li>
<li>Por que começar com o Esprima?</li>
</ul>
</li>
<li>Mão na Massa: Bora Codar!
<ul>
<li>Preparando o Terreno (Instalação)</li>
<li>Nosso Projetinho Simples</li>
<li>&ldquo;Parseando&rdquo;: Transformando Código em AST</li>
<li>&ldquo;Traversando&rdquo;: Dando um Rolê na AST</li>
<li>Analisando: Catando Informações Úteis</li>
</ul>
</li>
<li>Como Rodar Isso Aí
<ul>
<li>Instalando o Esprima (Moleza!)</li>
<li>Exemplo Básico pra Sentir o Gostinho</li>
<li>Botando pra Funcionar</li>
</ul>
</li>
<li>Detalhes Importantes (Pra Ficar Ligado!)
<ul>
<li>Performance: Roda Liso?</li>
<li>Entendendo as Peças do Quebra-Cabeça (ESTree)</li>
<li>E se o Código Tiver Erro?</li>
</ul>
</li>
<li>Próximos Níveis (O que mais dá pra fazer?)
<ul>
<li>Turbinando a Brincadeira</li>
<li>Outras Ferramentas na Caixa</li>
</ul>
</li>
<li>Onde Achar Mais Info (Links Úteis)</li>
</ul>
<p>Olá pessoal! 👋</p>]]></description>
      <content:encoded>&lt;![CDATA[<h2 id="desvendando-asts-com-javascript-typescript-e-esprima-um-guia-amigável-">Desvendando ASTs com JavaScript, TypeScript e Esprima: Um Guia Amigável 🤖</h2>
<p><em>Tempo de Leitura: uns 15-25 minutinhos (ou um café ☕)</em></p>
<p><strong>O que você vai encontrar aqui:</strong></p>
<ul>
<li>Introdução (Sem formalidades!)</li>
<li>ASTs: O Que Raios é Isso?
<ul>
<li>Traduzindo: O que é uma AST?</li>
<li>Pra que serve essa &ldquo;árvore&rdquo;?</li>
<li>Os 3 Passos Mágicos da AST</li>
<li>Por que começar com o Esprima?</li>
</ul>
</li>
<li>Mão na Massa: Bora Codar!
<ul>
<li>Preparando o Terreno (Instalação)</li>
<li>Nosso Projetinho Simples</li>
<li>&ldquo;Parseando&rdquo;: Transformando Código em AST</li>
<li>&ldquo;Traversando&rdquo;: Dando um Rolê na AST</li>
<li>Analisando: Catando Informações Úteis</li>
</ul>
</li>
<li>Como Rodar Isso Aí
<ul>
<li>Instalando o Esprima (Moleza!)</li>
<li>Exemplo Básico pra Sentir o Gostinho</li>
<li>Botando pra Funcionar</li>
</ul>
</li>
<li>Detalhes Importantes (Pra Ficar Ligado!)
<ul>
<li>Performance: Roda Liso?</li>
<li>Entendendo as Peças do Quebra-Cabeça (ESTree)</li>
<li>E se o Código Tiver Erro?</li>
</ul>
</li>
<li>Próximos Níveis (O que mais dá pra fazer?)
<ul>
<li>Turbinando a Brincadeira</li>
<li>Outras Ferramentas na Caixa</li>
</ul>
</li>
<li>Onde Achar Mais Info (Links Úteis)</li>
</ul>
<p>Olá pessoal! 👋</p>
<p>Neste artigo, vou desmistificar um pouco como navegar através de uma AST usando o Esprima em Javascript/Typescript. Desta maneira você pode manipular código evitando ao máximo usar expressões regulares (que em muitos casos são um pesadelo). Mas vamos começar pelo começo, o que é uma AST? Uma AST é uma representação abstrata da estrutura sintática de um programa. Ela é uma árvore de nós que representa a hierarquia e a relação entre as partes do código.</p>
<p>Isto é muito útil para uma diversidade de aplicações como:</p>
<ul>
<li>Análise de código - como o ESLint</li>
<li>Transformação de código - como o Babel</li>
<li>Verificação de padrões - como o Prettier</li>
</ul>
<p>E muito mais!</p>
<p>Abstraíndo um pouco o conceito, imagina que seu código é uma receita de bolo. A AST é tipo um <strong>diagrama ou um mapa mental</strong> dessa receita. Ela pega o texto puro do código e organiza ele numa estrutura de árvore, mostrando como cada pedacinho se conecta. Ela ignora coisas como espaços extras ou comentários (na maioria das vezes) e foca no que realmente importa: a estrutura<strong>lógica</strong> do código. Exemplo:</p>


  <pre><code class="language-javascript">const PI = 3.14;</code></pre>
 <p><strong>AST (Versão Super Simplificada):</strong></p>


  <pre><code class="language-json">- Program {
    type: &#34;Program&#34;,
    start: 0,
    end: 16,
    body: [
        - VariableDeclaration {
            type: &#34;VariableDeclaration&#34;,
            start: 0,
            end: 16,
            declarations: [
                - VariableDeclarator {
                    type: &#34;VariableDeclarator&#34;,
                    start: 6,
                    end: 15,
                    id: Identifier {
                        type: &#34;Identifier&#34;,
                        start: 6,
                        end: 8,
                        name: &#34;PI&#34;
                    },
                    init: Literal {
                        type: &#34;Literal&#34;,
                        start: 11,
                        end: 15,
                        value: 3.14,
                        raw: &#34;3.14&#34;
                    }
                }
            ]
            kind: &#34;const&#34;
        }
    ]
    sourceType: &#34;script&#34;
}

Viu só? Cada parte do código (o `const`, o nome `PI`, o número `3.14`) vira um **&#34;nó&#34;** nessa árvore. Cada nó tem um `type` dizendo o que ele é (`VariableDeclaration`, `Identifier`, `Literal`) e outras informações pra dar mais detalhes. É basicamente **código falando sobre código**!

Você pode estar se perguntando: Blz, mas pra que serve essa &#34;árvore&#34;? ASTs são o coração de muitas ferramentas que usamos:

*   **Linters (ESLint):** Ele &#34;lê&#34; a AST (o mapa do código) pra ver se você seguiu as regras de estilo ou se tem algum erro bobo ali, *sem precisar rodar o código*.
*   **Transpilers (Babel):** Quer usar código JavaScript moderno que o navegador antigo não entende? O Babel olha a AST, &#34;reescreve&#34; as partes modernas de um jeito mais antigo, e depois gera o código JS compatível. Pura mágica da AST!
*   **Bundlers (Webpack, Rollup):** Eles olham os `import` e `export` na AST pra entender quais arquivos dependem de quais e juntar tudo num pacote só.
*   **Formatadores (Prettier):** Ele não liga pro seu estilo, ele olha a AST (a estrutura lógica) e reescreve o código do jeito *dele*, todo formatadinho.
*   **Refatoração em IDEs:** Sabe quando você renomeia uma variável e a IDE magicamente atualiza em todos os lugares? Adivinha? AST em ação!

&gt; **Resumindo:** É muito mais fácil pra um programa analisar ou modificar outro programa usando a AST do que tentando entender a string de texto puro. É o jeito inteligente de fazer as coisas! 😉

#### Os 3 Passos Mágicos da AST

Geralmente, trabalhar com AST envolve 3 etapas:

**AST: O Fluxo**

Código (Texto) ➡️ **1. Parsing** ➡️ AST (Mapa) ➡️ **2. Traversal** ➡️ Visita aos Nós ➡️ **3. Análise/Transformação** ➡️ Info Útil / Código Novo

1.  **Parsing (Tradução):** É pegar o textão do código e transformar ele na estrutura de árvore (a AST). Quem faz isso é um carinha chamado **Parser** (tipo o Esprima). Ele verifica se o código tá certinho (sintaxe) e monta o mapa.
2.  **Traversal (Passeio):** Com o mapa (AST) pronto, a gente precisa &#34;andar&#34; por ele pra visitar os nós (as partes do código). O jeito comum é usar o **Visitor Pattern**: você define funções tipo &#34;ei, quando encontrar um nó do tipo `FunctionDeclaration`, faça isso aqui!&#34;. É como ter um guia turístico pra cada tipo de lugar no mapa.
3.  **Análise/Transformação (Ação):** Enquanto passeia pelos nós, você pode fazer coisas:
    *   **Análise:** Só olhar e coletar informações (Ex: contar quantas funções tem, achar todos os `console.log`).
    *   **Transformação:** Mudar a própria AST (Ex: renomear uma variável, trocar um nó por outro). Aí depois você pode gerar código novo a partir da AST modificada.


### Mão na Massa: Bora Codar!

#### Preparando o Terreno (Instalação)

Você vai precisar do **Node.js** instalado (com npm ou yarn).

No terminal, dentro da pasta do seu projeto, manda bala:

```bash
# Com npm
npm install esprima
npm install --save-dev @types/esprima @types/estree # Se for usar TypeScript

# Ou com yarn
yarn add esprima
yarn add --dev @types/esprima @types/estree # Se for usar TypeScript</code></pre>
 <p><em>Dica:</em> <code>@types/estree</code> são as definições de tipo pro padrão ESTree, super útil em TS!</p>
<p>Moleza, né? 😉</p>
<h4 id="nosso-projetinho-simples">Nosso Projetinho Simples</h4>
<p>Cria uma pasta e um arquivo <code>index.js</code> (ou <code>index.ts</code>) dentro dela. Algo tipo:</p>


  <pre><code class="language-">meu-projeto-ast/
├── node_modules/
├── index.js   # Ou index.ts
└── package.json</code></pre>
 <h4 id="parseando-transformando-código-em-ast">&ldquo;Parseando&rdquo;: Transformando Código em AST</h4>
<p>A principal função do Esprima é a <code>parseScript</code> (pra código JS normal) ou <code>parseModule</code> (se tiver <code>import</code>/<code>export</code>).</p>
<p><strong>Exemplo em JavaScript (<code>index.js</code>):</strong></p>


  <pre><code class="language-javascript">const esprima = require(&#39;esprima&#39;);

const codigo = &#39;const ANO = 2024; console.log(&#34;Olá, AST!&#34;);&#39;;

try {
  // A mágica acontece aqui!
  const ast = esprima.parseScript(codigo, {
    loc: true, // Quero saber a linha/coluna de cada nó
    range: true // Quero saber o índice de início/fim no texto original
  });

  // Imprime a AST toda bonitona (é um objeto gigante!)
  console.log(&#34;AST Gerada:&#34;);
  console.log(JSON.stringify(ast, null, 2));

} catch (e) {
  // Se der erro de sintaxe no código, ele cai aqui
  console.error(&#34;Eita, deu erro no parsing:&#34;, e.description);
  console.error(` &gt;&gt; Na linha ${e.lineNumber}, coluna ${e.column}`);
}</code></pre>
 <p><strong>Exemplo em TypeScript (<code>index.ts</code>):</strong></p>


  <pre><code class="language-typescript">import * as esprima from &#39;esprima&#39;;
import { Program, Node } from &#39;estree&#39;; // Tipos pra deixar o TS feliz

const codigo: string = &#39;let message = &#34;TypeScript &#43; AST = ❤️&#34;;&#39;;

try {
  const ast: Program = esprima.parseScript(codigo, {
    loc: true,
    range: true,
    tokens: true // Opcional: Me dá uma lista de todos os &#34;pedaços&#34; (palavras-chave, nomes, etc.)
  });

  console.log(&#34;AST Gerada (TS):&#34;);
  console.log(JSON.stringify(ast, null, 2));

} catch (e: any) { // Captura o erro
  console.error(&#34;Ops, erro no parsing (TS):&#34;, e.description);
  console.error(` &gt;&gt; Na linha ${e.lineNumber}, coluna ${e.column}`);
}</code></pre>
 <ul>
<li><strong><code>parseScript</code> vs <code>parseModule</code>:</strong> Lembra: <code>parseModule</code> se tiver <code>import</code>/<code>export</code>.</li>
<li><strong>Opções úteis:</strong>
<ul>
<li><code>loc</code>/<code>range</code>: Pra saber <em>onde</em> cada parte da AST está no código original (ótimo pra mostrar erros!).</li>
<li><code>tokens</code>: Te dá uma lista de todos os &ldquo;tokens&rdquo; (tipo <code>const</code>, <code>ANO</code>, <code>=</code>, <code>2024</code>, <code>;</code>). Útil pra algumas análises, mas gasta mais memória.</li>
<li><code>comment</code>: Pra incluir os comentários na AST.</li>
<li><code>jsx</code>: Se tiver código React/JSX.</li>
</ul>
</li>
</ul>
<p>Roda isso e você vai ver a estrutura da AST impressa! É um JSONzão, mas ali tá todo o seu código organizado.</p>
<h4 id="traversando-dando-um-rolê-na-ast">&ldquo;Traversando&rdquo;: Dando um Rolê na AST</h4>
<p>Beleza, temos o mapa (AST). Como a gente &ldquo;anda&rdquo; por ele pra ver o que tem em cada lugar? Podemos fazer isso com uma função recursiva simples, ou usar bibliotecas prontas (como <code>estraverse</code>).</p>
<p>Vamos criar nossa função de &ldquo;passeio&rdquo; (um Visitor Pattern bem simples):</p>
<p><strong>JavaScript (<code>index.js</code> - continuação):</strong></p>


  <pre><code class="language-javascript">// ... (código do parsing ali em cima) ...

// Função pra &#34;passear&#34; na árvore
function traverse(node, visitor) {
  // 1. Visita o nó atual: Se o visitor tiver algo pra esse tipo de nó, chama!
  if (visitor[node.type]) {
    visitor[node.type](node);
  }

  // 2. Visita os filhos: Olha todas as propriedades do nó
  for (const key in node) {
    if (node.hasOwnProperty(key)) {
      const child = node[key];
      // Se for um objeto ou array...
      if (typeof child === &#39;object&#39; &amp;&amp; child !== null) {
        // Se for um array de nós, visita cada um
        if (Array.isArray(child)) {
          child.forEach(subChild =&gt; {
            if (subChild &amp;&amp; subChild.type) { // Garante que é um nó AST válido
              traverse(subChild, visitor);
            }
          });
        }
        // Se for um único nó filho, visita ele
        else if (child.type) {
          traverse(child, visitor);
        }
      }
    }
  }
}

// Nosso &#34;guia turístico&#34;: o que fazer quando encontrar cada tipo de nó
const meuVisitor = {
  // Quando achar uma declaração de função...
  FunctionDeclaration(node) {
    console.log(`\n==&gt; Achei uma função! Nome: ${node.id.name}, Linha: ${node.loc.start.line}`);
  },
  // Quando achar uma chamada de função...
  CallExpression(node) {
    // Verifica se tá chamando direto um nome (tipo console.log)
    if (node.callee.type === &#39;Identifier&#39;) {
      console.log(`\n==&gt; Opa, chamando a função: ${node.callee.name}()`);
    }
  }
  // Poderia adicionar mais: &#39;IfStatement&#39;, &#39;ForStatement&#39;, etc.
};

// Só roda a travessia se o parsing deu certo
if (typeof ast !== &#39;undefined&#39;) {
  console.log(&#34;\nBora passear pela AST e analisar...&#34;);
  traverse(ast, meuVisitor);
}</code></pre>
 <p><strong>TypeScript (<code>index.ts</code> - continuação):</strong></p>


  <pre><code class="language-typescript">// ... (código do parsing ali em cima) ...

// Interface pro nosso Visitor (pra ajudar o TS)
interface Visitor {
  [nodeType: string]: (node: Node) =&gt; void; // Aceita qualquer tipo de nó do ESTree
}

// Função traverse (igual a de JS, mas com um pouco de tipagem)
function traverse(node: Node, visitor: Visitor): void {
  // ... (lógica igual à da versão JS) ...
  if (visitor[node.type]) {
    visitor[node.type](node);
  }
  for (const key in node) {
    // ... (restante da lógica recursiva) ...
  }
}


const meuVisitorTS: Visitor = {
  VariableDeclarator(node: any) { // Usando &#39;any&#39; pra simplificar o acesso às props
    console.log(`\n==&gt; Variável declarada (TS)! Nome: ${node.id.name}, Linha: ${node.loc?.start.line}`);
  },
  Literal(node: any) {
    if (typeof node.value === &#39;string&#39;) {
      console.log(`\n==&gt; Achei um texto (string literal): &#34;${node.value}&#34;`);
    }
  }
};

declare const ast: Program | undefined; // Avisa pro TS que &#39;ast&#39; existe

if (ast) { // Verifica se ast não é undefined
  console.log(&#34;\nPasseando pela AST (TS)...&#34;);
  traverse(ast, meuVisitorTS);
}</code></pre>
 <p>Essa função <code>traverse</code> é bem básica. Bibliotecas como <code>estraverse</code> são tipo um &ldquo;GPS mais chique&rdquo;, te dão mais controle (tipo avisar quando <em>entra</em> e quando <em>sai</em> de um nó).</p>
<h4 id="analisando-catando-informações-úteis">Analisando: Catando Informações Úteis</h4>
<p>A &ldquo;análise&rdquo; acontece dentro das funções que a gente colocou no <code>visitor</code>. Viu ali no <code>meuVisitor</code>? Quando ele encontra um nó <code>FunctionDeclaration</code>, ele imprime o nome e a linha. Quando acha um <code>CallExpression</code>, imprime o nome da função chamada.</p>
<p>É aí que a mágica acontece! Você pode criar visitors pra:</p>
<ul>
<li>Pegar todos os nomes de variáveis.</li>
<li>Verificar se alguém usou <code>eval</code> (geralmente não é legal!).</li>
<li>Contar quantas vezes <code>console.log</code> foi chamado.</li>
<li>Achar todos os links (<code>&lt;a&gt;</code> em JSX, por exemplo).</li>
<li>Medir a complexidade do código (contando <code>if</code>, <code>for</code>, etc.).</li>
<li>O céu é o limite! 🚀</li>
</ul>
<h3 id="como-rodar-isso-aí">Como Rodar Isso Aí</h3>
<h4 id="instalando-o-esprima-moleza">Instalando o Esprima (Moleza!)</h4>
<p>Já fizemos lá em cima, né? Só garantir que o Node.js tá aí e rodar <code>npm install esprima</code> ou <code>yarn add esprima</code>.</p>
<h4 id="exemplo-básico-pra-sentir-o-gostinho">Exemplo Básico pra Sentir o Gostinho</h4>
<p>Pega o código completo (parsing + traverse + visitor) que montamos acima e salva num arquivo <code>index.js</code> ou <code>index.ts</code>.</p>
<p><strong>Exemplo Completo Simples (JS - pra facilitar o copiar/colar):</strong></p>


  <pre><code class="language-javascript">// index.js
const esprima = require(&#39;esprima&#39;);

// Nosso código de exemplo
const codigo = `
function calcularArea(largura, altura) {
  // Função simples
  if (largura &lt;= 0 || altura &lt;= 0) {
    return null; // Não calcula área inválida
  }
  const area = largura * altura;
  console.log(&#34;Área calculada:&#34;, area);
  return area;
}

let resultado = calcularArea(10, 5);
let nome = &#34;AST Explorer&#34;; // Uma string literal
`;

// Função pra &#34;passear&#34; na árvore (copie daqui se precisar)
function traverse(node, visitor) {
  if (!node) return; // Segurança extra
  if (visitor[node.type]) {
    visitor[node.type](node);
  }
  for (const key in node) {
    if (node.hasOwnProperty(key)) {
      const child = node[key];
      if (typeof child === &#39;object&#39; &amp;&amp; child !== null) {
        if (Array.isArray(child)) {
          child.forEach(subChild =&gt; {
            if (subChild &amp;&amp; subChild.type) { traverse(subChild, visitor); }
          });
        } else if (child.type) {
          traverse(child, visitor);
        }
      }
    }
  }
}


// Nosso &#34;guia turístico&#34;
const meuVisitor = {
  FunctionDeclaration(node) {
    console.log(`\n[INFO] Função encontrada: &#39;${node.id.name}&#39; com ${node.params.length} params. Linha: ${node.loc.start.line}`);
  },
  VariableDeclarator(node) {
    console.log(`[INFO] Var declarada: &#39;${node.id.name}&#39;. Tipo: ${node.kind || &#39;var/let/const&#39;}.`); // kind só em VariableDeclaration
  },
  CallExpression(node) {
    if (node.callee.type === &#39;Identifier&#39;) {
      console.log(`[INFO] Chamada de função: ${node.callee.name}(). Linha: ${node.loc.start.line}`);
    } else if (node.callee.type === &#39;MemberExpression&#39;) { // tipo console.log
        if (node.callee.object.type === &#39;Identifier&#39; &amp;&amp; node.callee.property.type === &#39;Identifier&#39;) {
             console.log(`[INFO] Chamada de método: ${node.callee.object.name}.${node.callee.property.name}(). Linha: ${node.loc.start.line}`);
        }
    }
  },
  IfStatement(node) {
    console.log(`[INFO] Encontrado um &#39;if&#39;. Linha: ${node.loc.start.line}`);
  },
  Literal(node) {
      if(typeof node.value === &#39;string&#39; &amp;&amp; node.value.length &gt; 0) {
        console.log(`[INFO] String encontrada: &#34;${node.value}&#34;. Linha: ${node.loc.start.line}`);
      }
  }
};

// --- Roda Tudo ---
try {
  console.log(&#34;--- Analisando o Código ---&#34;);
  const ast = esprima.parseScript(codigo, { loc: true, range: true });

  // Descomente pra ver a ASTzona completa:
  // console.log(&#34;\n--- AST Completa ---&#34;);
  // console.log(JSON.stringify(ast, null, 2));

  console.log(&#34;\n--- Iniciando Análise com Visitor ---&#34;);
  traverse(ast, meuVisitor);
  console.log(&#34;\n--- Análise Concluída ---&#34;);

} catch (e) {
  console.error(&#34;\n--- ERRO ---&#34;);
  console.error(&#34;Deu ruim no parsing:&#34;, e.description);
  console.error(`Local: Linha ${e.lineNumber}, Coluna ${e.column}`);
}</code></pre>
 <h4 id="botando-pra-funcionar">Botando pra Funcionar</h4>
<p>Abre o terminal na pasta do projeto e manda ver:</p>


  <pre><code class="language-bash"># Se for JavaScript
node index.js

# Se for TypeScript (precisa do ts-node ou compilar antes)
# Instala globalmente (se não tiver): npm install -g ts-node
ts-node index.ts
# Ou compila e roda:
# tsc index.ts
# node index.js</code></pre>
 <p>E pronto! Você vai ver a saída da nossa &ldquo;análise&rdquo; no console, mostrando as funções, variáveis, chamadas e o que mais a gente pediu pro visitor procurar. Legal, né? 😎</p>
<h3 id="detalhes-importantes-pra-ficar-ligado">Detalhes Importantes (Pra Ficar Ligado!)</h3>
<h4 id="performance-roda-liso">Performance: Roda Liso?</h4>
<ul>
<li><strong>Arquivos Gigantes:</strong> Parsear arquivos JS muito, muito grandes pode consumir bastante memória e processador. O Esprima é rápido, mas pra projetos gigantescos, pode ser um ponto a otimizar.</li>
<li><strong>Opções Ligadas:</strong> Ligar opções como <code>tokens</code>, <code>loc</code>, <code>range</code>, <code>comment</code> deixa o processo um pouco mais lento e a AST maior. Só ligue se for usar mesmo.</li>
<li><strong>Recuperação de Erros:</strong> O Esprima padrão para no primeiro erro de sintaxe. Ferramentas mais avançadas tentam continuar analisando mesmo com erros, mas isso é bem mais complexo.</li>
</ul>
<h4 id="entendendo-as-peças-do-quebra-cabeça-estree">Entendendo as Peças do Quebra-Cabeça (ESTree)</h4>
<p>O segredo pra explorar ASTs é sacar o <strong>ESTree</strong>. É ele que define todos os tipos de nós que podem aparecer (<code>Identifier</code>, <code>Literal</code>, <code>IfStatement</code>, <code>ForStatement</code>, etc.) e o que cada um tem dentro.</p>
<p><strong>Como analisar um nó:</strong></p>
<p>Seu Visitor pega um <code>node</code> ➡️ Olha o <code>node.type</code> ➡️ Sabendo o tipo, você sabe quais propriedades procurar (ex: um <code>IfStatement</code> tem <code>test</code>, <code>consequent</code>, <code>alternate</code>) ➡️ Pega a informação que você quer!</p>
<ul>
<li><strong>Exemplo: <code>IfStatement</code> (o nó do <code>if</code>)</strong>
<ul>
<li><code>type</code>: &ldquo;IfStatement&rdquo;</li>
<li><code>test</code>: É a condição dentro do <code>if (...)</code>. Geralmente outro nó, tipo uma comparação (<code>BinaryExpression</code>).</li>
<li><code>consequent</code>: É o bloco de código <code>{...}</code> que roda se a condição for verdadeira. Geralmente um <code>BlockStatement</code>.</li>
<li><code>alternate</code>: É o bloco do <code>else</code> (ou <code>else if</code>). Pode ser outro <code>IfStatement</code>, um <code>BlockStatement</code> ou <code>null</code> se não tiver <code>else</code>.</li>
</ul>
</li>
</ul>
<p>Vale muito a pena dar uma olhada na documentação do ESTree (link lá no final) e brincar no <a href="https://astexplorer.net/">astexplorer.net</a> pra ver como diferentes códigos viram ASTs.</p>
<h4 id="e-se-o-código-tiver-erro">E se o Código Tiver Erro?</h4>
<p>Se você tentar parsear um código com erro de sintaxe (tipo esqueceu uma vírgula), o Esprima vai dar pau e jogar um erro (Exception). Por isso é <strong>fundamental</strong> colocar o <code>esprima.parseScript(...)</code> dentro de um bloco <code>try...catch</code>.</p>


  <pre><code class="language-javascript">try {
  const ast = esprima.parseScript(&#34;let x = oops&#34;); // Erro aqui!
} catch (e) {
  // O &#39;e&#39; tem infos úteis!
  console.error(&#34;Deu erro de sintaxe!&#34;);
  console.error(&#34;Mensagem:&#34;, e.description);
  console.error(&#34;Onde:&#34;, `Linha ${e.lineNumber}, Coluna ${e.column}`);
}</code></pre>
 <p>Assim seu programa não quebra inteiro e você pode tratar o erro direitinho.</p>
<h3 id="próximos-níveis-o-que-mais-dá-pra-fazer">Próximos Níveis (O que mais dá pra fazer?)</h3>
<p>Curtiu a brincadeira? Dá pra ir muito além!</p>
<h4 id="turbinando-a-brincadeira">Turbinando a Brincadeira</h4>
<ul>
<li><strong>Passeio Turbinado:</strong> Dá uma olhada na biblioteca <code>estraverse</code>. Ela te dá mais controle sobre o passeio na AST.</li>
<li><strong>Análises Mais Ninjas:</strong>
<ul>
<li>Achar variáveis que nunca são usadas.</li>
<li>Calcular a &ldquo;complexidade&rdquo; de uma função (quantos <code>if</code>s, <code>for</code>s aninhados?).</li>
<li>Mapear quem chama quem no seu código.</li>
</ul>
</li>
<li><strong>Brincar de Transformar:</strong> Modifica a AST (com cuidado!) e usa uma lib tipo <code>escodegen</code> pra gerar o código JS de volta a partir da AST modificada. Imagina renomear todas as variáveis <code>i</code> de um loop pra <code>index</code> automaticamente!</li>
<li><strong>Falar TypeScript de Verdade:</strong> Pra analisar código TS <em>com tipos</em>, o Esprima não serve. Aí você teria que usar a API do próprio compilador TypeScript (<code>tsc</code>) ou ferramentas como <code>typescript-eslint-parser</code>. É mais complexo, mas te dá acesso aos tipos!</li>
</ul>
<h4 id="outras-ferramentas-na-caixa">Outras Ferramentas na Caixa</h4>
<ul>
<li><strong>Acorn:</strong> Parser JS moderno e rápido, base do Babel. Tem plugins! É tipo o Esprima anabolizado.</li>
<li><strong>Babel (@babel/parser):</strong> O parser do Babel. Entende de tudo, até das features mais novas do JS e JSX. Se você já usa Babel, pode usar o parser dele.</li>
<li><strong>TypeScript Compiler API:</strong> Acesso total à AST do TypeScript, incluindo tipos. Poderoso, mas com curva de aprendizado maior.</li>
<li><strong>AST Explorer (astexplorer.net):</strong> <strong>Use isso!</strong> É um site onde você cola seu código e vê a AST gerada por vários parsers. Melhor jeito de aprender e testar.</li>
</ul>
<p>Esprima é show pra começar, mas pra coisas mais sérias ou específicas (principalmente com TS), talvez valha a pena olhar essas outras.</p>
<h3 id="onde-achar-mais-info-links-úteis">Onde Achar Mais Info (Links Úteis)</h3>
<ul>
<li><strong>Esprima (Site Oficial):</strong> <a href="http://esprima.org/">esprima.org</a></li>
<li><strong>ESTree (A &ldquo;Gramática&rdquo; das ASTs):</strong> <a href="https://github.com/estree/estree">github.com/estree/estree</a></li>
<li><strong>AST Explorer (Seu Melhor Amigo!):</strong> <a href="https://astexplorer.net/">astexplorer.net</a></li>
<li><strong>Estraverse (Pra Passear Melhor):</strong> <a href="https://github.com/estools/estraverse">github.com/estools/estraverse</a></li>
<li><strong>Escodegen (Pra Gerar Código da AST):</strong> <a href="https://github.com/estools/escodegen">github.com/estools/escodegen</a></li>
<li><strong>Acorn (Alternativa):</strong> <a href="https://github.com/acornjs/acorn">github.com/acornjs/acorn</a></li>
<li><strong>Babel Parser (Outra Alternativa):</strong> <a href="https://babeljs.io/docs/en/babel-parser">babeljs.io/docs/en/babel-parser</a></li>
</ul>
]]></content:encoded>
      
      
      
      
      
      
      
      
    </item>
    
    <item>
      <title></title>
      <link>https://scovl.github.io/1/01/01/rama/</link>
      <guid>https://scovl.github.io/1/01/01/rama/</guid>
      <pubDate>Mon, 01 Jan 0001 00:00:00 &#43;0000</pubDate>
      <description>&lt;![CDATA[<h1 id="next-level-backends-with-rama-storing-and-traversing-graphs-in-60-loc">Next-level Backends with Rama: Storing and Traversing Graphs in 60 LOC</h1>
<h2 id="introdução-ao-rama">Introdução ao Rama</h2>
<p>Rama é uma plataforma que permite a criação de backends escaláveis com uma quantidade mínima de código. Sistemas que normalmente exigiriam milhares de linhas de código podem ser implementados em algumas dezenas de linhas, oferecendo:</p>
<ul>
<li>Escalabilidade para milhões de leituras/escritas por segundo</li>
<li>Conformidade ACID</li>
<li>Alto desempenho</li>
<li>Tolerância a falhas através de replicação incremental</li>
<li>Implantação, atualização e escalonamento via simples comandos CLI</li>
<li>Monitoramento abrangente integrado</li>
</ul>
<h2 id="exemplo-armazenamento-e-travessia-de-grafos">Exemplo: Armazenamento e Travessia de Grafos</h2>
<p>Para demonstrar a potência do Rama, vamos analisar a implementação de um backend para armazenamento de grafos e execução de consultas rápidas de travessia. O exemplo específico é uma árvore genealógica, onde cada nó (pessoa) tem dois pais e qualquer número de filhos.</p>]]></description>
      <content:encoded>&lt;![CDATA[<h1 id="next-level-backends-with-rama-storing-and-traversing-graphs-in-60-loc">Next-level Backends with Rama: Storing and Traversing Graphs in 60 LOC</h1>
<h2 id="introdução-ao-rama">Introdução ao Rama</h2>
<p>Rama é uma plataforma que permite a criação de backends escaláveis com uma quantidade mínima de código. Sistemas que normalmente exigiriam milhares de linhas de código podem ser implementados em algumas dezenas de linhas, oferecendo:</p>
<ul>
<li>Escalabilidade para milhões de leituras/escritas por segundo</li>
<li>Conformidade ACID</li>
<li>Alto desempenho</li>
<li>Tolerância a falhas através de replicação incremental</li>
<li>Implantação, atualização e escalonamento via simples comandos CLI</li>
<li>Monitoramento abrangente integrado</li>
</ul>
<h2 id="exemplo-armazenamento-e-travessia-de-grafos">Exemplo: Armazenamento e Travessia de Grafos</h2>
<p>Para demonstrar a potência do Rama, vamos analisar a implementação de um backend para armazenamento de grafos e execução de consultas rápidas de travessia. O exemplo específico é uma árvore genealógica, onde cada nó (pessoa) tem dois pais e qualquer número de filhos.</p>
<h3 id="definindo-o-modelo-de-dados">Definindo o Modelo de Dados</h3>
<p>No Rama, os datastores indexados são chamados de <a href="https://docs.redplanetlabs.com/concepts/pstates.html">PStates</a> (&ldquo;partitioned state&rdquo;). Ao contrário de bancos de dados tradicionais com modelos fixos, os PStates permitem infinitos modelos de dados através da composição de estruturas simples:</p>


  <pre><code class="language-clojure">(declare-pstate
  topology
  $$family-tree
  {UUID (fixed-keys-schema
          {:parent1 UUID
           :parent2 UUID
           :name String
           :children #{UUID}})})</code></pre>
 <p>Na versão Java:</p>


  <pre><code class="language-java">topology.pstate(
  &#34;$$family-tree&#34;,
  PState.mapSchema(UUID.class,
                   PState.fixedKeysSchema(
                     &#34;parent1&#34;, UUID.class,
                     &#34;parent2&#34;, UUID.class,
                     &#34;name&#34;, String.class,
                     &#34;children&#34;, PState.setSchema(UUID.class)
                     )));</code></pre>
 <p>Este <a href="https://docs.redplanetlabs.com/concepts/pstates.html">PState</a> representa uma árvore genealógica onde cada pessoa é identificada por um UUID e possui campos para seus pais, nome e filhos.</p>
<h3 id="conceitos-do-rama">Conceitos do Rama</h3>
<p>Um aplicativo Rama é chamado de &ldquo;módulo&rdquo; e segue uma arquitetura baseada em eventos:</p>
<ol>
<li>Todos os dados entram através de um log distribuído chamado &ldquo;depot&rdquo;</li>
<li>Topologias ETL consomem dados desses depots para materializar PStates</li>
<li>Clientes interagem com o módulo anexando novos dados ao depot ou consultando PStates</li>
</ol>
<p>Um módulo é dividido em &ldquo;tarefas&rdquo; que rodam em vários processos e nós, permitindo escalonamento horizontal.</p>
<h3 id="materializando-o-pstate">Materializando o PState</h3>
<p>Primeiro, definimos o depot que receberá as informações de novas pessoas:</p>


  <pre><code class="language-clojure">(declare-depot setup *people-depot (hash-by :id))</code></pre>
 <p>Em seguida, implementamos a topologia para consumir dados do depot e materializar o PState:</p>


  <pre><code class="language-clojure">(&lt;&lt;sources topology
  (source&gt; *people-depot :&gt; {:keys [*id *parent1 *parent2] :as *person})
  (local-transform&gt;
    [(keypath *id) (termval (dissoc *person :id))]
    $$family-tree)
  (ops/explode [*parent1 *parent2] :&gt; *parent)
  (|hash *parent)
  (local-transform&gt;
    [(keypath *parent) :children NONE-ELEM (termval *id)]
    $$family-tree))</code></pre>
 <p>Este código:</p>
<ol>
<li>Cria um novo nó para a pessoa com seus atributos</li>
<li>Atualiza cada pai para listar a nova pessoa como filho</li>
<li>Utiliza particionamento para garantir eficiência e paralelismo</li>
</ol>
<h3 id="implementando-consultas-de-travessia-de-grafo">Implementando Consultas de Travessia de Grafo</h3>
<p>As duas consultas implementadas são:</p>
<ol>
<li>Encontrar todos os ancestrais de uma pessoa dentro de N gerações</li>
<li>Contar quantos descendentes diretos uma pessoa tem em cada geração sucessiva</li>
</ol>
<h4 id="consulta-de-ancestrais">Consulta de Ancestrais</h4>
<p>A implementação usa um loop que examina iterativamente os pais de um nó:</p>


  <pre><code class="language-clojure">(&lt;&lt;query-topology topologies &#34;ancestors&#34;
  [*start-id *num-generations :&gt; *ancestors]
  (loop&lt;- [*id *start-id
           *generation 0
           :&gt; *ancestor]
    (filter&gt; (&lt;= *generation *num-generations))
    (|hash *id)
    (local-select&gt; [(keypath *id) (multi-path :parent1 :parent2) some?]
      $$family-tree
      :&gt; *parent)
    (:&gt; *parent)
    (continue&gt; *parent (inc *generation)))
  (|origin)
  (aggs/&#43;set-agg *ancestor :&gt; *ancestors))</code></pre>
 <p>Esta implementação:</p>
<ol>
<li>Inicia com o ID fornecido e geração 0</li>
<li>Verifica se ainda está dentro do limite de gerações</li>
<li>Recupera os pais do nó atual</li>
<li>Continua a travessia com cada pai, incrementando a contagem de gerações</li>
<li>Agrega todos os ancestrais encontrados em um conjunto</li>
</ol>
<h4 id="consulta-de-contagem-de-descendentes">Consulta de Contagem de Descendentes</h4>
<p>Similar à consulta anterior, mas percorre os filhos e conta por geração:</p>


  <pre><code class="language-clojure">(&lt;&lt;query-topology topologies &#34;descendants-count&#34;
  [*start-id *num-generations :&gt; *result]
  (loop&lt;- [*id *start-id
           *generation 0 :&gt; *gen *count]
    (filter&gt; (&lt; *generation *num-generations))
    (|hash *id)
    (local-select&gt; [(keypath *id) :children] $$family-tree :&gt; *children)
    (:&gt; *generation (count *children))
    (ops/explode *children :&gt; *c)
    (continue&gt; *c (inc *generation)))
  (|origin)
  (&#43;compound {*gen (aggs/&#43;sum *count)} :&gt; *result))</code></pre>
 <p>O resultado é um mapa de números de geração para contagens de descendentes.</p>
<h2 id="conclusão">Conclusão</h2>
<p>Com Rama, é possível construir o equivalente a um banco de dados de grafos personalizado em apenas 60 linhas de código. Não há trabalho adicional necessário para implantação, atualização e escalonamento, pois tudo está integrado.</p>
<p>A arquitetura baseada em eventos do Rama proporciona:</p>
<ul>
<li>Log de auditoria de todas as mudanças</li>
<li>Capacidade de recomputar PStates (útil em caso de bugs que corrompam dados)</li>
<li>Tolerância a falhas superior a abordagens alternativas</li>
</ul>
<p>O Rama é gratuito para clusters de produção com até dois nós e pode ser baixado no site da Red Planet Labs.</p>
]]></content:encoded>
      
      
      
      
      
      
      
      
    </item>
    
  </channel>
</rss>