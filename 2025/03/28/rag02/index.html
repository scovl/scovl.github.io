<!DOCTYPE html>
<html lang="pt">
<head>
    <title>T√©cnicas Avan√ßadas para RAG em Produ√ß√£o | scovl</title>
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Explorando t√©cnicas para otimizar sistemas RAG para uso em produ√ß√£o">



<link rel="preload" href="/vendor/fonts/inter/Inter-400.ttf" as="font" type="font/ttf" crossorigin>
<link rel="preload" href="/vendor/fonts/inter/Inter-600.ttf" as="font" type="font/ttf" crossorigin>
<link rel="preload" href="/vendor/fonts/jetbrains-mono/JetBrainsMono-400.ttf" as="font" type="font/ttf" crossorigin>



<link rel="dns-prefetch" href="//giscus.app">
<link rel="preconnect" href="//giscus.app">



<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="format-detection" content="telephone=no"> 


<link rel="stylesheet" href="/css/main.css?v=1757091656">


<link rel="stylesheet" href="/vendor/fonts/fonts.css?v=1757091656">


<link rel="preload" href="/fonts/abril-fatface.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/rokkitt.woff2" as="font" type="font/woff2" crossorigin>


<link rel="stylesheet" href="/vendor/prism/prism-tomorrow.min.css?v=1757091656">



<script src="/vendor/mermaid/mermaid.min.js?v=1757091656"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        mermaid.initialize({
            startOnLoad: true,
            theme: 'light',
            align: 'center'
        });SS
    });
</script>












<script>

const I18n = {
    currentLang: 'pt',
    isRTL:  false ,
    
    
    formatDate(date, options = {}) {
        const defaultOptions = {
            year: 'numeric',
            month: 'long',
            day: 'numeric'
        };
        
        const locale = this.getLocale();
        const finalOptions = { ...defaultOptions, ...options };
        
        return new Intl.DateTimeFormat(locale, finalOptions).format(date);
    },
    
    
    formatNumber(number, options = {}) {
        const defaultOptions = {
            style: 'decimal',
            minimumFractionDigits: 0,
            maximumFractionDigits: 2
        };
        
        const locale = this.getLocale();
        const finalOptions = { ...defaultOptions, ...options };
        
        return new Intl.NumberFormat(locale, finalOptions).format(number);
    },
    
    
    formatCurrency(amount, currency = 'USD') {
        const locale = this.getLocale();
        return new Intl.NumberFormat(locale, {
            style: 'currency',
            currency: currency
        }).format(amount);
    },
    
    
    formatRelativeTime(date) {
        const locale = this.getLocale();
        const now = new Date();
        const diff = now - date;
        const diffInMinutes = Math.floor(diff / (1000 * 60));
        const diffInHours = Math.floor(diff / (1000 * 60 * 60));
        const diffInDays = Math.floor(diff / (1000 * 60 * 60 * 24));
        
        if (diffInMinutes < 1) {
            return new Intl.RelativeTimeFormat(locale).format(0, 'minute');
        } else if (diffInMinutes < 60) {
            return new Intl.RelativeTimeFormat(locale).format(-diffInMinutes, 'minute');
        } else if (diffInHours < 24) {
            return new Intl.RelativeTimeFormat(locale).format(-diffInHours, 'hour');
        } else {
            return new Intl.RelativeTimeFormat(locale).format(-diffInDays, 'day');
        }
    },
    
    
    getLocale() {
        const localeMap = {
            'en': 'en-US',
            'pt': 'pt-BR',
            'es': 'es-ES',
            'fr': 'fr-FR',
            'de': 'de-DE',
            'it': 'it-IT',
            'ar': 'ar-SA',
            'he': 'he-IL',
            'fa': 'fa-IR',
            'ur': 'ur-PK',
            'zh': 'zh-CN',
            'ja': 'ja-JP',
            'ko': 'ko-KR'
        };
        
        return localeMap[this.currentLang] || 'en-US';
    },
    
    
    t(key, params = {}) {
        const translations = {
            'en': {
                'read_more': 'Read more',
                'back_to_top': 'Back to top',
                'loading': 'Loading...',
                'error': 'Error',
                'success': 'Success',
                'warning': 'Warning',
                'info': 'Information',
                'comments': 'Comments',
                'related_posts': 'Related Posts',
                'tags': 'Tags',
                'categories': 'Categories',
                'search': 'Search',
                'menu': 'Menu',
                'close': 'Close',
                'language': 'Language',
                'theme': 'Theme',
                'dark_mode': 'Dark Mode',
                'light_mode': 'Light Mode'
            },
            'pt': {
                'read_more': 'Ler mais',
                'back_to_top': 'Voltar ao topo',
                'loading': 'Carregando...',
                'error': 'Erro',
                'success': 'Sucesso',
                'warning': 'Aviso',
                'info': 'Informa√ß√£o',
                'comments': 'Coment√°rios',
                'related_posts': 'Posts Relacionados',
                'tags': 'Tags',
                'categories': 'Categorias',
                'search': 'Pesquisar',
                'menu': 'Menu',
                'close': 'Fechar',
                'language': 'Idioma',
                'theme': 'Tema',
                'dark_mode': 'Modo Escuro',
                'light_mode': 'Modo Claro'
            },
            'es': {
                'read_more': 'Leer m√°s',
                'back_to_top': 'Volver arriba',
                'loading': 'Cargando...',
                'error': 'Error',
                'success': '√âxito',
                'warning': 'Advertencia',
                'info': 'Informaci√≥n',
                'comments': 'Comentarios',
                'related_posts': 'Posts Relacionados',
                'tags': 'Etiquetas',
                'categories': 'Categor√≠as',
                'search': 'Buscar',
                'menu': 'Men√∫',
                'close': 'Cerrar',
                'language': 'Idioma',
                'theme': 'Tema',
                'dark_mode': 'Modo Oscuro',
                'light_mode': 'Modo Claro'
            },
            'ar': {
                'read_more': 'ÿßŸÇÿ±ÿ£ ÿßŸÑŸÖÿ≤ŸäÿØ',
                'back_to_top': 'ÿßŸÑÿπŸàÿØÿ© ÿ•ŸÑŸâ ÿßŸÑÿ£ÿπŸÑŸâ',
                'loading': 'ÿ¨ÿßÿ±Ÿä ÿßŸÑÿ™ÿ≠ŸÖŸäŸÑ...',
                'error': 'ÿÆÿ∑ÿ£',
                'success': 'ŸÜÿ¨ÿ≠',
                'warning': 'ÿ™ÿ≠ÿ∞Ÿäÿ±',
                'info': 'ŸÖÿπŸÑŸàŸÖÿßÿ™',
                'comments': 'ÿßŸÑÿ™ÿπŸÑŸäŸÇÿßÿ™',
                'related_posts': 'ÿßŸÑŸÖŸÇÿßŸÑÿßÿ™ ÿ∞ÿßÿ™ ÿßŸÑÿµŸÑÿ©',
                'tags': 'ÿßŸÑÿπŸÑÿßŸÖÿßÿ™',
                'categories': 'ÿßŸÑŸÅÿ¶ÿßÿ™',
                'search': 'ÿ®ÿ≠ÿ´',
                'menu': 'ÿßŸÑŸÇÿßÿ¶ŸÖÿ©',
                'close': 'ÿ•ÿ∫ŸÑÿßŸÇ',
                'language': 'ÿßŸÑŸÑÿ∫ÿ©',
                'theme': 'ÿßŸÑŸÖÿ∏Ÿáÿ±',
                'dark_mode': 'ÿßŸÑŸàÿ∂ÿπ ÿßŸÑŸÖÿ∏ŸÑŸÖ',
                'light_mode': 'ÿßŸÑŸàÿ∂ÿπ ÿßŸÑŸÅÿßÿ™ÿ≠'
            }
        };
        
        const langTranslations = translations[this.currentLang] || translations['en'];
        let text = langTranslations[key] || key;
        
        
        Object.keys(params).forEach(param => {
            text = text.replace(`{${param}}`, params[param]);
        });
        
        return text;
    },
    
    
    init() {
        this.updatePageDirection();
        this.updateDateFormats();
        this.updateNumberFormats();
        this.updateTranslations();
    },
    
    
    updatePageDirection() {
        if (this.isRTL) {
            document.documentElement.setAttribute('dir', 'rtl');
            document.documentElement.setAttribute('lang', this.currentLang);
        }
    },
    
    
    updateDateFormats() {
        const dateElements = document.querySelectorAll('[data-date]');
        dateElements.forEach(element => {
            const date = new Date(element.getAttribute('data-date'));
            const format = element.getAttribute('data-date-format') || 'default';
            
            let formattedDate;
            switch (format) {
                case 'relative':
                    formattedDate = this.formatRelativeTime(date);
                    break;
                case 'short':
                    formattedDate = this.formatDate(date, { month: 'short', day: 'numeric' });
                    break;
                case 'long':
                    formattedDate = this.formatDate(date, { 
                        weekday: 'long',
                        year: 'numeric',
                        month: 'long',
                        day: 'numeric'
                    });
                    break;
                default:
                    formattedDate = this.formatDate(date);
            }
            
            element.textContent = formattedDate;
        });
    },
    
    
    updateNumberFormats() {
        const numberElements = document.querySelectorAll('[data-number]');
        numberElements.forEach(element => {
            const number = parseFloat(element.getAttribute('data-number'));
            const format = element.getAttribute('data-number-format') || 'decimal';
            
            let formattedNumber;
            switch (format) {
                case 'currency':
                    const currency = element.getAttribute('data-currency') || 'USD';
                    formattedNumber = this.formatCurrency(number, currency);
                    break;
                case 'percent':
                    formattedNumber = this.formatNumber(number / 100, { style: 'percent' });
                    break;
                default:
                    formattedNumber = this.formatNumber(number);
            }
            
            element.textContent = formattedNumber;
        });
    },
    
    
    updateTranslations() {
        const translationElements = document.querySelectorAll('[data-i18n]');
        translationElements.forEach(element => {
            const key = element.getAttribute('data-i18n');
            const params = {};
            
            
            const paramAttributes = element.getAttribute('data-i18n-params');
            if (paramAttributes) {
                try {
                    Object.assign(params, JSON.parse(paramAttributes));
                } catch (e) {
                    console.warn('Invalid i18n params:', paramAttributes);
                }
            }
            
            element.textContent = this.t(key, params);
        });
    }
};


window.I18n = I18n;
</script>


<script>
    if ('serviceWorker' in navigator) {
        window.addEventListener('load', function() {
            navigator.serviceWorker.register('\/sw.js')
                .then(function(registration) {
                    console.log('Service Worker registrado com sucesso:', registration.scope);
                })
                .catch(function(error) {
                    console.log('Falha no registro do Service Worker:', error);
                });
        });
    }
</script> 
</head>
<body>
    
    
    <header class="header">
    <div class="container">
        <div class="header-content">
            <a href="https://scovl.github.io/" class="site-title">scovl</a>
            
            <div class="header-actions">
                
                <nav class="nav-menu">
                    <ul>
                        
                        <li><a href="/page/about/">About</a></li>
                        
                        <li><a href="/page/contact/">Contact</a></li>
                        
                    </ul>
                </nav>
                
                
                



<div class="language-switcher" id="language-switcher">
    <button class="language-btn" onclick="toggleLanguageMenu()">
        <span class="current-lang">Portugu√™s</span>
        <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
            <polyline points="6,9 12,15 18,9"></polyline>
        </svg>
    </button>
    <div class="language-menu" id="language-menu">
        
            
                <a href="https://scovl.github.io/" class="language-option active">
                    Portugu√™s
                </a>
            
        
            
                <a href="https://scovl.github.io/en/" class="language-option ">
                    English
                </a>
            
        
    </div>
</div>
                
                
                <button id="dark-mode-toggle" class="theme-toggle" aria-label="Toggle dark mode">
                    <svg class="sun-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <line x1="12" y1="1" x2="12" y2="3"/>
                        <line x1="12" y1="21" x2="12" y2="23"/>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/>
                        <line x1="1" y1="12" x2="3" y2="12"/>
                        <line x1="21" y1="12" x2="23" y2="12"/>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/>
                    </svg>
                    <svg class="moon-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </div>
</header> 
    
    
    
    <main>
        <div class="container">
            
<article class="post">
    <header class="post-header">
        <h1 class="post-title">T√©cnicas Avan√ßadas para RAG em Produ√ß√£o</h1>
        
        <div class="post-meta">
            <time datetime="2025-03-28T12:00:00Z">
                üìÖ 28/03/2025
            </time>
            
            <span>üë§ Vitor Lobo Ramos</span>
            
            
            
            <div class="post-tags">
                
                <a href="/tags/rag" class="tag">RAG</a>
                
                <a href="/tags/llm" class="tag">LLM</a>
                
                <a href="/tags/ai" class="tag">AI</a>
                
                <a href="/tags/optimiza%C3%A7%C3%A3o" class="tag">Optimiza√ß√£o</a>
                
                <a href="/tags/produ%C3%A7%C3%A3o" class="tag">Produ√ß√£o</a>
                
                <a href="/tags/postgresql" class="tag">PostgreSQL</a>
                
                <a href="/tags/ollama" class="tag">Ollama</a>
                
            </div>
            
        </div>
        
    </header>
    
    
    















  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  



<aside class="toc" id="toc" aria-labelledby="toc-title">
    <div class="toc-container">
        <div class="toc-header">
            <h3 id="toc-title" class="toc-title">
                <svg class="toc-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <path d="M3 6h18M3 12h18M3 18h18"/>
                </svg>
                Sum√°rio
            </h3>
            <button class="toc-toggle" id="toc-toggle" aria-expanded="true" aria-controls="toc-content" aria-label="Mostrar/Ocultar Sum√°rio">
                <svg class="toc-toggle-icon" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <path d="m6 9 6 6 6-6"/>
                </svg>
            </button>
        </div>
        <div class="toc-content" id="toc-content">
            <div class="toc-progress">
                <div class="toc-progress-bar" id="toc-progress-bar"></div>
            </div>
            <nav class="toc-nav">
                <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#introdu√ß√£o">Introdu√ß√£o</a></li>
        <li><a href="#da-teoria-√†-produ√ß√£o-os-desafios-reais">Da Teoria √† Produ√ß√£o: Os Desafios Reais</a></li>
        <li><a href="#armadilhas-comuns-e-como-evit√°-las">Armadilhas Comuns e Como Evit√°-las</a>
          <ul>
            <li><a href="#armadilha-1-a-falsa-sensa√ß√£o-de-relev√¢ncia">Armadilha 1: A Falsa Sensa√ß√£o de Relev√¢ncia</a></li>
            <li><a href="#armadilha-2-tamanho-inadequado-de-chunks">Armadilha 2: Tamanho Inadequado de Chunks</a></li>
            <li><a href="#armadilha-3-falta-de-monitoramento-cont√≠nuo">Armadilha 3: Falta de Monitoramento Cont√≠nuo</a></li>
            <li><a href="#armadilha-4-consultas-complexas-em-pipelines-simples">Armadilha 4: Consultas Complexas em Pipelines Simples</a></li>
          </ul>
        </li>
        <li><a href="#t√©cnicas-avan√ßadas-de-otimiza√ß√£o">T√©cnicas Avan√ßadas de Otimiza√ß√£o</a>
          <ul>
            <li><a href="#re-ranqueamento-de-chunks">Re-ranqueamento de Chunks</a></li>
            <li><a href="#estrat√©gias-de-chunking-din√¢mico">Estrat√©gias de Chunking Din√¢mico</a></li>
            <li><a href="#workflows-com-agentes-para-consultas-complexas">Workflows com Agentes para Consultas Complexas</a>
              <ul>
                <li><a href="#arquitetura-de-agentes-avan√ßada">Arquitetura de Agentes Avan√ßada</a></li>
                <li><a href="#casos-de-uso-para-workflows-de-agentes">Casos de Uso para Workflows de Agentes</a></li>
              </ul>
            </li>
            <li><a href="#pipelines-multimodais">Pipelines Multimodais</a>
              <ul>
                <li><a href="#arquitetura-multimodal-completa">Arquitetura Multimodal Completa</a></li>
                <li><a href="#esquema-postgresql-para-dados-multimodais">Esquema PostgreSQL para Dados Multimodais</a></li>
                <li><a href="#desafios-de-implementa√ß√£o-multimodal">Desafios de Implementa√ß√£o Multimodal</a></li>
              </ul>
            </li>
            <li><a href="#estrat√©gias-de-cache">Estrat√©gias de Cache</a>
              <ul>
                <li><a href="#estrat√©gias-avan√ßadas-de-cache-para-rag">Estrat√©gias Avan√ßadas de Cache para RAG</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#monitoramento-e-m√©tricas-llmops-na-pr√°tica">Monitoramento e M√©tricas: LLMOps na Pr√°tica</a>
          <ul>
            <li><a href="#m√©tricas-de-qualidade-espec√≠ficas-para-rag">M√©tricas de Qualidade Espec√≠ficas para RAG</a>
              <ul>
                <li><a href="#1-m√©tricas-de-relev√¢ncia-do-contexto">1. M√©tricas de Relev√¢ncia do Contexto</a></li>
                <li><a href="#2-m√©tricas-de-qualidade-da-resposta">2. M√©tricas de Qualidade da Resposta</a></li>
                <li><a href="#3-m√©tricas-de-consenso-entre-modelos">3. M√©tricas de Consenso entre Modelos</a></li>
              </ul>
            </li>
            <li><a href="#automa√ß√£o-da-avalia√ß√£o-com-llms-como-ju√≠zes">Automa√ß√£o da Avalia√ß√£o com LLMs como Ju√≠zes</a>
              <ul>
                <li><a href="#configura√ß√£o-de-um-dashboard-de-qualidade-rag">Configura√ß√£o de um Dashboard de Qualidade RAG</a></li>
              </ul>
            </li>
            <li><a href="#integra√ß√£o-com-sistemas-de-feedback-do-usu√°rio">Integra√ß√£o com Sistemas de Feedback do Usu√°rio</a></li>
          </ul>
        </li>
        <li><a href="#implementando-no-docai">Implementando no DocAI</a>
          <ul>
            <li><a href="#arquitetura-atual-do-docai">Arquitetura Atual do DocAI</a></li>
            <li><a href="#diferenciais-do-docai">Diferenciais do DocAI</a></li>
            <li><a href="#pr√≥ximos-passos-para-o-docai">Pr√≥ximos Passos para o DocAI</a></li>
          </ul>
        </li>
        <li><a href="#integra√ß√£o-com-o-ecossistema">Integra√ß√£o com o Ecossistema</a></li>
        <li><a href="#conclus√£o">Conclus√£o</a></li>
        <li><a href="#refer√™ncias">Refer√™ncias</a></li>
      </ul>
    </li>
  </ul>
</nav>
            </nav>
        </div>
    </div>
    
    
    <button class="toc-mobile-toggle" id="toc-mobile-toggle" aria-label="Mostrar Sum√°rio">
        <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <path d="M3 6h18M3 12h18M3 18h18"/>
        </svg>
    </button>
</aside>



    
    <div class="post-content">
        <h2 id="introdu√ß√£o">Introdu√ß√£o</h2>
<p>Ol√° pessoal! üëã</p>
<p>Nos artigos anteriores, exploramos como <a href="/2025/03/23/rag/">implementar um RAG b√°sico em Clojure</a> em mem√≥ria e como <a href="/2025/03/25/semantic-postgresql/">construir um sistema de busca sem√¢ntica com PostgreSQL e Ollama</a>. Agora, vamos dar o pr√≥ximo passo: transformar nosso prot√≥tipo em um sistema RAG pronto para produ√ß√£o.</p>
<p>Como muitos desenvolvedores j√° descobriram, criar um prot√≥tipo funcional de <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> com alguns documentos √© relativamente simples. O verdadeiro desafio come√ßa quando precisamos escalar esse sistema para lidar com milhares de documentos, garantir respostas precisas e manter o desempenho sob carga. Neste artigo, vamos explorar t√©cnicas avan√ßadas para superar esses desafios e levar nosso <a href="https://github.com/scovl/docai">DocAI</a> para um novo patamar de qualidade e confiabilidade.</p>
<h2 id="da-teoria-√†-produ√ß√£o-os-desafios-reais">Da Teoria √† Produ√ß√£o: Os Desafios Reais</h2>
<blockquote>
<p>&ldquo;No papel, implementar um sistema <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> parece simples‚Äîconectar um banco de dados vetorial, processar documentos, incorporar os dados, incorporar a consulta, consultar o <a href="https://en.wikipedia.org/wiki/Vector_database">banco de dados vetorial</a> e gerar a resposta com o <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a>. Mas na pr√°tica, transformar um prot√≥tipo em uma aplica√ß√£o de alto desempenho √© um desafio completamente diferente.&rdquo;</p></blockquote>
<p>Ao migrarmos do <a href="/2025/03/23/rag/">TF-IDF em mem√≥ria</a> para <a href="/2025/03/25/semantic-postgresql/">PostgreSQL/pgvector/pgai</a>, demos um grande salto de qualidade. Por√©m, √† medida que o volume de dados cresce e os casos de uso se tornam mais complexos, novos desafios surgem:</p>
<ul>
<li><strong>Escalabilidade</strong>: Como lidar com milh√µes de documentos sem degradar o desempenho?</li>
<li><strong>Precis√£o</strong>: Como garantir que estamos recuperando o contexto mais relevante para cada consulta?</li>
<li><strong>Efici√™ncia</strong>: Como reduzir lat√™ncia e custos de processamento?</li>
<li><strong>Confiabilidade</strong>: Como evitar alucina√ß√µes e respostas incorretas?</li>
<li><strong>Manuten√ß√£o</strong>: Como monitorar e melhorar continuamente o sistema?</li>
</ul>
<p>Antes de mergulharmos nas t√©cnicas avan√ßadas, precisamos entender que o impacto mais significativo no desempenho de um sistema <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> n√£o vem apenas de usar o modelo de linguagem mais recente. Os verdadeiros ganhos v√™m de tr√™s fatores fundamentais:</p>
<ul>
<li><strong>Qualidade dos dados</strong>: Dados bem estruturados e relevantes s√£o a base de todo sistema RAG eficaz.</li>
<li><strong>Prepara√ß√£o adequada</strong>: Como os dados s√£o processados, limpos e organizados.</li>
<li><strong>Processamento eficiente</strong>: Como os dados s√£o recuperados e utilizados durante a infer√™ncia.</li>
</ul>
<p>Mesmo com o avan√ßo dos <a href="https://en.wikipedia.org/wiki/Large_language_model">LLMs</a>, esperar que modelos maiores corrijam magicamente problemas em dados defeituosos n√£o √© uma estrat√©gia vi√°vel. O futuro da <a href="https://en.wikipedia.org/wiki/Artificial_intelligence">IA</a> n√£o est√° em um √∫nico modelo que sabe tudo, mas em sistemas que combinam <a href="https://en.wikipedia.org/wiki/Large_language_model">LLMs</a>, modelos multimodais e ferramentas de suporte que trabalham juntos de forma integrada. Dito isto, para construir um sistema <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> robusto, precisamos responder a v√°rias perguntas importantes como:</p>
<ul>
<li>Como construir <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">mecanismos de recupera√ß√£o robustos</a>?</li>
<li>Qual o papel da <a href="https://en.wikipedia.org/wiki/Embedding_model">qualidade dos embeddings</a> no desempenho da recupera√ß√£o?</li>
<li>Como adaptar estrat√©gias de <a href="https://en.wikipedia.org/wiki/Chunking_%28data_storage%29">chunking</a> dinamicamente?</li>
<li>Como o <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a> pode interpretar dados de forma eficaz?</li>
<li>Uma cadeia de <a href="https://en.wikipedia.org/wiki/Large_language_model">LLMs</a> ajudaria a refinar as respostas? Vale o custo?</li>
<li>Como prevenir alucina√ß√µes mantendo a diversidade das respostas?</li>
<li>Como integrar entradas <a href="https://en.wikipedia.org/wiki/Multimodal_learning">multimodais</a> (texto, imagens, tabelas) em um pipeline <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a>?</li>
<li>Quais estrat√©gias de <a href="https://en.wikipedia.org/wiki/Cache_%28computing%29">cache</a> reduzem chamadas de API redundantes e lat√™ncia?</li>
<li>Como automatizar a <a href="https://en.wikipedia.org/wiki/Evaluation_of_retrieval_systems">avalia√ß√£o da recupera√ß√£o</a> para melhoria cont√≠nua?</li>
</ul>
<h2 id="armadilhas-comuns-e-como-evit√°-las">Armadilhas Comuns e Como Evit√°-las</h2>
<p>Baseado na nossa experi√™ncia com o <a href="https://github.com/scovl/docai">DocAI</a> e nos desafios relatados pela comunidade, identificamos quatro armadilhas principais que podem comprometer sistemas RAG:</p>
<h3 id="armadilha-1-a-falsa-sensa√ß√£o-de-relev√¢ncia">Armadilha 1: A Falsa Sensa√ß√£o de Relev√¢ncia</h3>
<p>Uma busca por vizinhos mais pr√≥ximos sempre retornar√° algum resultado, mas como saber se √© realmente √∫til? Alguns documentos podem parecer relevantes com base na similaridade vetorial, mas n√£o fornecem o contexto adequado para responder √† pergunta do usu√°rio.</p>
<blockquote>
<p><strong>Solu√ß√£o</strong>: Implementar verifica√ß√£o de relev√¢ncia p√≥s-recupera√ß√£o usando <a href="https://huggingface.co/cross-encoder">cross-encoders</a> ou filtros baseados em regras. No <a href="https://www.postgresql.org/">PostgreSQL</a>, podemos fazer isso com:</p></blockquote>


  <pre><code class="language-sql">-- Primeiro recuperamos candidatos usando busca vetorial
WITH candidatos AS (
  SELECT id, titulo, conteudo, embedding &lt;=&gt; query_embedding AS distancia
  FROM documentos_embeddings
  ORDER BY distancia
  LIMIT 20
),
-- Depois aplicamos filtro secund√°rio para verificar relev√¢ncia real
filtrados AS (
  SELECT id, titulo, conteudo, distancia
  FROM candidatos
  WHERE 
    -- Filtro baseado em regras (exemplo: deve conter palavras-chave)
    conteudo ILIKE &#39;%&#39; || &#39;palavra_chave&#39; || &#39;%&#39;
    -- Ou usar um modelo secund√°rio para avaliar relev√¢ncia
    -- ai.evaluate_relevance(conteudo, &#39;consulta_original&#39;) &gt; 0.7  -- ‚ö†Ô∏è Nota: Fun√ß√£o experimental no pgai
)
SELECT * FROM filtrados ORDER BY distancia LIMIT 5;</code></pre>
 <p>Este c√≥digo SQL demonstra uma abordagem de duas fases para melhorar a qualidade da recupera√ß√£o em sistemas RAG. Na primeira fase, utilizamos a <a href="https://en.wikipedia.org/wiki/Vector_database">busca vetorial</a> para recuperar 20 candidatos iniciais ordenados por <a href="https://en.wikipedia.org/wiki/Vector_database">similaridade vetorial</a> (usando o operador <code>&lt;=&gt;</code> do <a href="https://en.wikipedia.org/wiki/Vector_database">pgvector</a> para calcular a dist√¢ncia entre embeddings). Esta etapa prioriza a velocidade e a amplitude da recupera√ß√£o.</p>
<p>Na segunda fase, aplicamos filtros mais refinados para verificar a relev√¢ncia real dos documentos recuperados. Isso pode incluir filtros baseados em regras (como busca por palavras-chave usando <code>ILIKE</code>) ou at√© mesmo modelos secund√°rios de avalia√ß√£o de relev√¢ncia (como sugerido no coment√°rio sobre a fun√ß√£o experimental do <a href="https://github.com/timescale/pgai">pgai</a>). Esta abordagem em duas etapas equilibra efici√™ncia e precis√£o, permitindo que o sistema primeiro capture um conjunto amplo de candidatos potenciais e depois refine os resultados para apresentar apenas os documentos verdadeiramente relevantes para a consulta do usu√°rio.</p>
<h3 id="armadilha-2-tamanho-inadequado-de-chunks">Armadilha 2: Tamanho Inadequado de Chunks</h3>
<p>Dividir documentos em chunks menores √© uma pr√°tica padr√£o, mas qual √© o tamanho ideal?</p>
<ul>
<li>Chunks muito pequenos perdem contexto crucial</li>
<li>Chunks muito grandes diluem a recupera√ß√£o com detalhes irrelevantes</li>
</ul>
<blockquote>
<p><strong>Solu√ß√£o</strong>: Adaptar a estrat√©gia de chunking ao tipo de conte√∫do. No nosso <a href="/2025/03/25/semantic-postgresql/">PostgreSQL RAG</a>, usamos chunking recursivo:</p></blockquote>


  <pre><code class="language-sql">-- Podemos ajustar os par√¢metros de chunking para diferentes tipos de documentos
SELECT ai.create_vectorizer(
   &#39;documentos_tecnicos&#39;::regclass,
   destination =&gt; &#39;embeddings_tecnicos&#39;,
   embedding =&gt; ai.embedding_ollama(&#39;nomic-embed-text&#39;, 768),
   -- Chunks maiores para documentos t√©cnicos que precisam de mais contexto
   chunking =&gt; ai.chunking_recursive_character_text_splitter(&#39;conteudo&#39;, 
                                                           chunk_size =&gt; 1500, 
                                                           chunk_overlap =&gt; 200)
);</code></pre>
 <p>Para documentos t√©cnicos, que geralmente cont√™m informa√ß√µes densas e interconectadas, configuramos chunks maiores (1500 caracteres) com uma sobreposi√ß√£o significativa (200 caracteres).</p>
<p>Isso permite preservar mais contexto dentro de cada chunk, o que √© crucial para a compreens√£o de conceitos t√©cnicos complexos. O uso do <code>chunking_recursive_character_text_splitter</code> implementa uma estrat√©gia de divis√£o recursiva que respeita a estrutura natural do texto, enquanto o modelo de embedding <code>nomic-embed-text</code> com 768 dimens√µes captura as nuances sem√¢nticas do conte√∫do t√©cnico. Esta <a href="https://en.wikipedia.org/wiki/Chunking_%28data_storage%29">abordagem adaptativa de chunking</a> √© fundamental para equilibrar a granularidade da recupera√ß√£o com a preserva√ß√£o do contexto necess√°rio para respostas precisas em sistemas <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a>.</p>
<h3 id="armadilha-3-falta-de-monitoramento-cont√≠nuo">Armadilha 3: Falta de Monitoramento Cont√≠nuo</h3>
<p>Como garantir que seu sistema permane√ßa eficaz ao longo do tempo? <a href="https://www.databricks.com/br/glossary/llmops">LLMOps</a> n√£o √© apenas sobre implanta√ß√£o, mas sobre o monitoramento cont√≠nuo da qualidade.</p>
<blockquote>
<p><strong>Solu√ß√£o</strong>: Implementar m√©tricas de avalia√ß√£o como:</p>
<ul>
<li>Compara√ß√µes com respostas conhecidas (ground truth)</li>
<li>Detec√ß√£o de drift em embeddings</li>
<li>Monitoramento de lat√™ncia e taxa de falhas</li>
</ul></blockquote>
<h3 id="armadilha-4-consultas-complexas-em-pipelines-simples">Armadilha 4: Consultas Complexas em Pipelines Simples</h3>
<p>Muitas consultas do mundo real s√£o complexas demais para uma √∫nica etapa de recupera√ß√£o. Se uma pergunta requer sintetizar v√°rias informa√ß√µes, um pipeline RAG padr√£o pode falhar.</p>
<p><strong>Solu√ß√£o</strong>: Implementar fluxos de trabalho mais sofisticados:</p>
<ul>
<li>Workflows com agentes</li>
<li>Recupera√ß√£o multi-hop</li>
<li>Gera√ß√£o din√¢mica de prompts</li>
</ul>
<h2 id="t√©cnicas-avan√ßadas-de-otimiza√ß√£o">T√©cnicas Avan√ßadas de Otimiza√ß√£o</h2>
<p>Agora que entendemos os fundamentos e as armadilhas comuns, vamos explorar t√©cnicas espec√≠ficas para melhorar cada componente do nosso sistema RAG.</p>
<h3 id="re-ranqueamento-de-chunks">Re-ranqueamento de Chunks</h3>


  
    
  
  <div class="mermaid">flowchart LR
    subgraph &#34;Primeira Fase&#34;
        Q[Consulta] --&gt; EMB[Embedding da Consulta]
        EMB --&gt; SIM[Busca por Similaridade Vetorial]
        DB[(Base Vetorial)] --&gt; SIM
        SIM --&gt; IC[Chunks Iniciais]
    end
    
    subgraph &#34;Re-ranqueamento&#34;
        IC --&gt; PAIR[Pares Consulta-Chunk]
        Q2[Consulta Original] --&gt; PAIR
        PAIR --&gt; CENC[Cross-Encoder]
        CENC --&gt; SCORE[Scores de Relev√¢ncia]
        SCORE --&gt; SORT[Ordena√ß√£o por Relev√¢ncia]
        SORT --&gt; RC[Chunks Re-ranqueados]
    end
    
    IC -.-&gt; |Top-K Chunks| PAIR
    RC --&gt; GEN[Gera√ß√£o de Resposta]
    
    style Q fill:#f9f,stroke:#333,stroke-width:2px
    style Q2 fill:#f9f,stroke:#333,stroke-width:2px
    style CENC fill:#ffc,stroke:#333,stroke-width:2px
    style RC fill:#9f9,stroke:#333,stroke-width:2px
    style GEN fill:#99f,stroke:#333,stroke-width:2px</div>
 <p>O diagrama acima ilustra o processo de re-ranqueamento em um sistema RAG, dividido em duas fases principais:</p>
<ol>
<li>
<p>Na &ldquo;Primeira Fase&rdquo;, o fluxo come√ßa com a consulta do usu√°rio que √© transformada em um embedding vetorial. Este embedding √© ent√£o utilizado para realizar uma busca por <a href="https://en.wikipedia.org/wiki/Vector_database">similaridade vetorial</a> na base de dados vetoriais, resultando em um conjunto inicial de chunks relevantes.</p>
</li>
<li>
<p>A segunda fase, &ldquo;Re-ranqueamento&rdquo;, representa o refinamento desses resultados iniciais. Os chunks recuperados s√£o combinados com a consulta original para formar pares consulta-chunk. Estes pares s√£o processados por um <a href="https://en.wikipedia.org/wiki/Cross-encoder">cross-encoder</a>, um modelo especializado que avalia a relev√¢ncia contextual entre a consulta e cada chunk. O cross-encoder gera scores de relev√¢ncia que permitem uma ordena√ß√£o mais precisa, resultando em chunks re-ranqueados que s√£o finalmente utilizados para a gera√ß√£o da resposta final.</p>
</li>
</ol>
<p>Esta abordagem em duas etapas combina a efici√™ncia computacional dos embeddings (que permitem busca r√°pida em grandes bases de dados) com a precis√£o dos cross-encoders (que capturam melhor as rela√ß√µes sem√¢nticas entre consulta e documento), superando as limita√ß√µes de cada m√©todo quando usado isoladamente. Abordagem conceitual de como implementar re-ranqueamento com cross-encoder em Clojure:</p>


  <pre><code class="language-clojure">;; Exemplo conceitual de como implementar re-ranqueamento com cross-encoder
(defn rerank-results
  &#34;Re-classifica resultados usando cross-encoder para melhorar a precis√£o&#34;
  [query initial-results n]
  (let [;; Em um cen√°rio real, usar√≠amos uma biblioteca Clojure para acessar modelos
        ;; Como o clj-huggingface ou wrapper Java para transformers
        cross-encoder (load-cross-encoder &#34;cross-encoder/ms-marco-MiniLM-L-6-v2&#34;)
        
        ;; Preparar pares de consulta-documento para avalia√ß√£o
        pairs (map (fn [doc] [query (:conteudo doc)]) initial-results)
        
        ;; Obter scores de relev√¢ncia do cross-encoder
        scores (predict-with-cross-encoder cross-encoder pairs)
        
        ;; Associar scores aos resultados originais
        results-with-scores (map-indexed 
                              (fn [idx doc] 
                                (assoc doc :relevance_score (nth scores idx)))
                              initial-results)
        
        ;; Ordenar por score de relev√¢ncia (do maior para o menor)
        reranked-results (sort-by :relevance_score &gt; results-with-scores)]
    
    ;; Retornar apenas os top-n resultados
    (take n reranked-results)))

;; Fun√ß√µes auxiliares (implementa√ß√µes dependeriam da biblioteca espec√≠fica usada)
(defn load-cross-encoder [model-name]
  ;; Carregar modelo cross-encoder usando Java interop ou biblioteca espec√≠fica
  (println &#34;Carregando modelo&#34; model-name)
  {:model-name model-name})

(defn predict-with-cross-encoder [model pairs]
  ;; Executar predi√ß√£o do cross-encoder nos pares consulta-documento
  ;; Retorna um vetor de scores de relev√¢ncia
  (println &#34;Avaliando&#34; (count pairs) &#34;pares com&#34; (:model-name model))
  (vec (repeatedly (count pairs) #(rand))))</code></pre>
 <p>No contexto do <a href="/2025/03/25/semantic-postgresql/">DocAI com PostgreSQL</a>, podemos implementar isso como:</p>


  <pre><code class="language-clojure">;; Exemplo de implementa√ß√£o de re-ranqueamento em Clojure para DocAI
(defn rerank-results
  &#34;Re-classifica resultados usando cross-encoder&#34;
  [query initial-results]
  (let [conn (jdbc/get-connection db-spec)
        ;; Construir array de IDs para consulta SQL
        ids (str/join &#34;,&#34; (map :id initial-results))
        ;; Consulta SQL que utiliza fun√ß√£o do pgai para re-classifica√ß√£o
        sql (str &#34;SELECT d.id, d.titulo, d.conteudo, 
                 ai.relevance_score(&#39;&#34; query &#34;&#39;, d.conteudo) AS relevance  -- ‚ö†Ô∏è Nota: Fun√ß√£o experimental no pgai
                 FROM documentos d 
                 WHERE d.id IN (&#34; ids &#34;) 
                 ORDER BY relevance DESC&#34;)]
    (jdbc/execute! conn [sql])))</code></pre>
 <p>O primeiro c√≥digo demonstra uma implementa√ß√£o conceitual de re-ranqueamento usando um cross-encoder em Clojure. Ele recebe uma consulta e resultados iniciais, utiliza um modelo cross-encoder para avaliar a relev√¢ncia de cada documento em rela√ß√£o √† consulta, e ent√£o reordena os resultados com base nos scores obtidos. As fun√ß√µes auxiliares simulam a integra√ß√£o com modelos de machine learning, embora em um cen√°rio real seria necess√°rio utilizar bibliotecas espec√≠ficas para acessar modelos de linguagem.</p>
<p>O segundo exemplo mostra uma implementa√ß√£o mais pr√°tica no contexto de um sistema <a href="/2025/03/25/semantic-postgresql/">DocAI integrado com PostgreSQL</a>. Neste caso, o re-ranqueamento √© delegado a uma fun√ß√£o SQL (<code>ai.relevance_score</code>) que avalia a relev√¢ncia entre a consulta e o conte√∫do do documento diretamente no banco de dados. Esta abordagem aproveita as capacidades de IA incorporadas no PostgreSQL atrav√©s de extens√µes como pgai, simplificando a arquitetura ao mover o processamento de relev√¢ncia para o banco de dados.</p>
<p>Ambas as implementa√ß√µes ilustram diferentes estrat√©gias para melhorar a precis√£o dos resultados em sistemas RAG. A primeira abordagem oferece mais controle e flexibilidade ao processar o re-ranqueamento na aplica√ß√£o, enquanto a segunda aproveita as capacidades do banco de dados para simplificar a arquitetura e potencialmente melhorar o desempenho ao reduzir a transfer√™ncia de dados entre a aplica√ß√£o e o banco de dados. A escolha entre estas abordagens depender√° dos requisitos espec√≠ficos do sistema, incluindo considera√ß√µes de desempenho, escalabilidade e facilidade de manuten√ß√£o.</p>
<hr>
<h3 id="estrat√©gias-de-chunking-din√¢mico">Estrat√©gias de Chunking Din√¢mico</h3>
<p>Em vez de usar um tamanho fixo para todos os chunks, podemos implementar estrat√©gias din√¢micas que se adaptam ao conte√∫do:</p>
<ul>
<li><strong>Chunking Sem√¢ntico</strong>: Dividir o texto em unidades semanticamente coerentes</li>
<li><strong>Chunking Hier√°rquico</strong>: Manter m√∫ltiplas granularidades do mesmo conte√∫do</li>
<li><strong>Chunking Adaptativo</strong>: Ajustar tamanho com base em caracter√≠sticas do documento</li>
</ul>


  <pre><code class="language-clojure">;; Fun√ß√£o conceitual para chunking hier√°rquico
(defn create-hierarchical-chunks
  &#34;Cria chunks em m√∫ltiplos n√≠veis de granularidade&#34;
  [document]
  (let [;; Divis√£o em par√°grafos
        paragraphs (split-paragraphs document)
        ;; Divis√£o em se√ß√µes
        sections (split-sections document)
        ;; Documento completo
        full-doc [{:content document :level &#34;document&#34;}]
        ;; Combinar todos os n√≠veis
        all-chunks (concat full-doc
                          (map #(hash-map :content % :level &#34;section&#34;) sections)
                          (map #(hash-map :content % :level &#34;paragraph&#34;) paragraphs))]
    ;; Inserir no PostgreSQL com metadados sobre o n√≠vel
    (doseq [chunk all-chunks]
      (jdbc/execute! db-spec
                    [&#34;INSERT INTO documentos_hierarquicos 
                     (conteudo, nivel_granularidade) VALUES (?, ?)&#34;
                     (:content chunk) (:level chunk)]))))</code></pre>
 <p>O c√≥digo acima implementa uma estrat√©gia de <a href="https://en.wikipedia.org/wiki/Chunking_%28data_storage%29">chunking hier√°rquico</a> em <a href="https://clojure.org/">Clojure</a>, uma t√©cnica avan√ßada para sistemas <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> que mant√©m m√∫ltiplas representa√ß√µes do mesmo conte√∫do em diferentes n√≠veis de granularidade. A fun√ß√£o <code>create-hierarchical-chunks</code> divide um documento em tr√™s n√≠veis: documento completo, se√ß√µes e par√°grafos, preservando assim tanto o contexto amplo quanto os detalhes espec√≠ficos.</p>
<p>Esta abordagem permite que o sistema de recupera√ß√£o escolha a granularidade mais apropriada dependendo da consulta, oferecendo flexibilidade que um chunking de tamanho fixo n√£o consegue proporcionar.</p>
<p>A implementa√ß√£o utiliza fun√ß√µes auxiliares como <code>split-paragraphs</code> e <code>split-sections</code> (n√£o mostradas no c√≥digo) para segmentar o documento de forma inteligente, respeitando a estrutura sem√¢ntica do texto. Cada <a href="https://en.wikipedia.org/wiki/Chunk_%28data_storage%29">chunk</a> √© armazenado no <a href="https://www.postgresql.org/">PostgreSQL</a> junto com metadados sobre seu n√≠vel de granularidade, permitindo consultas que podem priorizar diferentes n√≠veis dependendo do tipo de pergunta.</p>
<p>Esta t√©cnica √© particularmente valiosa para documentos longos e estruturados, como artigos t√©cnicos ou documenta√ß√£o, onde tanto o contexto geral quanto detalhes espec√≠ficos podem ser relevantes dependendo da natureza da consulta do usu√°rio.</p>
<hr>
<h3 id="workflows-com-agentes-para-consultas-complexas">Workflows com Agentes para Consultas Complexas</h3>
<p>Para consultas que exigem racioc√≠nio em v√°rias etapas, podemos implementar agentes que decomp√µem o problema:</p>


  
  <div class="mermaid">flowchart TB
    Q[Consulta Original] --&gt; AN[Analisador de Consulta]
    AN --&gt; SQ1[Sub-quest√£o 1]
    AN --&gt; SQ2[Sub-quest√£o 2]
    AN --&gt; SQ3[Sub-quest√£o 3]
    
    SQ1 --&gt; R1[RAG Espec√≠fico 1]
    SQ2 --&gt; R2[RAG Espec√≠fico 2]
    SQ3 --&gt; R3[RAG Espec√≠fico 3]
    
    R1 --&gt; A1[Resposta Parcial 1]
    R2 --&gt; A2[Resposta Parcial 2]
    R3 --&gt; A3[Resposta Parcial 3]
    
    A1 --&gt; S[Sintetizador]
    A2 --&gt; S
    A3 --&gt; S
    
    S --&gt; FR[Resposta Final]
    
    style Q fill:#f9f,stroke:#333,stroke-width:2px
    style S fill:#bbf,stroke:#333,stroke-width:2px
    style FR fill:#bfb,stroke:#333,stroke-width:2px</div>
 <p>Este diagrama ilustra uma arquitetura de <a href="https://en.wikipedia.org/wiki/Workflow">workflow</a> baseada em agentes para processamento de consultas complexas em sistemas <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a>. O fluxo come√ßa com uma consulta do usu√°rio que √© analisada por um componente <a href="https://en.wikipedia.org/wiki/Query_parser">Analisador</a>, respons√°vel por decompor a pergunta original em sub-quest√µes mais espec√≠ficas e gerenci√°veis. Cada sub-quest√£o √© ent√£o direcionada para um pipeline <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> especializado, permitindo recupera√ß√µes contextuais mais precisas.</p>
<p>A abordagem <a href="https://en.wikipedia.org/wiki/Divide_and_conquer_algorithm">divide-e-conquista</a> demonstrada no diagrama permite que o sistema lide com perguntas que exigiriam conhecimento de diferentes dom√≠nios ou documentos. Cada <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> especializado pode utilizar diferentes bases de conhecimento, estrat√©gias de recupera√ß√£o ou at√© mesmo modelos de linguagem otimizados para dom√≠nios espec√≠ficos, resultando em respostas parciais de alta qualidade para cada aspecto da consulta.</p>
<p>O componente Sintetizador atua como o elemento integrador final, combinando as respostas parciais em uma resposta coerente e abrangente. Esta arquitetura modular n√£o apenas melhora a precis√£o das respostas para consultas complexas, mas tamb√©m oferece maior transpar√™ncia no processo de racioc√≠nio, permitindo identificar quais fontes contribu√≠ram para cada parte da resposta final. O resultado √© um sistema RAG mais robusto, capaz de lidar com consultas que exigem racioc√≠nio em m√∫ltiplas etapas e integra√ß√£o de informa√ß√µes de diversas fontes.</p>


  <pre><code class="language-clojure">(defn agent-rag-workflow
  &#34;Implementa um workflow de agente para consultas complexas&#34;
  [query]
  (let [;; Passo 1: Analisar a consulta e identificar sub-quest√µes
        sub-questions (analyze-query query)
        ;; Passo 2: Buscar informa√ß√µes para cada sub-quest√£o
        sub-answers (map #(retrieve-and-generate %) sub-questions)
        ;; Passo 3: Sintetizar respostas parciais em uma resposta final
        final-context (str/join &#34;\n\n&#34; sub-answers)
        final-prompt (str &#34;Com base nas seguintes informa√ß√µes:\n\n&#34; 
                         final-context 
                         &#34;\n\nResponda √† pergunta original: &#34; query)
        final-answer (generate-response final-prompt)]
    final-answer))

(defn analyze-query
  &#34;Divide uma consulta complexa em sub-quest√µes&#34;
  [query]
  (let [prompt (str &#34;Divida a seguinte pergunta em sub-quest√µes independentes:\n\n&#34; query)
        response (call-ollama-api prompt)
        ;; Parsear a resposta para extrair as sub-quest√µes
        sub-questions (parse-sub-questions response)]
    sub-questions))</code></pre>
 <p>Uma implementa√ß√£o mais robusta de workflows com agentes envolve v√°rias etapas adicionais. Trataremos deste assunto em um pr√≥ximo artigo.</p>
<hr>
<h4 id="arquitetura-de-agentes-avan√ßada">Arquitetura de Agentes Avan√ßada</h4>
<p>Os sistemas de agentes RAG mais sofisticados aplicam o conceito de <strong>ReAct</strong> (Racioc√≠nio + A√ß√£o) para processar consultas complexas:</p>


  
  <div class="mermaid">flowchart TB
    subgraph &#34;Arquitetura ReAct para RAG&#34;
    Q[Consulta do Usu√°rio] --&gt; PL[Planejador]
    PL --&gt; PLAN[Plano de Execu√ß√£o]
    PLAN --&gt; RT[Roteador]
    
    RT --&gt;|Sub-tarefa 1| AS[Agente de Pesquisa]
    RT --&gt;|Sub-tarefa 2| AR[Agente de Racioc√≠nio]
    RT --&gt;|Sub-tarefa 3| AC[Agente de C√°lculo]
    
    AS --&gt; OR[Orquestrador]
    AR --&gt; OR
    AC --&gt; OR
    
    OR --&gt; SI[Sintetizador]
    SI --&gt; RES[Resposta Final]
    end
    
    subgraph &#34;Ferramentas e Recursos&#34;
    AS -.-&gt; VDB[(Base Vetorial)]
    AR -.-&gt; LLM[Modelo de Linguagem]
    AC -.-&gt; CALC[Ferramentas de C√°lculo]
    end
    
    style Q fill:#f9f,stroke:#333,stroke-width:2px
    style PLAN fill:#ffc,stroke:#333,stroke-width:2px
    style RT fill:#9cf,stroke:#333,stroke-width:2px
    style AS fill:#bbf,stroke:#333,stroke-width:2px
    style AR fill:#bbf,stroke:#333,stroke-width:2px
    style AC fill:#bbf,stroke:#333,stroke-width:2px
    style SI fill:#bfb,stroke:#333,stroke-width:2px
    style RES fill:#f99,stroke:#333,stroke-width:2px</div>
 <p>Este diagrama ilustra uma arquitetura avan√ßada ReAct para sistemas <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a>, mostrando como uma consulta complexa √© processada atrav√©s de m√∫ltiplos componentes especializados. O fluxo come√ßa com a consulta do usu√°rio sendo analisada por um <a href="https://en.wikipedia.org/wiki/Workflow">Planejador</a>, que cria um plano estruturado de execu√ß√£o.</p>
<p>Este plano √© ent√£o gerenciado por um <a href="https://en.wikipedia.org/wiki/Routing">Roteador</a> que distribui sub-tarefas para agentes especializados (Pesquisa, Racioc√≠nio e C√°lculo), cada um interagindo com recursos espec√≠ficos como bases de dados vetoriais, LLMs ou ferramentas de c√°lculo.</p>
<p>A for√ßa desta arquitetura est√° na sua capacidade de decompor problemas complexos em tarefas gerenci√°veis e especializadas, permitindo que cada componente se concentre no que faz melhor. O <a href="https://en.wikipedia.org/wiki/Orchestration">Orquestrador</a> coordena os resultados dos diferentes agentes, enquanto o <a href="https://en.wikipedia.org/wiki/Synthesis">Sintetizador</a> integra todas as informa√ß√µes em uma resposta final coerente. Esta abordagem modular n√£o apenas melhora a precis√£o das respostas, mas tamb√©m aumenta a transpar√™ncia do processo de racioc√≠nio e facilita a depura√ß√£o e otimiza√ß√£o de componentes individuais do sistema RAG.</p>
<ul>
<li><strong>Planejador</strong>: Analisa a consulta e cria um plano de execu√ß√£o</li>
<li><strong>Roteador</strong>: Direciona sub-consultas para ferramentas especializadas</li>
<li><strong>Agentes Especializados</strong>: Executam tarefas espec√≠ficas
<ul>
<li>Agente de Pesquisa: Recupera informa√ß√µes da base de conhecimento</li>
<li>Agente de Racioc√≠nio: Realiza infer√™ncias l√≥gicas sobre os dados recuperados</li>
<li>Agente de C√°lculo: Processa c√°lculos e an√°lises num√©ricas</li>
</ul>
</li>
<li><strong>Orquestrador</strong>: Gerencia o fluxo de informa√ß√µes entre agentes</li>
<li><strong>Sintetizador</strong>: Combina as respostas em um resultado coerente</li>
</ul>
<p>Vamos analisar o c√≥digo abaixo para entender como funciona um sistema ReAct para RAG:</p>


  <pre><code class="language-clojure">;; Exemplo conceitual de um sistema ReAct para RAG
(defn react-agent
  &#34;Implementa um agente ReAct para consultas complexas&#34;
  [query]
  (let [;; Determinar se a consulta precisa de um plano
        plan-needed? (complex-query? query)
        ;; Se necess√°rio, criar um plano
        execution-plan (when plan-needed?
                         (create-execution-plan query))
        ;; Executar o plano ou a consulta direta
        result (if plan-needed?
                 (execute-plan execution-plan)
                 (simple-rag-query query))]
    result))

(defn execute-plan
  &#34;Executa um plano com agentes especializados&#34;
  [plan]
  (loop [steps (:steps plan)
         context {}
         responses []]
    (if (empty? steps)
      ;; Sintetizar respostas em um resultado final
      (synthesize-responses responses (:query plan))
      (let [current-step (first steps)
            agent-type (:agent current-step)
            ;; Determinar qual agente especializado usar
            agent-fn (case agent-type
                       :search search-agent
                       :reasoning reasoning-agent
                       :calculation calculation-agent
                       :default default-agent)
            ;; Executar o agente com o contexto atual
            step-result (agent-fn (:input current-step) context)
            ;; Atualizar o contexto com o resultado
            updated-context (assoc context (:id current-step) step-result)]
        (recur (rest steps) 
               updated-context 
               (conj responses step-result))))))</code></pre>
 <p>O c√≥digo implementa um agente ReAct (Reasoning + Acting) para consultas complexas em um sistema <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a>. A fun√ß√£o principal <code>react-agent</code> avalia se a consulta requer um plano de execu√ß√£o complexo ou pode ser processada diretamente. Para consultas complexas, cria-se um plano estruturado que √© executado pela fun√ß√£o <code>execute-plan</code>, que utiliza um loop para processar cada etapa do plano sequencialmente.</p>
<p>O sistema emprega agentes especializados (busca, racioc√≠nio, c√°lculo) selecionados dinamicamente com base no tipo de tarefa. Cada agente contribui com resultados parciais que s√£o acumulados em um contexto compartilhado, permitindo que etapas posteriores utilizem informa√ß√µes de etapas anteriores. Finalmente, todas as respostas s√£o sintetizadas em um resultado coerente.</p>
<p>Esta arquitetura modular permite decompor problemas complexos em tarefas gerenci√°veis, melhorando a precis√£o e facilitando a manuten√ß√£o do sistema.Para implementa√ß√µes detalhadas de sistemas de agentes RAG, consulte:</p>
<ul>
<li><a href="https://docs.llamaindex.ai/en/stable/examples/agent/react_agent.html">LlamaIndex - Implementando ReAct Agents</a></li>
<li><a href="https://python.langchain.com/docs/modules/agents/agent_types/multi_agent">LangChain - Multi-Agent Systems</a></li>
<li><a href="https://huggingface.co/blog/autonomous-agents">HuggingFace - Agentes Aut√¥nomos</a></li>
</ul>
<h4 id="casos-de-uso-para-workflows-de-agentes">Casos de Uso para Workflows de Agentes</h4>
<p>Os workflows com agentes s√£o particularmente √∫teis em cen√°rios como:</p>
<ul>
<li><strong>Pesquisa Cient√≠fica</strong>: Onde diversas fontes precisam ser consultadas e relacionadas</li>
<li><strong>Diagn√≥stico de Problemas</strong>: Quando √© necess√°rio seguir uma √°rvore de decis√£o</li>
<li><strong>An√°lise de Documentos Complexos</strong>: Como contratos ou documenta√ß√£o t√©cnica</li>
<li><strong>Planejamento Estrat√©gico</strong>: Onde m√∫ltiplas dimens√µes precisam ser consideradas</li>
</ul>
<hr>
<h3 id="pipelines-multimodais">Pipelines Multimodais</h3>
<p>Integrar entradas multimodais (texto, imagens, tabelas) em um pipeline RAG pode enriquecer significativamente o contexto:</p>


  
  <div class="mermaid">flowchart LR
    subgraph &#34;Documento Misto&#34;
    TXT[Texto]
    IMG[Imagens]
    TBL[Tabelas]
    end
    
    subgraph &#34;Processadores Espec√≠ficos&#34;
    TXT --&gt; TXT_P[Processador de Texto]
    IMG --&gt; IMG_P[Processador de Imagem]
    TBL --&gt; TBL_P[Processador de Tabela]
    end
    
    subgraph &#34;Embeddings&#34;
    TXT_P --&gt; TXT_E[Embedding de Texto]
    IMG_P --&gt; IMG_E[Embedding de Imagem]
    TBL_P --&gt; TBL_E[Embedding de Tabela]
    end
    
    TXT_E --&gt; FUS[Fus√£o de Representa√ß√µes]
    IMG_E --&gt; FUS
    TBL_E --&gt; FUS
    
    FUS --&gt; DB[(Base de Dados Multimodal)]
    Q[Consulta do Usu√°rio] --&gt; Q_PROC[Processador de Consulta]
    Q_PROC --&gt; RAG[Motor RAG]
    DB --&gt; RAG
    RAG --&gt; RES[Resposta Multimodal]
    
    style TXT fill:#f9f,stroke:#333,stroke-width:2px
    style IMG fill:#9cf,stroke:#333,stroke-width:2px
    style TBL fill:#fcf,stroke:#333,stroke-width:2px
    style FUS fill:#ff9,stroke:#333,stroke-width:2px
    style DB fill:#9f9,stroke:#333,stroke-width:2px
    style RES fill:#f99,stroke:#333,stroke-width:2px</div>
 <p>O diagrama acima ilustra uma arquitetura de pipeline multimodal para sistemas <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a>, demonstrando como diferentes tipos de conte√∫do (texto, imagens e tabelas) podem ser processados e integrados em um √∫nico sistema de recupera√ß√£o. O fluxo come√ßa com a extra√ß√£o desses diferentes elementos de um documento misto, cada um seguindo para processadores especializados que compreendem as caracter√≠sticas √∫nicas de cada modalidade.</p>
<p>Na camada de embeddings, cada tipo de conte√∫do √© transformado em representa√ß√µes vetoriais espec√≠ficas para sua modalidade - textos s√£o processados por modelos de linguagem, imagens por modelos de vis√£o computacional, e tabelas por processadores estruturados. O componente de fus√£o de representa√ß√µes √© crucial nesta arquitetura, pois combina estas diferentes representa√ß√µes vetoriais em um formato unificado que pode ser armazenado e consultado eficientemente na base de dados multimodal.</p>
<p>Quando uma consulta do usu√°rio √© recebida, ela passa pelo processador de consulta que determina quais modalidades s√£o relevantes para a pergunta, e o motor <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> recupera as informa√ß√µes apropriadas da base de dados multimodal. Esta abordagem permite que o sistema forne√ßa respostas enriquecidas que incorporam conhecimento de m√∫ltiplas modalidades, resultando em uma experi√™ncia mais completa e contextualmente relevante para o usu√°rio, especialmente para consultas que se beneficiam de informa√ß√µes visuais ou estruturadas al√©m do texto puro.</p>


  <pre><code class="language-clojure">(defn process-multimodal-document
  &#34;Processa um documento que cont√©m texto e imagens&#34;
  [doc-path]
  (let [;; Extrair texto
        text-content (extract-text doc-path)
        ;; Identificar e extrair imagens
        image-paths (extract-images doc-path)
        ;; Gerar descri√ß√µes para as imagens usando um modelo de vis√£o
        image-descriptions (map #(describe-image %) image-paths)
        ;; Combinar texto e descri√ß√µes de imagens
        enriched-content (str text-content &#34;\n\n&#34;
                             &#34;O documento cont√©m as seguintes imagens:\n&#34;
                             (str/join &#34;\n&#34; image-descriptions))]
    ;; Inserir no banco de dados
    (jdbc/execute! db-spec
                  [&#34;INSERT INTO documentos (titulo, conteudo) VALUES (?, ?)&#34;
                   (extract-title doc-path) enriched-content])))</code></pre>
 <hr>
<h4 id="arquitetura-multimodal-completa">Arquitetura Multimodal Completa</h4>
<p>Uma implementa√ß√£o mais completa de pipelines multimodais requer v√°rios componentes especializados:</p>


  
  <div class="mermaid">flowchart TD
    DOC[Documento Multimodal] --&gt; DETECT[Detector de Tipo]
    DETECT --&gt; EXTRACT[Extra√ß√£o de Componentes]
    
    EXTRACT --&gt; TX[Componentes de Texto]
    EXTRACT --&gt; IMG[Componentes de Imagem]
    EXTRACT --&gt; TBL[Componentes de Tabela]
    EXTRACT --&gt; AUD[Componentes de √Åudio]
    
    TX --&gt; TX_PROC[Processador de Texto]
    IMG --&gt; IMG_PROC[Processador de Imagem]
    TBL --&gt; TBL_PROC[Processador de Tabela]
    AUD --&gt; AUD_PROC[Processador de √Åudio]
    
    TX_PROC --&gt; TX_EMB[Embedding de Texto]
    IMG_PROC --&gt; IMG_EMB[Embedding de Imagem]
    TBL_PROC --&gt; TBL_EMB[Embedding de Tabela]
    AUD_PROC --&gt; AUD_EMB[Embedding de √Åudio]
    
    TX_EMB --&gt; FUSION[Fus√£o de Representa√ß√µes]
    IMG_EMB --&gt; FUSION
    TBL_EMB --&gt; FUSION
    AUD_EMB --&gt; FUSION
    
    FUSION --&gt; META[Adi√ß√£o de Metadados]
    META --&gt; STORE[Armazenamento em PostgreSQL]
    
    subgraph &#34;Modelos Espec√≠ficos&#34;
        TX_PROC -.- TEXT_MODEL[Modelo de Texto]
        IMG_PROC -.- CLIP[CLIP]
        TBL_PROC -.- TABLE_MODEL[Modelo de Tabela]
        AUD_PROC -.- AUDIO_MODEL[Modelo de √Åudio]
        FUSION -.- FLAMINGO[Flamingo]
    end
    
    style DOC fill:#f9f,stroke:#333,stroke-width:2px
    style FUSION fill:#ff9,stroke:#333,stroke-width:2px
    style META fill:#9cf,stroke:#333,stroke-width:2px
    style STORE fill:#9f9,stroke:#333,stroke-width:2px</div>
 <p>O diagrama acima ilustra uma arquitetura para <a href="https://en.wikipedia.org/wiki/Multimodal_AI">processamento de documentos multimodais</a> em sistemas RAG avan√ßados. O fluxo come√ßa com um documento multimodal que passa por um <a href="https://en.wikipedia.org/wiki/Type_detection">detector de tipo</a>, seguido pela <a href="https://en.wikipedia.org/wiki/Component_extraction">extra√ß√£o de componentes</a> que separa o conte√∫do em diferentes modalidades: texto, imagem, tabela e √°udio. Cada tipo de componente √© ent√£o direcionado para um processador especializado, projetado para extrair informa√ß√µes significativas espec√≠ficas daquela modalidade.</p>
<p>Ap√≥s o processamento inicial, cada componente √© transformado em uma <a href="https://en.wikipedia.org/wiki/Embedding_model">representa√ß√£o vetorial (embedding)</a> usando modelos especializados para cada modalidade - <a href="https://en.wikipedia.org/wiki/Text_embedding">modelos de texto para componentes textuais</a>, <a href="https://en.wikipedia.org/wiki/CLIP">CLIP para imagens</a>, <a href="https://en.wikipedia.org/wiki/Table_embedding">modelos espec√≠ficos para tabelas</a> e <a href="https://en.wikipedia.org/wiki/Audio_embedding">√°udio</a>. Estes embeddings s√£o ent√£o combinados atrav√©s de um processo de fus√£o de representa√ß√µes, que cria uma compreens√£o unificada e coerente do documento multimodal, potencialmente utilizando modelos como o <a href="https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model">Flamingo</a> que s√£o projetados para integra√ß√£o multimodal.</p>
<p>A etapa final do pipeline envolve a adi√ß√£o de <a href="https://en.wikipedia.org/wiki/Metadata">metadados estruturados</a> √† <a href="https://en.wikipedia.org/wiki/Unified_representation">representa√ß√£o unificada</a> e seu armazenamento em um banco de dados <a href="https://www.postgresql.org/">PostgreSQL</a> otimizado para <a href="https://en.wikipedia.org/wiki/Vector_database">busca vetorial</a> com <a href="https://github.com/pgvector/pgvector">pgvector</a>.</p>
<p>Esta arquitetura modular permite que o sistema <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> processe eficientemente documentos complexos contendo m√∫ltiplos tipos de m√≠dia, mantendo as rela√ß√µes sem√¢nticas entre diferentes componentes e possibilitando recupera√ß√£o mais precisa quando consultado. Os modelos espec√≠ficos destacados no diagrama (<code>TEXT_MODEL</code>, <code>CLIP</code>, <code>TABLE_MODEL</code>, <code>AUDIO_MODEL</code> e <code>FLAMINGO</code>) representam as tecnologias de ponta que podem ser empregadas em cada etapa do processamento.</p>
<p>O c√≥digo abaixo implementa um pipeline avan√ßado para processamento de documentos multimodais em <a href="https://clojure.org/">Clojure</a>, demonstrando uma abordagem sofisticada para lidar com conte√∫do heterog√™neo em sistemas <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a>:</p>


  <pre><code class="language-clojure">;; Exemplo de pipeline multimodal mais elaborado
(defn advanced-multimodal-processor
  &#34;Pipeline completo para processamento multimodal&#34;
  [document-path]
  (let [;; Determinar tipo de documento
        doc-type (detect-document-type document-path)
        
        ;; Extrair componentes por tipo
        components (case doc-type
                     :pdf (extract-pdf-components document-path)
                     :doc (extract-doc-components document-path)
                     :webpage (extract-webpage-components document-path)
                     (extract-text-components document-path))
        
        ;; Processar cada componente com seu processador especializado
        processed-components (map process-component components)
        
        ;; Gerar embeddings multimodais
        embeddings (map #(generate-multimodal-embedding % doc-type) processed-components)
        
        ;; Criar representa√ß√£o unificada
        unified-representation {:components processed-components
                               :embeddings embeddings
                               :metadata {:doc-type doc-type
                                         :path document-path
                                         :extracted-at (java.util.Date.)}}]
    
    ;; Armazenar no PostgreSQL com schema adequado para multimodalidade
    (store-multimodal-document unified-representation)))

(defn process-component
  &#34;Processa um componente baseado em seu tipo&#34;
  [component]
  (case (:type component)
    :text (process-text (:content component))
    :image (process-image (:content component))
    :table (process-table (:content component))
    :chart (process-chart (:content component))
    :audio (process-audio (:content component))
    (:content component))) ;; Fallback para tipos desconhecidos</code></pre>
 <p>A fun√ß√£o principal <code>advanced-multimodal-processor</code> orquestra todo o fluxo, come√ßando pela detec√ß√£o do tipo de documento, seguida pela extra√ß√£o de componentes espec√≠ficos para cada formato (PDF, DOC, p√°ginas web), processamento especializado de cada componente, gera√ß√£o de embeddings multimodais e finalmente o armazenamento da representa√ß√£o unificada no PostgreSQL. Esta arquitetura modular permite que o sistema processe de forma inteligente diferentes tipos de m√≠dia dentro do mesmo documento.</p>
<p>A fun√ß√£o auxiliar <code>process-component</code> exemplifica o tratamento especializado para cada modalidade, direcionando o conte√∫do para processadores espec√≠ficos com base no tipo do componente (texto, imagem, tabela, gr√°fico ou √°udio). Esta abordagem granular garante que cada tipo de conte√∫do receba o tratamento mais apropriado, maximizando a qualidade da informa√ß√£o extra√≠da e sua representa√ß√£o vetorial.</p>
<p>O resultado √© um sistema <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> verdadeiramente <a href="https://en.wikipedia.org/wiki/Multimodal_AI">multimodal</a>, capaz de compreender e recuperar informa√ß√µes de documentos complexos que combinam texto, elementos visuais e dados estruturados, proporcionando respostas mais completas e contextualmente ricas para as consultas dos usu√°rios.</p>
<hr>
<h4 id="esquema-postgresql-para-dados-multimodais">Esquema PostgreSQL para Dados Multimodais</h4>
<p>Para armazenar e recuperar eficientemente dados multimodais no PostgreSQL:</p>


  <pre><code class="language-sql">-- Tabela principal para documentos multimodais
CREATE TABLE documentos_multimodais (
    id SERIAL PRIMARY KEY,
    titulo TEXT NOT NULL,
    doc_type TEXT,
    metadata JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Tabela para componentes espec√≠ficos
CREATE TABLE componentes_documento (
    id SERIAL PRIMARY KEY,
    documento_id INTEGER REFERENCES documentos_multimodais(id) ON DELETE CASCADE,
    tipo_componente TEXT NOT NULL,
    conteudo TEXT,
    posicao INTEGER,
    metadados JSONB
);

-- Tabela para embeddings de texto
CREATE TABLE embeddings_texto (
    id SERIAL PRIMARY KEY,
    componente_id INTEGER REFERENCES componentes_documento(id) ON DELETE CASCADE,
    embedding VECTOR(768)
);

-- Tabela para embeddings de imagem
CREATE TABLE embeddings_imagem (
    id SERIAL PRIMARY KEY,
    componente_id INTEGER REFERENCES componentes_documento(id) ON DELETE CASCADE,
    embedding VECTOR(512)
);</code></pre>
 <p>Este esquema (scheme) <a href="https://en.wikipedia.org/wiki/SQL">SQL</a> estabelece uma estrutura robusta para armazenar e gerenciar documentos multimodais no <a href="https://www.postgresql.org/">PostgreSQL</a>. A arquitetura √© composta por quatro tabelas interconectadas: uma tabela principal (<code>documentos_multimodais</code>) que armazena metadados gerais dos documentos, uma tabela para componentes espec√≠ficos (<code>componentes_documento</code>) que fragmenta cada documento em suas partes constituintes (texto, imagens, etc.), e duas tabelas especializadas para armazenar embeddings vetoriais de diferentes modalidades (<code>embeddings_texto</code> e <code>embeddings_imagem</code>). Esta estrutura relacional permite uma organiza√ß√£o hier√°rquica do conte√∫do, mantendo a integridade referencial atrav√©s de chaves estrangeiras.</p>
<p>A separa√ß√£o dos <a href="https://en.wikipedia.org/wiki/Embedding_model">embeddings</a> por tipo de modalidade √© particularmente importante, pois diferentes tipos de conte√∫do geralmente requerem modelos de embedding distintos com dimensionalidades variadas (768 para texto e 512 para imagens no exemplo). Esta abordagem modular facilita a implementa√ß√£o de consultas multimodais eficientes, permitindo buscas por similaridade em cada modalidade separadamente ou de forma combinada.</p>
<p>Al√©m disso, o uso de campos <a href="https://www.postgresql.org/docs/current/datatype-json.html">JSONB</a> para metadados oferece flexibilidade para armazenar informa√ß√µes adicionais sem necessidade de alterar o esquema, tornando o sistema adapt√°vel a diferentes tipos de documentos e requisitos de aplica√ß√£o. Para implementa√ß√µes detalhadas de <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> multimodal, consulte:</p>
<ul>
<li><a href="https://docs.llamaindex.ai/en/stable/examples/multi_modal/">MultiModal RAG com LlamaIndex</a></li>
<li><a href="https://towardsdatascience.com/a-comprehensive-guide-to-multimodal-rag-ea72c387c6e8">Comprehensive Guide to MultiModal RAG</a></li>
<li><a href="https://huggingface.co/blog/idefics">Projeto IDEFICS para RAG Multimodal</a></li>
<li><a href="https://supabase.com/blog/image-search-using-ai-embeddings">Supabase - Image Search com pgvector</a></li>
</ul>
<hr>
<h4 id="desafios-de-implementa√ß√£o-multimodal">Desafios de Implementa√ß√£o Multimodal</h4>
<p>A implementa√ß√£o de pipelines multimodais traz desafios espec√≠ficos:</p>
<ol>
<li><strong>Alinhamento de Representa√ß√µes</strong>: Garantir que diferentes modalidades possam ser comparadas</li>
<li><strong>Gerenciamento de Recursos</strong>: Modelos multimodais s√£o computacionalmente exigentes</li>
<li><strong>Estrat√©gias de Fus√£o</strong>: Decidir quando fundir informa√ß√µes de diferentes modalidades
<ul>
<li>Fus√£o Precoce: Combinar antes do embedding</li>
<li>Fus√£o Tardia: Manter embeddings separados e combinar apenas no ranking final</li>
</ul>
</li>
</ol>
<blockquote>
<p>No pr√≥ximo artigo, exploraremos em profundidade como expandir o DocAI para oferecer suporte total a conte√∫do multimodal, com exemplos pr√°ticos de implementa√ß√£o e otimiza√ß√£o de desempenho.</p></blockquote>
<hr>
<h3 id="estrat√©gias-de-cache">Estrat√©gias de Cache</h3>
<p>Implementar caching pode reduzir drasticamente a lat√™ncia e os custos:</p>


  
  <div class="mermaid">flowchart TD
    Q[Consulta] --&gt; CH1{Cache L1?}
    CH1 --&gt;|Sim| RES1[Resposta do Cache L1]
    CH1 --&gt;|N√£o| CH2{Cache L2?}
    
    CH2 --&gt;|Sim| RES2[Resposta do Cache L2]
    CH2 --&gt;|N√£o| CH3{Cache L3?}
    
    CH3 --&gt;|Sim| RES3[Resposta do Cache L3]
    CH3 --&gt;|N√£o| PROC[Processamento RAG Completo]
    
    PROC --&gt; RES4[Nova Resposta]
    RES4 --&gt; STORE[Armazenar em Cache]
    STORE --&gt; RES[Resposta Final]
    
    RES1 --&gt; RES
    RES2 --&gt; RES
    RES3 --&gt; RES
    
    subgraph &#34;Camadas de Cache&#34;
    CH1
    CH2
    CH3
    end
    
    style Q fill:#f9f,stroke:#333,stroke-width:2px
    style PROC fill:#ffc,stroke:#333,stroke-width:2px
    style RES fill:#9f9,stroke:#333,stroke-width:2px
    style STORE fill:#9cf,stroke:#333,stroke-width:2px</div>
 <p>O diagrama acima ilustra uma estrat√©gia de cache em m√∫ltiplas camadas para sistemas RAG, uma t√©cnica fundamental para otimizar tanto a lat√™ncia quanto os custos operacionais. A arquitetura implementa tr√™s n√≠veis de cache <code>(L1, L2 e L3)</code>, cada um representando diferentes compromissos entre velocidade e abrang√™ncia. O cache <code>L1</code> tipicamente armazena respostas exatas para consultas id√™nticas, oferecendo resposta instant√¢nea quando h√° correspond√™ncia perfeita. O cache <code>L2</code> pode armazenar respostas para consultas semanticamente similares, enquanto o cache <code>L3</code> pode conter resultados parciais como embeddings pr√©-calculados ou chunks recuperados anteriormente.</p>
<p>Esta abordagem em cascata permite que o sistema evite o processamento <a href="https://en.wikipedia.org/wiki/Retrieval-Augmented_Generation">RAG</a> completo sempre que poss√≠vel, reduzindo significativamente o tempo de resposta e a carga computacional. Quando uma consulta n√£o encontra correspond√™ncia em nenhum n√≠vel de cache, apenas ent√£o o sistema executa o fluxo completo de <a href="https://en.wikipedia.org/wiki/Retrieval-Augmented_Generation">RAG</a>, que inclui gera√ß√£o de embeddings, recupera√ß√£o de contexto e infer√™ncia do <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a>.</p>
<blockquote>
<p>A nova resposta gerada √© ent√£o armazenada no cache apropriado para uso futuro, criando um sistema que se torna progressivamente mais eficiente √† medida que processa mais consultas. A implementa√ß√£o de uma estrat√©gia de cache multicamada como esta pode reduzir custos operacionais em at√© 70% em sistemas de produ√ß√£o com padr√µes de consulta repetitivos.</p></blockquote>
<p>Al√©m da economia de recursos, a redu√ß√£o na lat√™ncia melhora significativamente a experi√™ncia do usu√°rio, com respostas quase instant√¢neas para consultas frequentes. Para maximizar a efic√°cia, √© importante implementar pol√≠ticas de expira√ß√£o de cache e estrat√©gias de invalida√ß√£o para garantir que as informa√ß√µes permane√ßam atualizadas, especialmente em dom√≠nios onde os dados subjacentes mudam com frequ√™ncia. Abaixo, um exemplo de implementa√ß√£o de cache de dois n√≠veis em Clojure:</p>


  <pre><code class="language-clojure">;; Implementa√ß√£o de cache de dois n√≠veis em Clojure
(def embedding-cache (atom {}))
(def response-cache (atom {}))

(defn cached-embed
  &#34;Gera embedding para texto com cache&#34;
  [text]
  (if-let [cached (@embedding-cache text)]
    cached
    (let [embedding (generate-embedding text)]
      (swap! embedding-cache assoc text embedding)
      embedding)))

(defn cached-rag-query
  &#34;Executa consulta RAG com cache&#34;
  [query]
  (if-let [cached (@response-cache query)]
    (do
      (println &#34;Cache hit for query!&#34;)
      cached)
    (let [;; Processo RAG normal
          response (full-rag-process query)]
      ;; Armazenar no cache apenas para consultas n√£o-pessoais
      (when (not (personal-query? query))
        (swap! response-cache assoc query response))
      response)))</code></pre>
 <p>O c√≥digo acima implementa uma estrat√©gia de cache de dois n√≠veis em <a href="https://clojure.org/">Clojure</a> para otimizar sistemas <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a>. O primeiro n√≠vel (<code>embedding-cache</code>) armazena embeddings j√° calculados para textos, evitando a regenera√ß√£o desses vetores que √© computacionalmente intensiva. O segundo n√≠vel (<code>response-cache</code>) armazena respostas completas para consultas anteriores, permitindo retornar resultados instantaneamente quando uma consulta id√™ntica √© feita novamente.</p>
<p>A fun√ß√£o <code>cached-embed</code> verifica primeiro se o embedding j√° existe no cache antes de ger√°-lo, enquanto <code>cached-rag-query</code> implementa l√≥gica similar para respostas completas, incluindo uma verifica√ß√£o inteligente para evitar o cache de consultas pessoais.</p>
<p>Em produ√ß√£o com maior escala, esta abordagem poderia ser estendida para utilizar <a href="https://redis.io/">Redis</a> ou outras solu√ß√µes de cache distribu√≠do, mantendo os mesmos princ√≠pios fundamentais. Para o <a href="https://www.postgresql.org/">PostgreSQL</a>, podemos implementar <a href="https://www.postgresql.org/docs/current/pgvector-embeddings.html">cache de embeddings diretamente no banco</a>:</p>


  <pre><code class="language-sql">-- Criar tabela de cache para embeddings de consultas frequentes
CREATE TABLE IF NOT EXISTS query_embedding_cache (
  query_text TEXT PRIMARY KEY,
  embedding VECTOR(768),
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  hit_count INTEGER DEFAULT 1
);

-- Fun√ß√£o para obter embedding com cache
CREATE OR REPLACE FUNCTION get_cached_embedding(query TEXT)
RETURNS VECTOR AS $$
DECLARE
  cached_embedding VECTOR(768);
BEGIN
  -- Verificar se existe no cache
  SELECT embedding INTO cached_embedding
  FROM query_embedding_cache
  WHERE query_text = query;
  
  -- Se existe, atualizar contador e retornar
  IF FOUND THEN
    UPDATE query_embedding_cache 
    SET hit_count = hit_count &#43; 1 
    WHERE query_text = query;
    RETURN cached_embedding;
  ELSE
    -- Gerar novo embedding
    cached_embedding := ai.ollama_embed(&#39;nomic-embed-text&#39;, query);  -- ‚ö†Ô∏è Nota: Verifique a disponibilidade desta fun√ß√£o na sua instala√ß√£o
    
    -- Armazenar no cache
    INSERT INTO query_embedding_cache (query_text, embedding)
    VALUES (query, cached_embedding);
    
    RETURN cached_embedding;
  END IF;
END;
$$ LANGUAGE plpgsql;</code></pre>
 <p>Este c√≥digo SQL implementa um sistema de cache para embeddings de consultas no PostgreSQL, otimizando significativamente o desempenho de sistemas RAG em produ√ß√£o. A tabela <code>query_embedding_cache</code> armazena o texto da consulta como chave prim√°ria, junto com seu <a href="https://www.postgresql.org/docs/current/pgvector-embeddings.html">embedding vetorial</a>, <a href="https://www.postgresql.org/docs/current/functions-datetime.html">timestamp de cria√ß√£o</a> e um <a href="https://www.postgresql.org/docs/current/functions-math.html">contador de acessos</a>. Esta estrutura n√£o apenas evita o rec√°lculo de embeddings para consultas repetidas, mas tamb√©m fornece dados valiosos sobre padr√µes de uso atrav√©s do campo <code>hit_count</code>.</p>
<p>A fun√ß√£o <code>get_cached_embedding</code> encapsula a l√≥gica de cache com uma interface limpa: quando uma consulta √© recebida, ela primeiro verifica se o embedding j√° existe no cache. Se encontrado, incrementa o contador de acessos e retorna imediatamente o embedding armazenado, economizando o custo computacional da gera√ß√£o de embeddings. Caso contr√°rio, gera um novo embedding usando o modelo &rsquo;nomic-embed-text&rsquo; via <a href="https://ollama.com/">Ollama</a>, armazena-o no cache para uso futuro e o retorna.</p>
<p>Esta implementa√ß√£o reduz significativamente a lat√™ncia para consultas repetidas, diminui a carga nos servi√ßos de embedding, e proporciona uma base para an√°lises de desempenho e otimiza√ß√£o cont√≠nua. A abordagem √© particularmente eficaz em cen√°rios onde os usu√°rios tendem a fazer perguntas semelhantes ou quando o sistema processa grandes volumes de consultas, resultando em economia de recursos computacionais e melhoria na experi√™ncia do usu√°rio com respostas mais r√°pidas.</p>
<h4 id="estrat√©gias-avan√ßadas-de-cache-para-rag">Estrat√©gias Avan√ßadas de Cache para RAG</h4>
<p>Para sistemas RAG em produ√ß√£o, podemos implementar estrat√©gias de cache mais sofisticadas:</p>
<ol>
<li>
<p><a href="https://en.wikipedia.org/wiki/Multilevel_cache"><strong>Cache em M√∫ltiplas Camadas</strong></a>:</p>
<ul>
<li>L1: Cache em mem√≥ria para consultas muito frequentes</li>
<li>L2: Cache em banco de dados para persist√™ncia entre reinicializa√ß√µes</li>
<li>L3: Cache distribu√≠do (como <a href="https://redis.io/">Redis</a>) para sistemas escal√°veis</li>
</ul>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/Time_to_live"><strong>Pol√≠ticas de Expira√ß√£o Inteligentes</strong></a>:</p>
<ul>
<li>TTL (Time-to-Live) baseado na frequ√™ncia de uso</li>
<li>Invalida√ß√£o seletiva quando documentos relacionados s√£o atualizados</li>
<li>Cache sem√¢ntico que agrupa consultas similares</li>
</ul>
</li>
<li>
<p><strong>Pr√©-Computa√ß√£o e Cache Preditivo</strong>:</p>
<ul>
<li>Analisar padr√µes de consulta para pr√©-computar respostas prov√°veis</li>
<li>Gerar embeddings para varia√ß√µes comuns de consultas</li>
</ul>
</li>
</ol>


  <pre><code class="language-clojure">;; Exemplo de implementa√ß√£o de cache com Redis para alta disponibilidade
(defn distributed-cached-rag-query
  &#34;Executa consulta RAG com cache distribu√≠do&#34;
  [query]
  (let [cache-key (str &#34;rag:query:&#34; (digest/md5 query))
        ;; Verificar no Redis
        cached-response (redis/get cache-key)]
    (if cached-response
      ;; Usar resposta em cache
      (do
        (redis/incr (str cache-key &#34;:hits&#34;))
        (json/read-str cached-response))
      ;; Gerar nova resposta
      (let [response (full-rag-process query)
            ;; Serializar e armazenar no Redis com TTL
            _ (redis/setex cache-key 
                          (* 60 60 24) ;; 24 horas
                          (json/write-str response))
            ;; Registrar metadados para an√°lise
            _ (redis/hmset (str cache-key &#34;:meta&#34;)
                          {&#34;timestamp&#34; (System/currentTimeMillis)
                           &#34;query_length&#34; (count query)
                           &#34;query_type&#34; (determine-query-type query)})]
        response))))</code></pre>
 <p>Para implementa√ß√µes detalhadas de estrat√©gias de cache para RAG, consulte:</p>
<ul>
<li><a href="https://docs.llamaindex.ai/en/stable/module_guides/querying/query_engine/query_engine_caching">LlamaIndex - Query Engine Caching</a></li>
<li><a href="https://python.langchain.com/docs/modules/model_io/llms/llm_caching">LangChain - Caching para LLM Applications</a></li>
<li><a href="https://redis.io/docs/stack/search/reference/vectors/">Redis Vector Database for RAG</a></li>
</ul>
<hr>
<h2 id="monitoramento-e-m√©tricas-llmops-na-pr√°tica">Monitoramento e M√©tricas: LLMOps na Pr√°tica</h2>
<p>Para garantir que nosso sistema RAG continue funcionando bem em produ√ß√£o, precisamos monitorar m√©tricas chave:</p>


  
  <div class="mermaid">flowchart TB
    subgraph &#34;Ciclo de Monitoramento RAG&#34;
    direction TB
    LOG[Logs de Intera√ß√µes] --&gt; METR[C√°lculo de M√©tricas]
    METR --&gt; ANOM[Detec√ß√£o de Anomalias]
    ANOM --&gt; ALER[Alertas e Relat√≥rios]
    ALER --&gt; OPT[Otimiza√ß√£o do Sistema]
    OPT --&gt; LOG
    end
    
    subgraph &#34;M√©tricas RAG&#34;
    direction LR
    METR_OP[M√©tricas Operacionais]
    METR_Q[M√©tricas de Qualidade]
    METR_F[M√©tricas de Feedback]
    end
    
    METR --- METR_OP
    METR --- METR_Q
    METR --- METR_F
    
    style LOG fill:#f9f,stroke:#333,stroke-width:2px
    style METR fill:#ffc,stroke:#333,stroke-width:2px
    style ANOM fill:#f99,stroke:#333,stroke-width:2px
    style OPT fill:#9f9,stroke:#333,stroke-width:2px</div>
 <p>O diagrama acima ilustra o ciclo completo de <a href="https://en.wikipedia.org/wiki/Monitoring">monitoramento</a> para <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">sistemas RAG</a> em produ√ß√£o. No centro do processo est√£o os &ldquo;<a href="https://en.wikipedia.org/wiki/Log_file">Logs de Intera√ß√µes</a>&rdquo;, que capturam dados detalhados sobre cada <a href="https://en.wikipedia.org/wiki/Query">consulta</a> processada pelo sistema, incluindo a pergunta original, os <a href="https://en.wikipedia.org/wiki/Information_retrieval">documentos recuperados</a>, a <a href="https://en.wikipedia.org/wiki/Natural_language_generation">resposta gerada</a> e <a href="https://en.wikipedia.org/wiki/Performance_metric">m√©tricas de desempenho</a>.</p>
<p>Estes logs alimentam o &ldquo;<a href="https://en.wikipedia.org/wiki/Metric_%28mathematics%29">C√°lculo de M√©tricas</a>&rdquo;, que transforma <a href="https://en.wikipedia.org/wiki/Raw_data">dados brutos</a> em <a href="https://en.wikipedia.org/wiki/Key_performance_indicator">indicadores acion√°veis</a> distribu√≠dos em tr√™s categorias principais: <a href="https://en.wikipedia.org/wiki/Operational_efficiency">operacionais</a> (<a href="https://en.wikipedia.org/wiki/Latency_%28engineering%29">lat√™ncia</a>, <a href="https://en.wikipedia.org/wiki/Throughput">throughput</a>), <a href="https://en.wikipedia.org/wiki/Data_quality">qualidade</a> (<a href="https://en.wikipedia.org/wiki/Precision_and_recall">precis√£o</a>, <a href="https://en.wikipedia.org/wiki/Relevance_%28information_retrieval%29">relev√¢ncia</a>) e <a href="https://en.wikipedia.org/wiki/Feedback">feedback</a> (avalia√ß√µes dos usu√°rios). A &ldquo;<a href="https://en.wikipedia.org/wiki/Anomaly_detection">Detec√ß√£o de Anomalias</a>&rdquo; monitora continuamente estas m√©tricas para identificar desvios significativos dos padr√µes esperados, gerando &ldquo;<a href="https://en.wikipedia.org/wiki/Alert_management">Alertas e Relat√≥rios</a>&rdquo; que orientam a &ldquo;<a href="https://en.wikipedia.org/wiki/System_optimization">Otimiza√ß√£o do Sistema</a>&rdquo;, fechando assim o ciclo de <a href="https://en.wikipedia.org/wiki/Continuous_improvement">melhoria cont√≠nua</a>.</p>
<p>Este fluxo de trabalho representa a ess√™ncia do <a href="https://en.wikipedia.org/wiki/MLOps">LLMOps</a> aplicado a sistemas RAG, onde o monitoramento n√£o √© apenas <a href="https://en.wikipedia.org/wiki/Reactive_programming">reativo</a>, mas <a href="https://en.wikipedia.org/wiki/Proactive">proativo</a> na identifica√ß√£o de oportunidades de melhoria. A estrutura tripartite das m√©tricas garante uma <a href="https://en.wikipedia.org/wiki/Holism">vis√£o hol√≠stica</a> do desempenho: enquanto as m√©tricas operacionais asseguram a <a href="https://en.wikipedia.org/wiki/Technical_efficiency">efici√™ncia t√©cnica</a> do sistema, as m√©tricas de qualidade avaliam a <a href="https://en.wikipedia.org/wiki/Semantic_similarity">precis√£o sem√¢ntica</a> das respostas, e as m√©tricas de feedback incorporam a <a href="https://en.wikipedia.org/wiki/Human-centered_design">perspectiva humana</a> na avalia√ß√£o.</p>
<p>Esta abordagem <a href="https://en.wikipedia.org/wiki/System_integration">integrada</a> permite que <a href="https://en.wikipedia.org/wiki/Engineering_team">equipes de engenharia</a> identifiquem rapidamente <a href="https://en.wikipedia.org/wiki/Bottleneck_%28software%29">gargalos</a>, ajustem <a href="https://en.wikipedia.org/wiki/Information_retrieval">par√¢metros de recupera√ß√£o</a> e melhorem continuamente a <a href="https://en.wikipedia.org/wiki/User_experience">experi√™ncia do usu√°rio</a> final, mesmo √† medida que o <a href="https://en.wikipedia.org/wiki/Big_data">volume de dados</a> e a <a href="https://en.wikipedia.org/wiki/Query_complexity">complexidade das consultas</a> aumentam. O c√≥digo abaixo mostra como implementar o log e a avalia√ß√£o de respostas em <a href="https://en.wikipedia.org/wiki/Clojure">Clojure</a>:</p>


  <pre><code class="language-clojure">;; Estrutura para log e avalia√ß√£o de respostas
(defn log-rag-interaction
  &#34;Registra uma intera√ß√£o RAG para an√°lise posterior&#34;
  [query retrieved-docs response latency]
  (jdbc/execute! db-spec
                [&#34;INSERT INTO rag_logs 
                 (query, retrieved_docs, response, latency_ms, timestamp)
                 VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP)&#34;
                 query
                 (json/write-str retrieved-docs)
                 response
                 latency]))

;; Fun√ß√£o para calcular m√©tricas de desempenho
(defn calculate-rag-metrics
  &#34;Calcula m√©tricas de desempenho para um per√≠odo&#34;
  [start-date end-date]
  (let [logs (jdbc/execute! db-spec
                           [&#34;SELECT * FROM rag_logs 
                            WHERE timestamp BETWEEN ? AND ?&#34;
                            start-date end-date])
        ;; M√©tricas de lat√™ncia
        avg-latency (average-latency logs)
        p95-latency (percentile-latency logs 95)
        ;; Taxa de falhas (quando resposta cont√©m erros espec√≠ficos)
        failure-rate (failure-rate logs)
        ;; Distribui√ß√£o de consultas por t√≥pico
        topic-distribution (topic-distribution logs)]
    {:avg_latency avg-latency
     :p95_latency p95-latency
     :failure_rate failure-rate
     :topic_distribution topic-distribution}))</code></pre>
 <p>A fun√ß√£o <code>log-rag-interaction</code> captura cada aspecto da intera√ß√£o desde a consulta original at√© os documentos recuperados, a resposta gerada e o tempo de lat√™ncia armazenando-os em um banco de dados relacional para an√°lise posterior. Esta abordagem permite rastrear o hist√≥rico completo de intera√ß√µes, criando um registro valioso para depura√ß√£o, otimiza√ß√£o e avalia√ß√£o de desempenho ao longo do tempo.</p>
<p>A fun√ß√£o <code>calculate-rag-metrics</code> complementa o sistema de logging ao transformar os dados brutos em m√©tricas acion√°veis, calculando indicadores cr√≠ticos como lat√™ncia m√©dia, percentil 95 de lat√™ncia (importante para entender outliers), taxa de falhas e distribui√ß√£o de consultas por t√≥pico.</p>
<p>Esta an√°lise multidimensional permite que as equipes identifiquem n√£o apenas problemas t√©cnicos (como gargalos de desempenho), mas tamb√©m padr√µes de uso e √°reas tem√°ticas que podem requerer otimiza√ß√£o espec√≠fica. A combina√ß√£o destas duas fun√ß√µes estabelece um ciclo de feedback cont√≠nuo que √© essencial para <a href="https://en.wikipedia.org/wiki/Retrieval-Augmented_Generation">sistemas RAG em produ√ß√£o</a>, permitindo melhorias iterativas baseadas em dados reais de uso.</p>
<h3 id="m√©tricas-de-qualidade-espec√≠ficas-para-rag">M√©tricas de Qualidade Espec√≠ficas para RAG</h3>
<p>Al√©m das m√©tricas operacionais comuns (lat√™ncia, disponibilidade), sistemas RAG requerem m√©tricas espec√≠ficas para avaliar a qualidade das respostas:</p>
<h4 id="1-m√©tricas-de-relev√¢ncia-do-contexto">1. M√©tricas de Relev√¢ncia do Contexto</h4>
<ul>
<li><strong>Precision@K</strong>: Propor√ß√£o de chunks recuperados que s√£o realmente relevantes para a consulta</li>
<li><strong>Recall@K</strong>: Propor√ß√£o de chunks relevantes na base de conhecimento que foram recuperados</li>
<li><strong>NDCG (Normalized Discounted Cumulative Gain)</strong>: Avalia se os chunks mais relevantes est√£o no topo da lista</li>
</ul>
<p>Abaixo, um exemplo de implementa√ß√£o de m√©tricas de relev√¢ncia do contexto em Clojure:</p>


  <pre><code class="language-clojure">(defn calculate-precision-at-k
  &#34;Calcula Precision@K para uma consulta e seus chunks recuperados&#34;
  [query chunks k expert-judgments]
  (let [retrieved-top-k (take k chunks)
        relevant-count (count (filter #(is-chunk-relevant? % query expert-judgments) 
                                     retrieved-top-k))]
    (double (/ relevant-count (min k (count retrieved-top-k))))))

(defn calculate-recall-at-k
  &#34;Calcula Recall@K para uma consulta&#34;
  [query chunks k all-relevant-chunks expert-judgments]
  (let [retrieved-top-k (take k chunks)
        retrieved-relevant (filter #(is-chunk-relevant? % query expert-judgments) 
                                  retrieved-top-k)
        total-relevant-count (count all-relevant-chunks)]
    (if (pos? total-relevant-count)
      (double (/ (count retrieved-relevant) total-relevant-count))
      1.0))) ;; Se n√£o h√° chunks relevantes, recall √© 1</code></pre>
 <p>A fun√ß√£o <code>calculate-precision-at-k</code> mede a propor√ß√£o de chunks relevantes entre os <code>k</code> primeiros resultados recuperados, comparando-os com julgamentos de especialistas. J√° a fun√ß√£o <code>calculate-recall-at-k</code> avalia a propor√ß√£o de chunks relevantes que foram efetivamente recuperados em rela√ß√£o ao total de chunks relevantes dispon√≠veis.</p>
<p>Ambas as m√©tricas s√£o fundamentais para entender a efic√°cia do sistema de recupera√ß√£o: <code>precision</code> indica qu√£o precisa √© a recupera√ß√£o (minimizando falsos positivos), enquanto <code>recall</code> mostra qu√£o completa √© a recupera√ß√£o (minimizando falsos negativos). A implementa√ß√£o inclui tratamento para casos especiais, como quando n√£o h√° chunks relevantes dispon√≠veis, garantindo resultados matematicamente consistentes.</p>
<h4 id="2-m√©tricas-de-qualidade-da-resposta">2. M√©tricas de Qualidade da Resposta</h4>
<p>Para avaliar a qualidade das respostas geradas por sistemas RAG, √© essencial implementar m√©tricas espec√≠ficas que capturem diferentes dimens√µes de efic√°cia. Estas m√©tricas v√£o al√©m de simples avalia√ß√µes bin√°rias (correto/incorreto) e permitem uma an√°lise nuan√ßada da performance do sistema. Implementamos as seguintes m√©tricas qualitativas em nosso framework de avalia√ß√£o:</p>
<ul>
<li><strong>Faithfulness (Fidelidade)</strong>: O grau em que a resposta √© suportada pelo contexto fornecido, sem alucina√ß√µes</li>
<li><strong>Answer Relevancy (Relev√¢ncia da Resposta)</strong>: Qu√£o bem a resposta aborda a consulta do usu√°rio</li>
<li><strong>Contextual Precision (Precis√£o Contextual)</strong>: Propor√ß√£o do contexto utilizado que foi relevante para a resposta</li>
<li><strong>Helpfulness (Utilidade)</strong>: Avalia√ß√£o subjetiva de qu√£o √∫til foi a resposta para o usu√°rio</li>
</ul>
<p>Abaixo, um exemplo de implementa√ß√£o de m√©tricas de qualidade da resposta em Clojure:</p>


  <pre><code class="language-clojure">(defn evaluate-response-quality
  &#34;Avalia m√©tricas qualitativas de uma resposta RAG&#34;
  [query context response]
  (let [;; Usar LLM como avaliador
        prompt-faithfulness (str &#34;Avalie a fidelidade da seguinte resposta ao contexto fornecido.\n\n&#34;
                                &#34;Consulta: &#34; query &#34;\n\n&#34;
                                &#34;Contexto: &#34; context &#34;\n\n&#34;
                                &#34;Resposta: &#34; response &#34;\n\n&#34;
                                &#34;A resposta cont√©m informa√ß√µes que n√£o est√£o no contexto? &#34;
                                &#34;A resposta contradiz o contexto em algum ponto? &#34;
                                &#34;Atribua uma pontua√ß√£o de 1 a 10, onde 10 significa perfeita fidelidade ao contexto.&#34;)
        
        prompt-relevancy (str &#34;Avalie qu√£o relevante √© a resposta para a consulta.\n\n&#34;
                             &#34;Consulta: &#34; query &#34;\n\n&#34;
                             &#34;Resposta: &#34; response &#34;\n\n&#34;
                             &#34;A resposta aborda diretamente a consulta? &#34;
                             &#34;Alguma parte importante da consulta foi ignorada? &#34;
                             &#34;Atribua uma pontua√ß√£o de 1 a 10, onde 10 significa perfeitamente relevante.&#34;)
        
        ;; Chamar LLM para avalia√ß√£o
        faithfulness-result (parse-score (call-evaluation-llm prompt-faithfulness))
        relevancy-result (parse-score (call-evaluation-llm prompt-relevancy))]
    
    ;; Retornar resultados agregados
    {:faithfulness faithfulness-result
     :relevancy relevancy-result
     :composite_score (/ (&#43; faithfulness-result relevancy-result) 2.0)}))</code></pre>
 <p>A fun√ß√£o recebe tr√™s par√¢metros principais: a consulta original do usu√°rio (<code>query</code>), o contexto recuperado pelo sistema (<code>context</code>) e a resposta gerada pelo modelo (<code>response</code>). Utilizando esses inputs, a fun√ß√£o constr√≥i dois prompts espec√≠ficos para avaliar diferentes dimens√µes da qualidade da resposta.</p>
<p>O primeiro prompt avalia a &ldquo;fidelidade&rdquo; <a href="https://en.wikipedia.org/wiki/Faithfulness_%28literary_theory%29">(faithfulness)</a> da resposta, verificando se ela se mant√©m fiel ao contexto fornecido sem adicionar informa√ß√µes n√£o presentes ou contradizer o material de refer√™ncia. O segundo prompt avalia a &ldquo;relev√¢ncia&rdquo; <a href="https://en.wikipedia.org/wiki/Relevance_%28information_retrieval%29">(relevancy)</a>, analisando se a resposta aborda diretamente a consulta do usu√°rio e se cobre todos os aspectos importantes da pergunta. Ambos os prompts s√£o enviados para um <a href="https://github.com/langchain-ai/langchain/blob/main/libs/langchain-core/langchain_core/prompts/prompt.py">LLM avaliador atrav√©s da fun√ß√£o <code>call-evaluation-llm</code></a>, que retorna uma avalia√ß√£o textual que √© ent√£o convertida em uma pontua√ß√£o num√©rica pela fun√ß√£o <code>parse-score</code>.</p>
<p>Por fim, a fun√ß√£o agrega os resultados em um mapa contendo as pontua√ß√µes individuais de fidelidade e relev√¢ncia, al√©m de calcular uma pontua√ß√£o composta que √© a m√©dia das duas m√©tricas. Esta abordagem de &ldquo;LLM como avaliador&rdquo; representa uma t√©cnica avan√ßada no campo de RAG, permitindo avalia√ß√µes automatizadas que capturam nuances qualitativas dif√≠ceis de medir com m√©tricas puramente estat√≠sticas.</p>
<blockquote>
<p>O c√≥digo demonstra como implementar um sistema de avalia√ß√£o que pode ser usado para monitoramento cont√≠nuo da qualidade das respostas e identifica√ß√£o de √°reas para melhoria.</p></blockquote>
<h4 id="3-m√©tricas-de-consenso-entre-modelos">3. M√©tricas de Consenso entre Modelos</h4>
<p>Uma t√©cnica eficaz √© comparar respostas de m√∫ltiplos modelos ou configura√ß√µes:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Model_agreement"><strong>Model Agreement (Concord√¢ncia de Modelos)</strong></a>: Grau de concord√¢ncia entre diferentes LLMs para a mesma consulta/contexto</li>
<li><a href="https://en.wikipedia.org/wiki/Embedding"><strong>Embedding Stability (Estabilidade de Embeddings)</strong></a>: Consist√™ncia de embeddings entre atualiza√ß√µes de modelos</li>
<li><a href="https://en.wikipedia.org/wiki/Context_utilization_variance"><strong>Context Utilization Variance (Vari√¢ncia de Utiliza√ß√£o de Contexto)</strong></a>: Diferen√ßas na forma como os modelos utilizam o contexto</li>
</ul>
<p>Abaixo, um exemplo de implementa√ß√£o de m√©tricas de consenso entre modelos em Clojure:</p>


  <pre><code class="language-clojure">(defn measure-model-agreement
  &#34;Mede concord√¢ncia entre diferentes modelos para mesma consulta&#34;
  [query context models]
  (let [;; Gerar respostas de cada modelo
        responses (map #(generate-response-with-model % query context) models)
        
        ;; Calcular similaridade sem√¢ntica entre cada par de respostas
        similarities (for [i (range (count responses))
                          j (range (inc i) (count responses))]
                      (calculate-semantic-similarity 
                        (nth responses i) 
                        (nth responses j)))
        
        ;; M√©dia das similaridades como medida de concord√¢ncia
        avg-similarity (if (seq similarities)
                         (/ (reduce &#43; similarities) (count similarities))
                         1.0)]
    avg-similarity))</code></pre>
 <p>Esta fun√ß√£o implementa uma m√©trica de concord√¢ncia entre modelos, uma t√©cnica valiosa para avaliar a robustez de sistemas RAG. Ao gerar respostas para a mesma consulta usando diferentes modelos, a fun√ß√£o calcula a similaridade sem√¢ntica entre cada par de respostas. Uma alta concord√¢ncia (similaridade) entre modelos diversos sugere que a resposta √© mais confi√°vel, enquanto baixa concord√¢ncia pode indicar ambiguidade nos dados ou quest√µes com a recupera√ß√£o de contexto.</p>
<p>A implementa√ß√£o utiliza uma abordagem de compara√ß√£o par a par, onde cada resposta √© comparada com todas as outras. A fun√ß√£o <code>calculate-semantic-similarity</code> (n√£o mostrada) provavelmente utiliza embeddings para medir qu√£o semanticamente pr√≥ximas est√£o duas respostas. O resultado final √© uma pontua√ß√£o m√©dia de similaridade que quantifica o n√≠vel geral de consenso entre os modelos. Esta m√©trica √© particularmente √∫til para identificar consultas problem√°ticas onde diferentes modelos divergem significativamente, sinalizando potenciais √°reas para melhoria no pipeline RAG.</p>
<h3 id="automa√ß√£o-da-avalia√ß√£o-com-llms-como-ju√≠zes">Automa√ß√£o da Avalia√ß√£o com LLMs como Ju√≠zes</h3>


  
  <div class="mermaid">flowchart TD
    Q[Consulta do Usu√°rio] --&gt; RAG[Sistema RAG]
    CTX[Contexto Recuperado] --&gt; RAG
    
    RAG --&gt; RESP[Resposta Gerada]
    
    subgraph &#34;Avalia√ß√£o Automatizada&#34;
        RESP --&gt; JUDGE[LLM Avaliador]
        Q --&gt; JUDGE
        CTX --&gt; JUDGE
        CRIT[Crit√©rios de Avalia√ß√£o] --&gt; JUDGE
        
        JUDGE --&gt; EVAL[Avalia√ß√£o Estruturada]
        EVAL --&gt; DB[(Banco de Dados)]
        
        EVAL --&gt; METRICS[M√©tricas de Qualidade]
        METRICS --&gt; DASH[Dashboard]
        
        EVAL --&gt; INSIGHT[Insights para Melhoria]
        INSIGHT --&gt; REFINE[Refinamento do Sistema]
        REFINE -.-&gt; RAG
    end
    
    style Q fill:#f9f,stroke:#333,stroke-width:2px
    style RESP fill:#9cf,stroke:#333,stroke-width:2px
    style JUDGE fill:#fc9,stroke:#333,stroke-width:2px
    style EVAL fill:#9f9,stroke:#333,stroke-width:2px
    style REFINE fill:#f99,stroke:#333,stroke-width:2px</div>
 <p>O diagrama acima representando o fluxo desde a consulta do usu√°rio at√© o refinamento cont√≠nuo do sistema. No centro do processo est√° o &ldquo;LLM Avaliador&rdquo; (JUDGE), que recebe tr√™s entradas cruciais: a consulta original do usu√°rio, o contexto recuperado e a resposta gerada pelo sistema RAG. Adicionalmente, o avaliador utiliza crit√©rios de avalia√ß√£o predefinidos para realizar uma an√°lise estruturada e imparcial.</p>
<p>O aspecto mais valioso deste fluxo √© o ciclo de feedback que ele estabelece: a avalia√ß√£o estruturada n√£o apenas alimenta um banco de dados para registro hist√≥rico e gera m√©tricas de qualidade para visualiza√ß√£o em dashboards, mas tamb√©m produz insights acion√°veis que direcionam o refinamento do sistema. Esta abordagem c√≠clica permite que o sistema RAG evolua continuamente, aprendendo com suas pr√≥prias limita√ß√µes e melhorando progressivamente a qualidade das respostas, sem necessidade de interven√ß√£o humana constante em cada etapa do processo de avalia√ß√£o. Uma abordagem emergente √© usar LLMs como &ldquo;ju√≠zes&rdquo; para avaliar automaticamente a qualidade das respostas:</p>


  <pre><code class="language-clojure">(defn llm-judge-evaluation
  &#34;Utiliza LLM como juiz para avaliar respostas RAG&#34;
  [query context response evaluation-criteria]
  (let [;; Construir prompt para avalia√ß√£o
        evaluation-prompt (str &#34;Voc√™ √© um avaliador especializado em sistemas RAG. &#34;
                              &#34;Analise a seguinte intera√ß√£o e avalie de acordo com os crit√©rios especificados.\n\n&#34;
                              &#34;Consulta do usu√°rio: &#34; query &#34;\n\n&#34;
                              &#34;Contexto recuperado: &#34; context &#34;\n\n&#34;
                              &#34;Resposta gerada: &#34; response &#34;\n\n&#34;
                              &#34;Crit√©rios de avalia√ß√£o:\n&#34;
                              evaluation-criteria &#34;\n\n&#34;
                              &#34;Para cada crit√©rio, forne√ßa:\n&#34;
                              &#34;1. Uma pontua√ß√£o de 1-10\n&#34;
                              &#34;2. Justificativa para a pontua√ß√£o\n&#34;
                              &#34;3. Sugest√µes espec√≠ficas para melhoria\n&#34;
                              &#34;Formate sua resposta como JSON.&#34;)
        
        ;; Chamar LLM avaliador (preferivelmente um modelo diferente do usado para gerar a resposta para evitar vi√©s de auto-avalia√ß√£o)](https://en.wikipedia.org/wiki/Self-assessment)
        judge-response (call-evaluation-llm evaluation-prompt)
        
        ;; Parsear resposta estruturada
        evaluation-results (json/read-str judge-response)]
    
    ;; Registrar avalia√ß√£o no banco de dados
    (log-evaluation query context response evaluation-results)
    
    ;; Retornar resultados estruturados
    evaluation-results))</code></pre>
 <p>A implementa√ß√£o segue um padr√£o elegante e pr√°tico: primeiro constr√≥i um prompt detalhado que enquadra a tarefa de avalia√ß√£o, depois chama um modelo <a href="https://en.wikipedia.org/wiki/Self-assessment">LLM dedicado (preferencialmente diferente do usado na gera√ß√£o da resposta para evitar vi√©s de auto-avalia√ß√£o)</a>, processa a resposta estruturada e finalmente registra os resultados para an√°lise posterior. Esta abordagem permite avalia√ß√£o cont√≠nua e escal√°vel da qualidade do sistema RAG, fornecendo insights acion√°veis para refinamento do pipeline sem necessidade de interven√ß√£o humana constante.</p>
<p>A fun√ß√£o representa uma evolu√ß√£o importante nas pr√°ticas de avalia√ß√£o de RAG, combinando a capacidade de compreens√£o contextual dos LLMs com a necessidade de feedback estruturado e quantific√°vel.</p>
<h4 id="configura√ß√£o-de-um-dashboard-de-qualidade-rag">Configura√ß√£o de um Dashboard de Qualidade RAG</h4>
<p>Para monitoramento cont√≠nuo, √© essencial configurar um dashboard que acompanhe a evolu√ß√£o das m√©tricas ao longo do tempo:</p>


  <pre><code class="language-clojure">(defn generate-rag-quality-report
  &#34;Gera relat√≥rio di√°rio de qualidade do sistema RAG&#34;
  []
  (let [;; Per√≠odo de avalia√ß√£o (√∫ltimo dia)
        end-date (java.util.Date.)
        start-date (-&gt; (java.util.Calendar/getInstance)
                       (doto (.setTime end-date)
                             (.add java.util.Calendar/DAY_OF_MONTH -1))
                       (.getTime))
        
        ;; Recuperar logs do per√≠odo
        logs (jdbc/execute! db-spec
                           [&#34;SELECT * FROM rag_logs 
                             WHERE timestamp BETWEEN ? AND ?&#34;
                            start-date end-date])
        
        ;; Calcular m√©tricas operacionais
        operational-metrics (calculate-operational-metrics logs)
        
        ;; Selecionar amostra aleat√≥ria para avalia√ß√£o qualitativa
        evaluation-sample (take 50 (shuffle logs))
        
        ;; Avaliar qualidade das respostas na amostra
        quality-metrics (evaluate-sample-quality evaluation-sample)
        
        ;; Identificar tend√™ncias e anomalias
        trends (detect-quality-trends quality-metrics)
        anomalies (detect-quality-anomalies quality-metrics)
        
        ;; Compilar relat√≥rio
        report {:date (format-date end-date)
                :sample_size (count evaluation-sample)
                :operational_metrics operational-metrics
                :quality_metrics quality-metrics
                :trends trends
                :anomalies anomalies
                :recommendations (generate-recommendations trends anomalies)}]
    
    ;; Salvar relat√≥rio e enviar notifica√ß√µes se houver anomalias
    (save-quality-report report)
    (when (not-empty anomalies)
      (send-quality-alert report))
    
    report))</code></pre>
 <p>O c√≥digo acima implementa uma fun√ß√£o Clojure chamada <code>generate-rag-quality-report</code> que automatiza a gera√ß√£o de relat√≥rios di√°rios de qualidade para um sistema RAG. A fun√ß√£o come√ßa definindo um per√≠odo de avalia√ß√£o (√∫ltimo dia), recupera logs de intera√ß√µes RAG desse per√≠odo do banco de dados, e calcula m√©tricas operacionais b√°sicas. Em seguida, seleciona uma amostra aleat√≥ria de 50 intera√ß√µes para uma avalia√ß√£o qualitativa mais profunda.</p>
<p>O n√∫cleo da fun√ß√£o est√° na avalia√ß√£o da qualidade das respostas na amostra selecionada, seguida pela identifica√ß√£o de tend√™ncias e anomalias nos dados de qualidade. Isso permite que o sistema n√£o apenas me√ßa o desempenho atual, mas tamb√©m detecte padr√µes emergentes ou problemas que possam exigir aten√ß√£o. O relat√≥rio final √© estruturado como um <a href="https://clojure.org/reference/data_structures">mapa Clojure</a> contendo a data, tamanho da amostra, m√©tricas operacionais, m√©tricas de qualidade, tend√™ncias identificadas, anomalias detectadas e recomenda√ß√µes geradas automaticamente.</p>
<p>Um aspecto importante da fun√ß√£o √© seu mecanismo de alerta: ap√≥s salvar o relat√≥rio no sistema, ela verifica se foram detectadas anomalias e, em caso positivo, envia alertas para os respons√°veis. Esta abordagem proativa para monitoramento de qualidade permite que equipes de engenharia e produto intervenham rapidamente quando o desempenho do sistema RAG come√ßa a degradar, antes que os usu√°rios sejam significativamente afetados. O c√≥digo exemplifica uma implementa√ß√£o pr√°tica de <a href="https://en.wikipedia.org/wiki/LLMOps">LLMOps</a>, focando na avalia√ß√£o cont√≠nua e sistem√°tica da qualidade das respostas em um sistema RAG.</p>
<h3 id="integra√ß√£o-com-sistemas-de-feedback-do-usu√°rio">Integra√ß√£o com Sistemas de Feedback do Usu√°rio</h3>
<p>O feedback direto dos usu√°rios √© uma fonte valiosa para avaliar a qualidade das respostas:</p>


  <pre><code class="language-clojure">(defn process-user-feedback
  &#34;Processa feedback expl√≠cito do usu√°rio&#34;
  [query-id response-id feedback-type feedback-text]
  (let [;; Registrar feedback no banco de dados
        _ (jdbc/execute! db-spec
                        [&#34;INSERT INTO user_feedback 
                          (query_id, response_id, feedback_type, feedback_text, timestamp) 
                          VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP)&#34;
                         query-id response-id feedback-type feedback-text])
        
        ;; Recuperar detalhes da intera√ß√£o
        interaction (jdbc/execute-one! db-spec
                                     [&#34;SELECT query, retrieved_docs, response 
                                       FROM rag_logs WHERE id = ?&#34;
                                      query-id])
        
        ;; Analisar feedback para extrair insights
        feedback-analysis (analyze-user-feedback feedback-type 
                                               feedback-text 
                                               (:query interaction)
                                               (:response interaction))]
    
    ;; Atualizar m√©tricas agregadas
    (update-feedback-metrics feedback-type)
    
    ;; Para feedback negativo, adicionar √† fila de revis√£o manual
    (when (= feedback-type &#34;negative&#34;)
      (add-to-manual-review-queue query-id feedback-analysis))
    
    feedback-analysis))</code></pre>
 <p>A fun√ß√£o registra o feedback no banco de dados, recupera os detalhes da intera√ß√£o original, analisa o feedback para extrair insights valiosos e atualiza m√©tricas agregadas. Um aspecto importante √© o tratamento especial para <a href="https://en.wikipedia.org/wiki/Negative_feedback">feedback negativo</a>, que √© automaticamente adicionado a uma fila de revis√£o manual, permitindo que a equipe investigue e corrija problemas espec√≠ficos.</p>
<p>Esta implementa√ß√£o representa um componente crucial de um sistema LLMOps maduro, pois estabelece um ciclo de feedback cont√≠nuo entre usu√°rios e desenvolvedores. Ao capturar sistematicamente as avalia√ß√µes dos usu√°rios e vincul√°-las √†s consultas e respostas espec√≠ficas, a fun√ß√£o permite an√°lises detalhadas sobre o desempenho do sistema, identifica√ß√£o de padr√µes de falha e oportunidades de melhoria.</p>
<hr>
<h2 id="implementando-no-docai">Implementando no DocAI</h2>
<p>Agora que exploramos v√°rias t√©cnicas avan√ßadas, vamos ver como elas s√£o implementadas no projeto DocAI. Nosso sistema atual j√° incorpora muitas dessas t√©cnicas para criar um pipeline RAG avan√ßado.</p>
<blockquote>
<p>Caso n√£o saiba o que √© o DocAI, voc√™ pode ver os artigos anteriores <a href="https://scovl.github.io/2025/03/23/rag/">RAG Simples com Clojure e Ollama</a> e <a href="https://scovl.github.io/2025/03/25/semantic-postgresql/">Busca Sem√¢ntica com Ollama e PostgreSQL</a>.</p></blockquote>
<h3 id="arquitetura-atual-do-docai">Arquitetura Atual do DocAI</h3>
<p>A arquitetura do DocAI implementa um sistema RAG completo com suporte a agentes para consultas complexas. Os principais componentes s√£o:</p>
<ol>
<li>
<p><a href="https://github.com/docai-ai/docai/blob/main/src/core.clj"><strong>Core (core.clj)</strong></a>: Coordena√ß√£o central do sistema, implementando a interface CLI e gerenciando o fluxo de dados entre componentes.</p>
</li>
<li>
<p><a href="https://github.com/docai-ai/docai/blob/main/src/llm.clj"><strong>LLM (llm.clj)</strong></a>: Interface com o Ollama para gera√ß√£o de texto e embeddings, abstraindo detalhes de comunica√ß√£o com a API.</p>
</li>
<li>
<p><a href="https://github.com/docai-ai/docai/blob/main/src/pg.clj"><strong>PostgreSQL (pg.clj)</strong></a>: Implementa√ß√£o da busca sem√¢ntica com pgvector, incluindo configura√ß√£o e consultas otimizadas.</p>
</li>
<li>
<p><a href="https://github.com/docai-ai/docai/blob/main/src/document.clj"><strong>Processamento de Documentos (document.clj)</strong></a>: Respons√°vel pela extra√ß√£o, limpeza e prepara√ß√£o de texto de diferentes formatos.</p>
</li>
<li>
<p><a href="https://github.com/docai-ai/docai/blob/main/src/advanced_rag.clj"><strong>Advanced RAG (advanced_rag.clj)</strong></a>:</p>
<ul>
<li>Cache em m√∫ltiplos n√≠veis (embeddings e respostas)</li>
<li>Chunking din√¢mico adaptado ao tipo de documento</li>
<li>Re-ranqueamento de resultados para melhorar precis√£o</li>
</ul>
</li>
<li>
<p><a href="https://github.com/docai-ai/docai/blob/main/src/agents.clj"><strong>Sistema de Agentes (agents.clj)</strong></a>:</p>
<ul>
<li>An√°lise de complexidade de consultas</li>
<li>Decomposi√ß√£o em sub-tarefas</li>
<li>Agentes especializados (busca, racioc√≠nio, c√°lculo)</li>
<li>Verifica√ß√£o de qualidade das respostas</li>
<li>S√≠ntese de resultados parciais</li>
</ul>
</li>
<li>
<p><a href="https://github.com/docai-ai/docai/blob/main/src/metrics.clj"><strong>M√©tricas (metrics.clj)</strong></a>: Monitoramento de desempenho e qualidade das respostas.</p>
</li>
</ol>
<p>O fluxo de processamento de consultas inicia em <code>core.clj</code>, que identifica se a consulta requer um pipeline RAG simples ou avan√ßado com agentes:</p>


  <pre><code class="language-clojure">(defn query-advanced-rag
  &#34;Processa uma consulta usando o pipeline RAG avan√ßado&#34;
  [query]
  (println &#34;DEBUG - Processando query com RAG avan√ßado:&#34; query)
  (let [start-time (System/currentTimeMillis)
        ;; Verificar se a consulta precisa do workflow com agentes
        need-agents (agents/needs-agent-workflow? query)
        _ (when need-agents
            (println &#34;DEBUG - Consulta identificada como complexa, usando workflow com agentes&#34;))
        
        ;; Escolher o processamento adequado
        response (if need-agents
                   (agents/process-with-agents query)
                   (adv-rag/advanced-rag-query query))
        
        end-time (System/currentTimeMillis)
        latency (- end-time start-time)]
    
    ;; Registrar m√©tricas
    (metrics/log-rag-interaction query [] response latency)
    
    response))</code></pre>
 <p>Para consultas simples, o pipeline <code>advanced-rag-query</code> realiza:</p>
<ol>
<li>Verifica√ß√£o de cache</li>
<li>An√°lise de complexidade da consulta</li>
<li>Busca sem√¢ntica com chunking din√¢mico</li>
<li>Formata√ß√£o de prompt contextualizado</li>
<li>Gera√ß√£o de resposta com o LLM</li>
</ol>
<p>Para consultas complexas, o sistema de agentes em <code>agents.clj</code> entra em a√ß√£o:</p>


  <pre><code class="language-clojure">(defn execute-agent-workflow
  &#34;Executa o workflow completo de agentes para uma consulta complexa&#34;
  [query]
  (let [;; Verificar cache primeiro
        cached (@agent-cache query)]
    (if cached
      cached
      (let [start-time (System/currentTimeMillis)
            
            ;; Analisar a consulta para determinar inten√ß√£o e sub-quest√µes
            analysis (analyze-query query)
            primary-intent (get-agent-type (:intent analysis))
            subtasks (or (:sub_questions analysis) [query])
            
            ;; Resultados parciais
            results (atom [])
            
            ;; Executar cada subtarefa em sequ√™ncia
            _ (doseq [subtask subtasks]
                (let [agent-result (execute-subtask 
                                     subtask 
                                     primary-intent
                                     @results)]
                  (swap! results conj (:response agent-result))))
            
            ;; Gerar resposta final sintetizada
            synthesis-prompt (str &#34;Com base nas seguintes informa√ß√µes:\n\n&#34;
                                 (str/join &#34;\n\n&#34; @results)
                                 &#34;\n\nResponda √† pergunta original de forma completa e coerente: &#34; query)
            
            initial-response (llm/call-ollama-api synthesis-prompt)
            
            ;; Obter contexto combinado para verifica√ß√£o
            combined-context (str/join &#34;\n\n&#34; @results)
            
            ;; Verificar a qualidade da resposta
            final-response (verify-response query combined-context initial-response)
            
            duration (- (System/currentTimeMillis) start-time)]
        
        ;; Registrar m√©tricas e resultados
        final-response))))</code></pre>
 <p>O sistema de agentes implementa um workflow sofisticado para consultas complexas:</p>
<ol>
<li>An√°lise da consulta para identificar inten√ß√£o e subtarefas</li>
<li>Execu√ß√£o de cada subtarefa com agentes especializados</li>
<li>Acumula√ß√£o de resultados parciais</li>
<li>S√≠ntese de uma resposta final coerente</li>
<li>Verifica√ß√£o da qualidade da resposta</li>
<li>Armazenamento em cache para consultas futuras</li>
</ol>
<h3 id="diferenciais-do-docai">Diferenciais do DocAI</h3>
<p>O DocAI se destaca por implementar v√°rias t√©cnicas avan√ßadas de RAG em um sistema integrado e modular:</p>
<ul>
<li><strong>Chunking Adaptativo</strong>: Diferentes estrat√©gias de chunking baseadas no tipo de documento:


  <pre><code class="language-clojure">(defn adaptive-chunking-strategy
  &#34;Determina estrat√©gia de chunking com base no tipo de documento&#34;
  [document-type]
  (case document-type
    &#34;article&#34; {:chunk-size 1000 :chunk-overlap 150}
    &#34;code&#34; {:chunk-size 500 :chunk-overlap 50}
    &#34;legal&#34; {:chunk-size 1500 :chunk-overlap 200}
    &#34;qa&#34; {:chunk-size 800 :chunk-overlap 100}
    ;; Default
    {:chunk-size 1000 :chunk-overlap 100}))</code></pre>
 </li>
</ul>
<p>O sistema implementa estrat√©gias de <a href="https://en.wikipedia.org/wiki/Chunking_%28data_storage%29">chunking adaptativas</a> que otimizam a segmenta√ß√£o de documentos conforme seu tipo espec√≠fico. Esta abordagem reconhece que diferentes conte√∫dos possuem caracter√≠sticas √∫nicas que afetam como devem ser divididos para processamento:</p>
<ul>
<li><strong>Artigos</strong>: Chunks maiores (1000 tokens) com sobreposi√ß√£o significativa (150 tokens), preservando o fluxo narrativo e argumentativo</li>
<li><strong>C√≥digo-fonte</strong>: Chunks menores (500 tokens) com sobreposi√ß√£o reduzida (50 tokens), respeitando a estrutura modular do c√≥digo</li>
<li><strong>Documentos legais</strong>: Chunks extensos (1500 tokens) com alta sobreposi√ß√£o (200 tokens), mantendo intactas cl√°usulas e refer√™ncias cruzadas</li>
<li><strong>Conte√∫do Q&amp;A</strong>: Chunks de tamanho m√©dio (800 tokens) com sobreposi√ß√£o moderada (100 tokens), preservando pares de perguntas e respostas</li>
</ul>
<p>Esta estrat√©gia contextual melhora significativamente a qualidade da recupera√ß√£o, garantindo que cada tipo de documento seja processado de forma otimizada para seu formato e densidade informacional espec√≠ficos. A fun√ß√£o <code>adaptive-chunking-strategy</code> demonstra uma implementa√ß√£o elegante deste conceito, utilizando pattern matching para selecionar par√¢metros otimizados para cada categoria de documento.</p>
<p>Documentos legais, por exemplo, recebem chunks maiores (1500 tokens) devido √† sua natureza densa e interconectada, enquanto documentos de perguntas e respostas utilizam uma configura√ß√£o intermedi√°ria (800 tokens). Esta estrat√©gia de chunking contextual melhora significativamente a qualidade da recupera√ß√£o, garantindo que o contexto sem√¢ntico seja preservado de forma apropriada para cada tipo espec√≠fico de conte√∫do.</p>
<ul>
<li><strong>Cache Multin√≠vel</strong>: Implementa√ß√£o de cache para embeddings e respostas, reduzindo lat√™ncia e custos:


  <pre><code class="language-clojure">;; Cache para embeddings
(def embedding-cache (atom {}))
;; Cache para respostas
(def response-cache (atom {}))
;; Cache para resultados de agentes
(def agent-cache (atom {}))</code></pre>
 </li>
</ul>
<p>O sistema implementa uma estrat√©gia de <a href="https://en.wikipedia.org/wiki/Cache_hierarchy">cache multin√≠vel</a> para otimizar o desempenho e reduzir custos operacionais. Utilizando estruturas de dados at√¥micas <a href="https://en.wikipedia.org/wiki/Atom_%28data_structure%29">(<code>atom</code>)</a>, o <a href="https://github.com/scovl/docai">DocAI</a> mant√©m tr√™s camadas distintas de cache: para embeddings, respostas completas e resultados de agentes. Esta abordagem permite reutilizar c√°lculos computacionalmente intensivos como a gera√ß√£o de embeddings, evitando processamento redundante de textos id√™nticos.</p>
<p>O cache de respostas armazena resultados finais para consultas frequentes, enquanto o cache de agentes preserva resultados intermedi√°rios de subtarefas espec√≠ficas. Esta implementa√ß√£o reduz significativamente a lat√™ncia do sistema, especialmente para consultas recorrentes, e diminui custos associados a chamadas de API para modelos externos. A estrutura at√¥mica escolhida garante <a href="https://en.wikipedia.org/wiki/Thread_safety">thread-safety</a> em ambientes concorrentes, permitindo atualiza√ß√µes seguras do cache mesmo com m√∫ltiplas consultas simult√¢neas.</p>
<ul>
<li><strong>Verifica√ß√£o de Respostas</strong>: Sistema que avalia e melhora automaticamente as respostas:


  <pre><code class="language-clojure">(defn verify-response
  &#34;Usa um agente cr√≠tico para verificar e melhorar uma resposta&#34;
  [query context response]
  (let [prompt (str &#34;Avalie criticamente a seguinte resposta para a consulta do usu√°rio. 
                    Verifique se a resposta √©:\n&#34;
                    &#34;1. Fiel ao contexto fornecido\n&#34;
                    &#34;2. Completa (responde todos os aspectos da pergunta)\n&#34;
                    &#34;3. Precisa (n√£o cont√©m informa√ß√µes incorretas)\n\n&#34;
                    &#34;Consulta: &#34; query &#34;\n\n&#34;
                    &#34;Contexto: &#34; (if (&gt; (count context) 300) 
                                  (str (subs context 0 300) &#34;...&#34;) context) &#34;\n\n&#34;
                    &#34;Resposta: &#34; response &#34;\n\n&#34;
                    &#34;Se a resposta for adequada, apenas responda &#39;A resposta est√° correta&#39;. &#34;
                    &#34;Caso contr√°rio, forne√ßa uma vers√£o melhorada.&#34;)
        verification (llm/call-ollama-api prompt)]

    (if (str/includes? verification &#34;A resposta est√° correta&#34;)
      response
      (let [improved-version (str/replace verification 
                                         #&#34;(?i).*?\b(a resposta melhorada seria:|vers√£o melhorada:|resposta corrigida:|sugest√£o de resposta:|aqui est√° uma vers√£o melhorada:)\s*&#34; 
                                         &#34;&#34;)]
        improved-version))))</code></pre>
 </li>
</ul>
<p>O c√≥digo acima implementa um sistema de verifica√ß√£o e melhoria autom√°tica de respostas, um componente cr√≠tico em sistemas <a href="https://en.wikipedia.org/wiki/Retrieval-Augmented_Generation">RAG</a> avan√ßados. A fun√ß√£o <code>verify-response</code> atua como um &ldquo;agente cr√≠tico&rdquo; que avalia a qualidade das respostas geradas com base em tr√™s crit√©rios fundamentais: fidelidade ao contexto fornecido, completude em rela√ß√£o √† pergunta original e precis√£o factual. Este mecanismo de auto-verifica√ß√£o representa uma camada adicional de controle de qualidade que ajuda a mitigar alucina√ß√µes e imprecis√µes comuns em sistemas baseados em LLMs.</p>
<p>A implementa√ß√£o utiliza uma abordagem elegante de <a href="https://en.wikipedia.org/wiki/Prompt_engineering">prompt engineering</a>, onde o sistema solicita explicitamente uma avalia√ß√£o cr√≠tica da resposta original. O prompt estruturado inclui a consulta do usu√°rio, um resumo do contexto (limitado a 300 caracteres para evitar sobrecarga) e a resposta gerada, orientando o modelo a realizar uma an√°lise meticulosa. A fun√ß√£o ent√£o analisa o resultado da verifica√ß√£o, mantendo a resposta original quando considerada adequada ou extraindo uma vers√£o aprimorada quando necess√°rio, utilizando express√µes regulares para limpar metadados desnecess√°rios da resposta melhorada.</p>
<p>Este mecanismo de verifica√ß√£o representa uma implementa√ß√£o pr√°tica do conceito de <a href="https://en.wikipedia.org/wiki/Constitutional_AI">Constitutional AI</a> ou &ldquo;AI com princ√≠pios orientadores&rdquo;, onde um sistema √© projetado para avaliar criticamente suas pr√≥prias sa√≠das. Ao incorporar esta camada de verifica√ß√£o no pipeline <a href="https://en.wikipedia.org/wiki/Retrieval-Augmented_Generation">RAG</a>, o <a href="https://github.com/scovl/docai">DocAI</a> consegue oferecer respostas mais confi√°veis e precisas, reduzindo significativamente o risco de fornecer informa√ß√µes incorretas ou incompletas. Esta abordagem reflexiva √© particularmente valiosa em dom√≠nios onde a precis√£o √© crucial, como documenta√ß√£o t√©cnica, informa√ß√µes m√©dicas ou an√°lises legais.</p>
<ul>
<li><strong>M√©tricas Detalhadas</strong>: Sistema de monitoramento que registra todos os aspectos das intera√ß√µes:


  <pre><code class="language-clojure">(metrics/log-rag-interaction query [] response latency)</code></pre>
 </li>
</ul>
<p>O c√≥digo acima implementa um sistema de monitoramento que registra todos os aspectos das intera√ß√µes, incluindo a consulta do usu√°rio, o tempo de resposta, e a resposta gerada. Este sistema permite acompanhar o desempenho do sistema ao longo do tempo e identificar poss√≠veis problemas ou pontos de melhoria. Isso √© essencial para manter o sistema funcionando de forma eficiente e para continuar evoluindo para novas funcionalidades.</p>
<p>Estas implementa√ß√µes demonstram como as t√©cnicas avan√ßadas de RAG discutidas neste artigo podem ser integradas em um sistema coeso, resultando em um assistente de documenta√ß√£o mais inteligente e eficiente.</p>
<h3 id="pr√≥ximos-passos-para-o-docai">Pr√≥ximos Passos para o DocAI</h3>
<p>Conforme detalhado no <code>plan.md</code>, o DocAI evoluir√° para um sistema RAG Ag√™ntico mais completo, implementando as seguintes melhorias:</p>
<ol>
<li>
<p><strong>Reescrita de Consultas</strong></p>
<ul>
<li>M√≥dulo de reformula√ß√£o para melhorar a precis√£o da busca</li>
<li>Expans√£o de consultas curtas e foco em consultas abrangentes</li>
</ul>
</li>
<li>
<p><strong>Sele√ß√£o Din√¢mica de Fontes</strong></p>
<ul>
<li>Workflow de agentes aprimorado para decidir quais fontes consultar</li>
<li>Integra√ß√£o com APIs externas e pesquisa web</li>
</ul>
</li>
<li>
<p><strong>Framework de Ferramentas para Agentes</strong></p>
<ul>
<li>Sistema de ferramentas para a√ß√µes espec√≠ficas</li>
<li>Executores de c√≥digo, calculadoras e formatadores</li>
</ul>
</li>
<li>
<p><strong>Interface Multimodal</strong></p>
<ul>
<li>Processamento de imagens e gera√ß√£o de gr√°ficos</li>
<li>Suporte a diversos formatos al√©m de texto</li>
</ul>
</li>
</ol>
<p>Estas evolu√ß√µes manter√£o a arquitetura modular e extens√≠vel do DocAI, permitindo adapta√ß√£o a diferentes casos de uso e dom√≠nios de conhecimento.</p>
<h2 id="integra√ß√£o-com-o-ecossistema">Integra√ß√£o com o Ecossistema</h2>


  
  <div class="mermaid">flowchart TB
    subgraph &#34;Ecossistema DocAI&#34;
        direction TB
        
        DOCAI[Sistema DocAI] --- OLLAMA[Ollama]
        DOCAI --- POSTGRES[PostgreSQL &#43; pgvector]
        
        DOCAI --- API_GATE[API Gateway]
        API_GATE --- WEB_APP[Aplica√ß√£o Web]
        API_GATE --- CLI[Interface CLI]
        
        DOCAI --- MONITORING[Sistema de Monitoramento]
        MONITORING --- DASHBOARD[Dashboard de M√©tricas]
        
        DOCAI -.-&gt; FUTURE_INT[Integra√ß√µes Futuras]
        FUTURE_INT -.-&gt; EXT_API[APIs Externas]
        FUTURE_INT -.-&gt; SEARCH[Motores de Busca]
        FUTURE_INT -.-&gt; TOOLS[Ferramentas de Produtividade]
        
        style DOCAI fill:#f99,stroke:#333,stroke-width:3px
        style OLLAMA fill:#9f9,stroke:#333,stroke-width:2px
        style POSTGRES fill:#99f,stroke:#333,stroke-width:2px
        style FUTURE_INT fill:#ddd,stroke:#333,stroke-width:2px,stroke-dasharray: 5 5
    end</div>
 <p>O diagrama acima mostra como o DocAI se integra ao ecossistema mais amplo de ferramentas e servi√ßos. No centro est√° o sistema <a href="https://github.com/docai-ai/docai">DocAI</a>, que se conecta diretamente com <a href="https://github.com/ollama/ollama">Ollama</a> para gera√ß√£o de texto e embeddings, e com <a href="https://www.postgresql.org/">PostgreSQL</a> (com <a href="https://github.com/pgvector/pgvector">pgvector</a>) para armazenamento e recupera√ß√£o de dados vetoriais.</p>
<p>Para intera√ß√£o com usu√°rios, o DocAI se conecta a um <a href="https://en.wikipedia.org/wiki/API_gateway">API Gateway</a> que fornece acesso tanto para uma aplica√ß√£o web quanto para uma interface de linha de comando (CLI). Um sistema dedicado de monitoramento coleta m√©tricas e as exibe em um dashboard para an√°lise de desempenho.</p>
<p>As linhas tracejadas indicam integra√ß√µes futuras planejadas, incluindo <a href="https://en.wikipedia.org/wiki/API">APIs externas</a> para busca de informa√ß√µes adicionais, <a href="https://en.wikipedia.org/wiki/Search_engine">motores de busca</a> para ampliar o alcance de recupera√ß√£o, e <a href="https://en.wikipedia.org/wiki/Productivity">ferramentas de produtividade</a> para aumentar as capacidades do sistema.</p>
<p>Esta arquitetura modular permite que o DocAI se mantenha flex√≠vel e adapt√°vel, podendo ser expandido conforme novos requisitos e oportunidades surgem, sempre mantendo seu n√∫cleo robusto de funcionalidades RAG avan√ßadas.</p>
<hr>
<h2 id="conclus√£o">Conclus√£o</h2>
<p>Transformar um sistema <a href="https://en.wikipedia.org/wiki/Retrieval-Augmented_Generation">RAG</a> de prot√≥tipo para produ√ß√£o requer mais do que apenas escolher as melhores ferramentas - exige uma compreens√£o profunda de cada componente e como eles trabalham juntos para produzir resultados confi√°veis.</p>
<p>O projeto <a href="https://github.com/scovl/docai">DocAI</a> representa uma implementa√ß√£o robusta das t√©cnicas avan√ßadas de RAG discutidas neste artigo. Sua arquitetura modular, com componentes especializados em diferentes aspectos do processo (como Core, LLM, PostgreSQL, Sistema de Agentes e M√©tricas), demonstra a import√¢ncia de um design bem estruturado para sistemas RAG em produ√ß√£o.</p>
<p>As t√©cnicas que exploramos - desde re-ranqueamento e chunking din√¢mico at√© workflows com agentes e monitoramento avan√ßado - representam as pr√°ticas que separam implementa√ß√µes amadoras de sistemas robustos e prontos para uso em escala.</p>


  
  <div class="mermaid">flowchart LR
    subgraph &#34;Evolu√ß√£o do DocAI&#34;
    direction LR
    BASIC[RAG B√°sico com TF-IDF] --&gt; PGSQL[PostgreSQL &#43; Embeddings] --&gt; ADV[Sistema RAG Avan√ßado] --&gt; AGT[Sistema RAG Ag√™ntico]
    end
    
    style BASIC fill:#ddf,stroke:#333,stroke-width:2px
    style PGSQL fill:#fdf,stroke:#333,stroke-width:2px
    style ADV fill:#dfd,stroke:#333,stroke-width:2px
    style AGT fill:#ffd,stroke:#333,stroke-width:2px</div>
 <p>Nossa jornada com o <a href="https://github.com/scovl/docai">DocAI</a> evoluiu significativamente, de uma implementa√ß√£o b√°sica com TF-IDF, passando por um sistema com PostgreSQL e embeddings, e agora para uma arquitetura avan√ßada com agentes que pode lidar com casos de uso complexos do mundo real. O pr√≥ximo passo, conforme detalhado no plano de evolu√ß√£o, ser√° expandir ainda mais essas capacidades para criar um sistema RAG Ag√™ntico completo.</p>
<p>O futuro dos sistemas de IA n√£o est√° em modelos cada vez maiores, mas na combina√ß√£o inteligente de componentes especializados que trabalham juntos para superar limita√ß√µes individuais. O <a href="https://github.com/scovl/docai">DocAI</a> exemplifica esta abordagem, demonstrando como a integra√ß√£o de t√©cnicas avan√ßadas de RAG pode resultar em um sistema mais inteligente, preciso e √∫til para seus usu√°rios.</p>
<hr>
<h2 id="refer√™ncias">Refer√™ncias</h2>
<ul>
<li><a href="/2025/03/25/semantic-postgresql/">Artigo anterior: Busca Sem√¢ntica com Ollama e PostgreSQL</a> - Nossa implementa√ß√£o b√°sica com PostgreSQL.</li>
<li><a href="https://openai.com/research/clip">CLIP - OpenAI</a> - Modelo para unificar vis√£o e linguagem.</li>
<li><a href="https://towardsdatascience.com/a-comprehensive-guide-to-multimodal-rag-ea72c387c6e8">Comprehensive Guide to MultiModal RAG</a> - Guia detalhado para implementa√ß√£o de RAG multimodal.</li>
<li><a href="https://huggingface.co/cross-encoder">Cross-Encoders - Hugging Face</a> - Modelos para re-ranking em sistemas de recupera√ß√£o.</li>
<li><a href="https://dailydoseofds.com">Daily Dose of Data Science: RAG Techniques</a> - Artigo sobre t√©cnicas para otimizar sistemas RAG.</li>
<li><a href="https://github.com/timescale/pgai">Documenta√ß√£o do pgai</a> - Extens√£o do PostgreSQL para aplica√ß√µes de IA.</li>
<li><a href="https://github.com/pgvector/pgvector">Documenta√ß√£o do pgvector</a> - Extens√£o do PostgreSQL para embeddings vetoriais.</li>
<li><a href="https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model">Flamingo - DeepMind</a> - Modelo visual de linguagem para tarefas multimodais.</li>
<li><a href="https://www.postgresql.org/docs/current/datatype-json.html">JSONB no PostgreSQL</a> - Documenta√ß√£o sobre o tipo de dados JSONB.</li>
<li><a href="https://python.langchain.com/docs/modules/agents/agent_types/multi_agent">LangChain - Multi-Agent Systems</a> - Implementa√ß√£o de sistemas multi-agentes.</li>
<li><a href="https://python.langchain.com/">LangChain</a> - Biblioteca para desenvolvimento de aplica√ß√µes baseadas em LLM.</li>
<li><a href="https://docs.llamaindex.ai/en/stable/examples/agent/react_agent.html">LlamaIndex - Implementando ReAct Agents</a> - Guia para implementa√ß√£o de agentes ReAct.</li>
<li><a href="https://docs.llamaindex.ai/">LlamaIndex</a> - Framework para construir aplica√ß√µes alimentadas por LLM.</li>
<li><a href="https://docs.llamaindex.ai/en/stable/examples/multi_modal/">MultiModal RAG com LlamaIndex</a> - Exemplos de implementa√ß√£o multimodal.</li>
<li><a href="https://ollama.com/">Ollama - Rodando LLMs localmente</a> - Ferramenta para executar LLMs localmente.</li>
<li><a href="https://www.postgresql.org/">PostgreSQL</a> - Sistema de gerenciamento de banco de dados relacional.</li>
<li><a href="https://github.com/scovl/docai">Projeto DocAI</a> - Reposit√≥rio do projeto DocAI.</li>
</ul>

    </div>
    
    
    



    
    
    
        
            
            
            
            
                
                    
                
                    
                
                    
                
                    
                
                    
                
            
            
            
        
    
        
            
            
            
            
                
                    
                
                    
                
                    
                
                    
                
                    
                
            
            
            
        
    
        
            
            
            
            
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            
            
            
        
    
        
            
            
            
            
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            
            
            
        
    
        
            
            
            
            
                
                    
                        
                        
                    
                
                    
                        
                        
                    
                
                    
                        
                        
                    
                
                    
                
            
            
            
                
            
        
    
        
            
            
            
            
                
                    
                        
                        
                    
                
                    
                        
                        
                    
                
                    
                
                    
                
                    
                        
                        
                    
                
                    
                
            
            
            
                
            
        
    
        
            
            
            
            
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            
            
            
        
    
        
    
        
            
            
            
            
            
            
        
    
        
            
            
            
            
            
            
        
    
    
    
        
        
        
        <div class="related-posts">
            <h3 class="related-posts-title">üìö Posts Relacionados</h3>
            <div class="related-posts-grid">
                
                    
                    <article class="related-post-item animate-on-scroll">
                        <div class="related-post-content">
                            <h4 class="related-post-title">
                                <a href="/2025/03/23/rag/">01 - RAG Simples com Clojure e Ollama</a>
                            </h4>
                            <p class="related-post-excerpt">
                                <h2 id="introdu√ß√£o">Introdu√ß√£o</h2>
<p>Ol√°, pessoal! üëã</p>
<p>Neste artigo, vamos explorar como construir uma aplica√ß√£o <a href="https://pt.wikipedia.org/wiki/Gera%C3%A7%C3%A3o_aumentada_por_recupera%C3%A7%C3%A3o">RAG (Retrieval-Augmented Generation)</a> completa do zero usando <a href="https://clojure.org/">Clojure</a>. Vamos mergulhar em uma implementa√ß√£o pr√°tica que combina processamento de texto, busca sem√¢ntica e gera√ß√£o de respostas com LLMs locais. Se voc√™ est√° interessado em melhorar a precis√£o e relev√¢ncia das respostas dos seus modelos de linguagem com informa√ß√µes atualizadas, este guia √© para voc√™!</p>
<h2 id="fundamentos-do-rag">Fundamentos do RAG</h2>
<h3 id="o-que-√©-rag">O que √© RAG?</h3>
<p>Os Modelos de Linguagem de Grande Escala (<a href="https://en.wikipedia.org/wiki/Large_language_model">LLMs</a>), como o <a href="https://openai.com/api/">GPT</a>, <a href="https://openai.com/api/">ChatGPT</a> e outros, revolucionaram a forma como interagimos com a intelig√™ncia artificial. Eles s√£o capazes de gerar textos coerentes, responder perguntas complexas e at√© mesmo criar conte√∫do criativo. No entanto, esses modelos possuem uma limita√ß√£o fundamental: seu conhecimento √© &ldquo;congelado&rdquo; no tempo.</p>
                            </p>
                            <div class="related-post-meta">
                                <span class="related-post-date">23/03/2025</span>
                                
                                    <div class="related-post-tags">
                                        
                                            <span class="tag">RAG</span>
                                        
                                            <span class="tag">LLM</span>
                                        
                                    </div>
                                
                            </div>
                            <div class="related-post-score">
                                <modern-badge variant="info">3 tags em comum</modern-badge>
                            </div>
                        </div>
                    </article>
                
                    
                    <article class="related-post-item animate-on-scroll">
                        <div class="related-post-content">
                            <h4 class="related-post-title">
                                <a href="/2025/03/25/semantic-postgresql/">Busca Sem√¢ntica com Ollama e PostgreSQL</a>
                            </h4>
                            <p class="related-post-excerpt">
                                <p>Ol√°, pessoal! üëã</p>
<p>No <a href="/2025/03/23/rag/">artigo anterior</a>, exploramos como construir um sistema RAG (Retrieval-Augmented Generation) usando <a href="https://clojure.org/">Clojure</a> e <a href="https://ollama.com/">Ollama</a> com uma implementa√ß√£o simples de <a href="/post/tf-idf/">TF-IDF</a>. Embora essa abordagem seja excelente para aprender os fundamentos, quando pensamos em solu√ß√µes de produ√ß√£o, precisamos de algo mais robusto e escal√°vel.</p>
<p>Neste artigo, vamos descobrir como construir um sistema de busca sem√¢ntica poderoso usando <a href="https://ollama.com/">Ollama</a>, <a href="https://www.postgresql.org/">PostgreSQL</a> e suas extens√µes para manipula√ß√£o de vetores. Esta solu√ß√£o √© perfeitamente adequada para aplica√ß√µes de produ√ß√£o e pode servir como base para sistemas RAG, agentes de IA, assistentes em geral. Diferentemente do artigo anterior, vamos usar o <a href="https://ollama.com/">Ollama</a> via Docker assim como o <a href="https://www.postgresql.org/">PostgreSQL</a> e as extens√µes <a href="https://github.com/pgvector/pgvector">pgvector</a> e <a href="https://github.com/timescale/pgai">pgai</a>.</p>
                            </p>
                            <div class="related-post-meta">
                                <span class="related-post-date">25/03/2025</span>
                                
                                    <div class="related-post-tags">
                                        
                                            <span class="tag">RAG</span>
                                        
                                            <span class="tag">PostgreSQL</span>
                                        
                                    </div>
                                
                            </div>
                            <div class="related-post-score">
                                <modern-badge variant="info">3 tags em comum</modern-badge>
                            </div>
                        </div>
                    </article>
                
            </div>
        </div>
    

    
    
    
    
<div class="comments-section">
    <h3 class="comments-title">üí¨ Coment√°rios</h3>
    <div class="comments-container">
        <script src="https://giscus.app/client.js"
                data-repo="scovl/scovl.github.io"
                data-repo-id="MDEwOlJlcG9zaXRvcnkxMzg1OTI2ODA="
                data-category="General"
                data-category-id="DIC_kwDOCELBqM4CthUV"
                data-mapping="pathname"
                data-strict="0"
                data-reactions-enabled="1"
                data-emit-metadata="0"
                data-input-position="bottom"
                data-theme="light"
                data-lang="pt"
                crossorigin="anonymous"
                async>
        </script>
    </div>
</div>

    
</article>

        </div>
    </main>
    
    
    
    <footer class="footer">
    <div class="container">
        <div class="footer-content">
            <div class="footer-links">
                
                <a href="https://github.com/scovl" target="_blank" rel="noopener noreferrer" class="footer-link">
                    GitHub
                </a>
                
                
                
                <a href="https://linkedin.com/in/vitor-lobo" target="_blank" rel="noopener noreferrer" class="footer-link">
                    LinkedIn
                </a>
                
                
                
                <a href="mailto:lobocode@gmail.com" class="footer-link">
                    Email
                </a>
                

                
                <a href="https://hachyderm.io/@lobocode" target="_blank" rel="noopener noreferrer" class="footer-link">
                    Mastodon
                </a>
                

                
                <a href="https://scovl.github.io/index.xml" target="_blank" rel="noopener noreferrer" class="footer-link">
                    RSS
                </a>
                
            </div>
            
            
            <div class="back-to-top-container">
                <button id="back-to-top" class="back-to-top-btn" aria-label="Voltar ao topo">
                    <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                        <path d="m18 15-6-6-6 6"/>
                    </svg>
                    <span data-i18n="back_to_top">Voltar ao topo</span>
                </button>
            </div>
            
            <div class="copyright">
                &copy; 2025 scovl
            </div>
        </div>
    </div>
</footer> 
    
    
    
    <script src="/vendor/prism/prism-core.min.js?v=1757091656"></script>
    <script src="/vendor/prism/prism-clike.min.js?v=1757091656"></script>
    <script src="/vendor/prism/prism-c.min.js?v=1757091656"></script>
    <script src="/vendor/prism/prism-cpp.min.js?v=1757091656"></script>
    <script src="/vendor/prism/prism-rust.min.js?v=1757091656"></script>
    <script src="/vendor/prism/prism-clojure.min.js?v=1757091656"></script>
    <script src="/vendor/prism/prism-swift.min.js?v=1757091656"></script>
    <script src="/vendor/prism/prism-bash.min.js?v=1757091656"></script>
    <script src="/vendor/prism/prism-javascript.min.js?v=1757091656"></script>
    <script src="/vendor/prism/prism-typescript.min.js?v=1757091656"></script>
    <script src="/vendor/prism/prism-autoloader.min.js?v=1757091656"></script>
    
    
    <script src="/js/main-minimal.js?v=1757091656"></script>
    <script src="/js/lazy-loading.js?v=1757091656"></script>
    <script src="/js/toc.js?v=1757091656"></script>
    
    
    
</body>
</html> 