<!DOCTYPE html>
<html lang="pt">
<head>
    <title>01 - RAG Simples com Clojure e Ollama | scovl</title>
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Um prot√≥tipo funcional do zero">



<link rel="preload" href="/vendor/fonts/inter/Inter-400.ttf" as="font" type="font/ttf" crossorigin>
<link rel="preload" href="/vendor/fonts/inter/Inter-600.ttf" as="font" type="font/ttf" crossorigin>
<link rel="preload" href="/vendor/fonts/jetbrains-mono/JetBrainsMono-400.ttf" as="font" type="font/ttf" crossorigin>



<link rel="dns-prefetch" href="//giscus.app">
<link rel="preconnect" href="//giscus.app">



<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="format-detection" content="telephone=no"> 


<link rel="stylesheet" href="/css/main.css?v=1757193440">


<link rel="stylesheet" href="/vendor/fonts/fonts.css?v=1757193440">


<link rel="preload" href="/fonts/abril-fatface.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/rokkitt.woff2" as="font" type="font/woff2" crossorigin>


<link rel="stylesheet" href="/vendor/prism/prism-tomorrow.min.css?v=1757193440">



<script src="/vendor/mermaid/mermaid.min.js?v=1757193440"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        mermaid.initialize({
            startOnLoad: true,
            theme: 'light',
            align: 'center'
        });SS
    });
</script>












<script>

const I18n = {
    currentLang: 'pt',
    isRTL:  false ,
    
    
    formatDate(date, options = {}) {
        const defaultOptions = {
            year: 'numeric',
            month: 'long',
            day: 'numeric'
        };
        
        const locale = this.getLocale();
        const finalOptions = { ...defaultOptions, ...options };
        
        return new Intl.DateTimeFormat(locale, finalOptions).format(date);
    },
    
    
    formatNumber(number, options = {}) {
        const defaultOptions = {
            style: 'decimal',
            minimumFractionDigits: 0,
            maximumFractionDigits: 2
        };
        
        const locale = this.getLocale();
        const finalOptions = { ...defaultOptions, ...options };
        
        return new Intl.NumberFormat(locale, finalOptions).format(number);
    },
    
    
    formatCurrency(amount, currency = 'USD') {
        const locale = this.getLocale();
        return new Intl.NumberFormat(locale, {
            style: 'currency',
            currency: currency
        }).format(amount);
    },
    
    
    formatRelativeTime(date) {
        const locale = this.getLocale();
        const now = new Date();
        const diff = now - date;
        const diffInMinutes = Math.floor(diff / (1000 * 60));
        const diffInHours = Math.floor(diff / (1000 * 60 * 60));
        const diffInDays = Math.floor(diff / (1000 * 60 * 60 * 24));
        
        if (diffInMinutes < 1) {
            return new Intl.RelativeTimeFormat(locale).format(0, 'minute');
        } else if (diffInMinutes < 60) {
            return new Intl.RelativeTimeFormat(locale).format(-diffInMinutes, 'minute');
        } else if (diffInHours < 24) {
            return new Intl.RelativeTimeFormat(locale).format(-diffInHours, 'hour');
        } else {
            return new Intl.RelativeTimeFormat(locale).format(-diffInDays, 'day');
        }
    },
    
    
    getLocale() {
        const localeMap = {
            'en': 'en-US',
            'pt': 'pt-BR',
            'es': 'es-ES',
            'fr': 'fr-FR',
            'de': 'de-DE',
            'it': 'it-IT',
            'ar': 'ar-SA',
            'he': 'he-IL',
            'fa': 'fa-IR',
            'ur': 'ur-PK',
            'zh': 'zh-CN',
            'ja': 'ja-JP',
            'ko': 'ko-KR'
        };
        
        return localeMap[this.currentLang] || 'en-US';
    },
    
    
    t(key, params = {}) {
        const translations = {
            'en': {
                'read_more': 'Read more',
                'back_to_top': 'Back to top',
                'loading': 'Loading...',
                'error': 'Error',
                'success': 'Success',
                'warning': 'Warning',
                'info': 'Information',
                'comments': 'Comments',
                'related_posts': 'Related Posts',
                'tags': 'Tags',
                'categories': 'Categories',
                'search': 'Search',
                'menu': 'Menu',
                'close': 'Close',
                'language': 'Language',
                'theme': 'Theme',
                'dark_mode': 'Dark Mode',
                'light_mode': 'Light Mode'
            },
            'pt': {
                'read_more': 'Ler mais',
                'back_to_top': 'Voltar ao topo',
                'loading': 'Carregando...',
                'error': 'Erro',
                'success': 'Sucesso',
                'warning': 'Aviso',
                'info': 'Informa√ß√£o',
                'comments': 'Coment√°rios',
                'related_posts': 'Posts Relacionados',
                'tags': 'Tags',
                'categories': 'Categorias',
                'search': 'Pesquisar',
                'menu': 'Menu',
                'close': 'Fechar',
                'language': 'Idioma',
                'theme': 'Tema',
                'dark_mode': 'Modo Escuro',
                'light_mode': 'Modo Claro'
            },
            'es': {
                'read_more': 'Leer m√°s',
                'back_to_top': 'Volver arriba',
                'loading': 'Cargando...',
                'error': 'Error',
                'success': '√âxito',
                'warning': 'Advertencia',
                'info': 'Informaci√≥n',
                'comments': 'Comentarios',
                'related_posts': 'Posts Relacionados',
                'tags': 'Etiquetas',
                'categories': 'Categor√≠as',
                'search': 'Buscar',
                'menu': 'Men√∫',
                'close': 'Cerrar',
                'language': 'Idioma',
                'theme': 'Tema',
                'dark_mode': 'Modo Oscuro',
                'light_mode': 'Modo Claro'
            },
            'ar': {
                'read_more': 'ÿßŸÇÿ±ÿ£ ÿßŸÑŸÖÿ≤ŸäÿØ',
                'back_to_top': 'ÿßŸÑÿπŸàÿØÿ© ÿ•ŸÑŸâ ÿßŸÑÿ£ÿπŸÑŸâ',
                'loading': 'ÿ¨ÿßÿ±Ÿä ÿßŸÑÿ™ÿ≠ŸÖŸäŸÑ...',
                'error': 'ÿÆÿ∑ÿ£',
                'success': 'ŸÜÿ¨ÿ≠',
                'warning': 'ÿ™ÿ≠ÿ∞Ÿäÿ±',
                'info': 'ŸÖÿπŸÑŸàŸÖÿßÿ™',
                'comments': 'ÿßŸÑÿ™ÿπŸÑŸäŸÇÿßÿ™',
                'related_posts': 'ÿßŸÑŸÖŸÇÿßŸÑÿßÿ™ ÿ∞ÿßÿ™ ÿßŸÑÿµŸÑÿ©',
                'tags': 'ÿßŸÑÿπŸÑÿßŸÖÿßÿ™',
                'categories': 'ÿßŸÑŸÅÿ¶ÿßÿ™',
                'search': 'ÿ®ÿ≠ÿ´',
                'menu': 'ÿßŸÑŸÇÿßÿ¶ŸÖÿ©',
                'close': 'ÿ•ÿ∫ŸÑÿßŸÇ',
                'language': 'ÿßŸÑŸÑÿ∫ÿ©',
                'theme': 'ÿßŸÑŸÖÿ∏Ÿáÿ±',
                'dark_mode': 'ÿßŸÑŸàÿ∂ÿπ ÿßŸÑŸÖÿ∏ŸÑŸÖ',
                'light_mode': 'ÿßŸÑŸàÿ∂ÿπ ÿßŸÑŸÅÿßÿ™ÿ≠'
            }
        };
        
        const langTranslations = translations[this.currentLang] || translations['en'];
        let text = langTranslations[key] || key;
        
        
        Object.keys(params).forEach(param => {
            text = text.replace(`{${param}}`, params[param]);
        });
        
        return text;
    },
    
    
    init() {
        this.updatePageDirection();
        this.updateDateFormats();
        this.updateNumberFormats();
        this.updateTranslations();
    },
    
    
    updatePageDirection() {
        if (this.isRTL) {
            document.documentElement.setAttribute('dir', 'rtl');
            document.documentElement.setAttribute('lang', this.currentLang);
        }
    },
    
    
    updateDateFormats() {
        const dateElements = document.querySelectorAll('[data-date]');
        dateElements.forEach(element => {
            const date = new Date(element.getAttribute('data-date'));
            const format = element.getAttribute('data-date-format') || 'default';
            
            let formattedDate;
            switch (format) {
                case 'relative':
                    formattedDate = this.formatRelativeTime(date);
                    break;
                case 'short':
                    formattedDate = this.formatDate(date, { month: 'short', day: 'numeric' });
                    break;
                case 'long':
                    formattedDate = this.formatDate(date, { 
                        weekday: 'long',
                        year: 'numeric',
                        month: 'long',
                        day: 'numeric'
                    });
                    break;
                default:
                    formattedDate = this.formatDate(date);
            }
            
            element.textContent = formattedDate;
        });
    },
    
    
    updateNumberFormats() {
        const numberElements = document.querySelectorAll('[data-number]');
        numberElements.forEach(element => {
            const number = parseFloat(element.getAttribute('data-number'));
            const format = element.getAttribute('data-number-format') || 'decimal';
            
            let formattedNumber;
            switch (format) {
                case 'currency':
                    const currency = element.getAttribute('data-currency') || 'USD';
                    formattedNumber = this.formatCurrency(number, currency);
                    break;
                case 'percent':
                    formattedNumber = this.formatNumber(number / 100, { style: 'percent' });
                    break;
                default:
                    formattedNumber = this.formatNumber(number);
            }
            
            element.textContent = formattedNumber;
        });
    },
    
    
    updateTranslations() {
        const translationElements = document.querySelectorAll('[data-i18n]');
        translationElements.forEach(element => {
            const key = element.getAttribute('data-i18n');
            const params = {};
            
            
            const paramAttributes = element.getAttribute('data-i18n-params');
            if (paramAttributes) {
                try {
                    Object.assign(params, JSON.parse(paramAttributes));
                } catch (e) {
                    console.warn('Invalid i18n params:', paramAttributes);
                }
            }
            
            element.textContent = this.t(key, params);
        });
    }
};


window.I18n = I18n;
</script>


<script>
    if ('serviceWorker' in navigator) {
        window.addEventListener('load', function() {
            navigator.serviceWorker.register('\/sw.js')
                .then(function(registration) {
                    console.log('Service Worker registrado com sucesso:', registration.scope);
                })
                .catch(function(error) {
                    console.log('Falha no registro do Service Worker:', error);
                });
        });
    }
</script> 
</head>
<body>
    
    
    <header class="header">
    <div class="container">
        <div class="header-content">
            <a href="https://scovl.github.io/" class="site-title">scovl</a>
            
            <div class="header-actions">
                
                <nav class="nav-menu">
                    <ul>
                        
                        <li><a href="/page/about/">About</a></li>
                        
                        <li><a href="/page/contact/">Contact</a></li>
                        
                    </ul>
                </nav>
                
                
                



<div class="language-switcher" id="language-switcher">
    <button class="language-btn" onclick="toggleLanguageMenu()">
        <span class="current-lang">Portugu√™s</span>
        <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
            <polyline points="6,9 12,15 18,9"></polyline>
        </svg>
    </button>
    <div class="language-menu" id="language-menu">
        
            
                <a href="https://scovl.github.io/" class="language-option active">
                    Portugu√™s
                </a>
            
        
            
                <a href="https://scovl.github.io/en/" class="language-option ">
                    English
                </a>
            
        
    </div>
</div>
                
                
                <button id="dark-mode-toggle" class="theme-toggle" aria-label="Toggle dark mode">
                    <svg class="sun-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <line x1="12" y1="1" x2="12" y2="3"/>
                        <line x1="12" y1="21" x2="12" y2="23"/>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/>
                        <line x1="1" y1="12" x2="3" y2="12"/>
                        <line x1="21" y1="12" x2="23" y2="12"/>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/>
                    </svg>
                    <svg class="moon-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </div>
</header> 
    
    
    
    <main>
        <div class="container">
            
<article class="post">
    <header class="post-header">
        <h1 class="post-title">01 - RAG Simples com Clojure e Ollama</h1>
        
        <div class="post-meta">
            <time datetime="2025-03-23T19:00:00Z">
                üìÖ 23/03/2025
            </time>
            
            <span>üë§ Vitor Lobo Ramos</span>
            
            
            
            <div class="post-tags">
                
                <a href="/tags/rag" class="tag">RAG</a>
                
                <a href="/tags/llm" class="tag">LLM</a>
                
                <a href="/tags/ai" class="tag">AI</a>
                
                <a href="/tags/langchain" class="tag">Langchain</a>
                
            </div>
            
        </div>
        
    </header>
    
    
    















  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  



<aside class="toc" id="toc" aria-labelledby="toc-title">
    <div class="toc-container">
        <div class="toc-header">
            <h3 id="toc-title" class="toc-title">
                <svg class="toc-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <path d="M3 6h18M3 12h18M3 18h18"/>
                </svg>
                Sum√°rio
            </h3>
            <button class="toc-toggle" id="toc-toggle" aria-expanded="true" aria-controls="toc-content" aria-label="Mostrar/Ocultar Sum√°rio">
                <svg class="toc-toggle-icon" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <path d="m6 9 6 6 6-6"/>
                </svg>
            </button>
        </div>
        <div class="toc-content" id="toc-content">
            <div class="toc-progress">
                <div class="toc-progress-bar" id="toc-progress-bar"></div>
            </div>
            <nav class="toc-nav">
                <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#introdu√ß√£o">Introdu√ß√£o</a></li>
        <li><a href="#fundamentos-do-rag">Fundamentos do RAG</a>
          <ul>
            <li><a href="#o-que-√©-rag">O que √© RAG?</a></li>
            <li><a href="#por-que-precisamos-do-rag">Por que precisamos do RAG?</a></li>
            <li><a href="#os-tr√™s-pilares-do-rag">Os Tr√™s Pilares do RAG</a></li>
            <li><a href="#rag-em-produ√ß√£o">RAG em Produ√ß√£o</a></li>
            <li><a href="#por-que-o-deepseek-r1">Por que o DeepSeek R1?</a></li>
          </ul>
        </li>
        <li><a href="#implementa√ß√£o-pr√°tica">Implementa√ß√£o Pr√°tica</a>
          <ul>
            <li><a href="#preparando-o-ambiente">Preparando o Ambiente</a></li>
            <li><a href="#estrutura-do-projeto">Estrutura do Projeto</a></li>
            <li><a href="#tf-idf">TF-IDF</a></li>
            <li><a href="#requisitos-m√≠nimos">Requisitos M√≠nimos</a></li>
            <li><a href="#configura√ß√£o-do-projeto">Configura√ß√£o do Projeto</a></li>
            <li><a href="#implementa√ß√£o-dos-componentes">Implementa√ß√£o dos Componentes</a>
              <ul>
                <li><a href="#processamento-de-documentos">Processamento de Documentos</a></li>
                <li><a href="#sistema-de-embeddings">Sistema de Embeddings</a></li>
                <li><a href="#interface-com-ollama">Interface com Ollama</a></li>
                <li><a href="#m√≥dulo-principal">M√≥dulo Principal</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#como-usar">Como Usar</a>
          <ul>
            <li><a href="#instala√ß√£o-do-ollama">Instala√ß√£o do Ollama</a></li>
            <li><a href="#executando-a-aplica√ß√£o">Executando a Aplica√ß√£o</a></li>
          </ul>
        </li>
        <li><a href="#considera√ß√µes-t√©cnicas">Considera√ß√µes T√©cnicas</a>
          <ul>
            <li><a href="#performance-e-otimiza√ß√µes">Performance e Otimiza√ß√µes</a></li>
            <li><a href="#melhorando-os-prompts">Melhorando os Prompts</a></li>
          </ul>
        </li>
        <li><a href="#pr√≥ximos-passos">Pr√≥ximos Passos</a>
          <ul>
            <li><a href="#melhorias-r√°pidas-implementa√ß√£o-imediata">Melhorias R√°pidas (Implementa√ß√£o Imediata)</a>
              <ul>
                <li><a href="#1-persist√™ncia-da-base-de-conhecimento"><strong>1. Persist√™ncia da Base de Conhecimento</strong></a></li>
                <li><a href="#2-testes-unit√°rios"><strong>2. Testes Unit√°rios</strong></a></li>
                <li><a href="#3-streaming-de-respostas"><strong>3. Streaming de Respostas</strong></a></li>
                <li><a href="#4-cache-de-embeddings"><strong>4. Cache de Embeddings</strong></a></li>
                <li><a href="#5-banco-vetorial-simples-bm25-manual"><strong>5. Banco Vetorial Simples (BM25 Manual)</strong></a></li>
              </ul>
            </li>
            <li><a href="#melhorias-avan√ßadas">Melhorias Avan√ßadas</a></li>
            <li><a href="#depend√™ncias-e-pr√≥ximos-passos">Depend√™ncias e Pr√≥ximos Passos</a>
              <ul>
                <li><a href="#depend√™ncias-recomendadas"><strong>Depend√™ncias Recomendadas</strong></a></li>
                <li><a href="#implementa√ß√£o-com-lucene"><strong>Implementa√ß√£o com Lucene</strong></a></li>
              </ul>
            </li>
            <li><a href="#upgrade-para-embeddings-densos">Upgrade para Embeddings Densos</a>
              <ul>
                <li><a href="#1-via-ollama-embeddings-api">1. <strong>Via Ollama Embeddings API</strong></a></li>
                <li><a href="#2-via-huggingface-transformers">2. <strong>Via HuggingFace Transformers</strong></a></li>
                <li><a href="#3-compara√ß√£o-de-performance">3. <strong>Compara√ß√£o de Performance</strong></a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#ap√™ndice">Ap√™ndice</a>
          <ul>
            <li><a href="#requisitos-de-hardware-detalhados">Requisitos de Hardware Detalhados</a>
              <ul>
                <li><a href="#configura√ß√µes-por-caso-de-uso"><strong>Configura√ß√µes por Caso de Uso</strong></a></li>
                <li><a href="#otimiza√ß√µes-por-hardware"><strong>Otimiza√ß√µes por Hardware</strong></a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#refer√™ncias">Refer√™ncias</a></li>
      </ul>
    </li>
  </ul>
</nav>
            </nav>
        </div>
    </div>
    
    
    <button class="toc-mobile-toggle" id="toc-mobile-toggle" aria-label="Mostrar Sum√°rio">
        <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <path d="M3 6h18M3 12h18M3 18h18"/>
        </svg>
    </button>
</aside>



    
    <div class="post-content">
        <h2 id="introdu√ß√£o">Introdu√ß√£o</h2>
<p>Ol√°, pessoal! üëã</p>
<p>Neste artigo, vamos explorar como construir uma aplica√ß√£o <a href="https://pt.wikipedia.org/wiki/Gera%C3%A7%C3%A3o_aumentada_por_recupera%C3%A7%C3%A3o">RAG (Retrieval-Augmented Generation)</a> completa do zero usando <a href="https://clojure.org/">Clojure</a>. Vamos mergulhar em uma implementa√ß√£o pr√°tica que combina processamento de texto, busca sem√¢ntica e gera√ß√£o de respostas com LLMs locais. Se voc√™ est√° interessado em melhorar a precis√£o e relev√¢ncia das respostas dos seus modelos de linguagem com informa√ß√µes atualizadas, este guia √© para voc√™!</p>
<h2 id="fundamentos-do-rag">Fundamentos do RAG</h2>
<h3 id="o-que-√©-rag">O que √© RAG?</h3>
<p>Os Modelos de Linguagem de Grande Escala (<a href="https://en.wikipedia.org/wiki/Large_language_model">LLMs</a>), como o <a href="https://openai.com/api/">GPT</a>, <a href="https://openai.com/api/">ChatGPT</a> e outros, revolucionaram a forma como interagimos com a intelig√™ncia artificial. Eles s√£o capazes de gerar textos coerentes, responder perguntas complexas e at√© mesmo criar conte√∫do criativo. No entanto, esses modelos possuem uma limita√ß√£o fundamental: seu conhecimento √© &ldquo;congelado&rdquo; no tempo.</p>


  
    
  
  <div class="mermaid">graph TD
    A[LLM Treinado] --&gt; B[Data de Corte]
    B --&gt; C[Conhecimento Congelado]
    C --&gt; D[Limita√ß√µes]
    D --&gt; E[N√£o sabe eventos recentes]
    D --&gt; F[N√£o tem dados atualizados]
    D --&gt; G[N√£o conhece novas tecnologias]</div>
 <h3 id="por-que-precisamos-do-rag">Por que precisamos do RAG?</h3>
<p>Ao desenvolver aplica√ß√µes inteligentes, como assistentes financeiros que precisam de cota√ß√µes de a√ß√µes em tempo real, chatbots de suporte que devem conhecer os produtos mais recentes da empresa ou sistemas de recomenda√ß√£o que se baseiam nas √∫ltimas tend√™ncias, nos deparamos com uma limita√ß√£o crucial dos Modelos de Linguagem de Grande Escala (<a href="https://en.wikipedia.org/wiki/Large_language_model">LLMs</a>) tradicionais: seu conhecimento est√°tico.</p>
<p>O problema fundamental reside no fato de que esses modelos, por mais sofisticados que sejam, possuem uma base de conhecimento &ldquo;congelada&rdquo; no momento de seu treinamento. Eles carecem de acesso inerente a informa√ß√µes atualizadas, o que restringe drasticamente sua aplicabilidade em cen√°rios que exigem dados em tempo real ou conhecimento sobre eventos recentes.</p>
<blockquote>
<p>Confiar exclusivamente em um <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM &ldquo;puro&rdquo;</a> nesses contextos resultar√° em respostas desatualizadas, potencialmente imprecisas e, consequentemente, em uma experi√™ncia do usu√°rio comprometida. A efic√°cia da aplica√ß√£o √© diretamente afetada.</p></blockquote>
<h3 id="os-tr√™s-pilares-do-rag">Os Tr√™s Pilares do RAG</h3>


  
  <div class="mermaid">graph LR
    A[RAG] --&gt; B[Base de Dados Atual]
    A --&gt; C[Pesquisa em Tempo Real]
    A --&gt; D[Combina√ß√£o de Conhecimento]
    
    B --&gt; E[Documentos Atualizados]
    B --&gt; F[Dados em Tempo Real]
    
    C --&gt; G[Busca Ativa]
    C --&gt; H[Sele√ß√£o de Informa√ß√µes]
    
    D --&gt; I[Integra√ß√£o com LLM]
    D --&gt; J[Contextualiza√ß√£o]</div>
 <ol>
<li><strong>Conex√£o com uma base de dados atual:</strong> Em vez de depender apenas do conhecimento est√°tico adquirido durante seu treinamento (que pode se tornar obsoleto rapidamente), o <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a> ganha acesso a uma fonte de informa√ß√µes din√¢mica e constantemente atualizada. Isso pode ser uma base de dados de not√≠cias, um reposit√≥rio de documentos corporativos, uma cole√ß√£o de artigos cient√≠ficos, ou qualquer outra fonte relevante para a tarefa em quest√£o.</li>
<li><strong>Pesquisa em tempo real:</strong> O <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a> n√£o est√° mais limitado a &ldquo;lembrar&rdquo; de informa√ß√µes. Ele adquire a capacidade de &ldquo;procurar&rdquo; ativamente por dados relevantes para responder a uma pergunta ou gerar um texto. Isso √© semelhante a como n√≥s, humanos, usamos mecanismos de busca para encontrar informa√ß√µes que n√£o temos memorizadas. O <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a>, equipado com RAG, pode formular consultas, analisar os resultados e selecionar as informa√ß√µes mais pertinentes.</li>
<li><strong>Combina√ß√£o de conhecimento base com dados novos:</strong> Este √© o ponto crucial que diferencia o <a href="https://pt.wikipedia.org/wiki/Gera%C3%A7%C3%A3o_aumentada_por_recupera%C3%A7%C3%A3o">RAG</a> de uma simples busca em uma base de dados. O <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a> n√£o apenas recupera informa√ß√µes, mas tamb√©m as integra ao seu conhecimento pr√©-existente. Ele usa sua capacidade de racioc√≠nio e compreens√£o para contextualizar os novos dados, identificar contradi√ß√µes, e formular respostas coerentes e informadas.</li>
</ol>
<h3 id="rag-em-produ√ß√£o">RAG em Produ√ß√£o</h3>
<p>Sistemas RAG em produ√ß√£o frequentemente incluem etapas adicionais para melhorar a precis√£o: <strong>re-ranking</strong> (onde um modelo especializado re-avalia a relev√¢ncia dos documentos recuperados) e <strong>merge-rerank</strong> (que combina resultados de m√∫ltiplas estrat√©gias de busca como sem√¢ntica, lexical e h√≠brida). Essas t√©cnicas aumentam significativamente a qualidade das respostas, mas adicionam complexidade ao sistema.</p>
<blockquote>
<p><strong>Nota</strong>: Nossa implementa√ß√£o atual usa apenas busca sem√¢ntica simples com TF-IDF, focando na compreens√£o dos fundamentos do RAG. Para aplica√ß√µes em produ√ß√£o, considere implementar essas t√©cnicas avan√ßadas.</p></blockquote>
<p>Segundo um <a href="https://arxiv.org/abs/2309.01066">whitepaper recente dos pesquisadores do Google</a>, existem v√°rias t√©cnicas para turbinar o desempenho dos <a href="https://en.wikipedia.org/wiki/Large_language_model">LLMs</a>, e o RAG √© uma das mais promissoras. Isso ocorre porque o RAG aborda algumas das limita√ß√µes fundamentais desses modelos:</p>
<p>O RAG resolve v√°rios problemas de uma vez s√≥: diminui aquelas &ldquo;viagens&rdquo; dos <a href="https://en.wikipedia.org/wiki/Large_language_model">LLMs</a> quando inventam respostas (as famosas alucina√ß√µes), mant√©m tudo atualizado em vez de ficar preso no passado, deixa as respostas mais transparentes porque voc√™ sabe de onde veio a informa√ß√£o, e ainda melhora o desempenho do modelo quando ele precisa lidar com documentos ou dados espec√≠ficos da sua empresa. √â como dar ao modelo um Google particular que ele pode consultar antes de responder!</p>
<blockquote>
<p>O RAG representa um avan√ßo significativo na evolu√ß√£o dos <a href="https://en.wikipedia.org/wiki/Large_language_model">LLMs</a>, permitindo que eles se tornem ferramentas mais confi√°veis, precisas e √∫teis para uma ampla gama de aplica√ß√µes. Ele transforma o <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a> de um &ldquo;sabe-tudo&rdquo; desatualizado em um pesquisador √°gil e bem-informado, capaz de combinar conhecimento profundo com informa√ß√µes atualizadas em tempo real.</p></blockquote>
<h3 id="por-que-o-deepseek-r1">Por que o DeepSeek R1?</h3>
<p>Ele trabalha muito bem com documenta√ß√£o t√©cnica, o que √© perfeito para nosso sistema <a href="https://pt.wikipedia.org/wiki/Gera%C3%A7%C3%A3o_aumentada_por_recupera%C3%A7%C3%A3o">RAG</a> focado em docs t√©cnicas. O DeepSeek R1 consegue equilibrar qualidade e velocidade melhor que outros modelos do Ollama, rodando na sua m√°quina sem ficar alucinando com respostas que n√£o fazem sentido.</p>
<p>O modelo tamb√©m se d√° super bem com v√°rias linguagens de programa√ß√£o, incluindo <a href="https://clojure.org/">Clojure</a>, ent√£o ele responde numa boa sobre implementa√ß√µes t√©cnicas e documenta√ß√£o de c√≥digo. E o melhor: mesmo quando voc√™ joga informa√ß√µes pela metade ou todas bagun√ßadas, ele ainda consegue manter o contexto e dar respostas que fazem sentido. Por isso ele √© perfeito para o que estamos construindo!</p>
<h2 id="implementa√ß√£o-pr√°tica">Implementa√ß√£o Pr√°tica</h2>
<h3 id="preparando-o-ambiente">Preparando o Ambiente</h3>
<p>Pre-requisitos:</p>
<ul>
<li><a href="https://clojure.org/guides/getting_started">Clojure</a>: Linguagem de programa√ß√£o funcional que vamos usar para construir a aplica√ß√£o</li>
<li><a href="https://leiningen.org/">Leiningen</a>: Ferramenta de build para Clojure</li>
<li><a href="https://ollama.com/">Ollama</a>: Modelo de linguagem local</li>
</ul>
<h3 id="estrutura-do-projeto">Estrutura do Projeto</h3>
<p>Nossa aplica√ß√£o ter√° tr√™s componentes principais:</p>
<ol>
<li><strong>Processamento de documenta√ß√£o (Markdown/HTML)</strong>
<ul>
<li>Extra√ß√£o de texto</li>
<li>Pr√©-processamento de texto</li>
</ul>
</li>
<li><strong>Sistema de embeddings</strong>
<ul>
<li>Cria√ß√£o de embeddings para o texto usando <a href="https://pt.wikipedia.org/wiki/TF-IDF">TF-IDF</a></li>
<li>Busca por similaridade sem√¢ntica</li>
</ul>
</li>
<li><strong>Interface com o LLM</strong>
<ul>
<li>Gera√ß√£o de resposta usando o LLM</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>Observa√ß√£o:</strong> Embora o RAG moderno utilize embeddings densos gerados por modelos de linguagem para capturar a sem√¢ntica de forma mais rica, neste artigo, usaremos uma implementa√ß√£o simplificada de <a href="https://pt.wikipedia.org/wiki/TF-IDF">TF-IDF (Term Frequency-Inverse Document Frequency)</a> como <strong>prova de conceito</strong>. Para aplica√ß√µes em produ√ß√£o, recomendamos fortemente o uso de embeddings densos.</p></blockquote>
<h3 id="tf-idf">TF-IDF</h3>
<p>O <a href="https://pt.wikipedia.org/wiki/TF-IDF">TF-IDF</a> (Term Frequency-Inverse Document Frequency) √© uma t√©cnica estat√≠stica usada para avaliar a import√¢ncia de uma palavra em um documento, em rela√ß√£o a uma cole√ß√£o de documentos. Vamos entender como funciona:</p>
<ol>
<li>
<p><strong>Term Frequency (TF)</strong>: Mede a frequ√™ncia de uma palavra em um documento.</p>


  <pre><code class="language-">TF(termo) = (N√∫mero de vezes que o termo aparece no documento) / (Total de termos no documento)</code></pre>
 </li>
<li>
<p><strong>Inverse Document Frequency (IDF)</strong>: Mede a raridade de um termo na cole√ß√£o de documentos.</p>


  <pre><code class="language-">IDF(termo) = log(N√∫mero total de documentos / N√∫mero de documentos contendo o termo)</code></pre>
 </li>
<li>
<p><strong>TF-IDF</strong>: √â o produto desses dois valores.</p>


  <pre><code class="language-">TF-IDF(termo) = TF(termo) √ó IDF(termo)</code></pre>
 </li>
</ol>
<p>Vamos imaginar um cen√°rio pr√°tico com tr√™s documentos t√©cnicos:</p>
<ul>
<li>Doc1: &ldquo;Clojure √© uma linguagem funcional baseada em <a href="https://en.wikipedia.org/wiki/Lisp_%28programming_language%29">Lisp</a>&rdquo;</li>
<li>Doc2: &ldquo;Python √© uma linguagem de programa√ß√£o vers√°til&rdquo;</li>
<li>Doc3: &ldquo;Clojure e Python s√£o linguagens de programa√ß√£o populares&rdquo;</li>
</ul>
<p>O TF-IDF √© uma t√©cnica que nos ajuda a identificar quais palavras s√£o mais importantes em cada documento, comparando a frequ√™ncia de um termo no documento (TF) com a raridade desse termo em toda a cole√ß√£o (IDF). Por exemplo, se &ldquo;Clojure&rdquo; aparece uma vez em um documento de oito palavras, seu TF √© 0,125; como est√° presente em dois de tr√™s documentos, seu IDF √© log(3/2) ‚âà 0,176, resultando em um TF-IDF de aproximadamente 0,022. J√° termos muito comuns, como &ldquo;linguagem&rdquo;, acabam com TF-IDF zero, pois n√£o ajudam a diferenciar os documentos.</p>
<p>Esse m√©todo √© fundamental em sistemas de busca, pois destaca os termos que realmente caracterizam cada texto. No contexto do RAG, o TF-IDF permite indexar e encontrar rapidamente os documentos mais relevantes para uma consulta, servindo como uma base simples e eficiente para recupera√ß√£o de informa√ß√µes, que pode ser aprimorada com t√©cnicas mais avan√ßadas como embeddings densos.</p>
<h3 id="requisitos-m√≠nimos">Requisitos M√≠nimos</h3>
<p>Este experimento funciona com hardware b√°sico: <strong>4 cores de CPU e 8GB de RAM</strong> s√£o suficientes. Para m√°quinas mais lentas, use <code>ollama pull deepseek-r1:3b</code> (vers√£o otimizada).</p>
<blockquote>
<p>Para requisitos detalhados de produ√ß√£o e otimiza√ß√µes avan√ßadas, consulte o <a href="/2025/03/23/rag/#requisitos-de-hardware-detalhados">ap√™ndice de hardware</a> ao final do artigo.</p></blockquote>
<h3 id="configura√ß√£o-do-projeto">Configura√ß√£o do Projeto</h3>
<ol>
<li>Crie um novo projeto Clojure:</li>
</ol>


  <pre><code class="language-bash">lein new app docai
cd docai</code></pre>
 <ol start="2">
<li>Configure o <code>project.clj</code>:</li>
</ol>


  <pre><code class="language-clojure">(defproject docai &#34;0.1.0-SNAPSHOT&#34;
  :description &#34;Um assistente RAG para consulta de documenta√ß√£o t√©cnica&#34;
  :url &#34;http://example.com/FIXME&#34;
  :license {:name &#34;EPL-2.0 OR GPL-2.0-or-later WITH Classpath-exception-2.0&#34;
            :url &#34;https://www.eclipse.org/legal/epl-2.0/&#34;}
  :dependencies [[org.clojure/clojure &#34;1.11.1&#34;]
                 [markdown-to-hiccup &#34;0.6.2&#34;]
                 [hickory &#34;0.7.1&#34;]
                 [org.clojure/data.json &#34;2.4.0&#34;]
                 [http-kit &#34;2.6.0&#34;]
                 [org.clojure/tools.logging &#34;1.2.4&#34;]
                 [org.clojure/tools.namespace &#34;1.4.4&#34;]
                 [org.clojure/core.async &#34;1.6.681&#34;]
                 [org.clojure/core.memoize &#34;1.0.257&#34;]
                 [org.clojure/core.cache &#34;1.0.225&#34;]]
  :main ^:skip-aot docai.core
  :target-path &#34;target/%s&#34;
  :profiles {:uberjar {:aot :all
                       :jvm-opts [&#34;-Dclojure.compiler.direct-linking=true&#34;]}})</code></pre>
 <p>A estrutura do projeto acima define um aplicativo Clojure para RAG (Retrieval-Augmented Generation) com v√°rias depend√™ncias essenciais. Entre elas, <code>markdown-to-hiccup</code> e <code>hickory</code> s√£o usadas para processar documentos em Markdown e HTML, enquanto <code>data.json</code> e <code>http-kit</code> facilitam a comunica√ß√£o com APIs externas, como a do Ollama. Al√©m disso, <code>tools.logging</code> √© respons√°vel pelo registro de eventos e logs, e <code>tools.namespace</code> auxilia no gerenciamento de namespaces do projeto.</p>
<p>J√° <code>core.async</code> permite opera√ß√µes ass√≠ncronas, o que √© especialmente √∫til ao lidar com o processamento de documentos grandes. Por fim, <code>core.memoize</code> e <code>core.cache</code> s√£o utilizados para implementar cache de resultados, como embeddings ou respostas do LLM, melhorando significativamente a performance ao evitar rec√°lculos desnecess√°rios, principalmente em consultas repetidas ou similares.</p>
<h3 id="implementa√ß√£o-dos-componentes">Implementa√ß√£o dos Componentes</h3>
<p>Agora vamos implementar os tr√™s componentes principais do nosso sistema RAG e vamos come√ßar com o processamento de documentos. Pois, ele √© o ponto de entrada para o RAG onde vamos processar os documentos e extrair o texto para ser usado nos outros componentes.</p>
<h4 id="processamento-de-documentos">Processamento de Documentos</h4>


  <pre><code class="language-clojure">;; src/docai/document.clj
(ns docai.document
  (:require [markdown-to-hiccup.core :as md]
            [hickory.core :as html]
            [clojure.string :as str]))

(defn extract-text-from-markdown [content]
  (try
    (let [hiccup-result (md/md-&gt;hiccup content)
          text-nodes (filter string? (flatten hiccup-result))]
      text-nodes)
    (catch Exception e
      (println &#34;Erro ao processar Markdown:&#34; (.getMessage e))
      [content])))

(defn extract-text-from-html [content]
  (try
    (let [dom (html/parse content)
          hiccup-result (html/as-hiccup dom)
          text-nodes (filter string? (flatten hiccup-result))]
      text-nodes)
    (catch Exception e
      (println &#34;Erro ao processar HTML:&#34; (.getMessage e))
      [content])))

;; Declare functions that will be defined later
(declare create-token-aware-chunks)

(defn extract-text
  &#34;Extrai texto de documenta√ß√£o (Markdown ou HTML)&#34;
  [doc-path]
  (println &#34;Extraindo texto de:&#34; doc-path)
  (let [content (slurp doc-path)
        _ (println &#34;Tamanho do conte√∫do:&#34; (count content) &#34;caracteres&#34;)
        _ (println &#34;Amostra do conte√∫do:&#34; (subs content 0 (min 100 (count content))))
        text (if (.endsWith doc-path &#34;.md&#34;)
               (extract-text-from-markdown content)
               (extract-text-from-html content))
        _ (println &#34;Quantidade de n√≥s de texto extra√≠dos:&#34; (count text))
        ;; Usar tokens reais em vez de caracteres para chunking preciso
        chunks (create-token-aware-chunks text 512)]
    (println &#34;Quantidade de chunks gerados:&#34; (count chunks))
    chunks))

(defn count-tokens
  &#34;Conta tokens usando heur√≠stica (para desenvolvimento)&#34;
  [text]
  ;; ‚ö†Ô∏è ATEN√á√ÉO: Esta √© uma heur√≠stica aproximada
  ;; Para produ√ß√£o, use [clojure-tiktoken](https://github.com/justone/clojure-tiktoken)
  ;; ou API do Ollama para contagem precisa
  (try
    (let [words (str/split text #&#34;\s&#43;&#34;)
          ;; Estimativa melhorada para portugu√™s brasileiro
          ;; Ainda pode errar 2x em textos muito curtos/longos
          estimated-tokens (reduce &#43; 
                                 (map (fn [word]
                                        (cond
                                          ;; Palavras muito longas (composi√ß√£o)
                                          (&gt; (count word) 15) (* (count word) 0.8)
                                          ;; Palavras longas (deriva√ß√£o)
                                          (&gt; (count word) 10) (* (count word) 0.6)
                                          ;; Palavras m√©dias
                                          (&gt; (count word) 5) (* (count word) 0.4)
                                          ;; Palavras curtas
                                          :else 1.0))
                                      words))]
      (int estimated-tokens))
    (catch Exception e
      (println &#34;Erro ao contar tokens:&#34; (.getMessage e))
      ;; Fallback conservador: 1 token por caractere
      (count text))))

(defn create-token-aware-chunks
  &#34;Cria chunks baseados em tokens reais, n√£o caracteres&#34;
  [text-nodes max-tokens]
  (loop [nodes text-nodes
         current-chunk []
         current-tokens 0
         all-chunks []]
    (if (empty? nodes)
      (if (seq current-chunk)
        (conj all-chunks (str/join &#34; &#34; current-chunk))
        all-chunks)
      (let [node (first nodes)
            node-tokens (count-tokens node)
            new-total (&#43; current-tokens node-tokens)]
        (if (and (&gt; new-total max-tokens) (seq current-chunk))
          ;; Chunk cheio, salva e inicia novo
          (recur (rest nodes)
                 [node]
                 node-tokens
                 (conj all-chunks (str/join &#34; &#34; current-chunk)))
          ;; Adiciona ao chunk atual
          (recur (rest nodes)
                 (conj current-chunk node)
                 new-total
                 all-chunks))))))

(defn preprocess-chunks
  &#34;Limpa e prepara os chunks de texto&#34;
  [chunks]
  (let [processed (map #(-&gt; %
                            (str/replace #&#34;\s&#43;&#34; &#34; &#34;)
                            (str/trim))
                       chunks)]
    (println &#34;Primeiro chunk processado:&#34; (first processed))
    processed))</code></pre>
 <p>Este trecho de c√≥digo implementa a parte de processamento de documentos do nosso sistema RAG. Basicamente, ele pega arquivos Markdown ou HTML e extrai o texto puro deles para que possamos usar depois na busca sem√¢ntica. O c√≥digo usa bibliotecas como <code>markdown-to-hiccup</code> e <code>hickory</code> para converter os documentos em estruturas de dados que facilitam a extra√ß√£o do texto.</p>


  
  <div class="mermaid">graph TD
    A[Documento] --&gt; B{√â Markdown?}
    B --&gt;|Sim| C[Processa Markdown]
    B --&gt;|N√£o| D[Processa HTML]
    C --&gt; E[Extrai Texto]
    D --&gt; E
    E --&gt; F[Divide em Chunks]
    F --&gt; G[Limpa e Formata]
    G --&gt; H[Chunks Prontos]</div>
 <p>O fluxo √© bem direto: primeiro verificamos se estamos lidando com Markdown ou HTML, depois extra√≠mos o texto usando a fun√ß√£o apropriada, dividimos em peda√ßos menores (chunks) baseados em tokens reais (n√£o caracteres), e finalmente limpamos esses chunks removendo espa√ßos extras e formatando tudo direitinho.</p>
<p>O c√≥digo tamb√©m inclui bastante logging para ajudar a depurar o processo, mostrando informa√ß√µes como o tamanho do documento, quantidade de texto extra√≠do e n√∫mero de chunks gerados.</p>
<p>Essa abordagem de dividir o texto em peda√ßos menores √© crucial para o RAG, j√° que permite processar documentos grandes sem sobrecarregar o modelo de linguagem.</p>
<blockquote>
<p><strong>Importante</strong>: Dividimos o texto em chunks usando tokens (n√£o caracteres) para n√£o ultrapassar o limite do modelo. A contagem de tokens √© aproximada. Para produ√ß√£o, use uma biblioteca como <a href="https://github.com/justone/clojure-tiktoken">clojure-tiktoken</a> para maior precis√£o.</p></blockquote>
<h4 id="sistema-de-embeddings">Sistema de Embeddings</h4>
<p>Agora vamos implementar o sistema de embeddings. Ele √© respons√°vel por criar embeddings para o texto para que possamos usar na busca sem√¢ntica.</p>


  <pre><code class="language-clojure">;; src/docai/embedding.clj
(ns docai.embedding
  (:require [clojure.string :as str]
            [clojure.core.memoize :as memo]))

;; Implementa√ß√£o de embeddings usando TF-IDF simples
;; N√£o depende de modelos externos, ao contr√°rio do Ollama que usa o deepseek-r1 para o LLM

(defn tokenize
  &#34;Divide o texto em tokens&#34;
  [text]
  (if (string? text)
    (-&gt; text
        str/lower-case
        (str/split #&#34;\s&#43;&#34;)
        (-&gt;&gt; (filter #(&gt; (count %) 2))))
    []))

(defn term-freq
  &#34;Calcula a frequ√™ncia dos termos&#34;
  [tokens]
  (frequencies tokens))



(defn doc-freq
  &#34;Calcula a frequ√™ncia dos documentos&#34;
  [docs]
  (let [string-docs (filter string? docs)  ; Use Clojure&#39;s built-in string? function
        _ (println (str &#34;Processando &#34; (count string-docs) &#34; documentos v√°lidos de &#34; (count docs) &#34; total&#34;))
        doc-tokens (map tokenize string-docs)  
        all-tokens (distinct (flatten doc-tokens))
        doc-count (count string-docs)]
    (if (zero? doc-count)
      {}
      (zipmap all-tokens
              (map #(count (filter (fn [tokens] (some #{%} tokens)) doc-tokens))
                   all-tokens)))))

(defn tf-idf
  &#34;Calcula TF-IDF para um documento&#34;
  [doc doc-freq doc-count]
  (if (empty? doc-freq)
    {}
    (let [tokens (tokenize doc)
          tf (term-freq tokens)]
      (zipmap (keys tf)
              (map #(* (get tf %) (Math/log (/ doc-count (get doc-freq % 1))))
                   (keys tf))))))

(defn vectorize
  &#34;Converte um documento em um vetor TF-IDF&#34;
  [doc doc-freq doc-count vocab]
  (let [tf-idf-scores (tf-idf doc doc-freq doc-count)]
    (if (empty? vocab)
      []
      (map #(get tf-idf-scores % 0.0) vocab))))

(defn create-embeddings
  &#34;Gera embeddings para uma lista de textos usando TF-IDF&#34;
  [texts]
  (try
    (let [doc-freq (doc-freq texts)
          doc-count (count (filter string? texts))
          ;; Vocabul√°rio ordenado para garantir ordem est√°vel
          vocab (sort (keys doc-freq))]
      (map #(vectorize % doc-freq doc-count vocab) texts))
    (catch Exception e
      (println &#34;Erro ao criar embeddings: &#34; (.getMessage e))
      (vec (repeat (count texts) [])))))

(defn cosine-similarity
  &#34;Calcula a similaridade do cosseno entre dois vetores&#34;
  [v1 v2]
  (if (or (empty? v1) (empty? v2))
    0.0
    (let [dot-product (reduce &#43; (map * v1 v2))
          norm1 (Math/sqrt (reduce &#43; (map #(* % %) v1)))
          norm2 (Math/sqrt (reduce &#43; (map #(* % %) v2)))]
      (if (or (zero? norm1) (zero? norm2))
        0.0
        (/ dot-product (* norm1 norm2))))))

(defn similarity-search
  &#34;Encontra os N chunks mais similares&#34;
  [query-embedding doc-embeddings n]
  (if (or (empty? query-embedding) (empty? doc-embeddings))
    (take (min n (count doc-embeddings)) (range))
    (let [scores (map #(cosine-similarity query-embedding %) doc-embeddings)]
      (-&gt;&gt; (map vector scores (range))
           (sort-by first &gt;)
           (take n)
           (map second)))))</code></pre>
 <p>O c√≥digo acima implementa um sistema simples de embeddings usando TF-IDF (Term Frequency-Inverse Document Frequency) para transformar textos em vetores num√©ricos.</p>
<p>Basicamente, ele pega documentos de texto, quebra em palavras (tokens), calcula a import√¢ncia de cada palavra considerando tanto sua frequ√™ncia no documento quanto sua raridade na cole√ß√£o inteira, e cria vetores que representam cada documento. √â como transformar textos em coordenadas matem√°ticas para que o computador possa entender a &ldquo;semelhan√ßa&rdquo; entre eles.</p>


  
  <div class="mermaid">graph TD
    A[Documentos] --&gt;|Tokeniza√ß√£o| B[Tokens]
    B --&gt;|TF-IDF| C[Vetores Num√©ricos]
    C --&gt;|Similaridade do Cosseno| D[Documentos Similares]</div>
 <p>A parte mais legal √© a fun√ß√£o <code>similarity_search</code>, que usa a similaridade do cosseno para encontrar documentos parecidos com uma consulta. Imagine que cada documento √© um ponto num espa√ßo multidimensional ‚Äì quanto menor o √¢ngulo entre dois pontos, mais similares eles s√£o.</p>
<p>O c√≥digo n√£o usa nenhum modelo de IA sofisticado para isso, apenas matem√°tica b√°sica, o que o torna leve e r√°pido, embora menos poderoso que embeddings modernos baseados em redes neurais. √â como um GPS simples que te leva ao destino sem todos os recursos de um Google Maps.</p>
<p>O TF-IDF transforma textos em vetores num√©ricos ao combinar a frequ√™ncia de cada palavra em um documento (TF) com o quanto essa palavra √© rara em toda a cole√ß√£o (IDF): palavras comuns como &ldquo;linguagem&rdquo; t√™m peso baixo, enquanto termos mais exclusivos como &ldquo;Clojure&rdquo; ganham peso alto, permitindo que o computador compare documentos de forma eficiente e encontre os mais relevantes para cada consulta.</p>
<p>Outra abordagem, √© por meio da similaridade do cosseno, que compara dois vetores TF-IDF calculando o √¢ngulo entre eles: quanto menor o √¢ngulo, mais parecidos s√£o os textos, usando a f√≥rmula cos(Œ∏) = (A¬∑B) / (||A|| ||B||), onde A¬∑B √© o produto escalar e ||A|| e ||B|| s√£o os tamanhos dos vetores; por√©m, o TF-IDF tem limita√ß√µes, pois n√£o entende sin√¥nimos, contexto ou ordem das palavras, tratando termos como &ldquo;carro&rdquo; e &ldquo;autom√≥vel&rdquo; como diferentes e podendo gerar vetores grandes.</p>
<blockquote>
<p><strong>Importante</strong>: Esta implementa√ß√£o TF-IDF √© uma <strong>prova de conceito</strong> para demonstrar os fundamentos do RAG. Em aplica√ß√µes reais, embeddings densos modernos como <a href="https://www.sbert.net/">SBERT</a>, <a href="https://huggingface.co/intfloat/e5-large">E5</a>, <a href="https://huggingface.co/BAAI/bge-large-en">BGE</a> ou modelos via Ollama superam significativamente o TF-IDF em tarefas de busca sem√¢ntica e question-answering.</p></blockquote>
<h4 id="interface-com-ollama">Interface com Ollama</h4>
<p>Agora vamos implementar a interface com o Ollama. Ele √© respons√°vel por gerar a resposta para a query do usu√°rio (essa parte aqui √© super divertida, pois √© onde vamos usar o LLM).</p>


  <pre><code class="language-clojure">;; src/docai/llm.clj
(ns docai.llm
  (:require [clojure.data.json :as json]
            [org.httpkit.client :as http]))

(def ollama-url &#34;http://localhost:11434/api/generate&#34;)
(def model-name &#34;deepseek-r1&#34;) ; Modelo DeepSeek para melhor qualidade

(defn call-ollama-api
  &#34;Chama a API do Ollama para gerar uma resposta&#34;
  [prompt]
  (let [request-body {:model model-name
                      :prompt prompt
                      :stream false}
        options {:headers {&#34;Content-Type&#34; &#34;application/json&#34;}
                 :body (json/write-str request-body)}
        response @(http/post ollama-url options)]
    (if (= (:status response) 200)
      (-&gt; response
          :body
          (json/read-str :key-fn keyword)
          ;; Compat√≠vel com vers√µes antigas (:response) e novas (:message) do Ollama
          (#(or (:response %) (:message %))))
      (str &#34;Erro ao chamar a API do Ollama: &#34; (:status response) &#34; - &#34; (:body response)))))

;; Fun√ß√µes de utilidade para uso futuro:
;;
;; extract-code-blocks: Extrai blocos de c√≥digo do texto usando regex
;; exemplo de uso:
;;   (extract-code-blocks &#34;```clojure\n(&#43; 1 2)\n```&#34;) =&gt; [&#34;(&#43; 1 2)&#34;]
;;
;; extract-summary: Cria um resumo de texto com tamanho m√°ximo especificado
;; exemplo de uso:
;;   (extract-summary &#34;# T√≠tulo\nConte√∫do longo...&#34; 50) =&gt; &#34;Conte√∫do longo...&#34;

(defn format-prompt
  &#34;Formata o prompt para o LLM com delimita√ß√£o segura do contexto&#34;
  [context query]
  (str &#34;Voc√™ √© um assistente especializado em documenta√ß√£o t√©cnica. &#34;
       &#34;Use APENAS as informa√ß√µes do contexto fornecido para responder.\n\n&#34;
       &#34;DOCUMENTO:\n&#34;
       &#34;```\n&#34;
       context
       &#34;\n```\n\n&#34;
       &#34;Pergunta: &#34; query
       &#34;\n\n&#34;
       &#34;Instru√ß√µes:\n&#34;
       &#34;- Responda baseado APENAS no contexto fornecido\n&#34;
       &#34;- Se a informa√ß√£o n√£o estiver no contexto, indique claramente\n&#34;
       &#34;- Forne√ßa exemplos de c√≥digo quando relevante\n&#34;
       &#34;- Se o contexto for limitado, mencione essa limita√ß√£o\n&#34;
       &#34;- N√ÉO invente informa√ß√µes que n√£o est√£o no contexto&#34;))

(defn generate-response
  &#34;Gera resposta usando o LLM com base no contexto&#34;
  [query context]
  (try
    (let [prompt (format-prompt context query)]
      (println &#34;DEBUG - Enviando prompt para o Ollama usando o modelo&#34; model-name)
      (call-ollama-api prompt))
    (catch Exception e
      (str &#34;Erro ao gerar resposta: &#34; (.getMessage e) 
           &#34;\n\nPor favor, verifique se o Ollama est√° em execu√ß√£o no endere√ßo &#34; 
           ollama-url 
           &#34;\n\nVoc√™ pode iniciar o Ollama com o comando: ollama serve&#34;))))

;; Exemplo de prompt seguro gerado:
;; Voc√™ √© um assistente especializado em documenta√ß√£o t√©cnica. 
;; Use APENAS as informa√ß√µes do contexto fornecido para responder.
;;
;; DOCUMENTO:
;; ```
;; [contexto aqui]
;; ```
;;
;; Pergunta: [pergunta do usu√°rio]
;;
;; Instru√ß√µes:
;; - Responda baseado APENAS no contexto fornecido
;; - Se a informa√ß√£o n√£o estiver no contexto, indique claramente
;; - Forne√ßa exemplos de c√≥digo quando relevante
;; - Se o contexto for limitado, mencione essa limita√ß√£o
;; - N√ÉO invente informa√ß√µes que n√£o est√£o no contexto</code></pre>
 <p>A parte mais importante aqui √© a fun√ß√£o <code>call-ollama-api</code>, que faz uma requisi√ß√£o HTTP para o servidor Ollama rodando na m√°quina local. Ela envia um prompt de texto e recebe de volta a resposta gerada pelo modelo DeepSeek R1. O c√≥digo tamb√©m inclui uma fun√ß√£o <code>format-prompt</code> super importante, que estrutura a mensagem enviada ao modelo.</p>
<p>Ela combina o contexto (os trechos de documenta√ß√£o relevantes que encontramos) com a pergunta do usu√°rio, e adiciona instru√ß√µes espec√≠ficas para o modelo se comportar como um assistente t√©cnico. Essa &ldquo;engenharia de prompt&rdquo; √© crucial para obter respostas de qualidade - estamos essencialmente ensinando o modelo a responder no formato que queremos.</p>
<p>A fun√ß√£o <code>generate-response</code> amarra tudo isso, pegando a pergunta e o contexto, formatando o prompt, enviando para o Ollama e tratando poss√≠veis erros. Tem at√© uma mensagem amig√°vel caso o Ollama n√£o esteja rodando, sugerindo como iniciar o servi√ßo. √â um exemplo cl√°ssico de como interfaces com LLMs funcionam: voc√™ prepara um prompt bem estruturado, envia para o modelo, e recebe de volta texto gerado que (esperamos!) responda √† pergunta original com base no contexto fornecido.</p>
<h4 id="m√≥dulo-principal">M√≥dulo Principal</h4>
<p>Agora vamos implementar o m√≥dulo principal que vai ser o ponto de entrada para o RAG. Ele vai ser respons√°vel por carregar os documentos, processar os chunks, criar os embeddings e gerar a resposta para a query do usu√°rio.</p>


  <pre><code class="language-clojure">;; src/docai/core.clj
(ns docai.core
  (:require [docai.document :as doc]
            [docai.embedding :as emb]
            [docai.llm :as llm]
            [clojure.java.io :as io]
            [clojure.string :as str])
  (:gen-class))

(def docs-path &#34;resources/docs&#34;)

(defn load-documentation
  &#34;Carrega todos os arquivos de documenta√ß√£o do diret√≥rio&#34;
  []
  (-&gt;&gt; (file-seq (io/file docs-path))
       (filter #(.isFile %))
       (map #(.getPath %))))

(defn setup-knowledge-base
  &#34;Configura a base de conhecimento inicial&#34;
  []
  (let [doc-files (load-documentation)
        _ (when (empty? doc-files)
            (println &#34;Aviso: Nenhum arquivo de documenta√ß√£o encontrado em resources/docs/&#34;))
        _ (doseq [file doc-files]
            (println &#34;Arquivo encontrado:&#34; file))
        all-chunks (mapcat doc/extract-text doc-files)
        processed-chunks (doc/preprocess-chunks all-chunks)
        _ (println (str &#34;Processando &#34; (count processed-chunks) &#34; chunks de texto...&#34;))
        _ (when (&lt; (count processed-chunks) 5)
            (println &#34;DEBUG - Primeiros chunks:&#34;)
            (doseq [chunk (take 5 processed-chunks)]
              (println (str &#34;Chunk: &#39;&#34; (subs chunk 0 (min 50 (count chunk))) &#34;...&#39;&#34;))))
        doc-freq (emb/doc-freq processed-chunks)
        doc-count (count (filter string? processed-chunks))
        ;; Vocabul√°rio ordenado para garantir ordem est√°vel entre execu√ß√µes
        vocab (sort (keys doc-freq))
        embeddings (map #(emb/vectorize % doc-freq doc-count vocab) processed-chunks)]
          {:chunks processed-chunks
       :embeddings embeddings
       :doc-freq doc-freq
       :doc-count doc-count
            :vocab vocab  ; Persistir vocabul√°rio ordenado
     :original-files doc-files}))

;; Fun√ß√£o para for√ßar rec√°lculo (√∫til para desenvolvimento)
(defn force-recalculate-kb []
  (let [kb-file &#34;resources/knowledge-base.json&#34;]
    (when (.exists (io/file kb-file))
      (.delete (io/file kb-file)))
  (setup-knowledge-base))

(defn get-file-content
  &#34;L√™ o conte√∫do completo de um arquivo&#34;
  [file-path]
  (try
    (slurp file-path)
    (catch Exception _
      (println &#34;Erro ao ler arquivo:&#34; file-path)
      &#34;&#34;)))

(defn get-limited-fallback-content
  &#34;Obt√©m conte√∫do limitado para fallback (evita estourar contexto)&#34;
  [file-path]
  (try
    (let [content (slurp file-path)
          max-chars 8000  ; Limite de ~8KB para evitar estourar contexto
          limited-content (if (&gt; (count content) max-chars)
                           (str (subs content 0 max-chars) 
                                &#34;\n\n[Conte√∫do truncado - arquivo muito grande]&#34;)
                           content)]
      (str &#34;Informa√ß√µes limitadas da documenta√ß√£o:\n\n&#34; limited-content))
    (catch Exception _
      (println &#34;Erro ao ler arquivo para fallback:&#34; file-path)
      &#34;N√£o foi poss√≠vel acessar a documenta√ß√£o.&#34;)))

(defn query-rag
  &#34;Processa uma query usando o pipeline RAG&#34;
  [knowledge-base query]
  (cond
    (str/blank? query)
    &#34;Por favor, digite uma pergunta v√°lida.&#34;
    
    (and (seq (:chunks knowledge-base)) 
         (seq (:embeddings knowledge-base)))
    (let [query-emb (emb/vectorize query (:doc-freq knowledge-base) (:doc-count knowledge-base) (:vocab knowledge-base))
          similar-idxs (emb/similarity-search query-emb 
                                            (:embeddings knowledge-base)
                                            3)
          _ (println &#34;DEBUG - √çndices similares:&#34; similar-idxs)
          
          ;; Obter contexto relevante
          context-chunks (-&gt;&gt; similar-idxs
                              (map #(nth (:chunks knowledge-base) %))
                              (str/join &#34;\n\n&#34;))
          
          ;; Se n√£o houver chunks relevantes, use fallback inteligente
          context (if (str/blank? context-chunks)
                    (if (seq (:original-files knowledge-base))
                      (get-limited-fallback-content (first (:original-files knowledge-base)))
                      &#34;N√£o foi poss√≠vel encontrar informa√ß√µes relevantes.&#34;)
                    context-chunks)]
      
      (println &#34;DEBUG - Tamanho do contexto:&#34; (count context) &#34;caracteres&#34;)
      (println &#34;DEBUG - Amostra do contexto:&#34; (subs context 0 (min 200 (count context))) &#34;...&#34;)
      
      ;; Gerar resposta usando o LLM
      (llm/generate-response query context))
    
    :else
    &#34;N√£o foi poss√≠vel encontrar informa√ß√µes relevantes na base de conhecimento.&#34;))

(defn -main
  &#34;Fun√ß√£o principal que inicializa a aplica√ß√£o DocAI&#34;
  [&amp; _]
  (println &#34;Inicializando DocAI...&#34;)
  
  ;; Verificar se o Ollama est√° acess√≠vel
  (println &#34;Para usar o Ollama, certifique-se de que ele est√° em execu√ß√£o com o comando: ollama serve&#34;)
  (println &#34;Usando o modelo deepseek-r1. Se voc√™ ainda n√£o o baixou, execute: ollama pull deepseek-r1&#34;)
  
  (let [kb (setup-knowledge-base)]
    (println &#34;Base de conhecimento pronta! Fa√ßa sua pergunta:&#34;)
    (try
      (loop []
        (when-let [input (read-line)]
          (cond
            (= input &#34;sair&#34;) 
            (println &#34;Obrigado por usar o DocAI. At√© a pr√≥xima!&#34;)
            
            (str/blank? input)
            (do
              (println &#34;Digite uma pergunta ou &#39;sair&#39; para terminar.&#34;)
              (recur))
            
            :else
            (do
              (println &#34;Processando...&#34;)
              (println (query-rag kb input))
              (println &#34;\nPr√≥xima pergunta (ou &#39;sair&#39; para terminar):&#34;)
              (recur)))))
      (catch Exception e
        (println &#34;Erro: &#34; (.getMessage e))
        (println &#34;Detalhes: &#34; (ex-data e))))
    (println &#34;Obrigado por usar o DocAI. At√© a pr√≥xima!&#34;)))</code></pre>
 <p>Basicamente, quando voc√™ faz uma pergunta, o sistema primeiro transforma sua pergunta em n√∫meros (embeddings) e depois procura nos documentos quais partes s√£o mais parecidas com o que voc√™ perguntou.</p>
<p>√â como se ele estivesse destacando os trechos mais relevantes de um livro para responder sua d√∫vida. Voc√™ pode ver isso acontecendo quando ele imprime os &ldquo;√≠ndices similares&rdquo; no console - s√£o as posi√ß√µes dos peda√ßos de texto que ele achou mais √∫teis.</p>
<p>Depois de encontrar os trechos relevantes, o sistema junta tudo em um &ldquo;contexto&rdquo; - que √© basicamente um resumo das informa√ß√µes importantes. Se ele n√£o achar nada parecido com sua pergunta, ele tenta usar o documento inteiro ou avisa que n√£o tem informa√ß√£o suficiente.</p>
<p>D√° para ver que ele √© bem transparente, mostrando no console o tamanho do contexto e at√© uma amostra do que encontrou, para voc√™ entender o que est√° acontecendo nos bastidores.</p>
<p>Por fim, ele passa sua pergunta original junto com o contexto encontrado para o modelo de linguagem (LLM) gerar uma resposta personalizada. √â como dar a um especialista tanto a sua pergunta quanto as p√°ginas relevantes de um manual t√©cnico - assim ele pode dar uma resposta muito mais precisa e fundamentada.</p>
<p>Todo esse processo acontece em segundos, permitindo que voc√™ tenha uma conversa fluida com seus documentos, como se estivesse conversando com algu√©m que leu tudo e est√° pronto para responder suas d√∫vidas espec√≠ficas.</p>
<hr>
<h2 id="como-usar">Como Usar</h2>
<p>Abaixo um guia para voc√™ instalar e usar o DocAI (e ver o processo em a√ß√£o).</p>
<h3 id="instala√ß√£o-do-ollama">Instala√ß√£o do Ollama</h3>
<ol>
<li>
<p><strong>Instala√ß√£o</strong>:</p>
<ul>
<li><strong>Windows</strong>: Baixe o instalador do <a href="https://ollama.com/download">site oficial do Ollama</a> e execute-o</li>
<li><strong>Linux</strong>: Execute o comando:


  <pre><code class="language-bash">curl https://ollama.ai/install.sh | sh</code></pre>
 </li>
<li><strong>macOS</strong>: Use o Homebrew:


  <pre><code class="language-bash">brew install ollama</code></pre>
 </li>
</ul>
</li>
<li>
<p><strong>Iniciando o Servidor</strong>:</p>


  <pre><code class="language-bash">ollama serve</code></pre>
 </li>
<li>
<p><strong>Baixando o Modelo</strong>:</p>


  <pre><code class="language-bash">ollama pull deepseek-r1</code></pre>
 </li>
<li>
<p><strong>Verificando a Instala√ß√£o</strong>:</p>
<ul>
<li>Execute um teste simples:


  <pre><code class="language-bash">ollama run deepseek-r1 &#34;Ol√°! Como voc√™ est√°?&#34;</code></pre>
 </li>
<li>Se tudo estiver funcionando, voc√™ receber√° uma resposta do modelo</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>Dica</strong>: O Ollama mant√©m os modelos em cache local. Se voc√™ precisar liberar espa√ßo, pode usar <code>ollama rm deepseek-r1</code> para remover o modelo.</p></blockquote>
<h3 id="executando-a-aplica√ß√£o">Executando a Aplica√ß√£o</h3>
<ol>
<li>Coloque seus documentos na pasta <code>resources/docs/</code> (j√° inclu√≠mos dois exemplos: <code>example.md</code>)</li>
<li>Execute o projeto:</li>
</ol>


  <pre><code class="language-bash">lein run</code></pre>
 <ol start="3">
<li>Fa√ßa suas perguntas! Exemplo:</li>
</ol>


  <pre><code class="language-bash">Como implementar autentica√ß√£o JWT em Clojure?
Como implementar auth saml em python?
Como integrar o auth0 em uma aplica√ß√£o Clojure?
etc...</code></pre>
 <p>O DocAI processa sua pergunta em v√°rias etapas:</p>


  
  <div class="mermaid">flowchart TD
    A[In√≠cio] --&gt; B[Carrega Documenta√ß√£o]
    B --&gt; C[Processa Documentos]
    C --&gt; D[Gera Embeddings]
    D --&gt; E[Base de Conhecimento]
    
    F[Consulta do Usu√°rio] --&gt; G[Processa Consulta]
    G --&gt; H[Gera Embedding da Consulta]
    H --&gt; I[Busca Similaridade]
    I --&gt; J[Seleciona Chunks Relevantes]
    J --&gt; K[Combina Contexto]
    K --&gt; L[Gera Prompt]
    L --&gt; M[LLM DeepSeek R1]
    M --&gt; N[Resposta Final]
    
    E --&gt; I
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style N fill:#9ff,stroke:#333,stroke-width:2px</div>
 <ol>
<li><strong>Processamento da Consulta</strong>: A pergunta √© convertida em um vetor TF-IDF</li>
<li><strong>Busca por Similaridade</strong>: O sistema encontra os chunks mais relevantes</li>
<li><strong>Gera√ß√£o de Contexto</strong>: Os chunks s√£o combinados em um contexto coeso</li>
<li><strong>Gera√ß√£o de Resposta</strong>: O LLM gera uma resposta baseada no contexto</li>
</ol>
<p>Voc√™ pode ver o processo em a√ß√£o nos logs:</p>


  <pre><code class="language-bash">DEBUG - Processando query: Como implementar autentica√ß√£o JWT em Clojure?
DEBUG - √çndices similares: [2, 5, 8]
DEBUG - Tamanho do contexto: 1234 caracteres
DEBUG - Amostra do contexto: &#34;Para implementar autentica√ß√£o JWT em Clojure...&#34;</code></pre>
 <blockquote>
<p><strong>NOTA:</strong> A prop√≥sito, o projeto docai est√° dispon√≠vel no <a href="https://github.com/scovl/docai">https://github.com/scovl/docai</a> caso voc√™ queira contribuir com o projeto ou usar em outro projeto.</p></blockquote>
<hr>
<h2 id="considera√ß√µes-t√©cnicas">Considera√ß√µes T√©cnicas</h2>
<h3 id="performance-e-otimiza√ß√µes">Performance e Otimiza√ß√µes</h3>
<p>Nossa implementa√ß√£o atual oferece uma base funcional, mas pode ser significativamente otimizada em termos de performance atrav√©s da ado√ß√£o de bancos de dados vetoriais como <a href="https://milvus.io/">Milvus</a> ou <a href="https://github.com/facebookresearch/faiss">FAISS</a>, implementa√ß√£o de cache de embeddings e paraleliza√ß√£o do processamento de chunks, permitindo consultas mais r√°pidas mesmo com grandes volumes de dados.</p>
<p>Para lidar com documenta√ß√µes extensas, recomendo estrat√©gias espec√≠ficas de gerenciamento de mem√≥ria, como o processamento de chunks em lotes menores, implementa√ß√£o de indexa√ß√£o incremental que constr√≥i a base de conhecimento gradualmente, e utiliza√ß√£o de t√©cnicas de streaming para processar arquivos grandes sem sobrecarregar a mem√≥ria dispon√≠vel.</p>
<p>Quanto √† escolha de modelos no ecossistema Ollama, cada um apresenta caracter√≠sticas distintas que podem ser exploradas conforme a necessidade: o <a href="https://ollama.com/models/deepseek-r1">DeepSeek R1</a> destaca-se na compreens√£o geral e gera√ß√£o de texto, o <a href="https://ollama.com/models/deepseek-coder">DeepSeek Coder</a> √© especializado em c√≥digo, o <a href="https://ollama.com/models/llama3">Llama 3</a> serve como excelente alternativa geral, o <a href="https://ollama.com/models/mistral">Mistral</a> demonstra efic√°cia em tarefas espec√≠ficas, enquanto o <a href="https://ollama.com/models/gemma">Gemma</a> oferece uma solu√ß√£o leve e eficiente para ambientes com recursos limitados.</p>
<p>Outra quest√£o importante √© como estou tratando os erros. O sistema implementa v√°rias camadas de tratamento de erros para lidar com diferentes cen√°rios:</p>
<ol>
<li>
<p><strong>Ollama Offline</strong></p>
<ul>
<li><strong>Sintoma</strong>: O sistema n√£o consegue se conectar ao servidor Ollama</li>
<li><strong>Tratamento</strong>: O c√≥digo verifica a disponibilidade do servidor e fornece mensagens claras de erro:</li>
</ul>


  <pre><code class="language-clojure">(catch Exception e
  (str &#34;Erro ao gerar resposta: &#34; (.getMessage e) 
       &#34;\n\nPor favor, verifique se o Ollama est√° em execu√ß√£o no endere√ßo &#34; 
       ollama-url 
       &#34;\n\nVoc√™ pode iniciar o Ollama com o comando: ollama serve&#34;))</code></pre>
 </li>
<li>
<p><strong>Documenta√ß√£o Muito Grande</strong></p>
<ul>
<li><strong>Sintoma</strong>: Arquivos de documenta√ß√£o que excedem a mem√≥ria dispon√≠vel</li>
<li><strong>Tratamento</strong>: O sistema implementa:
<ul>
<li>Chunking de documentos (512 tokens por chunk)</li>
<li>Processamento em lotes</li>
<li>Logs de progresso para monitoramento</li>
</ul>
</li>
</ul>


  <pre><code class="language-clojure">(let [content (slurp doc-path)
      chunks (partition-all 512 text)]
  (println &#34;Quantidade de chunks gerados:&#34; (count chunks)))</code></pre>
 </li>
<li>
<p><strong>Consultas sem Rela√ß√£o com a Documenta√ß√£o</strong></p>
<ul>
<li><strong>Sintoma</strong>: Nenhum chunk relevante √© encontrado para a consulta</li>
<li><strong>Tratamento</strong>: O sistema:
<ul>
<li>Verifica se h√° chunks dispon√≠veis</li>
<li>Usa fallback para conte√∫do original se necess√°rio</li>
<li>Fornece feedback claro ao usu√°rio</li>
</ul>
</li>
</ul>


  <pre><code class="language-clojure">(if (str/blank? context-chunks)
  (if (seq (:original-files knowledge-base))
    (get-file-content (first (:original-files knowledge-base)))
    &#34;N√£o foi poss√≠vel encontrar informa√ß√µes relevantes.&#34;)
  context-chunks)</code></pre>
 </li>
<li>
<p><strong>Melhorias Futuras</strong> - Implementar <a href="https://en.wikipedia.org/wiki/Exponential_backoff">retry com backoff exponencial</a> para falhas de conex√£o, adicionar <a href="https://en.wikipedia.org/wiki/Cache_%28computing%29">cache de embeddings</a> para melhor performance, implementar <a href="https://en.wikipedia.org/wiki/Streaming_media">streaming</a> para arquivos muito grandes, adicionar <a href="https://en.wikipedia.org/wiki/Document_validation">valida√ß√£o de formato de documentos</a> e implementar <a href="https://en.wikipedia.org/wiki/Rate_limiting">rate limiting</a> para evitar sobrecarga do Ollama.</p>
</li>
</ol>
<hr>
<h3 id="melhorando-os-prompts">Melhorando os Prompts</h3>
<p>Para obter melhores respostas do sistema RAG, voc√™ pode usar prompts mais estruturados:</p>


  <pre><code class="language-clojure">(defn format-advanced-prompt
  &#34;Prompt otimizado com diretrizes claras&#34;
  [context query]
  (str &#34;Voc√™ √© um especialista em documenta√ß√£o t√©cnica de software.\n\n&#34;
       &#34;DOCUMENTO:\n```\n&#34; context &#34;\n```\n\n&#34;
       &#34;Pergunta: &#34; query &#34;\n\n&#34;
       &#34;Diretrizes:\n&#34;
       &#34;1. Use APENAS informa√ß√µes do contexto fornecido\n&#34;
       &#34;2. Seja preciso e t√©cnico\n&#34;
       &#34;3. Inclua exemplos de c√≥digo quando relevante\n&#34;
       &#34;4. Se a informa√ß√£o n√£o estiver no contexto, indique claramente\n&#34;
       &#34;5. Use formata√ß√£o Markdown para melhor legibilidade&#34;))</code></pre>
 <blockquote>
<p>Para t√©cnicas avan√ßadas de prompt engineering, consulte o <a href="https://www.promptingguide.ai/">Guia Completo de Prompt Engineering</a>.</p></blockquote>
<h2 id="pr√≥ximos-passos">Pr√≥ximos Passos</h2>
<p>Abaixo uma lista de melhorias que podem ser feitas no projeto atual.</p>
<h3 id="melhorias-r√°pidas-implementa√ß√£o-imediata">Melhorias R√°pidas (Implementa√ß√£o Imediata)</h3>
<h4 id="1-persist√™ncia-da-base-de-conhecimento"><strong>1. Persist√™ncia da Base de Conhecimento</strong></h4>


  <pre><code class="language-clojure">;; src/docai/persistence.clj
(ns docai.persistence
  (:require [clojure.data.json :as json]
            [clojure.edn :as edn]))

(defn calculate-checksum
  &#34;Calcula checksum dos arquivos de documenta√ß√£o&#34;
  [doc-files]
  (let [checksums (map #(hash (slurp %)) doc-files)]
    (hash checksums)))

(defn save-knowledge-base
  &#34;Salva a base de conhecimento em disco com checksum&#34;
  [kb filename]
  (let [doc-files (:original-files kb)
        checksum (calculate-checksum doc-files)
        serializable-kb (-&gt; kb
                           (select-keys [:chunks :embeddings :doc-freq :doc-count :vocab])
                           (assoc :checksum checksum :doc-files doc-files))]
    (spit filename (json/write-str serializable-kb))))

(defn load-knowledge-base
  &#34;Carrega a base de conhecimento do disco com verifica√ß√£o de mudan√ßas&#34;
  [filename doc-files]
  (try
    (let [content (slurp filename)
          data (json/read-str content :key-fn keyword)
          cached-checksum (:checksum data)
          current-checksum (calculate-checksum doc-files)]
      (if (= cached-checksum current-checksum)
        (do
          (println &#34;Cache v√°lido - carregando embeddings...&#34;)
          (assoc data :original-files doc-files))
        (do
          (println &#34;Arquivos modificados - recalculando embeddings...&#34;)
          nil)))
    (catch Exception e
      (println &#34;Erro ao carregar KB:&#34; (.getMessage e))
      nil)))

;; Uso no core.clj
(defn setup-knowledge-base
  &#34;Configura a base de conhecimento (com cache inteligente)&#34;
  []
  (let [kb-file &#34;resources/knowledge-base.json&#34;
        doc-files (load-documentation)]
    (if (.exists (io/file kb-file))
      (if-let [cached-kb (load-knowledge-base kb-file doc-files)]
        cached-kb
        (do
          (println &#34;Recriando KB devido a mudan√ßas nos arquivos...&#34;)
          (let [kb (create-knowledge-base)]
            (save-knowledge-base kb kb-file)
            kb)))
      (do
        (println &#34;Criando nova KB...&#34;)
        (let [kb (create-knowledge-base)]
          (save-knowledge-base kb kb-file)
          kb)))))</code></pre>
 <h4 id="2-testes-unit√°rios"><strong>2. Testes Unit√°rios</strong></h4>


  <pre><code class="language-clojure">;; test/docai/embedding_test.clj
(ns docai.embedding-test
  (:require [clojure.test :refer :all]
            [docai.embedding :as emb]))

(deftest test-tokenize
  (testing &#34;Tokeniza√ß√£o b√°sica&#34;
    (is (= [&#34;hello&#34; &#34;world&#34;] (emb/tokenize &#34;Hello World!&#34;)))
    (testing &#34;Filtra palavras curtas&#34;
      (is (= [] (emb/tokenize &#34;a b c&#34;)))))

(deftest test-tf-idf
  (testing &#34;C√°lculo TF-IDF&#34;
    (let [doc &#34;hello world hello&#34;
          doc-freq {&#34;hello&#34; 2 &#34;world&#34; 1}
          doc-count 2
          result (emb/tf-idf doc doc-freq doc-count)]
      (is (contains? result &#34;hello&#34;))
      (is (contains? result &#34;world&#34;)))))

(deftest test-cosine-similarity
  (testing &#34;Similaridade do cosseno&#34;
    (is (= 1.0 (emb/cosine-similarity [1 0] [1 0])))
    (is (= 0.0 (emb/cosine-similarity [1 0] [0 1])))
    (is (= 0.707 (emb/cosine-similarity [1 1] [1 0]) :delta 0.001))))</code></pre>
 <h4 id="3-streaming-de-respostas"><strong>3. Streaming de Respostas</strong></h4>


  <pre><code class="language-clojure">;; src/docai/streaming.clj
(ns docai.streaming
  (:require [clojure.data.json :as json]
            [org.httpkit.client :as http]))

(defn stream-ollama-response
  &#34;Streaming de resposta do Ollama&#34;
  [prompt]
  (let [url &#34;http://localhost:11434/api/generate&#34;
        request-body {:model &#34;deepseek-r1&#34;
                     :prompt prompt
                     :stream true}]
    (with-open [conn @(http/post url {:body (json/write-str request-body)
                                      :as :stream})]
      (doseq [line (line-seq (:body conn))]
        (when-not (str/blank? line)
          (let [data (json/read-str line :key-fn keyword)]
            ;; Compat√≠vel com vers√µes antigas (:response) e novas (:message) do Ollama
            (when-let [content (or (:response data) (:message data))]
              (print content)
              (flush))))))))</code></pre>
 <h4 id="4-cache-de-embeddings"><strong>4. Cache de Embeddings</strong></h4>


  <pre><code class="language-clojure">;; src/docai/cache.clj
(ns docai.cache
  (:require [clojure.core.cache :as cache]))

;; Cache LRU com limite de mem√≥ria (evita vazamentos)
(def embedding-cache (atom (cache/lru-cache-factory {} :threshold 1000))) ; M√°ximo 1000 embeddings

(defn cached-embedding
  &#34;Embedding com cache LRU&#34;
  [text doc-freq doc-count vocab]
  (if-let [cached (cache/lookup @embedding-cache text)]
    cached
    (let [embedding (emb/vectorize text doc-freq doc-count vocab)]
      (swap! embedding-cache cache/miss text embedding)
      embedding)))

;; Cache para respostas do LLM (tamb√©m LRU)
(def response-cache (atom (cache/lru-cache-factory {} :threshold 500))) ; M√°ximo 500 respostas

(defn cached-llm-response
  &#34;Resposta do LLM com cache LRU&#34;
  [prompt]
  (if-let [cached (cache/lookup @response-cache prompt)]
    cached
    (let [response (llm/call-ollama-api prompt)]
      (swap! response-cache cache/miss prompt response)
      response)))

;; Fun√ß√£o para limpar cache manualmente se necess√°rio
(defn clear-caches []
  (reset! embedding-cache (cache/lru-cache-factory {} :threshold 1000))
  (reset! response-cache (cache/lru-cache-factory {} :threshold 500))
  (println &#34;Caches limpos!&#34;))

;; Monitoramento de cache
(defn cache-stats []
  (let [embedding-size (count @embedding-cache)
        response-size (count @response-cache)]
    (println (str &#34;Embedding cache: &#34; embedding-size &#34;/1000&#34;))
    (println (str &#34;Response cache: &#34; response-size &#34;/500&#34;))))</code></pre>
 <p><strong>Vantagens do Cache LRU:</strong></p>
<ul>
<li><strong>üîÑ Auto-limpeza</strong>: Remove itens menos usados automaticamente</li>
<li><strong>üíæ Controle de mem√≥ria</strong>: Limite m√°ximo de itens</li>
<li><strong>‚ö° Performance</strong>: Acesso r√°pido a dados frequentes</li>
<li><strong>üõ°Ô∏è Estabilidade</strong>: Evita vazamentos de mem√≥ria</li>
</ul>
<p><strong>Cache Inteligente de Embeddings:</strong></p>
<ul>
<li><strong>üìÅ Persist√™ncia</strong>: Embeddings salvos em disco</li>
<li><strong>üîç Verifica√ß√£o de Mudan√ßas</strong>: Checksum dos arquivos</li>
<li><strong>‚ö° Recarregamento R√°pido</strong>: S√≥ recalcula se necess√°rio</li>
<li><strong>üîÑ Invalida√ß√£o Autom√°tica</strong>: Detecta modifica√ß√µes nos arquivos</li>
</ul>
<h4 id="5-banco-vetorial-simples-bm25-manual"><strong>5. Banco Vetorial Simples (BM25 Manual)</strong></h4>


  <pre><code class="language-clojure">;; src/docai/vector_store.clj
(ns docai.vector-store
  (:require [clojure.string :as str]))

(defn create-simple-vector-store
  &#34;Store vetorial simples com BM25 (implementa√ß√£o manual)&#34;
  [documents]
  (let [index (atom {})
        doc-freq (emb/doc-freq documents)
        vocab (sort (keys doc-freq))]  ; Vocabul√°rio ordenado
    (doseq [[idx doc] (map-indexed vector documents)]
      (let [tokens (emb/tokenize doc)
            tf (emb/term-freq tokens)]
        (swap! index assoc idx {:doc doc :tf tf})))
    {:index index :doc-freq doc-freq :vocab vocab}))

(defn calculate-bm25
  &#34;Calcula score BM25 para um documento&#34;
  [query-tokens doc-tf doc-freq]
  (let [k1 1.2  ; Par√¢metro de satura√ß√£o de termo
        b 0.75   ; Par√¢metro de normaliza√ß√£o de comprimento
        avg-doc-len 100  ; Comprimento m√©dio do documento (aproxima√ß√£o)
        doc-len (reduce &#43; (vals doc-tf))
        
        ;; IDF para cada termo da query
        idf-scores (map (fn [term]
                          (let [df (get doc-freq term 0)
                                n (count doc-freq)]
                            (if (zero? df)
                              0
                              (Math/log (/ (- n df 0.5) (&#43; df 0.5)))))
                        query-tokens)
        
        ;; TF para cada termo da query no documento
        tf-scores (map (fn [term]
                         (let [tf (get doc-tf term 0)]
                           (/ (* tf (&#43; k1 1))
                              (&#43; tf (* k1 (- 1 b (* b (/ doc-len avg-doc-len)))))))
                       query-tokens)]
    
    ;; Soma ponderada de IDF * TF
    (reduce &#43; (map * idf-scores tf-scores))))

(defn search-bm25
  &#34;Busca h√≠brida: BM25 &#43; similaridade sem√¢ntica&#34;
  [query vector-store top-k]
  (let [query-tokens (emb/tokenize query)
        query-embedding (emb/vectorize query (:doc-freq vector-store) (:doc-count vector-store) (:vocab vector-store))
        
        ;; BM25 scores
        bm25-scores (map-indexed 
                      (fn [idx {:keys [tf]}]
                        [idx (calculate-bm25 query-tokens tf (:doc-freq vector-store))])
                      (vals @(:index vector-store)))
        
        ;; Semantic scores
        semantic-scores (map-indexed
                          (fn [idx _]
                            [idx (emb/cosine-similarity query-embedding 
                                                       (emb/vectorize (get-in @(:index vector-store) [idx :doc])
                                                                      (:doc-freq vector-store)
                                                                      (:doc-count vector-store)
                                                                      (:vocab vector-store)))])
                          (vals @(:index vector-store)))
        
        ;; Combine scores (weighted average)
        combined-scores (map (fn [[idx bm25] [idx2 semantic]]
                              [idx (&#43; (* 0.3 bm25) (* 0.7 semantic))])
                            bm25-scores semantic-scores)]
    
    (-&gt;&gt; combined-scores
         (sort-by second &gt;)
         (take top-k)
         (map first))))</code></pre>
 <p><strong>Sobre o Algoritmo BM25:</strong></p>
<ul>
<li><strong>k1 = 1.2</strong>: Controla satura√ß√£o de frequ√™ncia de termos</li>
<li><strong>b = 0.75</strong>: Normaliza pelo comprimento do documento</li>
<li><strong>IDF</strong>: Mede raridade dos termos na cole√ß√£o</li>
<li><strong>TF</strong>: Frequ√™ncia dos termos no documento</li>
<li><strong>Combina√ß√£o</strong>: 30% BM25 + 70% similaridade sem√¢ntica</li>
</ul>
<p><strong>Nota</strong>: Esta √© uma implementa√ß√£o manual do BM25. Para produ√ß√£o, considere usar Apache Lucene (veja depend√™ncias acima) que oferece BM25 nativo e otimizado.</p>
<h3 id="melhorias-avan√ßadas">Melhorias Avan√ßadas</h3>


  
  <div class="mermaid">mindmap
  root((Melhorias))
    Tokeniza√ß√£o
      BPE
      WordPiece
      Tokenizador do Modelo
    Embeddings
      Pr√©-treinados
      Via Ollama
      Cache
    Banco de Dados
      Milvus
      FAISS
      Qdrant
    Cache
      Embeddings
      Respostas
    Erros
      Conex√£o
      Modelo
      Rede
    Logging
      Framework
      Rastreamento
    Testes
      Unit√°rios
      Integra√ß√£o
    Prompt
      Few-shot
      Chain-of-thought
      Formato</div>
 <h3 id="depend√™ncias-e-pr√≥ximos-passos">Depend√™ncias e Pr√≥ximos Passos</h3>
<h4 id="depend√™ncias-recomendadas"><strong>Depend√™ncias Recomendadas</strong></h4>
<p>Para implementar as funcionalidades avan√ßadas mencionadas no artigo, adicione estas depend√™ncias ao <code>project.clj</code>:</p>


  <pre><code class="language-clojure">;; Depend√™ncias para produ√ß√£o
[com.github.justone/clojure-tiktoken &#34;0.1.0&#34;]  ; Contagem precisa de tokens
[org.apache.lucene/lucene-core &#34;9.10.0&#34;]       ; Busca textual avan√ßada
[org.apache.lucene/lucene-analyzers-common &#34;9.10.0&#34;]  ; Analisadores de texto
[org.apache.lucene/lucene-queryparser &#34;9.10.0&#34;] ; Parser de queries
[com.github.clojure-lsp/clojure-lsp &#34;2024.01.15-20.32.45&#34;]  ; LSP para IDE</code></pre>
 <h4 id="implementa√ß√£o-com-lucene"><strong>Implementa√ß√£o com Lucene</strong></h4>


  <pre><code class="language-clojure">;; src/docai/lucene_store.clj
(ns docai.lucene-store
  (:import [org.apache.lucene.analysis.standard StandardAnalyzer]
           [org.apache.lucene.document Document Field Field$Store]
           [org.apache.lucene.index IndexWriter IndexWriterConfig DirectoryReader]
           [org.apache.lucene.search IndexSearcher QueryParser]
           [org.apache.lucene.store RAMDirectory]))

(defn create-lucene-index
  &#34;Cria √≠ndice Lucene para busca textual&#34;
  [documents]
  (let [analyzer (StandardAnalyzer.)
        directory (RAMDirectory.)
        config (IndexWriterConfig. analyzer)
        writer (IndexWriter. directory config)]
    
    ;; Adiciona documentos ao √≠ndice
    (doseq [[idx doc] (map-indexed vector documents)]
      (let [document (Document.)]
        (.add document (Field. &#34;content&#34; doc Field$Store/YES))
        (.add document (Field. &#34;id&#34; (str idx) Field$Store/YES))
        (.addDocument writer document)))
    
    (.close writer)
    
    {:directory directory
     :analyzer analyzer
     :reader (DirectoryReader/open directory)
     :searcher (IndexSearcher. (DirectoryReader/open directory))}))

(defn search-lucene
  &#34;Busca textual usando Lucene&#34;
  [index query top-k]
  (let [parser (QueryParser. &#34;content&#34; (:analyzer index))
        query-obj (.parse parser query)
        hits (.search (:searcher index) query-obj top-k)]
    (map #(.doc (:searcher index) %) (.scoreDocs hits))))</code></pre>
 <h3 id="upgrade-para-embeddings-densos">Upgrade para Embeddings Densos</h3>
<p>Para evoluir de TF-IDF para embeddings densos modernos, considere estas op√ß√µes:</p>
<h4 id="1-via-ollama-embeddings-api">1. <strong>Via Ollama Embeddings API</strong></h4>


  <pre><code class="language-clojure">;; Exemplo de upgrade usando Ollama embeddings
(defn create-dense-embeddings [texts]
  (let [embeddings-url &#34;http://localhost:11434/api/embeddings&#34;]
    (map #(call-ollama-embeddings embeddings-url %) texts)))

(defn call-ollama-embeddings [url text]
  (let [request-body {:model &#34;deepseek-r1&#34; :prompt text}
        response @(http/post url {:body (json/write-str request-body)})]
    (if (= (:status response) 200)
      (-&gt; response :body (json/read-str :key-fn keyword) :embedding)
      (throw (ex-info &#34;Erro ao gerar embedding&#34; {:status (:status response)})))))

;; Token counting preciso via Ollama API
(defn count-tokens-ollama [text]
  (let [url &#34;http://localhost:11434/api/generate&#34;
        request-body {:model &#34;deepseek-r1&#34; 
                     :prompt text 
                     :stream false
                     :options {:num_predict 0}}]
    (try
      (let [response @(http/post url {:body (json/write-str request-body)})]
        (if (= (:status response) 200)
          (-&gt; response :body (json/read-str :key-fn keyword) :eval_count)
          0))
      (catch Exception _ 0))))

;; Implementa√ß√£o com clojure-tiktoken (recomendado para produ√ß√£o)
(defn count-tokens-precise [text]
  (try
    ;; Requer: [com.github.justone/clojure-tiktoken &#34;0.1.0&#34;]
    ;; (require &#39;[com.github.justone.clojure-tiktoken :as tiktoken])
    ;; (tiktoken/count-tokens text &#34;cl100k_base&#34;)
    (count-tokens text) ; Fallback para implementa√ß√£o atual
    (catch Exception e
      (println &#34;Erro ao usar tiktoken:&#34; (.getMessage e))
      (count-tokens text))))

;; Exemplo de implementa√ß√£o com API do Ollama (mais preciso)
(defn count-tokens-ollama-precise [text]
  (let [url &#34;http://localhost:11434/api/generate&#34;
        request-body {:model &#34;deepseek-r1&#34; 
                     :prompt text 
                     :stream false
                     :options {:num_predict 0}}]
    (try
      (let [response @(http/post url {:body (json/write-str request-body)})]
        (if (= (:status response) 200)
          (-&gt; response :body (json/read-str :key-fn keyword) :eval_count)
          (count-tokens text))) ; Fallback para heur√≠stica
      (catch Exception _
        (count-tokens text))))) ; Fallback para heur√≠stica</code></pre>
 <h4 id="2-via-huggingface-transformers">2. <strong>Via HuggingFace Transformers</strong></h4>


  <pre><code class="language-clojure">;; Exemplo usando interop com Python/HuggingFace
(defn create-hf-embeddings [texts]
  (let [model-name &#34;sentence-transformers/all-MiniLM-L6-v2&#34;]
    ;; Usar interop com Python para carregar modelo
    ;; e gerar embeddings densos
    ))

;; Token counting preciso com tiktoken
(defn count-tokens-tiktoken [text]
  ;; Requer interop com Python tiktoken
  ;; pip install tiktoken
  ;; python -c &#34;import tiktoken; print(len(tiktoken.get_encoding(&#39;cl100k_base&#39;).encode(&#39;texto aqui&#39;)))&#34;
  )</code></pre>
 <h4 id="3-compara√ß√£o-de-performance">3. <strong>Compara√ß√£o de Performance</strong></h4>
<table>
  <thead>
      <tr>
          <th>M√©todo</th>
          <th>Sem√¢ntica</th>
          <th>Contexto</th>
          <th>Performance</th>
          <th>Complexidade</th>
          <th>Hardware M√≠nimo</th>
          <th>Precis√£o Tokens</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>TF-IDF</td>
          <td>‚ùå</td>
          <td>‚ùå</td>
          <td>‚ö°‚ö°‚ö°</td>
          <td>‚ö°</td>
          <td>CPU 4 cores, 8GB RAM</td>
          <td>‚ö†Ô∏è Heur√≠stica</td>
      </tr>
      <tr>
          <td>Ollama Embeddings</td>
          <td>‚úÖ</td>
          <td>‚úÖ</td>
          <td>‚ö°‚ö°</td>
          <td>‚ö°‚ö°</td>
          <td>CPU 8 cores, 16GB RAM</td>
          <td>‚úÖ Preciso</td>
      </tr>
      <tr>
          <td>SBERT/E5</td>
          <td>‚úÖ‚úÖ</td>
          <td>‚úÖ‚úÖ</td>
          <td>‚ö°</td>
          <td>‚ö°‚ö°‚ö°</td>
          <td>GPU 8GB VRAM, 32GB RAM</td>
          <td>‚úÖ Preciso</td>
      </tr>
  </tbody>
</table>
<blockquote>
<p><strong>Recomenda√ß√£o</strong>: Para aplica√ß√µes em produ√ß√£o, comece com Ollama embeddings (simples de implementar) e evolua para modelos especializados como SBERT ou E5 conforme necess√°rio. Considere seus recursos de hardware ao escolher a abordagem.</p>
<p><strong>‚ö†Ô∏è Importante</strong>: A contagem de tokens heur√≠stica pode errar at√© 2x. Para produ√ß√£o, use <code>count-tokens-ollama-precise</code> ou <code>clojure-tiktoken</code> para precis√£o.</p></blockquote>
<p>Olha, d√° pra turbinar esse nosso RAG de v√°rias formas! Primeiro, a gente poderia melhorar a tokeniza√ß√£o usando aqueles m√©todos mais avan√ßados tipo <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding">BPE</a> ou <a href="https://en.wikipedia.org/wiki/WordPiece">WordPiece</a> - idealmente o mesmo que o modelo usa.</p>
<p>E os embeddings? Seria muito mais eficiente pegar direto do Ollama em vez de fazer na m√£o. A diferen√ßa na busca sem√¢ntica seria absurda! O TF-IDF que implementamos √© √≥timo para entender os conceitos, mas embeddings densos modernos capturam nuances sem√¢nticas que fazem toda a diferen√ßa em aplica√ß√µes reais.</p>
<p>Quando o projeto crescer, vai ser essencial ter um banco de dados vetorial decente. Imagina lidar com milhares de documentos usando nossa implementa√ß√£o atual? Seria um pesadelo! <a href="https://milvus.io/">Milvus</a>, <a href="https://github.com/facebookresearch/faiss">FAISS</a> ou <a href="https://qdrant.tech/">Qdrant</a> resolveriam isso numa boa. E n√£o podemos esquecer do cache - tanto para embeddings quanto para respostas. Economiza um temp√£o e reduz a carga no sistema.</p>
<p>A parte de tratamento de erros e logging tamb√©m precisa de carinho. J√° pensou o usu√°rio esperando resposta e o Ollama t√° offline? Ou um arquivo corrompido? Precisamos de mensagens amig√°veis e um sistema de logging decente pra rastrear problemas. E claro, testes! Sem testes unit√°rios e de integra√ß√£o, qualquer mudan√ßa vira uma roleta-russa.</p>
<p>O prompt engineering √© outro ponto crucial - d√° pra refinar bastante o formato atual. Poder√≠amos experimentar com exemplos no prompt (few-shot), instru√ß√µes passo a passo (chain-of-thought), e ser mais espec√≠fico sobre o formato da resposta.</p>
<h2 id="ap√™ndice">Ap√™ndice</h2>
<h3 id="requisitos-de-hardware-detalhados">Requisitos de Hardware Detalhados</h3>
<p>A performance do sistema RAG depende significativamente do hardware dispon√≠vel. Aqui est√£o as configura√ß√µes recomendadas:</p>
<table>
  <thead>
      <tr>
          <th>Componente</th>
          <th>M√≠nimo</th>
          <th>Recomendado</th>
          <th>Alto Desempenho</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>CPU</strong></td>
          <td>4 cores (Intel i5/AMD Ryzen 5)</td>
          <td>8 cores (Intel i7/AMD Ryzen 7)</td>
          <td>16+ cores (Intel i9/AMD Ryzen 9)</td>
      </tr>
      <tr>
          <td><strong>RAM</strong></td>
          <td>8 GB</td>
          <td>16 GB</td>
          <td>32+ GB</td>
      </tr>
      <tr>
          <td><strong>GPU</strong></td>
          <td>Integrada</td>
          <td>NVIDIA RTX 3060 (8GB VRAM)</td>
          <td>NVIDIA RTX 4090 (24GB VRAM)</td>
      </tr>
      <tr>
          <td><strong>VRAM</strong></td>
          <td>-</td>
          <td>8 GB</td>
          <td>16+ GB</td>
      </tr>
      <tr>
          <td><strong>Storage</strong></td>
          <td>SSD 256 GB</td>
          <td>SSD 512 GB</td>
          <td>NVMe 1 TB+</td>
      </tr>
      <tr>
          <td><strong>Rede</strong></td>
          <td>100 Mbps</td>
          <td>1 Gbps</td>
          <td>10 Gbps</td>
      </tr>
  </tbody>
</table>
<h4 id="configura√ß√µes-por-caso-de-uso"><strong>Configura√ß√µes por Caso de Uso</strong></h4>
<p><strong>üü¢ Desenvolvimento/Teste</strong></p>
<ul>
<li>CPU: 4 cores, RAM: 8GB</li>
<li>Modelo: <code>deepseek-r1</code> (CPU only)</li>
<li>Documentos: &lt; 1GB</li>
<li>Performance: ~2-5 segundos por consulta</li>
</ul>
<p><strong>üü° Produ√ß√£o Pequena</strong></p>
<ul>
<li>CPU: 8 cores, RAM: 16GB, GPU: RTX 3060</li>
<li>Modelo: <code>deepseek-r1</code> (GPU)</li>
<li>Documentos: 1-10GB</li>
<li>Performance: ~1-3 segundos por consulta</li>
</ul>
<p><strong>üî¥ Produ√ß√£o Grande</strong></p>
<ul>
<li>CPU: 16+ cores, RAM: 32GB+, GPU: RTX 4090</li>
<li>Modelo: <code>deepseek-r1</code> + embeddings densos</li>
<li>Documentos: 10GB+</li>
<li>Performance: &lt; 1 segundo por consulta</li>
</ul>
<h4 id="otimiza√ß√µes-por-hardware"><strong>Otimiza√ß√µes por Hardware</strong></h4>
<p><strong>CPU Only:</strong></p>


  <pre><code class="language-bash"># Usar modelo otimizado para CPU
ollama pull deepseek-r1:3b  # Vers√£o menor</code></pre>
 <p><strong>GPU Dispon√≠vel:</strong></p>


  <pre><code class="language-bash"># Usar vers√£o completa com acelera√ß√£o GPU
ollama pull deepseek-r1</code></pre>
 <p><strong>M√∫ltiplas GPUs:</strong></p>


  <pre><code class="language-bash"># Distribuir carga entre GPUs
CUDA_VISIBLE_DEVICES=0,1 ollama serve</code></pre>
 <hr>
<h2 id="refer√™ncias">Refer√™ncias</h2>
<ul>
<li><a href="https://www.pinecone.io/learn/rag/">RAG</a> - Documenta√ß√£o do Pinecone</li>
<li><a href="https://www.pinecone.io/learn/embeddings/">Embedding</a> - Documenta√ß√£o do Pinecone</li>
<li><a href="https://www.pinecone.io/learn/llms/">LLM</a> - Documenta√ß√£o do Pinecone</li>
<li><a href="https://ollama.com/">Ollama</a> - Ferramenta para rodar LLMs localmente</li>
<li><a href="https://clojure.org/">Clojure</a> - Documenta√ß√£o do Clojure</li>
<li><a href="https://github.com/http-kit/http-kit">http-kit</a> - Cliente HTTP para Clojure</li>
<li><a href="https://github.com/clojure/data.json">data.json</a> - Biblioteca JSON para Clojure</li>
<li><a href="https://clojure.github.io/clojure/clojure.test-api.html">clojure.test</a> - Documenta√ß√£o da biblioteca de testes do Clojure</li>
<li><a href="https://github.com/clj-kondo/clj-kondo">clj-kondo</a> - Linter para Clojure</li>
</ul>

    </div>
    
    
    



    
    
    
        
            
            
            
            
                
                    
                
                    
                
                    
                
                    
                
                    
                
            
            
            
        
    
        
            
            
            
            
                
                    
                
                    
                
                    
                
                    
                
                    
                
            
            
            
        
    
        
            
            
            
            
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            
            
            
        
    
        
            
            
            
            
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            
            
            
        
    
        
    
        
            
            
            
            
                
                    
                        
                        
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            
            
            
                
            
        
    
        
            
            
            
            
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            
            
            
        
    
        
            
            
            
            
                
                    
                        
                        
                    
                
                    
                        
                        
                    
                
                    
                        
                        
                    
                
                    
                
                    
                
                    
                
                    
                
            
            
            
                
            
        
    
        
            
            
            
            
            
            
        
    
        
            
            
            
            
            
            
        
    
    
    
        
        
        
        <div class="related-posts">
            <h3 class="related-posts-title">üìö Posts Relacionados</h3>
            <div class="related-posts-grid">
                
                    
                    <article class="related-post-item animate-on-scroll">
                        <div class="related-post-content">
                            <h4 class="related-post-title">
                                <a href="/2025/03/28/rag02/">T√©cnicas Avan√ßadas para RAG em Produ√ß√£o</a>
                            </h4>
                            <p class="related-post-excerpt">
                                <h2 id="introdu√ß√£o">Introdu√ß√£o</h2>
<p>Ol√° pessoal! üëã</p>
<p>Nos artigos anteriores, exploramos como <a href="/2025/03/23/rag/">implementar um RAG b√°sico em Clojure</a> em mem√≥ria e como <a href="/2025/03/25/semantic-postgresql/">construir um sistema de busca sem√¢ntica com PostgreSQL e Ollama</a>. Agora, vamos dar o pr√≥ximo passo: transformar nosso prot√≥tipo em um sistema RAG pronto para produ√ß√£o.</p>
<p>Como muitos desenvolvedores j√° descobriram, criar um prot√≥tipo funcional de <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> com alguns documentos √© relativamente simples. O verdadeiro desafio come√ßa quando precisamos escalar esse sistema para lidar com milhares de documentos, garantir respostas precisas e manter o desempenho sob carga. Neste artigo, vamos explorar t√©cnicas avan√ßadas para superar esses desafios e levar nosso <a href="https://github.com/scovl/docai">DocAI</a> para um novo patamar de qualidade e confiabilidade.</p>
                            </p>
                            <div class="related-post-meta">
                                <span class="related-post-date">28/03/2025</span>
                                
                                    <div class="related-post-tags">
                                        
                                            <span class="tag">RAG</span>
                                        
                                            <span class="tag">LLM</span>
                                        
                                    </div>
                                
                            </div>
                            <div class="related-post-score">
                                <modern-badge variant="info">3 tags em comum</modern-badge>
                            </div>
                        </div>
                    </article>
                
                    
                    <article class="related-post-item animate-on-scroll">
                        <div class="related-post-content">
                            <h4 class="related-post-title">
                                <a href="/2025/03/25/semantic-postgresql/">Busca Sem√¢ntica com Ollama e PostgreSQL</a>
                            </h4>
                            <p class="related-post-excerpt">
                                <p>Ol√°, pessoal! üëã</p>
<p>No <a href="/2025/03/23/rag/">artigo anterior</a>, exploramos como construir um sistema RAG (Retrieval-Augmented Generation) usando <a href="https://clojure.org/">Clojure</a> e <a href="https://ollama.com/">Ollama</a> com uma implementa√ß√£o simples de <a href="/post/tf-idf/">TF-IDF</a>. Embora essa abordagem seja excelente para aprender os fundamentos, quando pensamos em solu√ß√µes de produ√ß√£o, precisamos de algo mais robusto e escal√°vel.</p>
<p>Neste artigo, vamos descobrir como construir um sistema de busca sem√¢ntica poderoso usando <a href="https://ollama.com/">Ollama</a>, <a href="https://www.postgresql.org/">PostgreSQL</a> e suas extens√µes para manipula√ß√£o de vetores. Esta solu√ß√£o √© perfeitamente adequada para aplica√ß√µes de produ√ß√£o e pode servir como base para sistemas RAG, agentes de IA, assistentes em geral. Diferentemente do artigo anterior, vamos usar o <a href="https://ollama.com/">Ollama</a> via Docker assim como o <a href="https://www.postgresql.org/">PostgreSQL</a> e as extens√µes <a href="https://github.com/pgvector/pgvector">pgvector</a> e <a href="https://github.com/timescale/pgai">pgai</a>.</p>
                            </p>
                            <div class="related-post-meta">
                                <span class="related-post-date">25/03/2025</span>
                                
                                    <div class="related-post-tags">
                                        
                                            <span class="tag">RAG</span>
                                        
                                            <span class="tag">PostgreSQL</span>
                                        
                                    </div>
                                
                            </div>
                            <div class="related-post-score">
                                <modern-badge variant="info">1 tags em comum</modern-badge>
                            </div>
                        </div>
                    </article>
                
            </div>
        </div>
    

    
    
    
    
<div class="comments-section">
    <h3 class="comments-title">üí¨ Coment√°rios</h3>
    <div class="comments-container">
        <script src="https://giscus.app/client.js"
                data-repo="scovl/scovl.github.io"
                data-repo-id="MDEwOlJlcG9zaXRvcnkxMzg1OTI2ODA="
                data-category="General"
                data-category-id="DIC_kwDOCELBqM4CthUV"
                data-mapping="pathname"
                data-strict="0"
                data-reactions-enabled="1"
                data-emit-metadata="0"
                data-input-position="bottom"
                data-theme="light"
                data-lang="pt"
                crossorigin="anonymous"
                async>
        </script>
    </div>
</div>

    
</article>

        </div>
    </main>
    
    
    
    <footer class="footer">
    <div class="container">
        <div class="footer-content">
            <div class="footer-links">
                
                <a href="https://github.com/scovl" target="_blank" rel="noopener noreferrer" class="footer-link">
                    GitHub
                </a>
                
                
                
                <a href="https://linkedin.com/in/vitor-lobo" target="_blank" rel="noopener noreferrer" class="footer-link">
                    LinkedIn
                </a>
                
                
                
                <a href="mailto:lobocode@gmail.com" class="footer-link">
                    Email
                </a>
                

                
                <a href="https://hachyderm.io/@lobocode" target="_blank" rel="noopener noreferrer" class="footer-link">
                    Mastodon
                </a>
                

                
                <a href="https://scovl.github.io/index.xml" target="_blank" rel="noopener noreferrer" class="footer-link">
                    RSS
                </a>
                
            </div>
            
            
            <div class="back-to-top-container">
                <button id="back-to-top" class="back-to-top-btn" aria-label="Voltar ao topo">
                    <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                        <path d="m18 15-6-6-6 6"/>
                    </svg>
                    <span data-i18n="back_to_top">Voltar ao topo</span>
                </button>
            </div>
            
            <div class="copyright">
                &copy; 2025 scovl
            </div>
        </div>
    </div>
</footer> 
    
    
    
    <script src="/vendor/prism/prism-core.min.js?v=1757193440"></script>
    <script src="/vendor/prism/prism-clike.min.js?v=1757193440"></script>
    <script src="/vendor/prism/prism-c.min.js?v=1757193440"></script>
    <script src="/vendor/prism/prism-cpp.min.js?v=1757193440"></script>
    <script src="/vendor/prism/prism-rust.min.js?v=1757193440"></script>
    <script src="/vendor/prism/prism-clojure.min.js?v=1757193440"></script>
    <script src="/vendor/prism/prism-swift.min.js?v=1757193440"></script>
    <script src="/vendor/prism/prism-bash.min.js?v=1757193440"></script>
    <script src="/vendor/prism/prism-javascript.min.js?v=1757193440"></script>
    <script src="/vendor/prism/prism-typescript.min.js?v=1757193440"></script>
    <script src="/vendor/prism/prism-autoloader.min.js?v=1757193440"></script>
    
    
    <script src="/js/main-minimal.js?v=1757193440"></script>
    <script src="/js/lazy-loading.js?v=1757193440"></script>
    <script src="/js/toc.js?v=1757193440"></script>
    
    
    
</body>
</html> 