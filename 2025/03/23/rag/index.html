<!DOCTYPE html>
<html lang="pt">
<head>
    <title>RAG Simples com Clojure e Ollama | scovl</title>
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Um prot√≥tipo funcional do zero">


<link rel="stylesheet" href="/css/styles.css">
<link rel="stylesheet" href="/css/syntax.css">


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-tomorrow.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-dark.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-clojure.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-sql.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-typescript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-javascript.min.js"></script>





<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: '\\(', right: '\\)', display: false},
                {left: '\\[', right: '\\]', display: true}
            ]
        });
    });
</script>



<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        mermaid.initialize({
            startOnLoad: true,
            theme: 'light',
            align: 'center'
        });
    });
</script>
 
</head>
<body>
    <div class="container">
        
        
        <header class="site-header">
    <div class="header-inner">
        <div class="site-branding">
            <a href="https://scovl.github.io/" class="site-title">scovl</a>
        </div>
        
        <nav class="site-nav">
            <ul>
                
                
                <li>
                    <a href="/page/about/" class="">
                        About
                    </a>
                </li>
                
                <li>
                    <a href="/page/contact/" class="">
                        Contact
                    </a>
                </li>
                
            </ul>
        </nav>
    </div>
</header> 
        
        
        
        <main>
            
<article class="post">
    <header class="post-header">
        <h1 class="post-title">RAG Simples com Clojure e Ollama</h1>
        <div class="post-meta">
            
            <time datetime="2025-03-23T19:00:00Z">
                Sun, Mar 23, 2025
            </time>
            
            
            
            <span class="post-author">por Vitor Lobo Ramos</span>
            
            
            
            <div class="post-tags">
                
                <a href="/tags/rag/" class="tag">RAG</a>
                
                <a href="/tags/llm/" class="tag">LLM</a>
                
                <a href="/tags/ai/" class="tag">AI</a>
                
                <a href="/tags/langchain/" class="tag">Langchain</a>
                
            </div>
            
            
            
            <div class="reading-time">
                Estimated reading time: 26 min
            </div>
            
            
            
            <div class="post-description">
                Um prot√≥tipo funcional do zero
            </div>
            
        </div>
    </header>
    
    <div class="post-content content-wrapper">
        <h1 id="sum√°rio">Sum√°rio</h1>
<ul>
<li><strong><a href="/2025/03/23/rag/#introdu%c3%a7%c3%a3o">Introdu√ß√£o</a></strong></li>
<li><strong><a href="/2025/03/23/rag/#fundamentos-do-rag">Fundamentos do RAG</a></strong>
<ul>
<li><a href="/2025/03/23/rag/#o-que-%c3%a9-rag">O que √© RAG?</a></li>
<li><a href="/2025/03/23/rag/#por-que-precisamos-do-rag">Por que precisamos do RAG?</a></li>
<li><a href="/2025/03/23/rag/#os-tr%c3%aas-pilares-do-rag">Os Tr√™s Pilares do RAG</a></li>
<li><a href="/2025/03/23/rag/#por-que-o-deepseek-r1">Por que o DeepSeek R1?</a></li>
</ul>
</li>
<li><strong><a href="/2025/03/23/rag/#implementa%c3%a7%c3%a3o-pr%c3%a1tica">Implementa√ß√£o Pr√°tica</a></strong>
<ul>
<li><a href="/2025/03/23/rag/#preparando-o-ambiente">Preparando o Ambiente</a></li>
<li><a href="/2025/03/23/rag/#estrutura-do-projeto">Estrutura do Projeto</a></li>
<li><a href="/2025/03/23/rag/#processamento-de-documentos">Processamento de Documentos</a></li>
<li><a href="/2025/03/23/rag/#sistema-de-embeddings">Sistema de Embeddings</a></li>
<li><a href="/2025/03/23/rag/#interface-com-ollama">Interface com Ollama</a></li>
<li><a href="/2025/03/23/rag/#m%c3%b3dulo-principal">M√≥dulo Principal</a></li>
</ul>
</li>
<li><strong><a href="/2025/03/23/rag/#como-usar">Como Usar</a></strong>
<ul>
<li><a href="/2025/03/23/rag/#instala%c3%a7%c3%a3o-do-ollama">Instala√ß√£o do Ollama</a></li>
<li><a href="/2025/03/23/rag/#tf-idf">TF-IDF</a></li>
<li><a href="/2025/03/23/rag/#configura%c3%a7%c3%a3o-do-projeto">Configura√ß√£o do Projeto</a></li>
<li><a href="/2025/03/23/rag/#executando-a-aplica%c3%a7%c3%a3o">Executando a Aplica√ß√£o</a></li>
</ul>
</li>
<li><strong><a href="/2025/03/23/rag/#considera%c3%a7%c3%b5es-t%c3%a9cnicas">Considera√ß√µes T√©cnicas</a></strong>
<ul>
<li><a href="/2025/03/23/rag/#performance-e-otimiza%c3%a7%c3%b5es">Performance e Otimiza√ß√µes</a></li>
<li><a href="/2025/03/23/rag/#prompt-engineering">Prompt Engineering</a></li>
</ul>
</li>
<li><strong><a href="/2025/03/23/rag/#pr%c3%b3ximos-passos">Pr√≥ximos Passos</a></strong>
<ul>
<li><a href="/2025/03/23/rag/#melhorias-propostas">Melhorias Propostas</a></li>
<li><a href="/2025/03/23/rag/#usando-langchain4j">Usando Langchain4j</a></li>
</ul>
</li>
<li><strong><a href="/2025/03/23/rag/#refer%c3%aancias">Refer√™ncias</a></strong></li>
</ul>
<h2 id="introdu√ß√£o">Introdu√ß√£o</h2>
<p>Ol√°, pessoal! üëã</p>
<p>Neste artigo, vamos explorar como construir uma aplica√ß√£o <a href="https://pt.wikipedia.org/wiki/Gera%C3%A7%C3%A3o_aumentada_por_recupera%C3%A7%C3%A3o">RAG (Retrieval-Augmented Generation)</a> completa do zero usando <a href="https://clojure.org/">Clojure</a>. Vamos mergulhar em uma implementa√ß√£o pr√°tica que combina processamento de texto, busca sem√¢ntica e gera√ß√£o de respostas com LLMs locais. Se voc√™ est√° interessado em melhorar a precis√£o e relev√¢ncia das respostas dos seus modelos de linguagem com informa√ß√µes atualizadas, este guia √© para voc√™!</p>
<h2 id="fundamentos-do-rag">Fundamentos do RAG</h2>
<h3 id="o-que-√©-rag">O que √© RAG?</h3>
<p>Os Modelos de Linguagem de Grande Escala (LLMs), como o GPT, ChatGPT e outros, revolucionaram a forma como interagimos com a intelig√™ncia artificial. Eles s√£o capazes de gerar textos coerentes, responder perguntas complexas e at√© mesmo criar conte√∫do criativo. No entanto, esses modelos possuem uma limita√ß√£o fundamental: seu conhecimento √© &ldquo;congelado&rdquo; no tempo.</p>


  
    
  
  <div class="mermaid">graph TD
    A[LLM Treinado] --&gt; B[Data de Corte]
    B --&gt; C[Conhecimento Congelado]
    C --&gt; D[Limita√ß√µes]
    D --&gt; E[N√£o sabe eventos recentes]
    D --&gt; F[N√£o tem dados atualizados]
    D --&gt; G[N√£o conhece novas tecnologias]</div>
 <h3 id="por-que-precisamos-do-rag">Por que precisamos do RAG?</h3>
<p>Ao desenvolver aplica√ß√µes inteligentes, como assistentes financeiros que precisam de cota√ß√µes de a√ß√µes em tempo real, chatbots de suporte que devem conhecer os produtos mais recentes da empresa ou sistemas de recomenda√ß√£o que se baseiam nas √∫ltimas tend√™ncias, nos deparamos com uma limita√ß√£o crucial dos Modelos de Linguagem de Grande Escala (LLMs) tradicionais: seu conhecimento est√°tico.</p>
<p>O problema fundamental reside no fato de que esses modelos, por mais sofisticados que sejam, possuem uma base de conhecimento &ldquo;congelada&rdquo; no momento de seu treinamento. Eles carecem de acesso inerente a informa√ß√µes atualizadas, o que restringe drasticamente sua aplicabilidade em cen√°rios que exigem dados em tempo real ou conhecimento sobre eventos recentes.</p>
<blockquote>
<p>Confiar exclusivamente em um LLM &ldquo;puro&rdquo; nesses contextos resultar√° em respostas desatualizadas, potencialmente imprecisas e, consequentemente, em uma experi√™ncia do usu√°rio comprometida. A efic√°cia da aplica√ß√£o √© diretamente afetada.</p></blockquote>
<h3 id="os-tr√™s-pilares-do-rag">Os Tr√™s Pilares do RAG</h3>


  
  <div class="mermaid">graph LR
    A[RAG] --&gt; B[Base de Dados Atual]
    A --&gt; C[Pesquisa em Tempo Real]
    A --&gt; D[Combina√ß√£o de Conhecimento]
    
    B --&gt; E[Documentos Atualizados]
    B --&gt; F[Dados em Tempo Real]
    
    C --&gt; G[Busca Ativa]
    C --&gt; H[Sele√ß√£o de Informa√ß√µes]
    
    D --&gt; I[Integra√ß√£o com LLM]
    D --&gt; J[Contextualiza√ß√£o]</div>
 <ol>
<li><strong>Conex√£o com uma base de dados atual:</strong> Em vez de depender apenas do conhecimento est√°tico adquirido durante seu treinamento (que pode se tornar obsoleto rapidamente), o LLM ganha acesso a uma fonte de informa√ß√µes din√¢mica e constantemente atualizada. Isso pode ser uma base de dados de not√≠cias, um reposit√≥rio de documentos corporativos, uma cole√ß√£o de artigos cient√≠ficos, ou qualquer outra fonte relevante para a tarefa em quest√£o.</li>
<li><strong>Pesquisa em tempo real:</strong> O LLM n√£o est√° mais limitado a &ldquo;lembrar&rdquo; de informa√ß√µes. Ele adquire a capacidade de &ldquo;procurar&rdquo; ativamente por dados relevantes para responder a uma pergunta ou gerar um texto. Isso √© semelhante a como n√≥s, humanos, usamos mecanismos de busca para encontrar informa√ß√µes que n√£o temos memorizadas. O LLM, equipado com RAG, pode formular consultas, analisar os resultados e selecionar as informa√ß√µes mais pertinentes.</li>
<li><strong>Combina√ß√£o de conhecimento base com dados novos:</strong> Este √© o ponto crucial que diferencia o RAG de uma simples busca em uma base de dados. O LLM n√£o apenas recupera informa√ß√µes, mas tamb√©m as integra ao seu conhecimento pr√©-existente. Ele usa sua capacidade de racioc√≠nio e compreens√£o para contextualizar os novos dados, identificar contradi√ß√µes, e formular respostas coerentes e informadas.</li>
</ol>
<p>Segundo um <a href="https://arxiv.org/abs/2309.01066">whitepaper recente dos pesquisadores do Google</a>, existem v√°rias t√©cnicas para turbinar o desempenho dos LLMs, e o RAG √© uma das mais promissoras. Isso ocorre porque o RAG aborda algumas das limita√ß√µes fundamentais desses modelos:</p>
<p>O RAG resolve v√°rios problemas de uma vez s√≥: diminui aquelas &ldquo;viagens&rdquo; dos LLMs quando inventam respostas (as famosas alucina√ß√µes), mant√©m tudo atualizado em vez de ficar preso no passado, deixa as respostas mais transparentes porque voc√™ sabe de onde veio a informa√ß√£o, e ainda melhora o desempenho do modelo quando ele precisa lidar com documentos ou dados espec√≠ficos da sua empresa. √â como dar ao modelo um Google particular que ele pode consultar antes de responder!</p>
<blockquote>
<p>O RAG representa um avan√ßo significativo na evolu√ß√£o dos LLMs, permitindo que eles se tornem ferramentas mais confi√°veis, precisas e √∫teis para uma ampla gama de aplica√ß√µes. Ele transforma o LLM de um &ldquo;sabe-tudo&rdquo; desatualizado em um pesquisador √°gil e bem-informado, capaz de combinar conhecimento profundo com informa√ß√µes atualizadas em tempo real.</p></blockquote>
<h3 id="por-que-o-deepseek-r1">Por que o DeepSeek R1?</h3>
<p>Ele trabalha muito bem com documenta√ß√£o t√©cnica, o que √© perfeito para nosso sistema <a href="https://pt.wikipedia.org/wiki/Gera%C3%A7%C3%A3o_aumentada_por_recupera%C3%A7%C3%A3o">RAG</a> focado em docs t√©cnicas. O DeepSeek R1 consegue equilibrar qualidade e velocidade melhor que outros modelos do Ollama, rodando na sua m√°quina sem ficar alucinando com respostas que n√£o fazem sentido.</p>
<p>O modelo tamb√©m se d√° super bem com v√°rias linguagens de programa√ß√£o, incluindo <a href="https://clojure.org/">Clojure</a>, ent√£o ele responde numa boa sobre implementa√ß√µes t√©cnicas e documenta√ß√£o de c√≥digo. E o melhor: mesmo quando voc√™ joga informa√ß√µes pela metade ou todas bagun√ßadas, ele ainda consegue manter o contexto e dar respostas que fazem sentido. Por isso ele √© perfeito para o que estamos construindo!</p>
<h2 id="implementa√ß√£o-pr√°tica">Implementa√ß√£o Pr√°tica</h2>
<h3 id="preparando-o-ambiente">Preparando o Ambiente</h3>
<p>Pre-requisitos:</p>
<ul>
<li><a href="https://clojure.org/guides/getting_started">Clojure</a>: Linguagem de programa√ß√£o funcional que vamos usar para construir a aplica√ß√£o</li>
<li><a href="https://leiningen.org/">Leiningen</a>: Ferramenta de build para Clojure</li>
<li><a href="https://ollama.com/">Ollama</a>: Modelo de linguagem local</li>
</ul>
<h3 id="estrutura-do-projeto">Estrutura do Projeto</h3>
<p>Nossa aplica√ß√£o ter√° tr√™s componentes principais:</p>
<ol>
<li><strong>Processamento de documenta√ß√£o (Markdown/HTML)</strong>
<ul>
<li>Extra√ß√£o de texto</li>
<li>Pr√©-processamento de texto</li>
</ul>
</li>
<li><strong>Sistema de embeddings</strong>
<ul>
<li>Cria√ß√£o de embeddings para o texto usando <a href="https://pt.wikipedia.org/wiki/TF-IDF">TF-IDF</a></li>
<li>Busca por similaridade sem√¢ntica</li>
</ul>
</li>
<li><strong>Interface com o LLM</strong>
<ul>
<li>Gera√ß√£o de resposta usando o LLM</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>Observa√ß√£o:</strong> Embora o RAG moderno utilize embeddings densos gerados por modelos de linguagem para capturar a sem√¢ntica de forma mais rica, neste artigo, usaremos uma implementa√ß√£o simplificada de <a href="https://pt.wikipedia.org/wiki/TF-IDF">TF-IDF (Term Frequency-Inverse Document Frequency)</a>.</p></blockquote>
<h3 id="tf-idf">TF-IDF</h3>
<p>O TF-IDF (Term Frequency-Inverse Document Frequency) √© uma t√©cnica estat√≠stica usada para avaliar a import√¢ncia de uma palavra em um documento, em rela√ß√£o a uma cole√ß√£o de documentos. Vamos entender como funciona:</p>
<ol>
<li>
<p><strong>Term Frequency (TF)</strong>: Mede a frequ√™ncia de uma palavra em um documento.</p>


  <pre><code class="language-">TF(termo) = (N√∫mero de vezes que o termo aparece no documento) / (Total de termos no documento)</code></pre>
 </li>
<li>
<p><strong>Inverse Document Frequency (IDF)</strong>: Mede a raridade de um termo na cole√ß√£o de documentos.</p>


  <pre><code class="language-">IDF(termo) = log(N√∫mero total de documentos / N√∫mero de documentos contendo o termo)</code></pre>
 </li>
<li>
<p><strong>TF-IDF</strong>: √â o produto desses dois valores.</p>


  <pre><code class="language-">TF-IDF(termo) = TF(termo) √ó IDF(termo)</code></pre>
 </li>
</ol>
<p><strong>Exemplo pr√°tico:</strong></p>
<p>Imagine que temos tr√™s documentos t√©cnicos:</p>
<ul>
<li>Doc1: &ldquo;Clojure √© uma linguagem funcional baseada em Lisp.&rdquo;</li>
<li>Doc2: &ldquo;Python √© uma linguagem de programa√ß√£o vers√°til.&rdquo;</li>
<li>Doc3: &ldquo;Clojure e Python s√£o linguagens de programa√ß√£o populares.&rdquo;</li>
</ul>
<p>Para calcular o TF-IDF da palavra &ldquo;Clojure&rdquo; no Doc1:</p>
<ol>
<li>TF(&ldquo;Clojure&rdquo;, Doc1) = 1/8 = 0.125 (aparece 1 vez em 8 palavras)</li>
<li>IDF(&ldquo;Clojure&rdquo;) = log(3/2) ‚âà 0.176 (aparece em 2 dos 3 documentos)</li>
<li>TF-IDF(&ldquo;Clojure&rdquo;, Doc1) = 0.125 √ó 0.176 ‚âà 0.022</li>
</ol>
<p>Comparando com a palavra &ldquo;linguagem&rdquo; no mesmo documento:</p>
<ol>
<li>TF(&ldquo;linguagem&rdquo;, Doc1) = 1/8 = 0.125</li>
<li>IDF(&ldquo;linguagem&rdquo;) = log(3/3) = 0 (aparece em todos os documentos)</li>
<li>TF-IDF(&ldquo;linguagem&rdquo;, Doc1) = 0.125 √ó 0 = 0</li>
</ol>
<p>Isso mostra que &ldquo;Clojure&rdquo; tem maior valor discriminativo que &ldquo;linguagem&rdquo; para o Doc1, pois &ldquo;linguagem&rdquo; √© comum em todos os documentos.</p>
<p>No nosso sistema RAG, usaremos o TF-IDF para:</p>
<ol>
<li>Indexar os documentos t√©cnicos</li>
<li>Encontrar os documentos mais relevantes para uma consulta</li>
<li>Priorizar termos distintivos ao buscar informa√ß√µes</li>
</ol>
<h3 id="configura√ß√£o-do-projeto">Configura√ß√£o do Projeto</h3>
<ol>
<li>Crie um novo projeto Clojure:</li>
</ol>


  <pre><code class="language-bash">lein new app docai
cd docai</code></pre>
 <ol start="2">
<li>Configure o <code>project.clj</code>:</li>
</ol>


  <pre><code class="language-clojure">(defproject docai &#34;0.1.0-SNAPSHOT&#34;
  :description &#34;Um assistente RAG para consulta de documenta√ß√£o t√©cnica&#34;
  :url &#34;http://example.com/FIXME&#34;
  :license {:name &#34;EPL-2.0 OR GPL-2.0-or-later WITH Classpath-exception-2.0&#34;
            :url &#34;https://www.eclipse.org/legal/epl-2.0/&#34;}
  :dependencies [[org.clojure/clojure &#34;1.11.1&#34;]
                 [markdown-to-hiccup &#34;0.6.2&#34;]
                 [hickory &#34;0.7.1&#34;]
                 [org.clojure/data.json &#34;2.4.0&#34;]
                 [http-kit &#34;2.6.0&#34;]
                 [org.clojure/tools.logging &#34;1.2.4&#34;]
                 [org.clojure/tools.namespace &#34;1.4.4&#34;]
                 [org.clojure/core.async &#34;1.6.681&#34;]
                 [org.clojure/core.memoize &#34;1.0.257&#34;]
                 [org.clojure/core.cache &#34;1.0.225&#34;]]
  :main ^:skip-aot docai.core
  :target-path &#34;target/%s&#34;
  :profiles {:uberjar {:aot :all
                       :jvm-opts [&#34;-Dclojure.compiler.direct-linking=true&#34;]}})</code></pre>
 <p>A estrutura do projeto acima define um aplicativo Clojure para RAG (Retrieval-Augmented Generation) com v√°rias depend√™ncias essenciais: <code>markdown-to-hiccup</code> e <code>hickory</code> para processar documentos em Markdown e HTML, <code>data.json</code> e <code>http-kit</code> para comunica√ß√£o com APIs (como a do Ollama), <code>tools.logging</code> para registro de eventos, <code>tools.namespace</code> para gerenciamento de namespaces, <code>core.async</code> para opera√ß√µes ass√≠ncronas (√∫til ao lidar com processamento de documentos grandes), e <code>core.memoize</code> e <code>core.cache</code> para implementar cache de resultados (como embeddings ou respostas do LLM), o que melhora significativamente a performance ao evitar rec√°lculos desnecess√°rios, especialmente em consultas repetidas ou similares.</p>
<h3 id="implementa√ß√£o-dos-componentes">Implementa√ß√£o dos Componentes</h3>
<p>Agora vamos implementar os tr√™s componentes principais do nosso sistema RAG e vamos come√ßar com o processamento de documentos. Pois, ele √© o ponto de entrada para o RAG onde vamos processar os documentos e extrair o texto para ser usado nos outros componentes.</p>
<h4 id="processamento-de-documentos">Processamento de Documentos</h4>


  <pre><code class="language-clojure">;; src/docai/document.clj
(ns docai.document
  (:require [markdown-to-hiccup.core :as md]
            [hickory.core :as html]
            [clojure.string :as str]))

(defn extract-text-from-markdown [content]
  (try
    (let [hiccup-result (md/md-&gt;hiccup content)
          text-nodes (filter is-string? (flatten hiccup-result))]
      text-nodes)
    (catch Exception e
      (println &#34;Erro ao processar Markdown:&#34; (.getMessage e))
      [content])))

(defn extract-text-from-html [content]
  (try
    (let [dom (html/parse content)
          hiccup-result (html/as-hiccup dom)
          text-nodes (filter is-string? (flatten hiccup-result))]
      text-nodes)
    (catch Exception e
      (println &#34;Erro ao processar HTML:&#34; (.getMessage e))
      [content])))

(defn extract-text
  &#34;Extrai texto de documenta√ß√£o (Markdown ou HTML)&#34;
  [doc-path]
  (println &#34;Extraindo texto de:&#34; doc-path)
  (let [content (slurp doc-path)
        _ (println &#34;Tamanho do conte√∫do:&#34; (count content) &#34;caracteres&#34;)
        _ (println &#34;Amostra do conte√∫do:&#34; (subs content 0 (min 100 (count content))))
        text (if (.endsWith doc-path &#34;.md&#34;)
               (extract-text-from-markdown content)
               (extract-text-from-html content))
        _ (println &#34;Quantidade de n√≥s de texto extra√≠dos:&#34; (count text))
        chunks (partition-all 512 text)]  ; 512 tokens por chunk
    (println &#34;Quantidade de chunks gerados:&#34; (count chunks))
    chunks))

(defn preprocess-chunks
  &#34;Limpa e prepara os chunks de texto&#34;
  [chunks]
  (let [processed (map #(-&gt; %
                            (str/join &#34; &#34;)
                            (str/replace #&#34;\s&#43;&#34; &#34; &#34;)
                            (str/trim))
                       chunks)]
    (println &#34;Primeiro chunk processado:&#34; (first processed))
    processed))</code></pre>
 <p>Este trecho de c√≥digo implementa a parte de processamento de documentos do nosso sistema RAG. Basicamente, ele pega arquivos Markdown ou HTML e extrai o texto puro deles para que possamos usar depois na busca sem√¢ntica. O c√≥digo usa bibliotecas como <code>markdown-to-hiccup</code> e <code>hickory</code> para converter os documentos em estruturas de dados que facilitam a extra√ß√£o do texto.</p>


  
  <div class="mermaid">graph TD
    A[Documento] --&gt; B{√â Markdown?}
    B --&gt;|Sim| C[Processa Markdown]
    B --&gt;|N√£o| D[Processa HTML]
    C --&gt; E[Extrai Texto]
    D --&gt; E
    E --&gt; F[Divide em Chunks]
    F --&gt; G[Limpa e Formata]
    G --&gt; H[Chunks Prontos]</div>
 <p>O fluxo √© bem direto: primeiro verificamos se estamos lidando com Markdown ou HTML, depois extra√≠mos o texto usando a fun√ß√£o apropriada, dividimos em peda√ßos menores (chunks) de 512 tokens cada, e finalmente limpamos esses chunks removendo espa√ßos extras e formatando tudo direitinho. O c√≥digo tamb√©m inclui bastante logging para ajudar a depurar o processo, mostrando informa√ß√µes como o tamanho do documento, quantidade de texto extra√≠do e n√∫mero de chunks gerados. Essa abordagem de dividir o texto em peda√ßos menores √© crucial para o RAG, j√° que permite processar documentos grandes sem sobrecarregar o modelo de linguagem.</p>
<h4 id="sistema-de-embeddings">Sistema de Embeddings</h4>
<p>Agora vamos implementar o sistema de embeddings. Ele √© respons√°vel por criar embeddings para o texto para que possamos usar na busca sem√¢ntica.</p>


  <pre><code class="language-clojure">;; src/docai/embedding.clj
(ns docai.embedding
  (:require [clojure.string :as str]
            [clojure.core.memoize :as memo]))

;; Implementa√ß√£o de embeddings usando TF-IDF simples
;; N√£o depende de modelos externos, ao contr√°rio do Ollama que usa o deepseek-r1 para o LLM

(defn tokenize
  &#34;Divide o texto em tokens&#34;
  [text]
  (if (string? text)
    (-&gt; text
        str/lower-case
        (str/split #&#34;\s&#43;&#34;)
        (-&gt;&gt; (filter #(&gt; (count %) 2))))
    []))

(defn term-freq
  &#34;Calcula a frequ√™ncia dos termos&#34;
  [tokens]
  (frequencies tokens))

(defn string-doc? [x]
  (instance? String x))

(defn doc-freq
  &#34;Calcula a frequ√™ncia dos documentos&#34;
  [docs]
  (let [string-docs (filter string-doc? docs)  ; Use our own predicate function
        _ (println (str &#34;Processando &#34; (count string-docs) &#34; documentos v√°lidos de &#34; (count docs) &#34; total&#34;))
        doc-tokens (map tokenize string-docs)  
        all-tokens (distinct (flatten doc-tokens))
        doc-count (count string-docs)]
    (if (zero? doc-count)
      {}
      (zipmap all-tokens
              (map #(count (filter (fn [tokens] (some #{%} tokens)) doc-tokens))
                   all-tokens)))))

(defn tf-idf
  &#34;Calcula TF-IDF para um documento&#34;
  [doc doc-freq]
  (if (empty? doc-freq)
    {}
    (let [tokens (tokenize doc)
          tf (term-freq tokens)
          n-docs (count (keys doc-freq))]
      (zipmap (keys tf)
              (map #(* (get tf %) (Math/log (/ n-docs (get doc-freq % 1))))
                   (keys tf))))))

(defn vectorize
  &#34;Converte um documento em um vetor TF-IDF&#34;
  [doc doc-freq]
  (let [tf-idf-scores (tf-idf doc doc-freq)]
    (if (empty? doc-freq)
      []
      (map #(get tf-idf-scores % 0.0)
           (keys doc-freq)))))

(defn create-embeddings
  &#34;Gera embeddings para uma lista de textos usando TF-IDF&#34;
  [texts]
  (try
    (let [doc-freq (doc-freq texts)]
      (map #(vectorize % doc-freq) texts))
    (catch Exception e
      (println &#34;Erro ao criar embeddings: &#34; (.getMessage e))
      (vec (repeat (count texts) [])))))

(defn cosine-similarity
  &#34;Calcula a similaridade do cosseno entre dois vetores&#34;
  [v1 v2]
  (if (or (empty? v1) (empty? v2))
    0.0
    (let [dot-product (reduce &#43; (map * v1 v2))
          norm1 (Math/sqrt (reduce &#43; (map #(* % %) v1)))
          norm2 (Math/sqrt (reduce &#43; (map #(* % %) v2)))]
      (if (or (zero? norm1) (zero? norm2))
        0.0
        (/ dot-product (* norm1 norm2))))))

(defn similarity-search
  &#34;Encontra os N chunks mais similares&#34;
  [query-embedding doc-embeddings n]
  (if (or (empty? query-embedding) (empty? doc-embeddings))
    (take (min n (count doc-embeddings)) (range))
    (let [scores (map #(cosine-similarity query-embedding %) doc-embeddings)]
      (-&gt;&gt; (map vector scores (range))
           (sort-by first &gt;)
           (take n)
           (map second)))))</code></pre>
 <p>O c√≥digo acima implementa um sistema simples de embeddings usando TF-IDF (Term Frequency-Inverse Document Frequency) para transformar textos em vetores num√©ricos. Basicamente, ele pega documentos de texto, quebra em palavras (tokens), calcula a import√¢ncia de cada palavra considerando tanto sua frequ√™ncia no documento quanto sua raridade na cole√ß√£o inteira, e cria vetores que representam cada documento. √â como transformar textos em coordenadas matem√°ticas para que o computador possa entender a &ldquo;semelhan√ßa&rdquo; entre eles.</p>


  
  <div class="mermaid">graph TD
    A[Documentos] --&gt;|Tokeniza√ß√£o| B[Tokens]
    B --&gt;|TF-IDF| C[Vetores Num√©ricos]
    C --&gt;|Similaridade do Cosseno| D[Documentos Similares]</div>
 <p>A parte mais legal √© a fun√ß√£o <code>similarity_search</code>, que usa a similaridade do cosseno para encontrar documentos parecidos com uma consulta. Imagine que cada documento √© um ponto num espa√ßo multidimensional - quanto menor o √¢ngulo entre dois pontos, mais similares eles s√£o. O c√≥digo n√£o usa nenhum modelo de IA sofisticado para isso, apenas matem√°tica b√°sica, o que o torna leve e r√°pido, embora menos poderoso que embeddings modernos baseados em redes neurais. √â como um GPS simples que te leva ao destino sem todos os recursos de um Google Maps.</p>
<h4 id="entendendo-o-tf-idf">Entendendo o TF-IDF</h4>
<p>O TF-IDF √© uma t√©cnica fundamental para representar documentos como vetores num√©ricos. Vamos entender como ele funciona atrav√©s de um exemplo pr√°tico:</p>
<h5 id="exemplo-num√©rico">Exemplo Num√©rico</h5>
<p>Suponha que temos tr√™s documentos sobre programa√ß√£o:</p>
<ol>
<li>Doc1: &ldquo;Clojure √© uma linguagem funcional&rdquo;</li>
<li>Doc2: &ldquo;Clojure √© uma linguagem Lisp&rdquo;</li>
<li>Doc3: &ldquo;Python √© uma linguagem din√¢mica&rdquo;</li>
</ol>
<p>Vamos calcular o TF-IDF passo a passo:</p>
<ol>
<li>
<p><strong>Tokeniza√ß√£o e TF (Term Frequency)</strong></p>
<ul>
<li>Primeiro, convertemos para min√∫sculas e dividimos em palavras</li>
<li>Removemos palavras muito curtas (menos de 3 caracteres)</li>
<li>Calculamos a frequ√™ncia de cada termo em cada documento</li>
</ul>


  <pre><code class="language-">Doc1: {&#34;clojure&#34;: 1, &#34;linguagem&#34;: 1, &#34;funcional&#34;: 1}
Doc2: {&#34;clojure&#34;: 1, &#34;linguagem&#34;: 1, &#34;lisp&#34;: 1}
Doc3: {&#34;python&#34;: 1, &#34;linguagem&#34;: 1, &#34;din√¢mica&#34;: 1}</code></pre>
 </li>
<li>
<p><strong>IDF (Inverse Document Frequency)</strong></p>
<ul>
<li>Contamos em quantos documentos cada termo aparece</li>
<li>Aplicamos a f√≥rmula: IDF = log(N/DF), onde:
<ul>
<li>N = n√∫mero total de documentos (3)</li>
<li>DF = n√∫mero de documentos que cont√™m o termo</li>
</ul>
</li>
</ul>


  <pre><code class="language-bash">&#34;clojure&#34;: log(3/2) = 0.405
&#34;linguagem&#34;: log(3/3) = 0
&#34;funcional&#34;: log(3/1) = 1.099
&#34;lisp&#34;: log(3/1) = 1.099
&#34;python&#34;: log(3/1) = 1.099
&#34;din√¢mica&#34;: log(3/1) = 1.099</code></pre>
 </li>
<li>
<p><strong>TF-IDF Final</strong></p>
<ul>
<li>Multiplicamos TF pelo IDF para cada termo</li>
</ul>


  <pre><code class="language-bash">Doc1: {&#34;clojure&#34;: 0.405, &#34;linguagem&#34;: 0, &#34;funcional&#34;: 1.099}
Doc2: {&#34;clojure&#34;: 0.405, &#34;linguagem&#34;: 0, &#34;lisp&#34;: 1.099}
Doc3: {&#34;python&#34;: 1.099, &#34;linguagem&#34;: 0, &#34;din√¢mica&#34;: 1.099}</code></pre>
 </li>
<li>
<p><strong>Vetoriza√ß√£o</strong></p>
<ul>
<li>Convertemos para vetores usando todos os termos √∫nicos como dimens√µes</li>
<li>Preenchemos com 0 para termos ausentes</li>
</ul>


  <pre><code class="language-bash">Doc1: [0.405, 0, 1.099, 0, 0, 0]
Doc2: [0.405, 0, 0, 1.099, 0, 0]
Doc3: [0, 0, 0, 0, 1.099, 1.099]</code></pre>
 </li>
</ol>
<h5 id="por-que-usar-logaritmo-no-idf">Por que usar logaritmo no IDF?</h5>
<p>O logaritmo no IDF serve para dois prop√≥sitos principais:</p>
<ol>
<li><strong>Suaviza√ß√£o</strong>: Reduz o impacto de termos muito raros ou muito comuns</li>
<li><strong>Escala</strong>: Mant√©m os valores em uma faixa mais gerenci√°vel</li>
</ol>
<p>Por exemplo, sem o logaritmo:</p>
<ul>
<li>Um termo que aparece em 1/1000 documentos teria IDF = 1000</li>
<li>Um termo que aparece em 1/2 documentos teria IDF = 2</li>
</ul>
<p>Com o logaritmo:</p>
<ul>
<li><code>log(1000) ‚âà 6.9</code></li>
<li><code>log(2) ‚âà 0.7</code></li>
</ul>
<h4 id="similaridade-do-cosseno">Similaridade do Cosseno</h4>
<p>A similaridade do cosseno mede o √¢ngulo entre dois vetores TF-IDF. Quanto menor o √¢ngulo, mais similares s√£o os documentos. A f√≥rmula √©:</p>


  <pre><code class="language-bash">cos(Œ∏) = (A¬∑B) / (||A|| ||B||)</code></pre>
 <p>Onde:</p>
<ul>
<li><code>A¬∑B</code> √© o produto escalar dos vetores</li>
<li><code>||A||</code> e <code>||B||</code> s√£o as normas (comprimentos) dos vetores</li>
</ul>
<h4 id="limita√ß√µes-do-tf-idf">Limita√ß√µes do TF-IDF</h4>
<ol>
<li><strong>Sem√¢ntica</strong>: TF-IDF n√£o captura o significado das palavras. Por exemplo:
<ul>
<li>&ldquo;carro&rdquo; e &ldquo;autom√≥vel&rdquo; s√£o tratados como palavras diferentes</li>
<li>&ldquo;bom&rdquo; e &ldquo;ruim&rdquo; s√£o tratados como palavras diferentes</li>
</ul>
</li>
<li><strong>Ordem</strong>: N√£o considera a ordem das palavras
<ul>
<li>&ldquo;gato come rato&rdquo; e &ldquo;rato come gato&rdquo; teriam o mesmo vetor TF-IDF</li>
</ul>
</li>
<li><strong>Contexto</strong>: N√£o captura o contexto das palavras
<ul>
<li>&ldquo;banco&rdquo; (financeiro) e &ldquo;banco&rdquo; (assento) s√£o tratados como a mesma palavra</li>
</ul>
</li>
<li><strong>Dimens√£o</strong>: O vetor final pode ser muito grande (uma dimens√£o para cada termo √∫nico)</li>
</ol>
<blockquote>
<p>Por isso, em sistemas RAG modernos, √© mais comum usar embeddings gerados por modelos de linguagem, que capturam melhor a sem√¢ntica e o contexto das palavras.</p></blockquote>
<h4 id="interface-com-ollama">Interface com Ollama</h4>
<p>Agora vamos implementar a interface com o Ollama. Ele √© respons√°vel por gerar a resposta para a query do usu√°rio (essa parte aqui √© super divertida, pois √© onde vamos usar o LLM).</p>


  <pre><code class="language-clojure">;; src/docai/llm.clj
(ns docai.llm
  (:require [clojure.data.json :as json]
            [org.httpkit.client :as http]))

(def ollama-url &#34;http://localhost:11434/api/generate&#34;)
(def model-name &#34;deepseek-r1&#34;) ; Modelo DeepSeek para melhor qualidade

(defn call-ollama-api
  &#34;Chama a API do Ollama para gerar uma resposta&#34;
  [prompt]
  (let [request-body {:model model-name
                      :prompt prompt
                      :stream false}
        options {:headers {&#34;Content-Type&#34; &#34;application/json&#34;}
                 :body (json/write-str request-body)}
        response @(http/post ollama-url options)]
    (if (= (:status response) 200)
      (-&gt; response
          :body
          (json/read-str :key-fn keyword)
          :response)
      (str &#34;Erro ao chamar a API do Ollama: &#34; (:status response) &#34; - &#34; (:body response)))))

;; Fun√ß√µes de utilidade para uso futuro:
;;
;; extract-code-blocks: Extrai blocos de c√≥digo do texto usando regex
;; exemplo de uso:
;;   (extract-code-blocks &#34;```clojure\n(&#43; 1 2)\n```&#34;) =&gt; [&#34;(&#43; 1 2)&#34;]
;;
;; extract-summary: Cria um resumo de texto com tamanho m√°ximo especificado
;; exemplo de uso:
;;   (extract-summary &#34;# T√≠tulo\nConte√∫do longo...&#34; 50) =&gt; &#34;Conte√∫do longo...&#34;

(defn format-prompt
  &#34;Formata o prompt para o LLM&#34;
  [context query]
  (str &#34;Voc√™ √© um assistente especializado em documenta√ß√£o t√©cnica. Com base no seguinte contexto da documenta√ß√£o:\n\n&#34;
       context
       &#34;\n\nPergunta: &#34; query
       &#34;\n\nForne√ßa uma resposta t√©cnica precisa e, se poss√≠vel, inclua exemplos de c√≥digo. &#34;
       &#34;Se a documenta√ß√£o n√£o contiver informa√ß√µes relevantes para a pergunta, &#34;
       &#34;indique isso claramente e forne√ßa uma resposta geral com base em seu conhecimento.&#34;))

(defn generate-response
  &#34;Gera resposta usando o LLM com base no contexto&#34;
  [query context]
  (try
    (let [prompt (format-prompt context query)]
      (println &#34;DEBUG - Enviando prompt para o Ollama usando o modelo&#34; model-name)
      (call-ollama-api prompt))
    (catch Exception e
      (str &#34;Erro ao gerar resposta: &#34; (.getMessage e) 
           &#34;\n\nPor favor, verifique se o Ollama est√° em execu√ß√£o no endere√ßo &#34; 
           ollama-url 
           &#34;\n\nVoc√™ pode iniciar o Ollama com o comando: ollama serve&#34;))))</code></pre>
 <p>A parte mais importante aqui √© a fun√ß√£o <code>call-ollama-api</code>, que faz uma requisi√ß√£o HTTP para o servidor Ollama rodando na m√°quina local. Ela envia um prompt de texto e recebe de volta a resposta gerada pelo modelo DeepSeek R1. O c√≥digo tamb√©m inclui uma fun√ß√£o <code>format-prompt</code> super importante, que estrutura a mensagem enviada ao modelo. Ela combina o contexto (os trechos de documenta√ß√£o relevantes que encontramos) com a pergunta do usu√°rio, e adiciona instru√ß√µes espec√≠ficas para o modelo se comportar como um assistente t√©cnico. Essa &ldquo;engenharia de prompt&rdquo; √© crucial para obter respostas de qualidade - estamos essencialmente ensinando o modelo a responder no formato que queremos.</p>
<p>A fun√ß√£o <code>generate-response</code> amarra tudo isso, pegando a pergunta e o contexto, formatando o prompt, enviando para o Ollama e tratando poss√≠veis erros. Tem at√© uma mensagem amig√°vel caso o Ollama n√£o esteja rodando, sugerindo como iniciar o servi√ßo. √â um exemplo cl√°ssico de como interfaces com LLMs funcionam: voc√™ prepara um prompt bem estruturado, envia para o modelo, e recebe de volta texto gerado que (esperamos!) responda √† pergunta original com base no contexto fornecido.</p>
<h4 id="m√≥dulo-principal">M√≥dulo Principal</h4>
<p>Agora vamos implementar o m√≥dulo principal que vai ser o ponto de entrada para o RAG. Ele vai ser respons√°vel por carregar os documentos, processar os chunks, criar os embeddings e gerar a resposta para a query do usu√°rio.</p>


  <pre><code class="language-clojure">;; src/docai/core.clj
(ns docai.core
  (:require [docai.document :as doc]
            [docai.embedding :as emb]
            [docai.llm :as llm]
            [clojure.java.io :as io]
            [clojure.string :as str])
  (:gen-class))

(def docs-path &#34;resources/docs&#34;)

(defn load-documentation
  &#34;Carrega todos os arquivos de documenta√ß√£o do diret√≥rio&#34;
  []
  (-&gt;&gt; (file-seq (io/file docs-path))
       (filter #(.isFile %))
       (map #(.getPath %))))

(defn setup-knowledge-base
  &#34;Configura a base de conhecimento inicial&#34;
  []
  (let [doc-files (load-documentation)
        _ (when (empty? doc-files)
            (println &#34;Aviso: Nenhum arquivo de documenta√ß√£o encontrado em resources/docs/&#34;))
        _ (doseq [file doc-files]
            (println &#34;Arquivo encontrado:&#34; file))
        all-chunks (mapcat doc/extract-text doc-files)
        processed-chunks (doc/preprocess-chunks all-chunks)
        _ (println (str &#34;Processando &#34; (count processed-chunks) &#34; chunks de texto...&#34;))
        _ (when (&lt; (count processed-chunks) 5)
            (println &#34;DEBUG - Primeiros chunks:&#34;)
            (doseq [chunk (take 5 processed-chunks)]
              (println (str &#34;Chunk: &#39;&#34; (subs chunk 0 (min 50 (count chunk))) &#34;...&#39;&#34;))))
        embeddings (emb/create-embeddings processed-chunks)]
    {:chunks processed-chunks
     :embeddings embeddings
     :original-files doc-files}))

(defn get-file-content
  &#34;L√™ o conte√∫do completo de um arquivo&#34;
  [file-path]
  (try
    (slurp file-path)
    (catch Exception _
      (println &#34;Erro ao ler arquivo:&#34; file-path)
      &#34;&#34;)))

(defn query-rag
  &#34;Processa uma query usando o pipeline RAG&#34;
  [knowledge-base query]
  (println &#34;DEBUG - Processando query:&#34; query)
  (if (and (seq (:chunks knowledge-base)) 
           (seq (:embeddings knowledge-base)))
    (let [query-emb (first (emb/create-embeddings [query]))
          similar-idxs (emb/similarity-search query-emb 
                                            (:embeddings knowledge-base)
                                            3)
          _ (println &#34;DEBUG - √çndices similares:&#34; similar-idxs)
          
          ;; Obter contexto relevante
          context-chunks (-&gt;&gt; similar-idxs
                              (map #(nth (:chunks knowledge-base) %))
                              (str/join &#34;\n\n&#34;))
          
          ;; Se n√£o houver chunks relevantes, use o conte√∫do original
          context (if (str/blank? context-chunks)
                    (if (seq (:original-files knowledge-base))
                      (get-file-content (first (:original-files knowledge-base)))
                      &#34;N√£o foi poss√≠vel encontrar informa√ß√µes relevantes.&#34;)
                    context-chunks)]
      
      (println &#34;DEBUG - Tamanho do contexto:&#34; (count context) &#34;caracteres&#34;)
      (println &#34;DEBUG - Amostra do contexto:&#34; (subs context 0 (min 200 (count context))) &#34;...&#34;)
      
      ;; Gerar resposta usando o LLM
      (llm/generate-response query context))
    &#34;N√£o foi poss√≠vel encontrar informa√ß√µes relevantes na base de conhecimento.&#34;))

(defn -main
  &#34;Fun√ß√£o principal que inicializa a aplica√ß√£o DocAI&#34;
  [&amp; _]
  (println &#34;Inicializando DocAI...&#34;)
  
  ;; Verificar se o Ollama est√° acess√≠vel
  (println &#34;Para usar o Ollama, certifique-se de que ele est√° em execu√ß√£o com o comando: ollama serve&#34;)
  (println &#34;Usando o modelo deepseek-r1. Se voc√™ ainda n√£o o baixou, execute: ollama pull deepseek-r1&#34;)
  
  (let [kb (setup-knowledge-base)]
    (println &#34;Base de conhecimento pronta! Fa√ßa sua pergunta:&#34;)
    (try
      (loop []
        (when-let [input (read-line)]
          (when-not (= input &#34;sair&#34;)
            (println &#34;Processando...&#34;)
            (println (query-rag kb input))
            (println &#34;\nPr√≥xima pergunta (ou &#39;sair&#39; para terminar):&#34;)
            (recur))))
      (catch Exception e
        (println &#34;Erro: &#34; (.getMessage e))
        (println &#34;Detalhes: &#34; (ex-data e))))
    (println &#34;Obrigado por usar o DocAI. At√© a pr√≥xima!&#34;)))</code></pre>
 <p>Basicamente, quando voc√™ faz uma pergunta, o sistema primeiro transforma sua pergunta em n√∫meros (embeddings) e depois procura nos documentos quais partes s√£o mais parecidas com o que voc√™ perguntou. √â como se ele estivesse destacando os trechos mais relevantes de um livro para responder sua d√∫vida. Voc√™ pode ver isso acontecendo quando ele imprime os &ldquo;√≠ndices similares&rdquo; no console - s√£o as posi√ß√µes dos peda√ßos de texto que ele achou mais √∫teis.</p>
<p>Depois de encontrar os trechos relevantes, o sistema junta tudo em um &ldquo;contexto&rdquo; - que √© basicamente um resumo das informa√ß√µes importantes. Se ele n√£o achar nada parecido com sua pergunta, ele tenta usar o documento inteiro ou avisa que n√£o tem informa√ß√£o suficiente. D√° para ver que ele √© bem transparente, mostrando no console o tamanho do contexto e at√© uma amostra do que encontrou, para voc√™ entender o que est√° acontecendo nos bastidores.</p>
<p>Por fim, ele passa sua pergunta original junto com o contexto encontrado para o modelo de linguagem (LLM) gerar uma resposta personalizada. √â como dar a um especialista tanto a sua pergunta quanto as p√°ginas relevantes de um manual t√©cnico - assim ele pode dar uma resposta muito mais precisa e fundamentada. Todo esse processo acontece em segundos, permitindo que voc√™ tenha uma conversa fluida com seus documentos, como se estivesse conversando com algu√©m que leu tudo e est√° pronto para responder suas d√∫vidas espec√≠ficas.</p>
<hr>
<h2 id="como-usar">Como Usar</h2>
<p>Abaixo um guia para voc√™ instalar e usar o DocAI (e ver o processo em a√ß√£o).</p>
<h3 id="instala√ß√£o-do-ollama">Instala√ß√£o do Ollama</h3>
<ol>
<li>
<p><strong>Instala√ß√£o</strong>:</p>
<ul>
<li><strong>Windows</strong>: Baixe o instalador do <a href="https://ollama.com/download">site oficial do Ollama</a> e execute-o</li>
<li><strong>Linux</strong>: Execute o comando:


  <pre><code class="language-bash">curl https://ollama.ai/install.sh | sh</code></pre>
 </li>
<li><strong>macOS</strong>: Use o Homebrew:


  <pre><code class="language-bash">brew install ollama</code></pre>
 </li>
</ul>
</li>
<li>
<p><strong>Iniciando o Servidor</strong>:</p>


  <pre><code class="language-bash">ollama serve</code></pre>
 </li>
<li>
<p><strong>Baixando o Modelo</strong>:</p>


  <pre><code class="language-bash">ollama pull deepseek-r1</code></pre>
 </li>
<li>
<p><strong>Verificando a Instala√ß√£o</strong>:</p>
<ul>
<li>Execute um teste simples:


  <pre><code class="language-bash">ollama run deepseek-r1 &#34;Ol√°! Como voc√™ est√°?&#34;</code></pre>
 </li>
<li>Se tudo estiver funcionando, voc√™ receber√° uma resposta do modelo</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>Dica</strong>: O Ollama mant√©m os modelos em cache local. Se voc√™ precisar liberar espa√ßo, pode usar <code>ollama rm deepseek-r1</code> para remover o modelo.</p></blockquote>
<h3 id="executando-a-aplica√ß√£o">Executando a Aplica√ß√£o</h3>
<ol>
<li>Coloque seus documentos na pasta <code>resources/docs/</code> (j√° inclu√≠mos dois exemplos: <code>example.md</code>)</li>
<li>Execute o projeto:</li>
</ol>


  <pre><code class="language-bash">lein run</code></pre>
 <ol start="3">
<li>Fa√ßa suas perguntas! Exemplo:</li>
</ol>


  <pre><code class="language-bash">Como implementar autentica√ß√£o JWT em Clojure?
Como implementar auth saml em python?
Como integrar o auth0 em uma aplica√ß√£o Clojure?
etc...</code></pre>
 <p>O DocAI processa sua pergunta em v√°rias etapas:</p>


  
  <div class="mermaid">flowchart TD
    A[In√≠cio] --&gt; B[Carrega Documenta√ß√£o]
    B --&gt; C[Processa Documentos]
    C --&gt; D[Gera Embeddings]
    D --&gt; E[Base de Conhecimento]
    
    F[Consulta do Usu√°rio] --&gt; G[Processa Consulta]
    G --&gt; H[Gera Embedding da Consulta]
    H --&gt; I[Busca Similaridade]
    I --&gt; J[Seleciona Chunks Relevantes]
    J --&gt; K[Combina Contexto]
    K --&gt; L[Gera Prompt]
    L --&gt; M[LLM DeepSeek R1]
    M --&gt; N[Resposta Final]
    
    E --&gt; I
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style N fill:#9ff,stroke:#333,stroke-width:2px</div>
 <ol>
<li><strong>Processamento da Consulta</strong>: A pergunta √© convertida em um vetor TF-IDF</li>
<li><strong>Busca por Similaridade</strong>: O sistema encontra os chunks mais relevantes</li>
<li><strong>Gera√ß√£o de Contexto</strong>: Os chunks s√£o combinados em um contexto coeso</li>
<li><strong>Gera√ß√£o de Resposta</strong>: O LLM gera uma resposta baseada no contexto</li>
</ol>
<p>Voc√™ pode ver o processo em a√ß√£o nos logs:</p>


  <pre><code class="language-bash">DEBUG - Processando query: Como implementar autentica√ß√£o JWT em Clojure?
DEBUG - √çndices similares: [2, 5, 8]
DEBUG - Tamanho do contexto: 1234 caracteres
DEBUG - Amostra do contexto: &#34;Para implementar autentica√ß√£o JWT em Clojure...&#34;</code></pre>
 <blockquote>
<p><strong>NOTA:</strong> A prop√≥sito, o projeto docai est√° dispon√≠vel no <a href="https://github.com/scovl/docai">https://github.com/scovl/docai</a> caso voc√™ queira contribuir com o projeto ou usar em outro projeto.</p></blockquote>
<hr>
<h2 id="considera√ß√µes-t√©cnicas">Considera√ß√µes T√©cnicas</h2>
<h3 id="performance-e-otimiza√ß√µes">Performance e Otimiza√ß√µes</h3>
<p>Nossa implementa√ß√£o atual oferece uma base funcional, mas pode ser significativamente otimizada em termos de performance atrav√©s da ado√ß√£o de bancos de dados vetoriais como <a href="https://milvus.io/">Milvus</a> ou <a href="https://github.com/facebookresearch/faiss">FAISS</a>, implementa√ß√£o de cache de embeddings e paraleliza√ß√£o do processamento de chunks, permitindo consultas mais r√°pidas mesmo com grandes volumes de dados.</p>
<p>Para lidar com documenta√ß√µes extensas, recomendo estrat√©gias espec√≠ficas de gerenciamento de mem√≥ria, como o processamento de chunks em lotes menores, implementa√ß√£o de indexa√ß√£o incremental que constr√≥i a base de conhecimento gradualmente, e utiliza√ß√£o de t√©cnicas de streaming para processar arquivos grandes sem sobrecarregar a mem√≥ria dispon√≠vel.</p>
<p>Quanto √† escolha de modelos no ecossistema Ollama, cada um apresenta caracter√≠sticas distintas que podem ser exploradas conforme a necessidade: o <a href="https://ollama.com/models/deepseek-r1">DeepSeek R1</a> destaca-se na compreens√£o geral e gera√ß√£o de texto, o <a href="https://ollama.com/models/deepseek-coder">DeepSeek Coder</a> √© especializado em c√≥digo, o <a href="https://ollama.com/models/llama3">Llama 3</a> serve como excelente alternativa geral, o <a href="https://ollama.com/models/mistral">Mistral</a> demonstra efic√°cia em tarefas espec√≠ficas, enquanto o <a href="https://ollama.com/models/gemma">Gemma</a> oferece uma solu√ß√£o leve e eficiente para ambientes com recursos limitados. Outra quest√£o importante √© como estou tratando os erros. O sistema implementa v√°rias camadas de tratamento de erros para lidar com diferentes cen√°rios:</p>
<ol>
<li>
<p><strong>Ollama Offline</strong></p>
<ul>
<li><strong>Sintoma</strong>: O sistema n√£o consegue se conectar ao servidor Ollama</li>
<li><strong>Tratamento</strong>: O c√≥digo verifica a disponibilidade do servidor e fornece mensagens claras de erro:</li>
</ul>


  <pre><code class="language-clojure">(catch Exception e
  (str &#34;Erro ao gerar resposta: &#34; (.getMessage e) 
       &#34;\n\nPor favor, verifique se o Ollama est√° em execu√ß√£o no endere√ßo &#34; 
       ollama-url 
       &#34;\n\nVoc√™ pode iniciar o Ollama com o comando: ollama serve&#34;))</code></pre>
 </li>
<li>
<p><strong>Documenta√ß√£o Muito Grande</strong></p>
<ul>
<li><strong>Sintoma</strong>: Arquivos de documenta√ß√£o que excedem a mem√≥ria dispon√≠vel</li>
<li><strong>Tratamento</strong>: O sistema implementa:
<ul>
<li>Chunking de documentos (512 tokens por chunk)</li>
<li>Processamento em lotes</li>
<li>Logs de progresso para monitoramento</li>
</ul>
</li>
</ul>


  <pre><code class="language-clojure">(let [content (slurp doc-path)
      chunks (partition-all 512 text)]
  (println &#34;Quantidade de chunks gerados:&#34; (count chunks)))</code></pre>
 </li>
<li>
<p><strong>Consultas sem Rela√ß√£o com a Documenta√ß√£o</strong></p>
<ul>
<li><strong>Sintoma</strong>: Nenhum chunk relevante √© encontrado para a consulta</li>
<li><strong>Tratamento</strong>: O sistema:
<ul>
<li>Verifica se h√° chunks dispon√≠veis</li>
<li>Usa fallback para conte√∫do original se necess√°rio</li>
<li>Fornece feedback claro ao usu√°rio</li>
</ul>
</li>
</ul>


  <pre><code class="language-clojure">(if (str/blank? context-chunks)
  (if (seq (:original-files knowledge-base))
    (get-file-content (first (:original-files knowledge-base)))
    &#34;N√£o foi poss√≠vel encontrar informa√ß√µes relevantes.&#34;)
  context-chunks)</code></pre>
 </li>
<li>
<p><strong>Melhorias Futuras</strong> - Implementar <a href="https://en.wikipedia.org/wiki/Exponential_backoff">retry com backoff exponencial</a> para falhas de conex√£o, adicionar <a href="https://en.wikipedia.org/wiki/Cache_%28computing%29">cache de embeddings</a> para melhor performance, implementar <a href="https://en.wikipedia.org/wiki/Streaming_media">streaming</a> para arquivos muito grandes, adicionar <a href="https://en.wikipedia.org/wiki/Document_validation">valida√ß√£o de formato de documentos</a> e implementar <a href="https://en.wikipedia.org/wiki/Rate_limiting">rate limiting</a> para evitar sobrecarga do Ollama.</p>
</li>
</ol>
<hr>
<h3 id="prompt-engineering">Prompt Engineering</h3>
<p>O Prompt Engineering √© uma habilidade crucial para obter bons resultados com LLMs. Um prompt bem estruturado pode fazer a diferen√ßa entre uma resposta vaga e uma resposta precisa e √∫til.</p>
<h4 id="estrutura-do-prompt">Estrutura do Prompt</h4>


  <pre><code class="language-clojure">(defn format-prompt
  &#34;Formata o prompt para o LLM&#34;
  [context query]
  (str &#34;Voc√™ √© um assistente especializado em documenta√ß√£o t√©cnica. &#34;
       &#34;Com base no seguinte contexto da documenta√ß√£o:\n\n&#34;
       context
       &#34;\n\nPergunta: &#34; query
       &#34;\n\nForne√ßa uma resposta t√©cnica precisa e, se poss√≠vel, &#34;
       &#34;inclua exemplos de c√≥digo. Se a documenta√ß√£o n√£o contiver &#34;
       &#34;informa√ß√µes relevantes para a pergunta, indique isso claramente &#34;
       &#34;e forne√ßa uma resposta geral com base em seu conhecimento.&#34;))</code></pre>
 <p>O c√≥digo acima define uma fun√ß√£o <code>format-prompt</code> que estrutura a comunica√ß√£o com o modelo de linguagem. Esta fun√ß√£o recebe dois par√¢metros principais: o <code>context</code>, que cont√©m os trechos relevantes da documenta√ß√£o recuperados pelo sistema RAG, e a <code>query</code>, que √© a pergunta do usu√°rio. A fun√ß√£o combina esses elementos em um prompt estruturado que orienta o comportamento do LLM.</p>
<p>A estrutura do prompt √© cuidadosamente projetada com v√°rios elementos estrat√©gicos: primeiro, define o papel do modelo como &ldquo;assistente especializado em documenta√ß√£o t√©cnica&rdquo;, estabelecendo o tom e a expectativa; em seguida, fornece o contexto extra√≠do da documenta√ß√£o para que o modelo tenha as informa√ß√µes necess√°rias; depois, apresenta claramente a pergunta do usu√°rio; e finalmente, inclui instru√ß√µes espec√≠ficas sobre como o modelo deve responder, incentivando respostas t√©cnicas precisas com exemplos de c√≥digo quando apropriado, al√©m de orientar como proceder quando a documenta√ß√£o n√£o cont√©m informa√ß√µes relevantes.</p>
<p>O prompt engineering utiliza diversas t√©cnicas para melhorar as respostas dos LLMs, incluindo: <a href="https://www.promptingguide.ai/techniques/role-prompting"><strong>Role Prompting</strong></a>, que define um papel espec√≠fico para o modelo (como &ldquo;Voc√™ √© um especialista em&hellip;&rdquo;); <a href="https://www.promptingguide.ai/techniques/few-shot"><strong>Few-shot Learning</strong></a>, que fornece exemplos de entradas e sa√≠das desejadas; <a href="https://www.promptingguide.ai/techniques/chain-of-thought"><strong>Chain of Thought</strong></a>, que solicita ao modelo explicar seu racioc√≠nio passo a passo; <a href="https://www.promptingguide.ai/techniques/format-specification"><strong>Format Specification</strong></a>, que especifica o formato exato desejado para a resposta; e <a href="https://www.promptingguide.ai/techniques/constraints"><strong>Constraints</strong></a>, que estabelece limites e requisitos espec√≠ficos que a resposta deve seguir.</p>
<h4 id="exemplo-de-prompt-avan√ßado">Exemplo de Prompt Avan√ßado</h4>
<p>Este c√≥digo implementa uma vers√£o avan√ßada de formata√ß√£o de prompt para o LLM, criando uma estrutura mais detalhada e direcionada. Ele fornece um conjunto de diretrizes numeradas que orientam o comportamento do modelo, incluindo requisitos para precis√£o t√©cnica, exemplos de c√≥digo, cita√ß√µes da documenta√ß√£o, transpar√™ncia sobre limita√ß√µes de conhecimento, concis√£o e uso de formata√ß√£o Markdown.</p>
<p>A estrutura deste prompt segue princ√≠pios de engenharia de prompts mais sofisticados, incorporando t√©cnicas como role prompting (defini√ß√£o clara do papel do modelo), constraint engineering (estabelecimento de diretrizes espec√≠ficas) e format specification (solicita√ß√£o de formata√ß√£o Markdown). Esta abordagem mais estruturada ajuda a obter respostas mais consistentes, informativas e bem formatadas do modelo, especialmente para consultas t√©cnicas complexas relacionadas √† documenta√ß√£o de Clojure.</p>


  <pre><code class="language-clojure">(defn format-advanced-prompt
  &#34;Formata um prompt mais sofisticado para o LLM&#34;
  [context query]
  (str &#34;Voc√™ √© um especialista em documenta√ß√£o t√©cnica de software, &#34;
       &#34;com foco em Clojure e desenvolvimento web.\n\n&#34;
       &#34;Contexto da documenta√ß√£o:\n&#34;
       context
       &#34;\n\nPergunta: &#34; query
       &#34;\n\nPor favor, siga estas diretrizes:\n&#34;
       &#34;1. Seja preciso e t√©cnico\n&#34;
       &#34;2. Inclua exemplos de c√≥digo quando relevante\n&#34;
       &#34;3. Cite as partes da documenta√ß√£o que voc√™ est√° usando\n&#34;
       &#34;4. Se a informa√ß√£o n√£o estiver na documenta√ß√£o, indique claramente\n&#34;
       &#34;5. Mantenha a resposta concisa mas completa\n&#34;
       &#34;6. Use formata√ß√£o Markdown para melhor legibilidade&#34;))</code></pre>
 <p>Para criar prompts efetivos, √© essencial ser espec√≠fico e claro nas instru√ß√µes, utilizar formata√ß√£o adequada para melhorar a legibilidade e incluir exemplos ilustrativos sempre que poss√≠vel. Tamb√©m √© recomend√°vel definir limites e restri√ß√µes claras, solicitar ao modelo que explique seu racioc√≠nio e utilizar um processo iterativo para refinar continuamente o prompt at√© obter os resultados desejados.</p>
<p>A avalia√ß√£o sistem√°tica de prompts envolve testar diferentes varia√ß√µes da mesma instru√ß√£o e comparar as respostas para identificar qual estrutura produz os melhores resultados. Este processo deve incluir a coleta de feedback dos usu√°rios finais e a manuten√ß√£o de um registro detalhado dos prompts que demonstraram bom desempenho, permitindo assim o desenvolvimento de um conjunto de melhores pr√°ticas espec√≠ficas para cada caso de uso.</p>
<blockquote>
<p><strong>Nota</strong>: O <a href="https://www.promptingguide.ai/">Prompt Engineering</a> √© uma √°rea em constante evolu√ß√£o. Novas t√©cnicas e melhores pr√°ticas surgem regularmente √† medida que os modelos evoluem.</p></blockquote>
<h2 id="pr√≥ximos-passos">Pr√≥ximos Passos</h2>
<p>Abaixo uma lista de melhorias que podem ser feitas no projeto atual.</p>
<h3 id="melhorias-propostas">Melhorias Propostas</h3>


  
  <div class="mermaid">mindmap
  root((Melhorias))
    Tokeniza√ß√£o
      BPE
      WordPiece
      Tokenizador do Modelo
    Embeddings
      Pr√©-treinados
      Via Ollama
      Cache
    Banco de Dados
      Milvus
      FAISS
      Qdrant
    Cache
      Embeddings
      Respostas
    Erros
      Conex√£o
      Modelo
      Rede
    Logging
      Framework
      Rastreamento
    Testes
      Unit√°rios
      Integra√ß√£o
    Prompt
      Few-shot
      Chain-of-thought
      Formato
    Langchain4j
      Abstra√ß√£o
      Integra√ß√£o</div>
 <p>Olha, d√° pra turbinar esse nosso RAG de v√°rias formas! Primeiro, a gente poderia melhorar a tokeniza√ß√£o usando aqueles m√©todos mais avan√ßados tipo <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding">BPE</a> ou <a href="https://en.wikipedia.org/wiki/WordPiece">WordPiece</a> - idealmente o mesmo que o modelo usa. E os embeddings? Seria muito mais eficiente pegar direto do Ollama em vez de fazer na m√£o. A diferen√ßa na busca sem√¢ntica seria absurda!</p>
<p>Quando o projeto crescer, vai ser essencial ter um banco de dados vetorial decente. Imagina lidar com milhares de documentos usando nossa implementa√ß√£o atual? Seria um pesadelo! <a href="https://milvus.io/">Milvus</a>, <a href="https://github.com/facebookresearch/faiss">FAISS</a> ou <a href="https://qdrant.tech/">Qdrant</a> resolveriam isso numa boa. E n√£o podemos esquecer do cache - tanto para embeddings quanto para respostas. Economiza um temp√£o e reduz a carga no sistema.</p>
<p>A parte de tratamento de erros e logging tamb√©m precisa de carinho. J√° pensou o usu√°rio esperando resposta e o Ollama t√° offline? Ou um arquivo corrompido? Precisamos de mensagens amig√°veis e um sistema de logging decente pra rastrear problemas. E claro, testes! Sem testes unit√°rios e de integra√ß√£o, qualquer mudan√ßa vira uma roleta-russa.</p>
<p>O prompt engineering √© outro ponto crucial - d√° pra refinar bastante o formato atual. Poder√≠amos experimentar com exemplos no prompt (few-shot), instru√ß√µes passo a passo (chain-of-thought), e ser mais espec√≠fico sobre o formato da resposta. Ah, e uma alternativa interessante seria usar o langchain4j via interop com Java. Ele j√° tem um monte de abstra√ß√µes prontas que economizariam muito c√≥digo!</p>
<h3 id="usando-langchain4j">Usando Langchain4j</h3>
<p>Langchain4j √© uma biblioteca Java que oferece uma abstra√ß√£o de alto n√≠vel para construir aplica√ß√µes de IA generativa, incluindo sistemas RAG. Ela se integra bem com Clojure atrav√©s da interoperabilidade Java.</p>
<p>Vantagens de usar Langchain4j:</p>
<ul>
<li><strong>Abstra√ß√£o</strong>: Fornece componentes pr√©-constru√≠dos para tarefas comuns</li>
<li><strong>Modularidade</strong>: Permite trocar implementa√ß√µes facilmente</li>
<li><strong>Integra√ß√£o</strong>: Oferece integra√ß√µes com v√°rias ferramentas e servi√ßos</li>
<li><strong>Comunidade e Suporte</strong>: Possui uma comunidade ativa e boa documenta√ß√£o</li>
</ul>
<blockquote>
<p>Em um pr√≥ximo artigo, escreverei sobre como usar <a href="https://github.com/langchain4j/langchain4j">Langchain4j</a> para criar um sistema RAG ainda neste mesmo projeto.</p></blockquote>
<h2 id="refer√™ncias">Refer√™ncias</h2>
<ul>
<li><a href="https://www.pinecone.io/learn/rag/">RAG</a> - Documenta√ß√£o do Pinecone</li>
<li><a href="https://www.pinecone.io/learn/embeddings/">Embedding</a> - Documenta√ß√£o do Pinecone</li>
<li><a href="https://www.pinecone.io/learn/llms/">LLM</a> - Documenta√ß√£o do Pinecone</li>
<li><a href="https://ollama.com/">Ollama</a> - Ferramenta para rodar LLMs localmente</li>
<li><a href="https://clojure.org/">Clojure</a> - Documenta√ß√£o do Clojure</li>
<li><a href="https://github.com/http-kit/http-kit">http-kit</a> - Cliente HTTP para Clojure</li>
<li><a href="https://github.com/clojure/data.json">data.json</a> - Biblioteca JSON para Clojure</li>
<li><a href="https://clojure.github.io/clojure/clojure.test-api.html">clojure.test</a> - Documenta√ß√£o da biblioteca de testes do Clojure</li>
<li><a href="https://github.com/clj-kondo/clj-kondo">clj-kondo</a> - Linter para Clojure</li>
</ul>

    </div>
    
    
    <div class="post-comments">
        <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "scovl" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    </div>
    
</article>

        </main>
        
        
        
        <footer class="site-footer">
    <div class="footer-inner">
        <div class="footer-content">
            <div class="copyright">
                &copy; 2025 Vitor Lobo
            </div>
            
            <div class="social-links">
                
                <a href="https://github.com/scovl" target="_blank" rel="noopener noreferrer" class="social-link">
                    GitHub
                </a>
                
                
                
                <a href="https://linkedin.com/in/vitor-lobo" target="_blank" rel="noopener noreferrer" class="social-link">
                    LinkedIn
                </a>
                
                
                
                <a href="mailto:lobocode@gmail.com" class="social-link">
                    Email
                </a>
                

                
                <a href="https://hachyderm.io/@lobocode" target="_blank" rel="noopener noreferrer" class="social-link">
                    Mastodon
                </a>
                

                
                <a href="https://scovl.github.io/index.xml" target="_blank" rel="noopener noreferrer" class="social-link">
                    RSS
                </a>
                

            </div>
        </div>
    </div>
</footer>


<script src="/js/code-escaper.js"></script>


<script>
  document.addEventListener("DOMContentLoaded", function() {
    
    if (typeof Prism !== 'undefined') {
      Prism.highlightAll();
    }
  });
</script>

</body>
</html> 
        
    </div>
    
    
    <script src="/js/main.js"></script>
    
    
    
</body>
</html> 