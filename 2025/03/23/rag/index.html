<!DOCTYPE html>
<html lang="pt">
<head>
    <title>01 - RAG Simples com Clojure e Ollama | scovl</title>
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Um protótipo funcional do zero">



<link rel="preload" href="/vendor/fonts/inter/Inter-400.ttf" as="font" type="font/ttf" crossorigin>
<link rel="preload" href="/vendor/fonts/inter/Inter-600.ttf" as="font" type="font/ttf" crossorigin>
<link rel="preload" href="/vendor/fonts/jetbrains-mono/JetBrainsMono-400.ttf" as="font" type="font/ttf" crossorigin>



<link rel="dns-prefetch" href="//giscus.app">
<link rel="preconnect" href="//giscus.app">



<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="format-detection" content="telephone=no"> 


<link rel="stylesheet" href="/css/main.css?v=1757193440">


<link rel="stylesheet" href="/vendor/fonts/fonts.css?v=1757193440">


<link rel="preload" href="/fonts/abril-fatface.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/rokkitt.woff2" as="font" type="font/woff2" crossorigin>


<link rel="stylesheet" href="/vendor/prism/prism-tomorrow.min.css?v=1757193440">



<script src="/vendor/mermaid/mermaid.min.js?v=1757193440"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        mermaid.initialize({
            startOnLoad: true,
            theme: 'light',
            align: 'center'
        });SS
    });
</script>












<script>

const I18n = {
    currentLang: 'pt',
    isRTL:  false ,
    
    
    formatDate(date, options = {}) {
        const defaultOptions = {
            year: 'numeric',
            month: 'long',
            day: 'numeric'
        };
        
        const locale = this.getLocale();
        const finalOptions = { ...defaultOptions, ...options };
        
        return new Intl.DateTimeFormat(locale, finalOptions).format(date);
    },
    
    
    formatNumber(number, options = {}) {
        const defaultOptions = {
            style: 'decimal',
            minimumFractionDigits: 0,
            maximumFractionDigits: 2
        };
        
        const locale = this.getLocale();
        const finalOptions = { ...defaultOptions, ...options };
        
        return new Intl.NumberFormat(locale, finalOptions).format(number);
    },
    
    
    formatCurrency(amount, currency = 'USD') {
        const locale = this.getLocale();
        return new Intl.NumberFormat(locale, {
            style: 'currency',
            currency: currency
        }).format(amount);
    },
    
    
    formatRelativeTime(date) {
        const locale = this.getLocale();
        const now = new Date();
        const diff = now - date;
        const diffInMinutes = Math.floor(diff / (1000 * 60));
        const diffInHours = Math.floor(diff / (1000 * 60 * 60));
        const diffInDays = Math.floor(diff / (1000 * 60 * 60 * 24));
        
        if (diffInMinutes < 1) {
            return new Intl.RelativeTimeFormat(locale).format(0, 'minute');
        } else if (diffInMinutes < 60) {
            return new Intl.RelativeTimeFormat(locale).format(-diffInMinutes, 'minute');
        } else if (diffInHours < 24) {
            return new Intl.RelativeTimeFormat(locale).format(-diffInHours, 'hour');
        } else {
            return new Intl.RelativeTimeFormat(locale).format(-diffInDays, 'day');
        }
    },
    
    
    getLocale() {
        const localeMap = {
            'en': 'en-US',
            'pt': 'pt-BR',
            'es': 'es-ES',
            'fr': 'fr-FR',
            'de': 'de-DE',
            'it': 'it-IT',
            'ar': 'ar-SA',
            'he': 'he-IL',
            'fa': 'fa-IR',
            'ur': 'ur-PK',
            'zh': 'zh-CN',
            'ja': 'ja-JP',
            'ko': 'ko-KR'
        };
        
        return localeMap[this.currentLang] || 'en-US';
    },
    
    
    t(key, params = {}) {
        const translations = {
            'en': {
                'read_more': 'Read more',
                'back_to_top': 'Back to top',
                'loading': 'Loading...',
                'error': 'Error',
                'success': 'Success',
                'warning': 'Warning',
                'info': 'Information',
                'comments': 'Comments',
                'related_posts': 'Related Posts',
                'tags': 'Tags',
                'categories': 'Categories',
                'search': 'Search',
                'menu': 'Menu',
                'close': 'Close',
                'language': 'Language',
                'theme': 'Theme',
                'dark_mode': 'Dark Mode',
                'light_mode': 'Light Mode'
            },
            'pt': {
                'read_more': 'Ler mais',
                'back_to_top': 'Voltar ao topo',
                'loading': 'Carregando...',
                'error': 'Erro',
                'success': 'Sucesso',
                'warning': 'Aviso',
                'info': 'Informação',
                'comments': 'Comentários',
                'related_posts': 'Posts Relacionados',
                'tags': 'Tags',
                'categories': 'Categorias',
                'search': 'Pesquisar',
                'menu': 'Menu',
                'close': 'Fechar',
                'language': 'Idioma',
                'theme': 'Tema',
                'dark_mode': 'Modo Escuro',
                'light_mode': 'Modo Claro'
            },
            'es': {
                'read_more': 'Leer más',
                'back_to_top': 'Volver arriba',
                'loading': 'Cargando...',
                'error': 'Error',
                'success': 'Éxito',
                'warning': 'Advertencia',
                'info': 'Información',
                'comments': 'Comentarios',
                'related_posts': 'Posts Relacionados',
                'tags': 'Etiquetas',
                'categories': 'Categorías',
                'search': 'Buscar',
                'menu': 'Menú',
                'close': 'Cerrar',
                'language': 'Idioma',
                'theme': 'Tema',
                'dark_mode': 'Modo Oscuro',
                'light_mode': 'Modo Claro'
            },
            'ar': {
                'read_more': 'اقرأ المزيد',
                'back_to_top': 'العودة إلى الأعلى',
                'loading': 'جاري التحميل...',
                'error': 'خطأ',
                'success': 'نجح',
                'warning': 'تحذير',
                'info': 'معلومات',
                'comments': 'التعليقات',
                'related_posts': 'المقالات ذات الصلة',
                'tags': 'العلامات',
                'categories': 'الفئات',
                'search': 'بحث',
                'menu': 'القائمة',
                'close': 'إغلاق',
                'language': 'اللغة',
                'theme': 'المظهر',
                'dark_mode': 'الوضع المظلم',
                'light_mode': 'الوضع الفاتح'
            }
        };
        
        const langTranslations = translations[this.currentLang] || translations['en'];
        let text = langTranslations[key] || key;
        
        
        Object.keys(params).forEach(param => {
            text = text.replace(`{${param}}`, params[param]);
        });
        
        return text;
    },
    
    
    init() {
        this.updatePageDirection();
        this.updateDateFormats();
        this.updateNumberFormats();
        this.updateTranslations();
    },
    
    
    updatePageDirection() {
        if (this.isRTL) {
            document.documentElement.setAttribute('dir', 'rtl');
            document.documentElement.setAttribute('lang', this.currentLang);
        }
    },
    
    
    updateDateFormats() {
        const dateElements = document.querySelectorAll('[data-date]');
        dateElements.forEach(element => {
            const date = new Date(element.getAttribute('data-date'));
            const format = element.getAttribute('data-date-format') || 'default';
            
            let formattedDate;
            switch (format) {
                case 'relative':
                    formattedDate = this.formatRelativeTime(date);
                    break;
                case 'short':
                    formattedDate = this.formatDate(date, { month: 'short', day: 'numeric' });
                    break;
                case 'long':
                    formattedDate = this.formatDate(date, { 
                        weekday: 'long',
                        year: 'numeric',
                        month: 'long',
                        day: 'numeric'
                    });
                    break;
                default:
                    formattedDate = this.formatDate(date);
            }
            
            element.textContent = formattedDate;
        });
    },
    
    
    updateNumberFormats() {
        const numberElements = document.querySelectorAll('[data-number]');
        numberElements.forEach(element => {
            const number = parseFloat(element.getAttribute('data-number'));
            const format = element.getAttribute('data-number-format') || 'decimal';
            
            let formattedNumber;
            switch (format) {
                case 'currency':
                    const currency = element.getAttribute('data-currency') || 'USD';
                    formattedNumber = this.formatCurrency(number, currency);
                    break;
                case 'percent':
                    formattedNumber = this.formatNumber(number / 100, { style: 'percent' });
                    break;
                default:
                    formattedNumber = this.formatNumber(number);
            }
            
            element.textContent = formattedNumber;
        });
    },
    
    
    updateTranslations() {
        const translationElements = document.querySelectorAll('[data-i18n]');
        translationElements.forEach(element => {
            const key = element.getAttribute('data-i18n');
            const params = {};
            
            
            const paramAttributes = element.getAttribute('data-i18n-params');
            if (paramAttributes) {
                try {
                    Object.assign(params, JSON.parse(paramAttributes));
                } catch (e) {
                    console.warn('Invalid i18n params:', paramAttributes);
                }
            }
            
            element.textContent = this.t(key, params);
        });
    }
};


window.I18n = I18n;
</script>


<script>
    if ('serviceWorker' in navigator) {
        window.addEventListener('load', function() {
            navigator.serviceWorker.register('\/sw.js')
                .then(function(registration) {
                    console.log('Service Worker registrado com sucesso:', registration.scope);
                })
                .catch(function(error) {
                    console.log('Falha no registro do Service Worker:', error);
                });
        });
    }
</script> 
</head>
<body>
    
    
    <header class="header">
    <div class="container">
        <div class="header-content">
            <a href="https://scovl.github.io/" class="site-title">scovl</a>
            
            <div class="header-actions">
                
                <nav class="nav-menu">
                    <ul>
                        
                        <li><a href="/page/about/">About</a></li>
                        
                        <li><a href="/page/contact/">Contact</a></li>
                        
                    </ul>
                </nav>
                
                
                



<div class="language-switcher" id="language-switcher">
    <button class="language-btn" onclick="toggleLanguageMenu()">
        <span class="current-lang">Português</span>
        <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
            <polyline points="6,9 12,15 18,9"></polyline>
        </svg>
    </button>
    <div class="language-menu" id="language-menu">
        
            
                <a href="https://scovl.github.io/" class="language-option active">
                    Português
                </a>
            
        
            
                <a href="https://scovl.github.io/en/" class="language-option ">
                    English
                </a>
            
        
    </div>
</div>
                
                
                <button id="dark-mode-toggle" class="theme-toggle" aria-label="Toggle dark mode">
                    <svg class="sun-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <line x1="12" y1="1" x2="12" y2="3"/>
                        <line x1="12" y1="21" x2="12" y2="23"/>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/>
                        <line x1="1" y1="12" x2="3" y2="12"/>
                        <line x1="21" y1="12" x2="23" y2="12"/>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/>
                    </svg>
                    <svg class="moon-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </div>
    </div>
</header> 
    
    
    
    <main>
        <div class="container">
            
<article class="post">
    <header class="post-header">
        <h1 class="post-title">01 - RAG Simples com Clojure e Ollama</h1>
        
        <div class="post-meta">
            <time datetime="2025-03-23T19:00:00Z">
                📅 23/03/2025
            </time>
            
            <span>👤 Vitor Lobo Ramos</span>
            
            
            
            <div class="post-tags">
                
                <a href="/tags/rag" class="tag">RAG</a>
                
                <a href="/tags/llm" class="tag">LLM</a>
                
                <a href="/tags/ai" class="tag">AI</a>
                
                <a href="/tags/langchain" class="tag">Langchain</a>
                
            </div>
            
        </div>
        
    </header>
    
    
    















  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  



<aside class="toc" id="toc" aria-labelledby="toc-title">
    <div class="toc-container">
        <div class="toc-header">
            <h3 id="toc-title" class="toc-title">
                <svg class="toc-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <path d="M3 6h18M3 12h18M3 18h18"/>
                </svg>
                Sumário
            </h3>
            <button class="toc-toggle" id="toc-toggle" aria-expanded="true" aria-controls="toc-content" aria-label="Mostrar/Ocultar Sumário">
                <svg class="toc-toggle-icon" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <path d="m6 9 6 6 6-6"/>
                </svg>
            </button>
        </div>
        <div class="toc-content" id="toc-content">
            <div class="toc-progress">
                <div class="toc-progress-bar" id="toc-progress-bar"></div>
            </div>
            <nav class="toc-nav">
                <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#introdução">Introdução</a></li>
        <li><a href="#fundamentos-do-rag">Fundamentos do RAG</a>
          <ul>
            <li><a href="#o-que-é-rag">O que é RAG?</a></li>
            <li><a href="#por-que-precisamos-do-rag">Por que precisamos do RAG?</a></li>
            <li><a href="#os-três-pilares-do-rag">Os Três Pilares do RAG</a></li>
            <li><a href="#rag-em-produção">RAG em Produção</a></li>
            <li><a href="#por-que-o-deepseek-r1">Por que o DeepSeek R1?</a></li>
          </ul>
        </li>
        <li><a href="#implementação-prática">Implementação Prática</a>
          <ul>
            <li><a href="#preparando-o-ambiente">Preparando o Ambiente</a></li>
            <li><a href="#estrutura-do-projeto">Estrutura do Projeto</a></li>
            <li><a href="#tf-idf">TF-IDF</a></li>
            <li><a href="#requisitos-mínimos">Requisitos Mínimos</a></li>
            <li><a href="#configuração-do-projeto">Configuração do Projeto</a></li>
            <li><a href="#implementação-dos-componentes">Implementação dos Componentes</a>
              <ul>
                <li><a href="#processamento-de-documentos">Processamento de Documentos</a></li>
                <li><a href="#sistema-de-embeddings">Sistema de Embeddings</a></li>
                <li><a href="#interface-com-ollama">Interface com Ollama</a></li>
                <li><a href="#módulo-principal">Módulo Principal</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#como-usar">Como Usar</a>
          <ul>
            <li><a href="#instalação-do-ollama">Instalação do Ollama</a></li>
            <li><a href="#executando-a-aplicação">Executando a Aplicação</a></li>
          </ul>
        </li>
        <li><a href="#considerações-técnicas">Considerações Técnicas</a>
          <ul>
            <li><a href="#performance-e-otimizações">Performance e Otimizações</a></li>
            <li><a href="#melhorando-os-prompts">Melhorando os Prompts</a></li>
          </ul>
        </li>
        <li><a href="#próximos-passos">Próximos Passos</a>
          <ul>
            <li><a href="#melhorias-rápidas-implementação-imediata">Melhorias Rápidas (Implementação Imediata)</a>
              <ul>
                <li><a href="#1-persistência-da-base-de-conhecimento"><strong>1. Persistência da Base de Conhecimento</strong></a></li>
                <li><a href="#2-testes-unitários"><strong>2. Testes Unitários</strong></a></li>
                <li><a href="#3-streaming-de-respostas"><strong>3. Streaming de Respostas</strong></a></li>
                <li><a href="#4-cache-de-embeddings"><strong>4. Cache de Embeddings</strong></a></li>
                <li><a href="#5-banco-vetorial-simples-bm25-manual"><strong>5. Banco Vetorial Simples (BM25 Manual)</strong></a></li>
              </ul>
            </li>
            <li><a href="#melhorias-avançadas">Melhorias Avançadas</a></li>
            <li><a href="#dependências-e-próximos-passos">Dependências e Próximos Passos</a>
              <ul>
                <li><a href="#dependências-recomendadas"><strong>Dependências Recomendadas</strong></a></li>
                <li><a href="#implementação-com-lucene"><strong>Implementação com Lucene</strong></a></li>
              </ul>
            </li>
            <li><a href="#upgrade-para-embeddings-densos">Upgrade para Embeddings Densos</a>
              <ul>
                <li><a href="#1-via-ollama-embeddings-api">1. <strong>Via Ollama Embeddings API</strong></a></li>
                <li><a href="#2-via-huggingface-transformers">2. <strong>Via HuggingFace Transformers</strong></a></li>
                <li><a href="#3-comparação-de-performance">3. <strong>Comparação de Performance</strong></a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#apêndice">Apêndice</a>
          <ul>
            <li><a href="#requisitos-de-hardware-detalhados">Requisitos de Hardware Detalhados</a>
              <ul>
                <li><a href="#configurações-por-caso-de-uso"><strong>Configurações por Caso de Uso</strong></a></li>
                <li><a href="#otimizações-por-hardware"><strong>Otimizações por Hardware</strong></a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#referências">Referências</a></li>
      </ul>
    </li>
  </ul>
</nav>
            </nav>
        </div>
    </div>
    
    
    <button class="toc-mobile-toggle" id="toc-mobile-toggle" aria-label="Mostrar Sumário">
        <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <path d="M3 6h18M3 12h18M3 18h18"/>
        </svg>
    </button>
</aside>



    
    <div class="post-content">
        <h2 id="introdução">Introdução</h2>
<p>Olá, pessoal! 👋</p>
<p>Neste artigo, vamos explorar como construir uma aplicação <a href="https://pt.wikipedia.org/wiki/Gera%C3%A7%C3%A3o_aumentada_por_recupera%C3%A7%C3%A3o">RAG (Retrieval-Augmented Generation)</a> completa do zero usando <a href="https://clojure.org/">Clojure</a>. Vamos mergulhar em uma implementação prática que combina processamento de texto, busca semântica e geração de respostas com LLMs locais. Se você está interessado em melhorar a precisão e relevância das respostas dos seus modelos de linguagem com informações atualizadas, este guia é para você!</p>
<h2 id="fundamentos-do-rag">Fundamentos do RAG</h2>
<h3 id="o-que-é-rag">O que é RAG?</h3>
<p>Os Modelos de Linguagem de Grande Escala (<a href="https://en.wikipedia.org/wiki/Large_language_model">LLMs</a>), como o <a href="https://openai.com/api/">GPT</a>, <a href="https://openai.com/api/">ChatGPT</a> e outros, revolucionaram a forma como interagimos com a inteligência artificial. Eles são capazes de gerar textos coerentes, responder perguntas complexas e até mesmo criar conteúdo criativo. No entanto, esses modelos possuem uma limitação fundamental: seu conhecimento é &ldquo;congelado&rdquo; no tempo.</p>


  
    
  
  <div class="mermaid">graph TD
    A[LLM Treinado] --&gt; B[Data de Corte]
    B --&gt; C[Conhecimento Congelado]
    C --&gt; D[Limitações]
    D --&gt; E[Não sabe eventos recentes]
    D --&gt; F[Não tem dados atualizados]
    D --&gt; G[Não conhece novas tecnologias]</div>
 <h3 id="por-que-precisamos-do-rag">Por que precisamos do RAG?</h3>
<p>Ao desenvolver aplicações inteligentes, como assistentes financeiros que precisam de cotações de ações em tempo real, chatbots de suporte que devem conhecer os produtos mais recentes da empresa ou sistemas de recomendação que se baseiam nas últimas tendências, nos deparamos com uma limitação crucial dos Modelos de Linguagem de Grande Escala (<a href="https://en.wikipedia.org/wiki/Large_language_model">LLMs</a>) tradicionais: seu conhecimento estático.</p>
<p>O problema fundamental reside no fato de que esses modelos, por mais sofisticados que sejam, possuem uma base de conhecimento &ldquo;congelada&rdquo; no momento de seu treinamento. Eles carecem de acesso inerente a informações atualizadas, o que restringe drasticamente sua aplicabilidade em cenários que exigem dados em tempo real ou conhecimento sobre eventos recentes.</p>
<blockquote>
<p>Confiar exclusivamente em um <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM &ldquo;puro&rdquo;</a> nesses contextos resultará em respostas desatualizadas, potencialmente imprecisas e, consequentemente, em uma experiência do usuário comprometida. A eficácia da aplicação é diretamente afetada.</p></blockquote>
<h3 id="os-três-pilares-do-rag">Os Três Pilares do RAG</h3>


  
  <div class="mermaid">graph LR
    A[RAG] --&gt; B[Base de Dados Atual]
    A --&gt; C[Pesquisa em Tempo Real]
    A --&gt; D[Combinação de Conhecimento]
    
    B --&gt; E[Documentos Atualizados]
    B --&gt; F[Dados em Tempo Real]
    
    C --&gt; G[Busca Ativa]
    C --&gt; H[Seleção de Informações]
    
    D --&gt; I[Integração com LLM]
    D --&gt; J[Contextualização]</div>
 <ol>
<li><strong>Conexão com uma base de dados atual:</strong> Em vez de depender apenas do conhecimento estático adquirido durante seu treinamento (que pode se tornar obsoleto rapidamente), o <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a> ganha acesso a uma fonte de informações dinâmica e constantemente atualizada. Isso pode ser uma base de dados de notícias, um repositório de documentos corporativos, uma coleção de artigos científicos, ou qualquer outra fonte relevante para a tarefa em questão.</li>
<li><strong>Pesquisa em tempo real:</strong> O <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a> não está mais limitado a &ldquo;lembrar&rdquo; de informações. Ele adquire a capacidade de &ldquo;procurar&rdquo; ativamente por dados relevantes para responder a uma pergunta ou gerar um texto. Isso é semelhante a como nós, humanos, usamos mecanismos de busca para encontrar informações que não temos memorizadas. O <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a>, equipado com RAG, pode formular consultas, analisar os resultados e selecionar as informações mais pertinentes.</li>
<li><strong>Combinação de conhecimento base com dados novos:</strong> Este é o ponto crucial que diferencia o <a href="https://pt.wikipedia.org/wiki/Gera%C3%A7%C3%A3o_aumentada_por_recupera%C3%A7%C3%A3o">RAG</a> de uma simples busca em uma base de dados. O <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a> não apenas recupera informações, mas também as integra ao seu conhecimento pré-existente. Ele usa sua capacidade de raciocínio e compreensão para contextualizar os novos dados, identificar contradições, e formular respostas coerentes e informadas.</li>
</ol>
<h3 id="rag-em-produção">RAG em Produção</h3>
<p>Sistemas RAG em produção frequentemente incluem etapas adicionais para melhorar a precisão: <strong>re-ranking</strong> (onde um modelo especializado re-avalia a relevância dos documentos recuperados) e <strong>merge-rerank</strong> (que combina resultados de múltiplas estratégias de busca como semântica, lexical e híbrida). Essas técnicas aumentam significativamente a qualidade das respostas, mas adicionam complexidade ao sistema.</p>
<blockquote>
<p><strong>Nota</strong>: Nossa implementação atual usa apenas busca semântica simples com TF-IDF, focando na compreensão dos fundamentos do RAG. Para aplicações em produção, considere implementar essas técnicas avançadas.</p></blockquote>
<p>Segundo um <a href="https://arxiv.org/abs/2309.01066">whitepaper recente dos pesquisadores do Google</a>, existem várias técnicas para turbinar o desempenho dos <a href="https://en.wikipedia.org/wiki/Large_language_model">LLMs</a>, e o RAG é uma das mais promissoras. Isso ocorre porque o RAG aborda algumas das limitações fundamentais desses modelos:</p>
<p>O RAG resolve vários problemas de uma vez só: diminui aquelas &ldquo;viagens&rdquo; dos <a href="https://en.wikipedia.org/wiki/Large_language_model">LLMs</a> quando inventam respostas (as famosas alucinações), mantém tudo atualizado em vez de ficar preso no passado, deixa as respostas mais transparentes porque você sabe de onde veio a informação, e ainda melhora o desempenho do modelo quando ele precisa lidar com documentos ou dados específicos da sua empresa. É como dar ao modelo um Google particular que ele pode consultar antes de responder!</p>
<blockquote>
<p>O RAG representa um avanço significativo na evolução dos <a href="https://en.wikipedia.org/wiki/Large_language_model">LLMs</a>, permitindo que eles se tornem ferramentas mais confiáveis, precisas e úteis para uma ampla gama de aplicações. Ele transforma o <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a> de um &ldquo;sabe-tudo&rdquo; desatualizado em um pesquisador ágil e bem-informado, capaz de combinar conhecimento profundo com informações atualizadas em tempo real.</p></blockquote>
<h3 id="por-que-o-deepseek-r1">Por que o DeepSeek R1?</h3>
<p>Ele trabalha muito bem com documentação técnica, o que é perfeito para nosso sistema <a href="https://pt.wikipedia.org/wiki/Gera%C3%A7%C3%A3o_aumentada_por_recupera%C3%A7%C3%A3o">RAG</a> focado em docs técnicas. O DeepSeek R1 consegue equilibrar qualidade e velocidade melhor que outros modelos do Ollama, rodando na sua máquina sem ficar alucinando com respostas que não fazem sentido.</p>
<p>O modelo também se dá super bem com várias linguagens de programação, incluindo <a href="https://clojure.org/">Clojure</a>, então ele responde numa boa sobre implementações técnicas e documentação de código. E o melhor: mesmo quando você joga informações pela metade ou todas bagunçadas, ele ainda consegue manter o contexto e dar respostas que fazem sentido. Por isso ele é perfeito para o que estamos construindo!</p>
<h2 id="implementação-prática">Implementação Prática</h2>
<h3 id="preparando-o-ambiente">Preparando o Ambiente</h3>
<p>Pre-requisitos:</p>
<ul>
<li><a href="https://clojure.org/guides/getting_started">Clojure</a>: Linguagem de programação funcional que vamos usar para construir a aplicação</li>
<li><a href="https://leiningen.org/">Leiningen</a>: Ferramenta de build para Clojure</li>
<li><a href="https://ollama.com/">Ollama</a>: Modelo de linguagem local</li>
</ul>
<h3 id="estrutura-do-projeto">Estrutura do Projeto</h3>
<p>Nossa aplicação terá três componentes principais:</p>
<ol>
<li><strong>Processamento de documentação (Markdown/HTML)</strong>
<ul>
<li>Extração de texto</li>
<li>Pré-processamento de texto</li>
</ul>
</li>
<li><strong>Sistema de embeddings</strong>
<ul>
<li>Criação de embeddings para o texto usando <a href="https://pt.wikipedia.org/wiki/TF-IDF">TF-IDF</a></li>
<li>Busca por similaridade semântica</li>
</ul>
</li>
<li><strong>Interface com o LLM</strong>
<ul>
<li>Geração de resposta usando o LLM</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>Observação:</strong> Embora o RAG moderno utilize embeddings densos gerados por modelos de linguagem para capturar a semântica de forma mais rica, neste artigo, usaremos uma implementação simplificada de <a href="https://pt.wikipedia.org/wiki/TF-IDF">TF-IDF (Term Frequency-Inverse Document Frequency)</a> como <strong>prova de conceito</strong>. Para aplicações em produção, recomendamos fortemente o uso de embeddings densos.</p></blockquote>
<h3 id="tf-idf">TF-IDF</h3>
<p>O <a href="https://pt.wikipedia.org/wiki/TF-IDF">TF-IDF</a> (Term Frequency-Inverse Document Frequency) é uma técnica estatística usada para avaliar a importância de uma palavra em um documento, em relação a uma coleção de documentos. Vamos entender como funciona:</p>
<ol>
<li>
<p><strong>Term Frequency (TF)</strong>: Mede a frequência de uma palavra em um documento.</p>


  <pre><code class="language-">TF(termo) = (Número de vezes que o termo aparece no documento) / (Total de termos no documento)</code></pre>
 </li>
<li>
<p><strong>Inverse Document Frequency (IDF)</strong>: Mede a raridade de um termo na coleção de documentos.</p>


  <pre><code class="language-">IDF(termo) = log(Número total de documentos / Número de documentos contendo o termo)</code></pre>
 </li>
<li>
<p><strong>TF-IDF</strong>: É o produto desses dois valores.</p>


  <pre><code class="language-">TF-IDF(termo) = TF(termo) × IDF(termo)</code></pre>
 </li>
</ol>
<p>Vamos imaginar um cenário prático com três documentos técnicos:</p>
<ul>
<li>Doc1: &ldquo;Clojure é uma linguagem funcional baseada em <a href="https://en.wikipedia.org/wiki/Lisp_%28programming_language%29">Lisp</a>&rdquo;</li>
<li>Doc2: &ldquo;Python é uma linguagem de programação versátil&rdquo;</li>
<li>Doc3: &ldquo;Clojure e Python são linguagens de programação populares&rdquo;</li>
</ul>
<p>O TF-IDF é uma técnica que nos ajuda a identificar quais palavras são mais importantes em cada documento, comparando a frequência de um termo no documento (TF) com a raridade desse termo em toda a coleção (IDF). Por exemplo, se &ldquo;Clojure&rdquo; aparece uma vez em um documento de oito palavras, seu TF é 0,125; como está presente em dois de três documentos, seu IDF é log(3/2) ≈ 0,176, resultando em um TF-IDF de aproximadamente 0,022. Já termos muito comuns, como &ldquo;linguagem&rdquo;, acabam com TF-IDF zero, pois não ajudam a diferenciar os documentos.</p>
<p>Esse método é fundamental em sistemas de busca, pois destaca os termos que realmente caracterizam cada texto. No contexto do RAG, o TF-IDF permite indexar e encontrar rapidamente os documentos mais relevantes para uma consulta, servindo como uma base simples e eficiente para recuperação de informações, que pode ser aprimorada com técnicas mais avançadas como embeddings densos.</p>
<h3 id="requisitos-mínimos">Requisitos Mínimos</h3>
<p>Este experimento funciona com hardware básico: <strong>4 cores de CPU e 8GB de RAM</strong> são suficientes. Para máquinas mais lentas, use <code>ollama pull deepseek-r1:3b</code> (versão otimizada).</p>
<blockquote>
<p>Para requisitos detalhados de produção e otimizações avançadas, consulte o <a href="/2025/03/23/rag/#requisitos-de-hardware-detalhados">apêndice de hardware</a> ao final do artigo.</p></blockquote>
<h3 id="configuração-do-projeto">Configuração do Projeto</h3>
<ol>
<li>Crie um novo projeto Clojure:</li>
</ol>


  <pre><code class="language-bash">lein new app docai
cd docai</code></pre>
 <ol start="2">
<li>Configure o <code>project.clj</code>:</li>
</ol>


  <pre><code class="language-clojure">(defproject docai &#34;0.1.0-SNAPSHOT&#34;
  :description &#34;Um assistente RAG para consulta de documentação técnica&#34;
  :url &#34;http://example.com/FIXME&#34;
  :license {:name &#34;EPL-2.0 OR GPL-2.0-or-later WITH Classpath-exception-2.0&#34;
            :url &#34;https://www.eclipse.org/legal/epl-2.0/&#34;}
  :dependencies [[org.clojure/clojure &#34;1.11.1&#34;]
                 [markdown-to-hiccup &#34;0.6.2&#34;]
                 [hickory &#34;0.7.1&#34;]
                 [org.clojure/data.json &#34;2.4.0&#34;]
                 [http-kit &#34;2.6.0&#34;]
                 [org.clojure/tools.logging &#34;1.2.4&#34;]
                 [org.clojure/tools.namespace &#34;1.4.4&#34;]
                 [org.clojure/core.async &#34;1.6.681&#34;]
                 [org.clojure/core.memoize &#34;1.0.257&#34;]
                 [org.clojure/core.cache &#34;1.0.225&#34;]]
  :main ^:skip-aot docai.core
  :target-path &#34;target/%s&#34;
  :profiles {:uberjar {:aot :all
                       :jvm-opts [&#34;-Dclojure.compiler.direct-linking=true&#34;]}})</code></pre>
 <p>A estrutura do projeto acima define um aplicativo Clojure para RAG (Retrieval-Augmented Generation) com várias dependências essenciais. Entre elas, <code>markdown-to-hiccup</code> e <code>hickory</code> são usadas para processar documentos em Markdown e HTML, enquanto <code>data.json</code> e <code>http-kit</code> facilitam a comunicação com APIs externas, como a do Ollama. Além disso, <code>tools.logging</code> é responsável pelo registro de eventos e logs, e <code>tools.namespace</code> auxilia no gerenciamento de namespaces do projeto.</p>
<p>Já <code>core.async</code> permite operações assíncronas, o que é especialmente útil ao lidar com o processamento de documentos grandes. Por fim, <code>core.memoize</code> e <code>core.cache</code> são utilizados para implementar cache de resultados, como embeddings ou respostas do LLM, melhorando significativamente a performance ao evitar recálculos desnecessários, principalmente em consultas repetidas ou similares.</p>
<h3 id="implementação-dos-componentes">Implementação dos Componentes</h3>
<p>Agora vamos implementar os três componentes principais do nosso sistema RAG e vamos começar com o processamento de documentos. Pois, ele é o ponto de entrada para o RAG onde vamos processar os documentos e extrair o texto para ser usado nos outros componentes.</p>
<h4 id="processamento-de-documentos">Processamento de Documentos</h4>


  <pre><code class="language-clojure">;; src/docai/document.clj
(ns docai.document
  (:require [markdown-to-hiccup.core :as md]
            [hickory.core :as html]
            [clojure.string :as str]))

(defn extract-text-from-markdown [content]
  (try
    (let [hiccup-result (md/md-&gt;hiccup content)
          text-nodes (filter string? (flatten hiccup-result))]
      text-nodes)
    (catch Exception e
      (println &#34;Erro ao processar Markdown:&#34; (.getMessage e))
      [content])))

(defn extract-text-from-html [content]
  (try
    (let [dom (html/parse content)
          hiccup-result (html/as-hiccup dom)
          text-nodes (filter string? (flatten hiccup-result))]
      text-nodes)
    (catch Exception e
      (println &#34;Erro ao processar HTML:&#34; (.getMessage e))
      [content])))

;; Declare functions that will be defined later
(declare create-token-aware-chunks)

(defn extract-text
  &#34;Extrai texto de documentação (Markdown ou HTML)&#34;
  [doc-path]
  (println &#34;Extraindo texto de:&#34; doc-path)
  (let [content (slurp doc-path)
        _ (println &#34;Tamanho do conteúdo:&#34; (count content) &#34;caracteres&#34;)
        _ (println &#34;Amostra do conteúdo:&#34; (subs content 0 (min 100 (count content))))
        text (if (.endsWith doc-path &#34;.md&#34;)
               (extract-text-from-markdown content)
               (extract-text-from-html content))
        _ (println &#34;Quantidade de nós de texto extraídos:&#34; (count text))
        ;; Usar tokens reais em vez de caracteres para chunking preciso
        chunks (create-token-aware-chunks text 512)]
    (println &#34;Quantidade de chunks gerados:&#34; (count chunks))
    chunks))

(defn count-tokens
  &#34;Conta tokens usando heurística (para desenvolvimento)&#34;
  [text]
  ;; ⚠️ ATENÇÃO: Esta é uma heurística aproximada
  ;; Para produção, use [clojure-tiktoken](https://github.com/justone/clojure-tiktoken)
  ;; ou API do Ollama para contagem precisa
  (try
    (let [words (str/split text #&#34;\s&#43;&#34;)
          ;; Estimativa melhorada para português brasileiro
          ;; Ainda pode errar 2x em textos muito curtos/longos
          estimated-tokens (reduce &#43; 
                                 (map (fn [word]
                                        (cond
                                          ;; Palavras muito longas (composição)
                                          (&gt; (count word) 15) (* (count word) 0.8)
                                          ;; Palavras longas (derivação)
                                          (&gt; (count word) 10) (* (count word) 0.6)
                                          ;; Palavras médias
                                          (&gt; (count word) 5) (* (count word) 0.4)
                                          ;; Palavras curtas
                                          :else 1.0))
                                      words))]
      (int estimated-tokens))
    (catch Exception e
      (println &#34;Erro ao contar tokens:&#34; (.getMessage e))
      ;; Fallback conservador: 1 token por caractere
      (count text))))

(defn create-token-aware-chunks
  &#34;Cria chunks baseados em tokens reais, não caracteres&#34;
  [text-nodes max-tokens]
  (loop [nodes text-nodes
         current-chunk []
         current-tokens 0
         all-chunks []]
    (if (empty? nodes)
      (if (seq current-chunk)
        (conj all-chunks (str/join &#34; &#34; current-chunk))
        all-chunks)
      (let [node (first nodes)
            node-tokens (count-tokens node)
            new-total (&#43; current-tokens node-tokens)]
        (if (and (&gt; new-total max-tokens) (seq current-chunk))
          ;; Chunk cheio, salva e inicia novo
          (recur (rest nodes)
                 [node]
                 node-tokens
                 (conj all-chunks (str/join &#34; &#34; current-chunk)))
          ;; Adiciona ao chunk atual
          (recur (rest nodes)
                 (conj current-chunk node)
                 new-total
                 all-chunks))))))

(defn preprocess-chunks
  &#34;Limpa e prepara os chunks de texto&#34;
  [chunks]
  (let [processed (map #(-&gt; %
                            (str/replace #&#34;\s&#43;&#34; &#34; &#34;)
                            (str/trim))
                       chunks)]
    (println &#34;Primeiro chunk processado:&#34; (first processed))
    processed))</code></pre>
 <p>Este trecho de código implementa a parte de processamento de documentos do nosso sistema RAG. Basicamente, ele pega arquivos Markdown ou HTML e extrai o texto puro deles para que possamos usar depois na busca semântica. O código usa bibliotecas como <code>markdown-to-hiccup</code> e <code>hickory</code> para converter os documentos em estruturas de dados que facilitam a extração do texto.</p>


  
  <div class="mermaid">graph TD
    A[Documento] --&gt; B{É Markdown?}
    B --&gt;|Sim| C[Processa Markdown]
    B --&gt;|Não| D[Processa HTML]
    C --&gt; E[Extrai Texto]
    D --&gt; E
    E --&gt; F[Divide em Chunks]
    F --&gt; G[Limpa e Formata]
    G --&gt; H[Chunks Prontos]</div>
 <p>O fluxo é bem direto: primeiro verificamos se estamos lidando com Markdown ou HTML, depois extraímos o texto usando a função apropriada, dividimos em pedaços menores (chunks) baseados em tokens reais (não caracteres), e finalmente limpamos esses chunks removendo espaços extras e formatando tudo direitinho.</p>
<p>O código também inclui bastante logging para ajudar a depurar o processo, mostrando informações como o tamanho do documento, quantidade de texto extraído e número de chunks gerados.</p>
<p>Essa abordagem de dividir o texto em pedaços menores é crucial para o RAG, já que permite processar documentos grandes sem sobrecarregar o modelo de linguagem.</p>
<blockquote>
<p><strong>Importante</strong>: Dividimos o texto em chunks usando tokens (não caracteres) para não ultrapassar o limite do modelo. A contagem de tokens é aproximada. Para produção, use uma biblioteca como <a href="https://github.com/justone/clojure-tiktoken">clojure-tiktoken</a> para maior precisão.</p></blockquote>
<h4 id="sistema-de-embeddings">Sistema de Embeddings</h4>
<p>Agora vamos implementar o sistema de embeddings. Ele é responsável por criar embeddings para o texto para que possamos usar na busca semântica.</p>


  <pre><code class="language-clojure">;; src/docai/embedding.clj
(ns docai.embedding
  (:require [clojure.string :as str]
            [clojure.core.memoize :as memo]))

;; Implementação de embeddings usando TF-IDF simples
;; Não depende de modelos externos, ao contrário do Ollama que usa o deepseek-r1 para o LLM

(defn tokenize
  &#34;Divide o texto em tokens&#34;
  [text]
  (if (string? text)
    (-&gt; text
        str/lower-case
        (str/split #&#34;\s&#43;&#34;)
        (-&gt;&gt; (filter #(&gt; (count %) 2))))
    []))

(defn term-freq
  &#34;Calcula a frequência dos termos&#34;
  [tokens]
  (frequencies tokens))



(defn doc-freq
  &#34;Calcula a frequência dos documentos&#34;
  [docs]
  (let [string-docs (filter string? docs)  ; Use Clojure&#39;s built-in string? function
        _ (println (str &#34;Processando &#34; (count string-docs) &#34; documentos válidos de &#34; (count docs) &#34; total&#34;))
        doc-tokens (map tokenize string-docs)  
        all-tokens (distinct (flatten doc-tokens))
        doc-count (count string-docs)]
    (if (zero? doc-count)
      {}
      (zipmap all-tokens
              (map #(count (filter (fn [tokens] (some #{%} tokens)) doc-tokens))
                   all-tokens)))))

(defn tf-idf
  &#34;Calcula TF-IDF para um documento&#34;
  [doc doc-freq doc-count]
  (if (empty? doc-freq)
    {}
    (let [tokens (tokenize doc)
          tf (term-freq tokens)]
      (zipmap (keys tf)
              (map #(* (get tf %) (Math/log (/ doc-count (get doc-freq % 1))))
                   (keys tf))))))

(defn vectorize
  &#34;Converte um documento em um vetor TF-IDF&#34;
  [doc doc-freq doc-count vocab]
  (let [tf-idf-scores (tf-idf doc doc-freq doc-count)]
    (if (empty? vocab)
      []
      (map #(get tf-idf-scores % 0.0) vocab))))

(defn create-embeddings
  &#34;Gera embeddings para uma lista de textos usando TF-IDF&#34;
  [texts]
  (try
    (let [doc-freq (doc-freq texts)
          doc-count (count (filter string? texts))
          ;; Vocabulário ordenado para garantir ordem estável
          vocab (sort (keys doc-freq))]
      (map #(vectorize % doc-freq doc-count vocab) texts))
    (catch Exception e
      (println &#34;Erro ao criar embeddings: &#34; (.getMessage e))
      (vec (repeat (count texts) [])))))

(defn cosine-similarity
  &#34;Calcula a similaridade do cosseno entre dois vetores&#34;
  [v1 v2]
  (if (or (empty? v1) (empty? v2))
    0.0
    (let [dot-product (reduce &#43; (map * v1 v2))
          norm1 (Math/sqrt (reduce &#43; (map #(* % %) v1)))
          norm2 (Math/sqrt (reduce &#43; (map #(* % %) v2)))]
      (if (or (zero? norm1) (zero? norm2))
        0.0
        (/ dot-product (* norm1 norm2))))))

(defn similarity-search
  &#34;Encontra os N chunks mais similares&#34;
  [query-embedding doc-embeddings n]
  (if (or (empty? query-embedding) (empty? doc-embeddings))
    (take (min n (count doc-embeddings)) (range))
    (let [scores (map #(cosine-similarity query-embedding %) doc-embeddings)]
      (-&gt;&gt; (map vector scores (range))
           (sort-by first &gt;)
           (take n)
           (map second)))))</code></pre>
 <p>O código acima implementa um sistema simples de embeddings usando TF-IDF (Term Frequency-Inverse Document Frequency) para transformar textos em vetores numéricos.</p>
<p>Basicamente, ele pega documentos de texto, quebra em palavras (tokens), calcula a importância de cada palavra considerando tanto sua frequência no documento quanto sua raridade na coleção inteira, e cria vetores que representam cada documento. É como transformar textos em coordenadas matemáticas para que o computador possa entender a &ldquo;semelhança&rdquo; entre eles.</p>


  
  <div class="mermaid">graph TD
    A[Documentos] --&gt;|Tokenização| B[Tokens]
    B --&gt;|TF-IDF| C[Vetores Numéricos]
    C --&gt;|Similaridade do Cosseno| D[Documentos Similares]</div>
 <p>A parte mais legal é a função <code>similarity_search</code>, que usa a similaridade do cosseno para encontrar documentos parecidos com uma consulta. Imagine que cada documento é um ponto num espaço multidimensional – quanto menor o ângulo entre dois pontos, mais similares eles são.</p>
<p>O código não usa nenhum modelo de IA sofisticado para isso, apenas matemática básica, o que o torna leve e rápido, embora menos poderoso que embeddings modernos baseados em redes neurais. É como um GPS simples que te leva ao destino sem todos os recursos de um Google Maps.</p>
<p>O TF-IDF transforma textos em vetores numéricos ao combinar a frequência de cada palavra em um documento (TF) com o quanto essa palavra é rara em toda a coleção (IDF): palavras comuns como &ldquo;linguagem&rdquo; têm peso baixo, enquanto termos mais exclusivos como &ldquo;Clojure&rdquo; ganham peso alto, permitindo que o computador compare documentos de forma eficiente e encontre os mais relevantes para cada consulta.</p>
<p>Outra abordagem, é por meio da similaridade do cosseno, que compara dois vetores TF-IDF calculando o ângulo entre eles: quanto menor o ângulo, mais parecidos são os textos, usando a fórmula cos(θ) = (A·B) / (||A|| ||B||), onde A·B é o produto escalar e ||A|| e ||B|| são os tamanhos dos vetores; porém, o TF-IDF tem limitações, pois não entende sinônimos, contexto ou ordem das palavras, tratando termos como &ldquo;carro&rdquo; e &ldquo;automóvel&rdquo; como diferentes e podendo gerar vetores grandes.</p>
<blockquote>
<p><strong>Importante</strong>: Esta implementação TF-IDF é uma <strong>prova de conceito</strong> para demonstrar os fundamentos do RAG. Em aplicações reais, embeddings densos modernos como <a href="https://www.sbert.net/">SBERT</a>, <a href="https://huggingface.co/intfloat/e5-large">E5</a>, <a href="https://huggingface.co/BAAI/bge-large-en">BGE</a> ou modelos via Ollama superam significativamente o TF-IDF em tarefas de busca semântica e question-answering.</p></blockquote>
<h4 id="interface-com-ollama">Interface com Ollama</h4>
<p>Agora vamos implementar a interface com o Ollama. Ele é responsável por gerar a resposta para a query do usuário (essa parte aqui é super divertida, pois é onde vamos usar o LLM).</p>


  <pre><code class="language-clojure">;; src/docai/llm.clj
(ns docai.llm
  (:require [clojure.data.json :as json]
            [org.httpkit.client :as http]))

(def ollama-url &#34;http://localhost:11434/api/generate&#34;)
(def model-name &#34;deepseek-r1&#34;) ; Modelo DeepSeek para melhor qualidade

(defn call-ollama-api
  &#34;Chama a API do Ollama para gerar uma resposta&#34;
  [prompt]
  (let [request-body {:model model-name
                      :prompt prompt
                      :stream false}
        options {:headers {&#34;Content-Type&#34; &#34;application/json&#34;}
                 :body (json/write-str request-body)}
        response @(http/post ollama-url options)]
    (if (= (:status response) 200)
      (-&gt; response
          :body
          (json/read-str :key-fn keyword)
          ;; Compatível com versões antigas (:response) e novas (:message) do Ollama
          (#(or (:response %) (:message %))))
      (str &#34;Erro ao chamar a API do Ollama: &#34; (:status response) &#34; - &#34; (:body response)))))

;; Funções de utilidade para uso futuro:
;;
;; extract-code-blocks: Extrai blocos de código do texto usando regex
;; exemplo de uso:
;;   (extract-code-blocks &#34;```clojure\n(&#43; 1 2)\n```&#34;) =&gt; [&#34;(&#43; 1 2)&#34;]
;;
;; extract-summary: Cria um resumo de texto com tamanho máximo especificado
;; exemplo de uso:
;;   (extract-summary &#34;# Título\nConteúdo longo...&#34; 50) =&gt; &#34;Conteúdo longo...&#34;

(defn format-prompt
  &#34;Formata o prompt para o LLM com delimitação segura do contexto&#34;
  [context query]
  (str &#34;Você é um assistente especializado em documentação técnica. &#34;
       &#34;Use APENAS as informações do contexto fornecido para responder.\n\n&#34;
       &#34;DOCUMENTO:\n&#34;
       &#34;```\n&#34;
       context
       &#34;\n```\n\n&#34;
       &#34;Pergunta: &#34; query
       &#34;\n\n&#34;
       &#34;Instruções:\n&#34;
       &#34;- Responda baseado APENAS no contexto fornecido\n&#34;
       &#34;- Se a informação não estiver no contexto, indique claramente\n&#34;
       &#34;- Forneça exemplos de código quando relevante\n&#34;
       &#34;- Se o contexto for limitado, mencione essa limitação\n&#34;
       &#34;- NÃO invente informações que não estão no contexto&#34;))

(defn generate-response
  &#34;Gera resposta usando o LLM com base no contexto&#34;
  [query context]
  (try
    (let [prompt (format-prompt context query)]
      (println &#34;DEBUG - Enviando prompt para o Ollama usando o modelo&#34; model-name)
      (call-ollama-api prompt))
    (catch Exception e
      (str &#34;Erro ao gerar resposta: &#34; (.getMessage e) 
           &#34;\n\nPor favor, verifique se o Ollama está em execução no endereço &#34; 
           ollama-url 
           &#34;\n\nVocê pode iniciar o Ollama com o comando: ollama serve&#34;))))

;; Exemplo de prompt seguro gerado:
;; Você é um assistente especializado em documentação técnica. 
;; Use APENAS as informações do contexto fornecido para responder.
;;
;; DOCUMENTO:
;; ```
;; [contexto aqui]
;; ```
;;
;; Pergunta: [pergunta do usuário]
;;
;; Instruções:
;; - Responda baseado APENAS no contexto fornecido
;; - Se a informação não estiver no contexto, indique claramente
;; - Forneça exemplos de código quando relevante
;; - Se o contexto for limitado, mencione essa limitação
;; - NÃO invente informações que não estão no contexto</code></pre>
 <p>A parte mais importante aqui é a função <code>call-ollama-api</code>, que faz uma requisição HTTP para o servidor Ollama rodando na máquina local. Ela envia um prompt de texto e recebe de volta a resposta gerada pelo modelo DeepSeek R1. O código também inclui uma função <code>format-prompt</code> super importante, que estrutura a mensagem enviada ao modelo.</p>
<p>Ela combina o contexto (os trechos de documentação relevantes que encontramos) com a pergunta do usuário, e adiciona instruções específicas para o modelo se comportar como um assistente técnico. Essa &ldquo;engenharia de prompt&rdquo; é crucial para obter respostas de qualidade - estamos essencialmente ensinando o modelo a responder no formato que queremos.</p>
<p>A função <code>generate-response</code> amarra tudo isso, pegando a pergunta e o contexto, formatando o prompt, enviando para o Ollama e tratando possíveis erros. Tem até uma mensagem amigável caso o Ollama não esteja rodando, sugerindo como iniciar o serviço. É um exemplo clássico de como interfaces com LLMs funcionam: você prepara um prompt bem estruturado, envia para o modelo, e recebe de volta texto gerado que (esperamos!) responda à pergunta original com base no contexto fornecido.</p>
<h4 id="módulo-principal">Módulo Principal</h4>
<p>Agora vamos implementar o módulo principal que vai ser o ponto de entrada para o RAG. Ele vai ser responsável por carregar os documentos, processar os chunks, criar os embeddings e gerar a resposta para a query do usuário.</p>


  <pre><code class="language-clojure">;; src/docai/core.clj
(ns docai.core
  (:require [docai.document :as doc]
            [docai.embedding :as emb]
            [docai.llm :as llm]
            [clojure.java.io :as io]
            [clojure.string :as str])
  (:gen-class))

(def docs-path &#34;resources/docs&#34;)

(defn load-documentation
  &#34;Carrega todos os arquivos de documentação do diretório&#34;
  []
  (-&gt;&gt; (file-seq (io/file docs-path))
       (filter #(.isFile %))
       (map #(.getPath %))))

(defn setup-knowledge-base
  &#34;Configura a base de conhecimento inicial&#34;
  []
  (let [doc-files (load-documentation)
        _ (when (empty? doc-files)
            (println &#34;Aviso: Nenhum arquivo de documentação encontrado em resources/docs/&#34;))
        _ (doseq [file doc-files]
            (println &#34;Arquivo encontrado:&#34; file))
        all-chunks (mapcat doc/extract-text doc-files)
        processed-chunks (doc/preprocess-chunks all-chunks)
        _ (println (str &#34;Processando &#34; (count processed-chunks) &#34; chunks de texto...&#34;))
        _ (when (&lt; (count processed-chunks) 5)
            (println &#34;DEBUG - Primeiros chunks:&#34;)
            (doseq [chunk (take 5 processed-chunks)]
              (println (str &#34;Chunk: &#39;&#34; (subs chunk 0 (min 50 (count chunk))) &#34;...&#39;&#34;))))
        doc-freq (emb/doc-freq processed-chunks)
        doc-count (count (filter string? processed-chunks))
        ;; Vocabulário ordenado para garantir ordem estável entre execuções
        vocab (sort (keys doc-freq))
        embeddings (map #(emb/vectorize % doc-freq doc-count vocab) processed-chunks)]
          {:chunks processed-chunks
       :embeddings embeddings
       :doc-freq doc-freq
       :doc-count doc-count
            :vocab vocab  ; Persistir vocabulário ordenado
     :original-files doc-files}))

;; Função para forçar recálculo (útil para desenvolvimento)
(defn force-recalculate-kb []
  (let [kb-file &#34;resources/knowledge-base.json&#34;]
    (when (.exists (io/file kb-file))
      (.delete (io/file kb-file)))
  (setup-knowledge-base))

(defn get-file-content
  &#34;Lê o conteúdo completo de um arquivo&#34;
  [file-path]
  (try
    (slurp file-path)
    (catch Exception _
      (println &#34;Erro ao ler arquivo:&#34; file-path)
      &#34;&#34;)))

(defn get-limited-fallback-content
  &#34;Obtém conteúdo limitado para fallback (evita estourar contexto)&#34;
  [file-path]
  (try
    (let [content (slurp file-path)
          max-chars 8000  ; Limite de ~8KB para evitar estourar contexto
          limited-content (if (&gt; (count content) max-chars)
                           (str (subs content 0 max-chars) 
                                &#34;\n\n[Conteúdo truncado - arquivo muito grande]&#34;)
                           content)]
      (str &#34;Informações limitadas da documentação:\n\n&#34; limited-content))
    (catch Exception _
      (println &#34;Erro ao ler arquivo para fallback:&#34; file-path)
      &#34;Não foi possível acessar a documentação.&#34;)))

(defn query-rag
  &#34;Processa uma query usando o pipeline RAG&#34;
  [knowledge-base query]
  (cond
    (str/blank? query)
    &#34;Por favor, digite uma pergunta válida.&#34;
    
    (and (seq (:chunks knowledge-base)) 
         (seq (:embeddings knowledge-base)))
    (let [query-emb (emb/vectorize query (:doc-freq knowledge-base) (:doc-count knowledge-base) (:vocab knowledge-base))
          similar-idxs (emb/similarity-search query-emb 
                                            (:embeddings knowledge-base)
                                            3)
          _ (println &#34;DEBUG - Índices similares:&#34; similar-idxs)
          
          ;; Obter contexto relevante
          context-chunks (-&gt;&gt; similar-idxs
                              (map #(nth (:chunks knowledge-base) %))
                              (str/join &#34;\n\n&#34;))
          
          ;; Se não houver chunks relevantes, use fallback inteligente
          context (if (str/blank? context-chunks)
                    (if (seq (:original-files knowledge-base))
                      (get-limited-fallback-content (first (:original-files knowledge-base)))
                      &#34;Não foi possível encontrar informações relevantes.&#34;)
                    context-chunks)]
      
      (println &#34;DEBUG - Tamanho do contexto:&#34; (count context) &#34;caracteres&#34;)
      (println &#34;DEBUG - Amostra do contexto:&#34; (subs context 0 (min 200 (count context))) &#34;...&#34;)
      
      ;; Gerar resposta usando o LLM
      (llm/generate-response query context))
    
    :else
    &#34;Não foi possível encontrar informações relevantes na base de conhecimento.&#34;))

(defn -main
  &#34;Função principal que inicializa a aplicação DocAI&#34;
  [&amp; _]
  (println &#34;Inicializando DocAI...&#34;)
  
  ;; Verificar se o Ollama está acessível
  (println &#34;Para usar o Ollama, certifique-se de que ele está em execução com o comando: ollama serve&#34;)
  (println &#34;Usando o modelo deepseek-r1. Se você ainda não o baixou, execute: ollama pull deepseek-r1&#34;)
  
  (let [kb (setup-knowledge-base)]
    (println &#34;Base de conhecimento pronta! Faça sua pergunta:&#34;)
    (try
      (loop []
        (when-let [input (read-line)]
          (cond
            (= input &#34;sair&#34;) 
            (println &#34;Obrigado por usar o DocAI. Até a próxima!&#34;)
            
            (str/blank? input)
            (do
              (println &#34;Digite uma pergunta ou &#39;sair&#39; para terminar.&#34;)
              (recur))
            
            :else
            (do
              (println &#34;Processando...&#34;)
              (println (query-rag kb input))
              (println &#34;\nPróxima pergunta (ou &#39;sair&#39; para terminar):&#34;)
              (recur)))))
      (catch Exception e
        (println &#34;Erro: &#34; (.getMessage e))
        (println &#34;Detalhes: &#34; (ex-data e))))
    (println &#34;Obrigado por usar o DocAI. Até a próxima!&#34;)))</code></pre>
 <p>Basicamente, quando você faz uma pergunta, o sistema primeiro transforma sua pergunta em números (embeddings) e depois procura nos documentos quais partes são mais parecidas com o que você perguntou.</p>
<p>É como se ele estivesse destacando os trechos mais relevantes de um livro para responder sua dúvida. Você pode ver isso acontecendo quando ele imprime os &ldquo;índices similares&rdquo; no console - são as posições dos pedaços de texto que ele achou mais úteis.</p>
<p>Depois de encontrar os trechos relevantes, o sistema junta tudo em um &ldquo;contexto&rdquo; - que é basicamente um resumo das informações importantes. Se ele não achar nada parecido com sua pergunta, ele tenta usar o documento inteiro ou avisa que não tem informação suficiente.</p>
<p>Dá para ver que ele é bem transparente, mostrando no console o tamanho do contexto e até uma amostra do que encontrou, para você entender o que está acontecendo nos bastidores.</p>
<p>Por fim, ele passa sua pergunta original junto com o contexto encontrado para o modelo de linguagem (LLM) gerar uma resposta personalizada. É como dar a um especialista tanto a sua pergunta quanto as páginas relevantes de um manual técnico - assim ele pode dar uma resposta muito mais precisa e fundamentada.</p>
<p>Todo esse processo acontece em segundos, permitindo que você tenha uma conversa fluida com seus documentos, como se estivesse conversando com alguém que leu tudo e está pronto para responder suas dúvidas específicas.</p>
<hr>
<h2 id="como-usar">Como Usar</h2>
<p>Abaixo um guia para você instalar e usar o DocAI (e ver o processo em ação).</p>
<h3 id="instalação-do-ollama">Instalação do Ollama</h3>
<ol>
<li>
<p><strong>Instalação</strong>:</p>
<ul>
<li><strong>Windows</strong>: Baixe o instalador do <a href="https://ollama.com/download">site oficial do Ollama</a> e execute-o</li>
<li><strong>Linux</strong>: Execute o comando:


  <pre><code class="language-bash">curl https://ollama.ai/install.sh | sh</code></pre>
 </li>
<li><strong>macOS</strong>: Use o Homebrew:


  <pre><code class="language-bash">brew install ollama</code></pre>
 </li>
</ul>
</li>
<li>
<p><strong>Iniciando o Servidor</strong>:</p>


  <pre><code class="language-bash">ollama serve</code></pre>
 </li>
<li>
<p><strong>Baixando o Modelo</strong>:</p>


  <pre><code class="language-bash">ollama pull deepseek-r1</code></pre>
 </li>
<li>
<p><strong>Verificando a Instalação</strong>:</p>
<ul>
<li>Execute um teste simples:


  <pre><code class="language-bash">ollama run deepseek-r1 &#34;Olá! Como você está?&#34;</code></pre>
 </li>
<li>Se tudo estiver funcionando, você receberá uma resposta do modelo</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>Dica</strong>: O Ollama mantém os modelos em cache local. Se você precisar liberar espaço, pode usar <code>ollama rm deepseek-r1</code> para remover o modelo.</p></blockquote>
<h3 id="executando-a-aplicação">Executando a Aplicação</h3>
<ol>
<li>Coloque seus documentos na pasta <code>resources/docs/</code> (já incluímos dois exemplos: <code>example.md</code>)</li>
<li>Execute o projeto:</li>
</ol>


  <pre><code class="language-bash">lein run</code></pre>
 <ol start="3">
<li>Faça suas perguntas! Exemplo:</li>
</ol>


  <pre><code class="language-bash">Como implementar autenticação JWT em Clojure?
Como implementar auth saml em python?
Como integrar o auth0 em uma aplicação Clojure?
etc...</code></pre>
 <p>O DocAI processa sua pergunta em várias etapas:</p>


  
  <div class="mermaid">flowchart TD
    A[Início] --&gt; B[Carrega Documentação]
    B --&gt; C[Processa Documentos]
    C --&gt; D[Gera Embeddings]
    D --&gt; E[Base de Conhecimento]
    
    F[Consulta do Usuário] --&gt; G[Processa Consulta]
    G --&gt; H[Gera Embedding da Consulta]
    H --&gt; I[Busca Similaridade]
    I --&gt; J[Seleciona Chunks Relevantes]
    J --&gt; K[Combina Contexto]
    K --&gt; L[Gera Prompt]
    L --&gt; M[LLM DeepSeek R1]
    M --&gt; N[Resposta Final]
    
    E --&gt; I
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style N fill:#9ff,stroke:#333,stroke-width:2px</div>
 <ol>
<li><strong>Processamento da Consulta</strong>: A pergunta é convertida em um vetor TF-IDF</li>
<li><strong>Busca por Similaridade</strong>: O sistema encontra os chunks mais relevantes</li>
<li><strong>Geração de Contexto</strong>: Os chunks são combinados em um contexto coeso</li>
<li><strong>Geração de Resposta</strong>: O LLM gera uma resposta baseada no contexto</li>
</ol>
<p>Você pode ver o processo em ação nos logs:</p>


  <pre><code class="language-bash">DEBUG - Processando query: Como implementar autenticação JWT em Clojure?
DEBUG - Índices similares: [2, 5, 8]
DEBUG - Tamanho do contexto: 1234 caracteres
DEBUG - Amostra do contexto: &#34;Para implementar autenticação JWT em Clojure...&#34;</code></pre>
 <blockquote>
<p><strong>NOTA:</strong> A propósito, o projeto docai está disponível no <a href="https://github.com/scovl/docai">https://github.com/scovl/docai</a> caso você queira contribuir com o projeto ou usar em outro projeto.</p></blockquote>
<hr>
<h2 id="considerações-técnicas">Considerações Técnicas</h2>
<h3 id="performance-e-otimizações">Performance e Otimizações</h3>
<p>Nossa implementação atual oferece uma base funcional, mas pode ser significativamente otimizada em termos de performance através da adoção de bancos de dados vetoriais como <a href="https://milvus.io/">Milvus</a> ou <a href="https://github.com/facebookresearch/faiss">FAISS</a>, implementação de cache de embeddings e paralelização do processamento de chunks, permitindo consultas mais rápidas mesmo com grandes volumes de dados.</p>
<p>Para lidar com documentações extensas, recomendo estratégias específicas de gerenciamento de memória, como o processamento de chunks em lotes menores, implementação de indexação incremental que constrói a base de conhecimento gradualmente, e utilização de técnicas de streaming para processar arquivos grandes sem sobrecarregar a memória disponível.</p>
<p>Quanto à escolha de modelos no ecossistema Ollama, cada um apresenta características distintas que podem ser exploradas conforme a necessidade: o <a href="https://ollama.com/models/deepseek-r1">DeepSeek R1</a> destaca-se na compreensão geral e geração de texto, o <a href="https://ollama.com/models/deepseek-coder">DeepSeek Coder</a> é especializado em código, o <a href="https://ollama.com/models/llama3">Llama 3</a> serve como excelente alternativa geral, o <a href="https://ollama.com/models/mistral">Mistral</a> demonstra eficácia em tarefas específicas, enquanto o <a href="https://ollama.com/models/gemma">Gemma</a> oferece uma solução leve e eficiente para ambientes com recursos limitados.</p>
<p>Outra questão importante é como estou tratando os erros. O sistema implementa várias camadas de tratamento de erros para lidar com diferentes cenários:</p>
<ol>
<li>
<p><strong>Ollama Offline</strong></p>
<ul>
<li><strong>Sintoma</strong>: O sistema não consegue se conectar ao servidor Ollama</li>
<li><strong>Tratamento</strong>: O código verifica a disponibilidade do servidor e fornece mensagens claras de erro:</li>
</ul>


  <pre><code class="language-clojure">(catch Exception e
  (str &#34;Erro ao gerar resposta: &#34; (.getMessage e) 
       &#34;\n\nPor favor, verifique se o Ollama está em execução no endereço &#34; 
       ollama-url 
       &#34;\n\nVocê pode iniciar o Ollama com o comando: ollama serve&#34;))</code></pre>
 </li>
<li>
<p><strong>Documentação Muito Grande</strong></p>
<ul>
<li><strong>Sintoma</strong>: Arquivos de documentação que excedem a memória disponível</li>
<li><strong>Tratamento</strong>: O sistema implementa:
<ul>
<li>Chunking de documentos (512 tokens por chunk)</li>
<li>Processamento em lotes</li>
<li>Logs de progresso para monitoramento</li>
</ul>
</li>
</ul>


  <pre><code class="language-clojure">(let [content (slurp doc-path)
      chunks (partition-all 512 text)]
  (println &#34;Quantidade de chunks gerados:&#34; (count chunks)))</code></pre>
 </li>
<li>
<p><strong>Consultas sem Relação com a Documentação</strong></p>
<ul>
<li><strong>Sintoma</strong>: Nenhum chunk relevante é encontrado para a consulta</li>
<li><strong>Tratamento</strong>: O sistema:
<ul>
<li>Verifica se há chunks disponíveis</li>
<li>Usa fallback para conteúdo original se necessário</li>
<li>Fornece feedback claro ao usuário</li>
</ul>
</li>
</ul>


  <pre><code class="language-clojure">(if (str/blank? context-chunks)
  (if (seq (:original-files knowledge-base))
    (get-file-content (first (:original-files knowledge-base)))
    &#34;Não foi possível encontrar informações relevantes.&#34;)
  context-chunks)</code></pre>
 </li>
<li>
<p><strong>Melhorias Futuras</strong> - Implementar <a href="https://en.wikipedia.org/wiki/Exponential_backoff">retry com backoff exponencial</a> para falhas de conexão, adicionar <a href="https://en.wikipedia.org/wiki/Cache_%28computing%29">cache de embeddings</a> para melhor performance, implementar <a href="https://en.wikipedia.org/wiki/Streaming_media">streaming</a> para arquivos muito grandes, adicionar <a href="https://en.wikipedia.org/wiki/Document_validation">validação de formato de documentos</a> e implementar <a href="https://en.wikipedia.org/wiki/Rate_limiting">rate limiting</a> para evitar sobrecarga do Ollama.</p>
</li>
</ol>
<hr>
<h3 id="melhorando-os-prompts">Melhorando os Prompts</h3>
<p>Para obter melhores respostas do sistema RAG, você pode usar prompts mais estruturados:</p>


  <pre><code class="language-clojure">(defn format-advanced-prompt
  &#34;Prompt otimizado com diretrizes claras&#34;
  [context query]
  (str &#34;Você é um especialista em documentação técnica de software.\n\n&#34;
       &#34;DOCUMENTO:\n```\n&#34; context &#34;\n```\n\n&#34;
       &#34;Pergunta: &#34; query &#34;\n\n&#34;
       &#34;Diretrizes:\n&#34;
       &#34;1. Use APENAS informações do contexto fornecido\n&#34;
       &#34;2. Seja preciso e técnico\n&#34;
       &#34;3. Inclua exemplos de código quando relevante\n&#34;
       &#34;4. Se a informação não estiver no contexto, indique claramente\n&#34;
       &#34;5. Use formatação Markdown para melhor legibilidade&#34;))</code></pre>
 <blockquote>
<p>Para técnicas avançadas de prompt engineering, consulte o <a href="https://www.promptingguide.ai/">Guia Completo de Prompt Engineering</a>.</p></blockquote>
<h2 id="próximos-passos">Próximos Passos</h2>
<p>Abaixo uma lista de melhorias que podem ser feitas no projeto atual.</p>
<h3 id="melhorias-rápidas-implementação-imediata">Melhorias Rápidas (Implementação Imediata)</h3>
<h4 id="1-persistência-da-base-de-conhecimento"><strong>1. Persistência da Base de Conhecimento</strong></h4>


  <pre><code class="language-clojure">;; src/docai/persistence.clj
(ns docai.persistence
  (:require [clojure.data.json :as json]
            [clojure.edn :as edn]))

(defn calculate-checksum
  &#34;Calcula checksum dos arquivos de documentação&#34;
  [doc-files]
  (let [checksums (map #(hash (slurp %)) doc-files)]
    (hash checksums)))

(defn save-knowledge-base
  &#34;Salva a base de conhecimento em disco com checksum&#34;
  [kb filename]
  (let [doc-files (:original-files kb)
        checksum (calculate-checksum doc-files)
        serializable-kb (-&gt; kb
                           (select-keys [:chunks :embeddings :doc-freq :doc-count :vocab])
                           (assoc :checksum checksum :doc-files doc-files))]
    (spit filename (json/write-str serializable-kb))))

(defn load-knowledge-base
  &#34;Carrega a base de conhecimento do disco com verificação de mudanças&#34;
  [filename doc-files]
  (try
    (let [content (slurp filename)
          data (json/read-str content :key-fn keyword)
          cached-checksum (:checksum data)
          current-checksum (calculate-checksum doc-files)]
      (if (= cached-checksum current-checksum)
        (do
          (println &#34;Cache válido - carregando embeddings...&#34;)
          (assoc data :original-files doc-files))
        (do
          (println &#34;Arquivos modificados - recalculando embeddings...&#34;)
          nil)))
    (catch Exception e
      (println &#34;Erro ao carregar KB:&#34; (.getMessage e))
      nil)))

;; Uso no core.clj
(defn setup-knowledge-base
  &#34;Configura a base de conhecimento (com cache inteligente)&#34;
  []
  (let [kb-file &#34;resources/knowledge-base.json&#34;
        doc-files (load-documentation)]
    (if (.exists (io/file kb-file))
      (if-let [cached-kb (load-knowledge-base kb-file doc-files)]
        cached-kb
        (do
          (println &#34;Recriando KB devido a mudanças nos arquivos...&#34;)
          (let [kb (create-knowledge-base)]
            (save-knowledge-base kb kb-file)
            kb)))
      (do
        (println &#34;Criando nova KB...&#34;)
        (let [kb (create-knowledge-base)]
          (save-knowledge-base kb kb-file)
          kb)))))</code></pre>
 <h4 id="2-testes-unitários"><strong>2. Testes Unitários</strong></h4>


  <pre><code class="language-clojure">;; test/docai/embedding_test.clj
(ns docai.embedding-test
  (:require [clojure.test :refer :all]
            [docai.embedding :as emb]))

(deftest test-tokenize
  (testing &#34;Tokenização básica&#34;
    (is (= [&#34;hello&#34; &#34;world&#34;] (emb/tokenize &#34;Hello World!&#34;)))
    (testing &#34;Filtra palavras curtas&#34;
      (is (= [] (emb/tokenize &#34;a b c&#34;)))))

(deftest test-tf-idf
  (testing &#34;Cálculo TF-IDF&#34;
    (let [doc &#34;hello world hello&#34;
          doc-freq {&#34;hello&#34; 2 &#34;world&#34; 1}
          doc-count 2
          result (emb/tf-idf doc doc-freq doc-count)]
      (is (contains? result &#34;hello&#34;))
      (is (contains? result &#34;world&#34;)))))

(deftest test-cosine-similarity
  (testing &#34;Similaridade do cosseno&#34;
    (is (= 1.0 (emb/cosine-similarity [1 0] [1 0])))
    (is (= 0.0 (emb/cosine-similarity [1 0] [0 1])))
    (is (= 0.707 (emb/cosine-similarity [1 1] [1 0]) :delta 0.001))))</code></pre>
 <h4 id="3-streaming-de-respostas"><strong>3. Streaming de Respostas</strong></h4>


  <pre><code class="language-clojure">;; src/docai/streaming.clj
(ns docai.streaming
  (:require [clojure.data.json :as json]
            [org.httpkit.client :as http]))

(defn stream-ollama-response
  &#34;Streaming de resposta do Ollama&#34;
  [prompt]
  (let [url &#34;http://localhost:11434/api/generate&#34;
        request-body {:model &#34;deepseek-r1&#34;
                     :prompt prompt
                     :stream true}]
    (with-open [conn @(http/post url {:body (json/write-str request-body)
                                      :as :stream})]
      (doseq [line (line-seq (:body conn))]
        (when-not (str/blank? line)
          (let [data (json/read-str line :key-fn keyword)]
            ;; Compatível com versões antigas (:response) e novas (:message) do Ollama
            (when-let [content (or (:response data) (:message data))]
              (print content)
              (flush))))))))</code></pre>
 <h4 id="4-cache-de-embeddings"><strong>4. Cache de Embeddings</strong></h4>


  <pre><code class="language-clojure">;; src/docai/cache.clj
(ns docai.cache
  (:require [clojure.core.cache :as cache]))

;; Cache LRU com limite de memória (evita vazamentos)
(def embedding-cache (atom (cache/lru-cache-factory {} :threshold 1000))) ; Máximo 1000 embeddings

(defn cached-embedding
  &#34;Embedding com cache LRU&#34;
  [text doc-freq doc-count vocab]
  (if-let [cached (cache/lookup @embedding-cache text)]
    cached
    (let [embedding (emb/vectorize text doc-freq doc-count vocab)]
      (swap! embedding-cache cache/miss text embedding)
      embedding)))

;; Cache para respostas do LLM (também LRU)
(def response-cache (atom (cache/lru-cache-factory {} :threshold 500))) ; Máximo 500 respostas

(defn cached-llm-response
  &#34;Resposta do LLM com cache LRU&#34;
  [prompt]
  (if-let [cached (cache/lookup @response-cache prompt)]
    cached
    (let [response (llm/call-ollama-api prompt)]
      (swap! response-cache cache/miss prompt response)
      response)))

;; Função para limpar cache manualmente se necessário
(defn clear-caches []
  (reset! embedding-cache (cache/lru-cache-factory {} :threshold 1000))
  (reset! response-cache (cache/lru-cache-factory {} :threshold 500))
  (println &#34;Caches limpos!&#34;))

;; Monitoramento de cache
(defn cache-stats []
  (let [embedding-size (count @embedding-cache)
        response-size (count @response-cache)]
    (println (str &#34;Embedding cache: &#34; embedding-size &#34;/1000&#34;))
    (println (str &#34;Response cache: &#34; response-size &#34;/500&#34;))))</code></pre>
 <p><strong>Vantagens do Cache LRU:</strong></p>
<ul>
<li><strong>🔄 Auto-limpeza</strong>: Remove itens menos usados automaticamente</li>
<li><strong>💾 Controle de memória</strong>: Limite máximo de itens</li>
<li><strong>⚡ Performance</strong>: Acesso rápido a dados frequentes</li>
<li><strong>🛡️ Estabilidade</strong>: Evita vazamentos de memória</li>
</ul>
<p><strong>Cache Inteligente de Embeddings:</strong></p>
<ul>
<li><strong>📁 Persistência</strong>: Embeddings salvos em disco</li>
<li><strong>🔍 Verificação de Mudanças</strong>: Checksum dos arquivos</li>
<li><strong>⚡ Recarregamento Rápido</strong>: Só recalcula se necessário</li>
<li><strong>🔄 Invalidação Automática</strong>: Detecta modificações nos arquivos</li>
</ul>
<h4 id="5-banco-vetorial-simples-bm25-manual"><strong>5. Banco Vetorial Simples (BM25 Manual)</strong></h4>


  <pre><code class="language-clojure">;; src/docai/vector_store.clj
(ns docai.vector-store
  (:require [clojure.string :as str]))

(defn create-simple-vector-store
  &#34;Store vetorial simples com BM25 (implementação manual)&#34;
  [documents]
  (let [index (atom {})
        doc-freq (emb/doc-freq documents)
        vocab (sort (keys doc-freq))]  ; Vocabulário ordenado
    (doseq [[idx doc] (map-indexed vector documents)]
      (let [tokens (emb/tokenize doc)
            tf (emb/term-freq tokens)]
        (swap! index assoc idx {:doc doc :tf tf})))
    {:index index :doc-freq doc-freq :vocab vocab}))

(defn calculate-bm25
  &#34;Calcula score BM25 para um documento&#34;
  [query-tokens doc-tf doc-freq]
  (let [k1 1.2  ; Parâmetro de saturação de termo
        b 0.75   ; Parâmetro de normalização de comprimento
        avg-doc-len 100  ; Comprimento médio do documento (aproximação)
        doc-len (reduce &#43; (vals doc-tf))
        
        ;; IDF para cada termo da query
        idf-scores (map (fn [term]
                          (let [df (get doc-freq term 0)
                                n (count doc-freq)]
                            (if (zero? df)
                              0
                              (Math/log (/ (- n df 0.5) (&#43; df 0.5)))))
                        query-tokens)
        
        ;; TF para cada termo da query no documento
        tf-scores (map (fn [term]
                         (let [tf (get doc-tf term 0)]
                           (/ (* tf (&#43; k1 1))
                              (&#43; tf (* k1 (- 1 b (* b (/ doc-len avg-doc-len)))))))
                       query-tokens)]
    
    ;; Soma ponderada de IDF * TF
    (reduce &#43; (map * idf-scores tf-scores))))

(defn search-bm25
  &#34;Busca híbrida: BM25 &#43; similaridade semântica&#34;
  [query vector-store top-k]
  (let [query-tokens (emb/tokenize query)
        query-embedding (emb/vectorize query (:doc-freq vector-store) (:doc-count vector-store) (:vocab vector-store))
        
        ;; BM25 scores
        bm25-scores (map-indexed 
                      (fn [idx {:keys [tf]}]
                        [idx (calculate-bm25 query-tokens tf (:doc-freq vector-store))])
                      (vals @(:index vector-store)))
        
        ;; Semantic scores
        semantic-scores (map-indexed
                          (fn [idx _]
                            [idx (emb/cosine-similarity query-embedding 
                                                       (emb/vectorize (get-in @(:index vector-store) [idx :doc])
                                                                      (:doc-freq vector-store)
                                                                      (:doc-count vector-store)
                                                                      (:vocab vector-store)))])
                          (vals @(:index vector-store)))
        
        ;; Combine scores (weighted average)
        combined-scores (map (fn [[idx bm25] [idx2 semantic]]
                              [idx (&#43; (* 0.3 bm25) (* 0.7 semantic))])
                            bm25-scores semantic-scores)]
    
    (-&gt;&gt; combined-scores
         (sort-by second &gt;)
         (take top-k)
         (map first))))</code></pre>
 <p><strong>Sobre o Algoritmo BM25:</strong></p>
<ul>
<li><strong>k1 = 1.2</strong>: Controla saturação de frequência de termos</li>
<li><strong>b = 0.75</strong>: Normaliza pelo comprimento do documento</li>
<li><strong>IDF</strong>: Mede raridade dos termos na coleção</li>
<li><strong>TF</strong>: Frequência dos termos no documento</li>
<li><strong>Combinação</strong>: 30% BM25 + 70% similaridade semântica</li>
</ul>
<p><strong>Nota</strong>: Esta é uma implementação manual do BM25. Para produção, considere usar Apache Lucene (veja dependências acima) que oferece BM25 nativo e otimizado.</p>
<h3 id="melhorias-avançadas">Melhorias Avançadas</h3>


  
  <div class="mermaid">mindmap
  root((Melhorias))
    Tokenização
      BPE
      WordPiece
      Tokenizador do Modelo
    Embeddings
      Pré-treinados
      Via Ollama
      Cache
    Banco de Dados
      Milvus
      FAISS
      Qdrant
    Cache
      Embeddings
      Respostas
    Erros
      Conexão
      Modelo
      Rede
    Logging
      Framework
      Rastreamento
    Testes
      Unitários
      Integração
    Prompt
      Few-shot
      Chain-of-thought
      Formato</div>
 <h3 id="dependências-e-próximos-passos">Dependências e Próximos Passos</h3>
<h4 id="dependências-recomendadas"><strong>Dependências Recomendadas</strong></h4>
<p>Para implementar as funcionalidades avançadas mencionadas no artigo, adicione estas dependências ao <code>project.clj</code>:</p>


  <pre><code class="language-clojure">;; Dependências para produção
[com.github.justone/clojure-tiktoken &#34;0.1.0&#34;]  ; Contagem precisa de tokens
[org.apache.lucene/lucene-core &#34;9.10.0&#34;]       ; Busca textual avançada
[org.apache.lucene/lucene-analyzers-common &#34;9.10.0&#34;]  ; Analisadores de texto
[org.apache.lucene/lucene-queryparser &#34;9.10.0&#34;] ; Parser de queries
[com.github.clojure-lsp/clojure-lsp &#34;2024.01.15-20.32.45&#34;]  ; LSP para IDE</code></pre>
 <h4 id="implementação-com-lucene"><strong>Implementação com Lucene</strong></h4>


  <pre><code class="language-clojure">;; src/docai/lucene_store.clj
(ns docai.lucene-store
  (:import [org.apache.lucene.analysis.standard StandardAnalyzer]
           [org.apache.lucene.document Document Field Field$Store]
           [org.apache.lucene.index IndexWriter IndexWriterConfig DirectoryReader]
           [org.apache.lucene.search IndexSearcher QueryParser]
           [org.apache.lucene.store RAMDirectory]))

(defn create-lucene-index
  &#34;Cria índice Lucene para busca textual&#34;
  [documents]
  (let [analyzer (StandardAnalyzer.)
        directory (RAMDirectory.)
        config (IndexWriterConfig. analyzer)
        writer (IndexWriter. directory config)]
    
    ;; Adiciona documentos ao índice
    (doseq [[idx doc] (map-indexed vector documents)]
      (let [document (Document.)]
        (.add document (Field. &#34;content&#34; doc Field$Store/YES))
        (.add document (Field. &#34;id&#34; (str idx) Field$Store/YES))
        (.addDocument writer document)))
    
    (.close writer)
    
    {:directory directory
     :analyzer analyzer
     :reader (DirectoryReader/open directory)
     :searcher (IndexSearcher. (DirectoryReader/open directory))}))

(defn search-lucene
  &#34;Busca textual usando Lucene&#34;
  [index query top-k]
  (let [parser (QueryParser. &#34;content&#34; (:analyzer index))
        query-obj (.parse parser query)
        hits (.search (:searcher index) query-obj top-k)]
    (map #(.doc (:searcher index) %) (.scoreDocs hits))))</code></pre>
 <h3 id="upgrade-para-embeddings-densos">Upgrade para Embeddings Densos</h3>
<p>Para evoluir de TF-IDF para embeddings densos modernos, considere estas opções:</p>
<h4 id="1-via-ollama-embeddings-api">1. <strong>Via Ollama Embeddings API</strong></h4>


  <pre><code class="language-clojure">;; Exemplo de upgrade usando Ollama embeddings
(defn create-dense-embeddings [texts]
  (let [embeddings-url &#34;http://localhost:11434/api/embeddings&#34;]
    (map #(call-ollama-embeddings embeddings-url %) texts)))

(defn call-ollama-embeddings [url text]
  (let [request-body {:model &#34;deepseek-r1&#34; :prompt text}
        response @(http/post url {:body (json/write-str request-body)})]
    (if (= (:status response) 200)
      (-&gt; response :body (json/read-str :key-fn keyword) :embedding)
      (throw (ex-info &#34;Erro ao gerar embedding&#34; {:status (:status response)})))))

;; Token counting preciso via Ollama API
(defn count-tokens-ollama [text]
  (let [url &#34;http://localhost:11434/api/generate&#34;
        request-body {:model &#34;deepseek-r1&#34; 
                     :prompt text 
                     :stream false
                     :options {:num_predict 0}}]
    (try
      (let [response @(http/post url {:body (json/write-str request-body)})]
        (if (= (:status response) 200)
          (-&gt; response :body (json/read-str :key-fn keyword) :eval_count)
          0))
      (catch Exception _ 0))))

;; Implementação com clojure-tiktoken (recomendado para produção)
(defn count-tokens-precise [text]
  (try
    ;; Requer: [com.github.justone/clojure-tiktoken &#34;0.1.0&#34;]
    ;; (require &#39;[com.github.justone.clojure-tiktoken :as tiktoken])
    ;; (tiktoken/count-tokens text &#34;cl100k_base&#34;)
    (count-tokens text) ; Fallback para implementação atual
    (catch Exception e
      (println &#34;Erro ao usar tiktoken:&#34; (.getMessage e))
      (count-tokens text))))

;; Exemplo de implementação com API do Ollama (mais preciso)
(defn count-tokens-ollama-precise [text]
  (let [url &#34;http://localhost:11434/api/generate&#34;
        request-body {:model &#34;deepseek-r1&#34; 
                     :prompt text 
                     :stream false
                     :options {:num_predict 0}}]
    (try
      (let [response @(http/post url {:body (json/write-str request-body)})]
        (if (= (:status response) 200)
          (-&gt; response :body (json/read-str :key-fn keyword) :eval_count)
          (count-tokens text))) ; Fallback para heurística
      (catch Exception _
        (count-tokens text))))) ; Fallback para heurística</code></pre>
 <h4 id="2-via-huggingface-transformers">2. <strong>Via HuggingFace Transformers</strong></h4>


  <pre><code class="language-clojure">;; Exemplo usando interop com Python/HuggingFace
(defn create-hf-embeddings [texts]
  (let [model-name &#34;sentence-transformers/all-MiniLM-L6-v2&#34;]
    ;; Usar interop com Python para carregar modelo
    ;; e gerar embeddings densos
    ))

;; Token counting preciso com tiktoken
(defn count-tokens-tiktoken [text]
  ;; Requer interop com Python tiktoken
  ;; pip install tiktoken
  ;; python -c &#34;import tiktoken; print(len(tiktoken.get_encoding(&#39;cl100k_base&#39;).encode(&#39;texto aqui&#39;)))&#34;
  )</code></pre>
 <h4 id="3-comparação-de-performance">3. <strong>Comparação de Performance</strong></h4>
<table>
  <thead>
      <tr>
          <th>Método</th>
          <th>Semântica</th>
          <th>Contexto</th>
          <th>Performance</th>
          <th>Complexidade</th>
          <th>Hardware Mínimo</th>
          <th>Precisão Tokens</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>TF-IDF</td>
          <td>❌</td>
          <td>❌</td>
          <td>⚡⚡⚡</td>
          <td>⚡</td>
          <td>CPU 4 cores, 8GB RAM</td>
          <td>⚠️ Heurística</td>
      </tr>
      <tr>
          <td>Ollama Embeddings</td>
          <td>✅</td>
          <td>✅</td>
          <td>⚡⚡</td>
          <td>⚡⚡</td>
          <td>CPU 8 cores, 16GB RAM</td>
          <td>✅ Preciso</td>
      </tr>
      <tr>
          <td>SBERT/E5</td>
          <td>✅✅</td>
          <td>✅✅</td>
          <td>⚡</td>
          <td>⚡⚡⚡</td>
          <td>GPU 8GB VRAM, 32GB RAM</td>
          <td>✅ Preciso</td>
      </tr>
  </tbody>
</table>
<blockquote>
<p><strong>Recomendação</strong>: Para aplicações em produção, comece com Ollama embeddings (simples de implementar) e evolua para modelos especializados como SBERT ou E5 conforme necessário. Considere seus recursos de hardware ao escolher a abordagem.</p>
<p><strong>⚠️ Importante</strong>: A contagem de tokens heurística pode errar até 2x. Para produção, use <code>count-tokens-ollama-precise</code> ou <code>clojure-tiktoken</code> para precisão.</p></blockquote>
<p>Olha, dá pra turbinar esse nosso RAG de várias formas! Primeiro, a gente poderia melhorar a tokenização usando aqueles métodos mais avançados tipo <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding">BPE</a> ou <a href="https://en.wikipedia.org/wiki/WordPiece">WordPiece</a> - idealmente o mesmo que o modelo usa.</p>
<p>E os embeddings? Seria muito mais eficiente pegar direto do Ollama em vez de fazer na mão. A diferença na busca semântica seria absurda! O TF-IDF que implementamos é ótimo para entender os conceitos, mas embeddings densos modernos capturam nuances semânticas que fazem toda a diferença em aplicações reais.</p>
<p>Quando o projeto crescer, vai ser essencial ter um banco de dados vetorial decente. Imagina lidar com milhares de documentos usando nossa implementação atual? Seria um pesadelo! <a href="https://milvus.io/">Milvus</a>, <a href="https://github.com/facebookresearch/faiss">FAISS</a> ou <a href="https://qdrant.tech/">Qdrant</a> resolveriam isso numa boa. E não podemos esquecer do cache - tanto para embeddings quanto para respostas. Economiza um tempão e reduz a carga no sistema.</p>
<p>A parte de tratamento de erros e logging também precisa de carinho. Já pensou o usuário esperando resposta e o Ollama tá offline? Ou um arquivo corrompido? Precisamos de mensagens amigáveis e um sistema de logging decente pra rastrear problemas. E claro, testes! Sem testes unitários e de integração, qualquer mudança vira uma roleta-russa.</p>
<p>O prompt engineering é outro ponto crucial - dá pra refinar bastante o formato atual. Poderíamos experimentar com exemplos no prompt (few-shot), instruções passo a passo (chain-of-thought), e ser mais específico sobre o formato da resposta.</p>
<h2 id="apêndice">Apêndice</h2>
<h3 id="requisitos-de-hardware-detalhados">Requisitos de Hardware Detalhados</h3>
<p>A performance do sistema RAG depende significativamente do hardware disponível. Aqui estão as configurações recomendadas:</p>
<table>
  <thead>
      <tr>
          <th>Componente</th>
          <th>Mínimo</th>
          <th>Recomendado</th>
          <th>Alto Desempenho</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>CPU</strong></td>
          <td>4 cores (Intel i5/AMD Ryzen 5)</td>
          <td>8 cores (Intel i7/AMD Ryzen 7)</td>
          <td>16+ cores (Intel i9/AMD Ryzen 9)</td>
      </tr>
      <tr>
          <td><strong>RAM</strong></td>
          <td>8 GB</td>
          <td>16 GB</td>
          <td>32+ GB</td>
      </tr>
      <tr>
          <td><strong>GPU</strong></td>
          <td>Integrada</td>
          <td>NVIDIA RTX 3060 (8GB VRAM)</td>
          <td>NVIDIA RTX 4090 (24GB VRAM)</td>
      </tr>
      <tr>
          <td><strong>VRAM</strong></td>
          <td>-</td>
          <td>8 GB</td>
          <td>16+ GB</td>
      </tr>
      <tr>
          <td><strong>Storage</strong></td>
          <td>SSD 256 GB</td>
          <td>SSD 512 GB</td>
          <td>NVMe 1 TB+</td>
      </tr>
      <tr>
          <td><strong>Rede</strong></td>
          <td>100 Mbps</td>
          <td>1 Gbps</td>
          <td>10 Gbps</td>
      </tr>
  </tbody>
</table>
<h4 id="configurações-por-caso-de-uso"><strong>Configurações por Caso de Uso</strong></h4>
<p><strong>🟢 Desenvolvimento/Teste</strong></p>
<ul>
<li>CPU: 4 cores, RAM: 8GB</li>
<li>Modelo: <code>deepseek-r1</code> (CPU only)</li>
<li>Documentos: &lt; 1GB</li>
<li>Performance: ~2-5 segundos por consulta</li>
</ul>
<p><strong>🟡 Produção Pequena</strong></p>
<ul>
<li>CPU: 8 cores, RAM: 16GB, GPU: RTX 3060</li>
<li>Modelo: <code>deepseek-r1</code> (GPU)</li>
<li>Documentos: 1-10GB</li>
<li>Performance: ~1-3 segundos por consulta</li>
</ul>
<p><strong>🔴 Produção Grande</strong></p>
<ul>
<li>CPU: 16+ cores, RAM: 32GB+, GPU: RTX 4090</li>
<li>Modelo: <code>deepseek-r1</code> + embeddings densos</li>
<li>Documentos: 10GB+</li>
<li>Performance: &lt; 1 segundo por consulta</li>
</ul>
<h4 id="otimizações-por-hardware"><strong>Otimizações por Hardware</strong></h4>
<p><strong>CPU Only:</strong></p>


  <pre><code class="language-bash"># Usar modelo otimizado para CPU
ollama pull deepseek-r1:3b  # Versão menor</code></pre>
 <p><strong>GPU Disponível:</strong></p>


  <pre><code class="language-bash"># Usar versão completa com aceleração GPU
ollama pull deepseek-r1</code></pre>
 <p><strong>Múltiplas GPUs:</strong></p>


  <pre><code class="language-bash"># Distribuir carga entre GPUs
CUDA_VISIBLE_DEVICES=0,1 ollama serve</code></pre>
 <hr>
<h2 id="referências">Referências</h2>
<ul>
<li><a href="https://www.pinecone.io/learn/rag/">RAG</a> - Documentação do Pinecone</li>
<li><a href="https://www.pinecone.io/learn/embeddings/">Embedding</a> - Documentação do Pinecone</li>
<li><a href="https://www.pinecone.io/learn/llms/">LLM</a> - Documentação do Pinecone</li>
<li><a href="https://ollama.com/">Ollama</a> - Ferramenta para rodar LLMs localmente</li>
<li><a href="https://clojure.org/">Clojure</a> - Documentação do Clojure</li>
<li><a href="https://github.com/http-kit/http-kit">http-kit</a> - Cliente HTTP para Clojure</li>
<li><a href="https://github.com/clojure/data.json">data.json</a> - Biblioteca JSON para Clojure</li>
<li><a href="https://clojure.github.io/clojure/clojure.test-api.html">clojure.test</a> - Documentação da biblioteca de testes do Clojure</li>
<li><a href="https://github.com/clj-kondo/clj-kondo">clj-kondo</a> - Linter para Clojure</li>
</ul>

    </div>
    
    
    



    
    
    
        
            
            
            
            
                
                    
                
                    
                
                    
                
                    
                
                    
                
            
            
            
        
    
        
            
            
            
            
                
                    
                
                    
                
                    
                
                    
                
                    
                
            
            
            
        
    
        
            
            
            
            
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            
            
            
        
    
        
            
            
            
            
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            
            
            
        
    
        
    
        
            
            
            
            
                
                    
                        
                        
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            
            
            
                
            
        
    
        
            
            
            
            
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            
            
            
        
    
        
            
            
            
            
                
                    
                        
                        
                    
                
                    
                        
                        
                    
                
                    
                        
                        
                    
                
                    
                
                    
                
                    
                
                    
                
            
            
            
                
            
        
    
        
            
            
            
            
            
            
        
    
        
            
            
            
            
            
            
        
    
    
    
        
        
        
        <div class="related-posts">
            <h3 class="related-posts-title">📚 Posts Relacionados</h3>
            <div class="related-posts-grid">
                
                    
                    <article class="related-post-item animate-on-scroll">
                        <div class="related-post-content">
                            <h4 class="related-post-title">
                                <a href="/2025/03/28/rag02/">Técnicas Avançadas para RAG em Produção</a>
                            </h4>
                            <p class="related-post-excerpt">
                                <h2 id="introdução">Introdução</h2>
<p>Olá pessoal! 👋</p>
<p>Nos artigos anteriores, exploramos como <a href="/2025/03/23/rag/">implementar um RAG básico em Clojure</a> em memória e como <a href="/2025/03/25/semantic-postgresql/">construir um sistema de busca semântica com PostgreSQL e Ollama</a>. Agora, vamos dar o próximo passo: transformar nosso protótipo em um sistema RAG pronto para produção.</p>
<p>Como muitos desenvolvedores já descobriram, criar um protótipo funcional de <a href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">RAG</a> com alguns documentos é relativamente simples. O verdadeiro desafio começa quando precisamos escalar esse sistema para lidar com milhares de documentos, garantir respostas precisas e manter o desempenho sob carga. Neste artigo, vamos explorar técnicas avançadas para superar esses desafios e levar nosso <a href="https://github.com/scovl/docai">DocAI</a> para um novo patamar de qualidade e confiabilidade.</p>
                            </p>
                            <div class="related-post-meta">
                                <span class="related-post-date">28/03/2025</span>
                                
                                    <div class="related-post-tags">
                                        
                                            <span class="tag">RAG</span>
                                        
                                            <span class="tag">LLM</span>
                                        
                                    </div>
                                
                            </div>
                            <div class="related-post-score">
                                <modern-badge variant="info">3 tags em comum</modern-badge>
                            </div>
                        </div>
                    </article>
                
                    
                    <article class="related-post-item animate-on-scroll">
                        <div class="related-post-content">
                            <h4 class="related-post-title">
                                <a href="/2025/03/25/semantic-postgresql/">Busca Semântica com Ollama e PostgreSQL</a>
                            </h4>
                            <p class="related-post-excerpt">
                                <p>Olá, pessoal! 👋</p>
<p>No <a href="/2025/03/23/rag/">artigo anterior</a>, exploramos como construir um sistema RAG (Retrieval-Augmented Generation) usando <a href="https://clojure.org/">Clojure</a> e <a href="https://ollama.com/">Ollama</a> com uma implementação simples de <a href="/post/tf-idf/">TF-IDF</a>. Embora essa abordagem seja excelente para aprender os fundamentos, quando pensamos em soluções de produção, precisamos de algo mais robusto e escalável.</p>
<p>Neste artigo, vamos descobrir como construir um sistema de busca semântica poderoso usando <a href="https://ollama.com/">Ollama</a>, <a href="https://www.postgresql.org/">PostgreSQL</a> e suas extensões para manipulação de vetores. Esta solução é perfeitamente adequada para aplicações de produção e pode servir como base para sistemas RAG, agentes de IA, assistentes em geral. Diferentemente do artigo anterior, vamos usar o <a href="https://ollama.com/">Ollama</a> via Docker assim como o <a href="https://www.postgresql.org/">PostgreSQL</a> e as extensões <a href="https://github.com/pgvector/pgvector">pgvector</a> e <a href="https://github.com/timescale/pgai">pgai</a>.</p>
                            </p>
                            <div class="related-post-meta">
                                <span class="related-post-date">25/03/2025</span>
                                
                                    <div class="related-post-tags">
                                        
                                            <span class="tag">RAG</span>
                                        
                                            <span class="tag">PostgreSQL</span>
                                        
                                    </div>
                                
                            </div>
                            <div class="related-post-score">
                                <modern-badge variant="info">1 tags em comum</modern-badge>
                            </div>
                        </div>
                    </article>
                
            </div>
        </div>
    

    
    
    
    
<div class="comments-section">
    <h3 class="comments-title">💬 Comentários</h3>
    <div class="comments-container">
        <script src="https://giscus.app/client.js"
                data-repo="scovl/scovl.github.io"
                data-repo-id="MDEwOlJlcG9zaXRvcnkxMzg1OTI2ODA="
                data-category="General"
                data-category-id="DIC_kwDOCELBqM4CthUV"
                data-mapping="pathname"
                data-strict="0"
                data-reactions-enabled="1"
                data-emit-metadata="0"
                data-input-position="bottom"
                data-theme="light"
                data-lang="pt"
                crossorigin="anonymous"
                async>
        </script>
    </div>
</div>

    
</article>

        </div>
    </main>
    
    
    
    <footer class="footer">
    <div class="container">
        <div class="footer-content">
            <div class="footer-links">
                
                <a href="https://github.com/scovl" target="_blank" rel="noopener noreferrer" class="footer-link">
                    GitHub
                </a>
                
                
                
                <a href="https://linkedin.com/in/vitor-lobo" target="_blank" rel="noopener noreferrer" class="footer-link">
                    LinkedIn
                </a>
                
                
                
                <a href="mailto:lobocode@gmail.com" class="footer-link">
                    Email
                </a>
                

                
                <a href="https://hachyderm.io/@lobocode" target="_blank" rel="noopener noreferrer" class="footer-link">
                    Mastodon
                </a>
                

                
                <a href="https://scovl.github.io/index.xml" target="_blank" rel="noopener noreferrer" class="footer-link">
                    RSS
                </a>
                
            </div>
            
            
            <div class="back-to-top-container">
                <button id="back-to-top" class="back-to-top-btn" aria-label="Voltar ao topo">
                    <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                        <path d="m18 15-6-6-6 6"/>
                    </svg>
                    <span data-i18n="back_to_top">Voltar ao topo</span>
                </button>
            </div>
            
            <div class="copyright">
                &copy; 2025 scovl
            </div>
        </div>
    </div>
</footer> 
    
    
    
    <script src="/vendor/prism/prism-core.min.js?v=1757193440"></script>
    <script src="/vendor/prism/prism-clike.min.js?v=1757193440"></script>
    <script src="/vendor/prism/prism-c.min.js?v=1757193440"></script>
    <script src="/vendor/prism/prism-cpp.min.js?v=1757193440"></script>
    <script src="/vendor/prism/prism-rust.min.js?v=1757193440"></script>
    <script src="/vendor/prism/prism-clojure.min.js?v=1757193440"></script>
    <script src="/vendor/prism/prism-swift.min.js?v=1757193440"></script>
    <script src="/vendor/prism/prism-bash.min.js?v=1757193440"></script>
    <script src="/vendor/prism/prism-javascript.min.js?v=1757193440"></script>
    <script src="/vendor/prism/prism-typescript.min.js?v=1757193440"></script>
    <script src="/vendor/prism/prism-autoloader.min.js?v=1757193440"></script>
    
    
    <script src="/js/main-minimal.js?v=1757193440"></script>
    <script src="/js/lazy-loading.js?v=1757193440"></script>
    <script src="/js/toc.js?v=1757193440"></script>
    
    
    
</body>
</html> 